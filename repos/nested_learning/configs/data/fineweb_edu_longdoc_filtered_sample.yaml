name: fineweb_edu_longdoc_filtered_sample
tokenizer_output_dir: artifacts/tokenizer/fineweb_edu_longdoc
datasets:
  - name: fineweb_edu_longdoc
    dataset: text
    split: train
    text_column: text
    data_files: data/filtered/fineweb_edu_longdoc_en_sample.txt
    sample_limit: 5000
    seq_len: 4096
    sequences_per_shard: 1024
    output_dir: data/shards/fineweb_edu_longdoc_sample
    max_records: null

