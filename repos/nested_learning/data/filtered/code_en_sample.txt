############################################################################### ## ## Copyright (C) 2013-2014 Tavendo GmbH ## ## Licensed under the Apache License, Version 2.0 (the "License"); ## you may not use this file except in compliance with the License. ## You may obtain a copy of the License at ## ## http://www.apache.org/licenses/LICENSE-2.0 ## ## Unless required by applicable law or agreed to in writing, software ## distributed under the License is distributed on an "AS IS" BASIS, ## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. ## See the License for the specific language governing permissions and ## limitations under the License. ## ############################################################################### from autobahn.asyncio.websocket import WebSocketClientProtocol, \ WebSocketClientFactory import asyncio class MyClientProtocol(WebSocketClientProtocol): def onConnect(self, response): print("Server connected: {0}".format(response.peer)) @asyncio.coroutine def onOpen(self): print("WebSocket connection open.") ## start sending messages every second .. while True: self.sendMessage(u"Hello, world!".encode('utf8')) self.sendMessage(b"\x00\x01\x03\x04", isBinary = True) yield from asyncio.sleep(1) def onMessage(self, payload, isBinary): if isBinary: print("Binary message received: {0} bytes".format(len(payload))) else: print("Text message received: {0}".format(payload.decode('utf8'))) def onClose(self, wasClean, code, reason): print("WebSocket connection closed: {0}".format(reason)) if __name__ == '__main__': import asyncio factory = WebSocketClientFactory("ws://localhost:9000", debug = False) factory.protocol = MyClientProtocol loop = asyncio.get_event_loop() coro = loop.create_connection(factory, '127.0.0.1', 9000) loop.run_until_complete(coro) loop.run_forever() loop.close()
from itertools import chain from django.utils.itercompat import is_iterable class Tags: """ Built-in tags for internal checks. """ admin = 'admin' caches = 'caches' compatibility = 'compatibility' database = 'database' models = 'models' security = 'security' signals = 'signals' templates = 'templates' urls = 'urls' class CheckRegistry: def __init__(self): self.registered_checks = set() self.deployment_checks = set() def register(self, check=None, *tags, **kwargs): """ Can be used as a function or a decorator. Register given function `f` labeled with given `tags`. The function should receive **kwargs and return list of Errors and Warnings. Example:: registry = CheckRegistry() @registry.register('mytag', 'anothertag') def my_check(apps, **kwargs): # ... perform checks and collect `errors` ... return errors # or registry.register(my_check, 'mytag', 'anothertag') """ kwargs.setdefault('deploy', False) def inner(check): check.tags = tags checks = self.deployment_checks if kwargs['deploy'] else self.registered_checks checks.add(check) return check if callable(check): return inner(check) else: if check: tags += (check, ) return inner def run_checks(self, app_configs=None, tags=None, include_deployment_checks=False): """ Run all registered checks and return list of Errors and Warnings. """ errors = [] checks = self.get_checks(include_deployment_checks) if tags is not None: checks = [check for check in checks if not set(check.tags).isdisjoint(tags)] else: # By default, 'database'-tagged checks are not run as they do more # than mere static code analysis. checks = [check for check in checks if Tags.database not in check.tags] for check in checks: new_errors = check(app_configs=app_configs) assert is_iterable(new_errors), ( "The function %r did not return a list. All functions registered " "with the checks registry must return a list." % check) errors.extend(new_errors) return errors def tag_exists(self, tag, include_deployment_checks=False): return tag in self.tags_available(include_deployment_checks) def tags_available(self, deployment_checks=False): return set(chain.from_iterable( check.tags for check in self.get_checks(deployment_checks) )) def get_checks(self, include_deployment_checks=False): checks = list(self.registered_checks) if include_deployment_checks: checks.extend(self.deployment_checks) return checks registry = CheckRegistry() register = registry.register run_checks = registry.run_checks tag_exists = registry.tag_exists
""" The :mod:`sklearn.utils` module includes various utilites. """ from collections import Sequence import numpy as np from scipy.sparse import issparse import warnings from .murmurhash import murmurhash3_32 from .validation import (as_float_array, check_arrays, safe_asarray, assert_all_finite, array2d, atleast2d_or_csc, atleast2d_or_csr, warn_if_not_float, check_random_state) from .class_weight import compute_class_weight __all__ = ["murmurhash3_32", "as_float_array", "check_arrays", "safe_asarray", "assert_all_finite", "array2d", "atleast2d_or_csc", "atleast2d_or_csr", "warn_if_not_float", "check_random_state", "compute_class_weight"] # Make sure that DeprecationWarning get printed warnings.simplefilter("always", DeprecationWarning) class deprecated(object): """Decorator to mark a function or class as deprecated. Issue a warning when the function is called/the class is instantiated and adds a warning to the docstring. The optional extra argument will be appended to the deprecation message and the docstring. Note: to use this with the default value for extra, put in an empty of parentheses: >>> from sklearn.utils import deprecated >>> deprecated() # doctest: +ELLIPSIS <sklearn.utils.deprecated object at ...> >>> @deprecated() ... def some_function(): pass """ # Adapted from http://wiki.python.org/moin/PythonDecoratorLibrary, # but with many changes. def __init__(self, extra=''): """ Parameters ---------- extra: string to be added to the deprecation messages """ self.extra = extra def __call__(self, obj): if isinstance(obj, type): return self._decorate_class(obj) else: return self._decorate_fun(obj) def _decorate_class(self, cls): msg = "Class %s is deprecated" % cls.__name__ if self.extra: msg += "; %s" % self.extra # FIXME: we should probably reset __new__ for full generality init = cls.__init__ def wrapped(*args, **kwargs): warnings.warn(msg, category=DeprecationWarning) return init(*args, **kwargs) cls.__init__ = wrapped wrapped.__name__ = '__init__' wrapped.__doc__ = self._update_doc(init.__doc__) wrapped.deprecated_original = init return cls def _decorate_fun(self, fun): """Decorate function fun""" msg = "Function %s is deprecated" % fun.__name__ if self.extra: msg += "; %s" % self.extra def wrapped(*args, **kwargs): warnings.warn(msg, category=DeprecationWarning) return fun(*args, **kwargs) wrapped.__name__ = fun.__name__ wrapped.__dict__ = fun.__dict__ wrapped.__doc__ = self._update_doc(fun.__doc__) return wrapped def _update_doc(self, olddoc): newdoc = "DEPRECATED" if self.extra: newdoc = "%s: %s" % (newdoc, self.extra) if olddoc: newdoc = "%s\n\n%s" % (newdoc, olddoc) return newdoc def safe_mask(X, mask): """Return a mask which is safe to use on X. Parameters ---------- X : {array-like, sparse matrix} Data on which to apply mask. mask: array Mask to be used on X. Returns ------- mask """ mask = np.asanyarray(mask) if np.issubdtype(mask.dtype, np.int): return mask if hasattr(X, "toarray"): ind = np.arange(mask.shape[0]) mask = ind[mask] return mask def resample(*arrays, **options): """Resample arrays or sparse matrices in a consistent way The default strategy implements one step of the bootstrapping procedure. Parameters ---------- `*arrays` : sequence of arrays or scipy.sparse matrices with same shape[0] replace : boolean, True by default Implements resampling with replacement. If False, this will implement (sliced) random permutations. n_samples : int, None by default Number of samples to generate. If left to None this is automatically set to the first dimension of the arrays. random_state : int or RandomState instance Control the shuffling for reproducible behavior. Returns ------- Sequence of resampled views of the collections. The original arrays are not impacted. Examples -------- It is possible to mix sparse and dense arrays in the same run:: >>> X = [[1., 0.], [2., 1.], [0., 0.]] >>> y = np.array([0, 1, 2]) >>> from scipy.sparse import coo_matrix >>> X_sparse = coo_matrix(X) >>> from sklearn.utils import resample >>> X, X_sparse, y = resample(X, X_sparse, y, random_state=0) >>> X array([[ 1., 0.], [ 2., 1.], [ 1., 0.]]) >>> X_sparse # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE <3x2 sparse matrix of type '<... 'numpy.float64'>' with 4 stored elements in Compressed Sparse Row format> >>> X_sparse.toarray() array([[ 1., 0.], [ 2., 1.], [ 1., 0.]]) >>> y array([0, 1, 0]) >>> resample(y, n_samples=2, random_state=0) array([0, 1]) See also -------- :class:`sklearn.cross_validation.Bootstrap` :func:`sklearn.utils.shuffle` """ random_state = check_random_state(options.pop('random_state', None)) replace = options.pop('replace', True) max_n_samples = options.pop('n_samples', None) if options: raise ValueError("Unexpected kw arguments: %r" % options.keys()) if len(arrays) == 0: return None first = arrays[0] n_samples = first.shape[0] if hasattr(first, 'shape') else len(first) if max_n_samples is None: max_n_samples = n_samples if max_n_samples > n_samples: raise ValueError("Cannot sample %d out of arrays with dim %d" % ( max_n_samples, n_samples)) arrays = check_arrays(*arrays, sparse_format='csr') if replace: indices = random_state.randint(0, n_samples, size=(max_n_samples,)) else: indices = np.arange(n_samples) random_state.shuffle(indices) indices = indices[:max_n_samples] resampled_arrays = [] for array in arrays: array = array[indices] resampled_arrays.append(array) if len(resampled_arrays) == 1: # syntactic sugar for the unit argument case return resampled_arrays[0] else: return resampled_arrays def shuffle(*arrays, **options): """Shuffle arrays or sparse matrices in a consistent way This is a convenience alias to ``resample(*arrays, replace=False)`` to do random permutations of the collections. Parameters ---------- `*arrays` : sequence of arrays or scipy.sparse matrices with same shape[0] random_state : int or RandomState instance Control the shuffling for reproducible behavior. n_samples : int, None by default Number of samples to generate. If left to None this is automatically set to the first dimension of the arrays. Returns ------- Sequence of shuffled views of the collections. The original arrays are not impacted. Examples -------- It is possible to mix sparse and dense arrays in the same run:: >>> X = [[1., 0.], [2., 1.], [0., 0.]] >>> y = np.array([0, 1, 2]) >>> from scipy.sparse import coo_matrix >>> X_sparse = coo_matrix(X) >>> from sklearn.utils import shuffle >>> X, X_sparse, y = shuffle(X, X_sparse, y, random_state=0) >>> X array([[ 0., 0.], [ 2., 1.], [ 1., 0.]]) >>> X_sparse # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE <3x2 sparse matrix of type '<... 'numpy.float64'>' with 3 stored elements in Compressed Sparse Row format> >>> X_sparse.toarray() array([[ 0., 0.], [ 2., 1.], [ 1., 0.]]) >>> y array([2, 1, 0]) >>> shuffle(y, n_samples=2, random_state=0) array([0, 1]) See also -------- :func:`sklearn.utils.resample` """ options['replace'] = False return resample(*arrays, **options) def safe_sqr(X, copy=True): """Element wise squaring of array-likes and sparse matrices. Parameters ---------- X : array like, matrix, sparse matrix Returns ------- X ** 2 : element wise square """ X = safe_asarray(X) if issparse(X): if copy: X = X.copy() X.data **= 2 else: if copy: X = X ** 2 else: X **= 2 return X def gen_even_slices(n, n_packs): """Generator to create n_packs slices going up to n. Examples -------- >>> from sklearn.utils import gen_even_slices >>> list(gen_even_slices(10, 1)) [slice(0, 10, None)] >>> list(gen_even_slices(10, 10)) #doctest: +ELLIPSIS [slice(0, 1, None), slice(1, 2, None), ..., slice(9, 10, None)] >>> list(gen_even_slices(10, 5)) #doctest: +ELLIPSIS [slice(0, 2, None), slice(2, 4, None), ..., slice(8, 10, None)] >>> list(gen_even_slices(10, 3)) [slice(0, 4, None), slice(4, 7, None), slice(7, 10, None)] """ start = 0 for pack_num in range(n_packs): this_n = n // n_packs if pack_num < n % n_packs: this_n += 1 if this_n > 0: end = start + this_n yield slice(start, end, None) start = end def tosequence(x): """Cast iterable x to a Sequence, avoiding a copy if possible.""" if isinstance(x, np.ndarray): return np.asarray(x) elif isinstance(x, Sequence): return x else: return list(x) class ConvergenceWarning(Warning): "Custom warning to capture convergence problems"
#!/usr/bin/python # encoding: utf-8 -*- # Copyright: (c) 2013, Matthias Vogelgesang <matthias.vogelgesang@gmail.com> # GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt) from __future__ import absolute_import, division, print_function __metaclass__ = type ANSIBLE_METADATA = {'metadata_version': '1.1', 'status': ['preview'], 'supported_by': 'community'} DOCUMENTATION = ''' --- module: kernel_blacklist author: - Matthias Vogelgesang (@matze) version_added: '1.4' short_description: Blacklist kernel modules description: - Add or remove kernel modules from blacklist. options: name: description: - Name of kernel module to black- or whitelist. required: true state: description: - Whether the module should be present in the blacklist or absent. choices: [ absent, present ] default: present blacklist_file: description: - If specified, use this blacklist file instead of C(/etc/modprobe.d/blacklist-ansible.conf). ''' EXAMPLES = ''' - name: Blacklist the nouveau driver module kernel_blacklist: name: nouveau state: present ''' import os import re from ansible.module_utils.basic import AnsibleModule class Blacklist(object): def __init__(self, module, filename, checkmode): self.filename = filename self.module = module self.checkmode = checkmode def create_file(self): if not self.checkmode and not os.path.exists(self.filename): open(self.filename, 'a').close() return True elif self.checkmode and not os.path.exists(self.filename): self.filename = os.devnull return True else: return False def get_pattern(self): return r'^blacklist\s*' + self.module + '$' def readlines(self): f = open(self.filename, 'r') lines = f.readlines() f.close() return lines def module_listed(self): lines = self.readlines() pattern = self.get_pattern() for line in lines: stripped = line.strip() if stripped.startswith('#'): continue if re.match(pattern, stripped): return True return False def remove_module(self): lines = self.readlines() pattern = self.get_pattern() if self.checkmode: f = open(os.devnull, 'w') else: f = open(self.filename, 'w') for line in lines: if not re.match(pattern, line.strip()): f.write(line) f.close() def add_module(self): if self.checkmode: f = open(os.devnull, 'a') else: f = open(self.filename, 'a') f.write('blacklist %s\n' % self.module) f.close() def main(): module = AnsibleModule( argument_spec=dict( name=dict(type='str', required=True), state=dict(type='str', default='present', choices=['absent', 'present']), blacklist_file=dict(type='str') ), supports_check_mode=True, ) args = dict(changed=False, failed=False, name=module.params['name'], state=module.params['state']) filename = '/etc/modprobe.d/blacklist-ansible.conf' if module.params['blacklist_file']: filename = module.params['blacklist_file'] blacklist = Blacklist(args['name'], filename, module.check_mode) if blacklist.create_file(): args['changed'] = True else: args['changed'] = False if blacklist.module_listed(): if args['state'] == 'absent': blacklist.remove_module() args['changed'] = True else: if args['state'] == 'present': blacklist.add_module() args['changed'] = True module.exit_json(**args) if __name__ == '__main__': main()
from contextlib import contextmanager from datetime import datetime from inspect import isclass from celery import chord, group from celery_once import QueueOnce from celery.schedules import crontab from celery.utils.log import get_task_logger from sqlalchemy.orm.attributes import flag_modified # from pipet import celery from pipet.models import db, Organization from pipet.sources.zendesk import ZendeskAccount from pipet.sources.zendesk.models import ( Base, CLASS_REGISTRY, ) logger = get_task_logger(__name__) # @celery.task(base=QueueOnce, once={'graceful': True}) def sync(account_id): with app.app_context(): account = ZendeskAccount.query.get(account_id) session = account.organization.create_session() for cls in [m for n, m in CLASS_REGISTRY.items() if isclass(m) and issubclass(m, Base)]: # TODO: Make these parallel to speed up execution while True: conn = session.connection() statments, cursor, has_more = cls.sync(account) account.cursors[cls.__tablename__] = cursor flag_modified(account, 'cursors') for statement in statments: conn.execute(statement) session.commit() db.session.add(account) db.session.commit() if not has_more: break # @celery.task def sync_all(): job = group([sync.s(account.id) for account in ZendeskAccount.query.all()]) job.apply_async()
import os from importlib import import_module from django.core.exceptions import ImproperlyConfigured from django.utils.module_loading import module_has_submodule MODELS_MODULE_NAME = 'models' class AppConfig: """Class representing a Django application and its configuration.""" def __init__(self, app_name, app_module): # Full Python path to the application e.g. 'django.contrib.admin'. self.name = app_name # Root module for the application e.g. <module 'django.contrib.admin' # from 'django/contrib/admin/__init__.py'>. self.module = app_module # Reference to the Apps registry that holds this AppConfig. Set by the # registry when it registers the AppConfig instance. self.apps = None # The following attributes could be defined at the class level in a # subclass, hence the test-and-set pattern. # Last component of the Python path to the application e.g. 'admin'. # This value must be unique across a Django project. if not hasattr(self, 'label'): self.label = app_name.rpartition(".")[2] # Human-readable name for the application e.g. "Admin". if not hasattr(self, 'verbose_name'): self.verbose_name = self.label.title() # Filesystem path to the application directory e.g. # '/path/to/django/contrib/admin'. if not hasattr(self, 'path'): self.path = self._path_from_module(app_module) # Module containing models e.g. <module 'django.contrib.admin.models' # from 'django/contrib/admin/models.py'>. Set by import_models(). # None if the application doesn't have a models module. self.models_module = None # Mapping of lower case model names to model classes. Initially set to # None to prevent accidental access before import_models() runs. self.models = None def __repr__(self): return '<%s: %s>' % (self.__class__.__name__, self.label) def _path_from_module(self, module): """Attempt to determine app's filesystem path from its module.""" # See #21874 for extended discussion of the behavior of this method in # various cases. # Convert paths to list because Python's _NamespacePath doesn't support # indexing. paths = list(getattr(module, '__path__', [])) if len(paths) != 1: filename = getattr(module, '__file__', None) if filename is not None: paths = [os.path.dirname(filename)] else: # For unknown reasons, sometimes the list returned by __path__ # contains duplicates that must be removed (#25246). paths = list(set(paths)) if len(paths) > 1: raise ImproperlyConfigured( "The app module %r has multiple filesystem locations (%r); " "you must configure this app with an AppConfig subclass " "with a 'path' class attribute." % (module, paths)) elif not paths: raise ImproperlyConfigured( "The app module %r has no filesystem location, " "you must configure this app with an AppConfig subclass " "with a 'path' class attribute." % (module,)) return paths[0] @classmethod def create(cls, entry): """ Factory that creates an app config from an entry in INSTALLED_APPS. """ try: # If import_module succeeds, entry is a path to an app module, # which may specify an app config class with default_app_config. # Otherwise, entry is a path to an app config class or an error. module = import_module(entry) except ImportError: # Track that importing as an app module failed. If importing as an # app config class fails too, we'll trigger the ImportError again. module = None mod_path, _, cls_name = entry.rpartition('.') # Raise the original exception when entry cannot be a path to an # app config class. if not mod_path: raise else: try: # If this works, the app module specifies an app config class. entry = module.default_app_config except AttributeError: # Otherwise, it simply uses the default app config class. return cls(entry, module) else: mod_path, _, cls_name = entry.rpartition('.') # If we're reaching this point, we must attempt to load the app config # class located at <mod_path>.<cls_name> mod = import_module(mod_path) try: cls = getattr(mod, cls_name) except AttributeError: if module is None: # If importing as an app module failed, that error probably # contains the most informative traceback. Trigger it again. import_module(entry) else: raise # Check for obvious errors. (This check prevents duck typing, but # it could be removed if it became a problem in practice.) if not issubclass(cls, AppConfig): raise ImproperlyConfigured( "'%s' isn't a subclass of AppConfig." % entry) # Obtain app name here rather than in AppClass.__init__ to keep # all error checking for entries in INSTALLED_APPS in one place. try: app_name = cls.name except AttributeError: raise ImproperlyConfigured( "'%s' must supply a name attribute." % entry) # Ensure app_name points to a valid module. try: app_module = import_module(app_name) except ImportError: raise ImproperlyConfigured( "Cannot import '%s'. Check that '%s.%s.name' is correct." % ( app_name, mod_path, cls_name, ) ) # Entry is a path to an app config class. return cls(app_name, app_module) def get_model(self, model_name, require_ready=True): """ Return the model with the given case-insensitive model_name. Raise LookupError if no model exists with this name. """ if require_ready: self.apps.check_models_ready() else: self.apps.check_apps_ready() try: return self.models[model_name.lower()] except KeyError: raise LookupError( "App '%s' doesn't have a '%s' model." % (self.label, model_name)) def get_models(self, include_auto_created=False, include_swapped=False): """ Return an iterable of models. By default, the following models aren't included: - auto-created models for many-to-many relations without an explicit intermediate table, - models that have been swapped out. Set the corresponding keyword argument to True to include such models. Keyword arguments aren't documented; they're a private API. """ self.apps.check_models_ready() for model in self.models.values(): if model._meta.auto_created and not include_auto_created: continue if model._meta.swapped and not include_swapped: continue yield model def import_models(self): # Dictionary of models for this app, primarily maintained in the # 'all_models' attribute of the Apps this AppConfig is attached to. self.models = self.apps.all_models[self.label] if module_has_submodule(self.module, MODELS_MODULE_NAME): models_module_name = '%s.%s' % (self.name, MODELS_MODULE_NAME) self.models_module = import_module(models_module_name) def ready(self): """ Override this method in subclasses to run code when Django starts. """
# -*- coding: utf-8 -*- # # Copyright (c) 2017 F5 Networks Inc. # GNU General Public License v3.0 (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt) from __future__ import (absolute_import, division, print_function) __metaclass__ = type import os import json import sys from nose.plugins.skip import SkipTest if sys.version_info < (2, 7): raise SkipTest("F5 Ansible modules require Python >= 2.7") from ansible.compat.tests import unittest from ansible.compat.tests.mock import Mock from ansible.compat.tests.mock import patch from ansible.module_utils.basic import AnsibleModule try: from library.modules.bigip_gtm_datacenter import ApiParameters from library.modules.bigip_gtm_datacenter import ModuleParameters from library.modules.bigip_gtm_datacenter import ModuleManager from library.modules.bigip_gtm_datacenter import ArgumentSpec from library.module_utils.network.f5.common import F5ModuleError from library.module_utils.network.f5.common import iControlUnexpectedHTTPError from test.unit.modules.utils import set_module_args except ImportError: try: from ansible.modules.network.f5.bigip_gtm_datacenter import ApiParameters from ansible.modules.network.f5.bigip_gtm_datacenter import ModuleParameters from ansible.modules.network.f5.bigip_gtm_datacenter import ModuleManager from ansible.modules.network.f5.bigip_gtm_datacenter import ArgumentSpec from ansible.module_utils.network.f5.common import F5ModuleError from ansible.module_utils.network.f5.common import iControlUnexpectedHTTPError from units.modules.utils import set_module_args except ImportError: raise SkipTest("F5 Ansible modules require the f5-sdk Python library") fixture_path = os.path.join(os.path.dirname(__file__), 'fixtures') fixture_data = {} def load_fixture(name): path = os.path.join(fixture_path, name) if path in fixture_data: return fixture_data[path] with open(path) as f: data = f.read() try: data = json.loads(data) except Exception: pass fixture_data[path] = data return data class TestParameters(unittest.TestCase): def test_module_parameters(self): args = dict( state='present', contact='foo', description='bar', location='baz', name='datacenter' ) p = ModuleParameters(params=args) assert p.state == 'present' def test_api_parameters(self): args = load_fixture('load_gtm_datacenter_default.json') p = ApiParameters(params=args) assert p.name == 'asd' def test_module_parameters_state_present(self): args = dict( state='present' ) p = ModuleParameters(params=args) assert p.state == 'present' assert p.enabled is True def test_module_parameters_state_absent(self): args = dict( state='absent' ) p = ModuleParameters(params=args) assert p.state == 'absent' def test_module_parameters_state_enabled(self): args = dict( state='enabled' ) p = ModuleParameters(params=args) assert p.state == 'enabled' assert p.enabled is True assert p.disabled is None def test_module_parameters_state_disabled(self): args = dict( state='disabled' ) p = ModuleParameters(params=args) assert p.state == 'disabled' assert p.enabled is None assert p.disabled is True class TestManager(unittest.TestCase): def setUp(self): self.spec = ArgumentSpec() def test_create_datacenter(self, *args): set_module_args(dict( state='present', password='admin', server='localhost', user='admin', name='foo' )) module = AnsibleModule( argument_spec=self.spec.argument_spec, supports_check_mode=self.spec.supports_check_mode ) mm = ModuleManager(module=module) # Override methods to force specific logic in the module to happen mm.exists = Mock(side_effect=[False, True]) mm.create_on_device = Mock(return_value=True) results = mm.exec_module() assert results['changed'] is True assert results['state'] == 'present' def test_create_disabled_datacenter(self, *args): set_module_args(dict( state='disabled', password='admin', server='localhost', user='admin', name='foo' )) module = AnsibleModule( argument_spec=self.spec.argument_spec, supports_check_mode=self.spec.supports_check_mode ) mm = ModuleManager(module=module) # Override methods to force specific logic in the module to happen mm.exists = Mock(side_effect=[False, True]) mm.create_on_device = Mock(return_value=True) results = mm.exec_module() assert results['changed'] is True assert results['enabled'] is False assert results['disabled'] is True def test_create_enabled_datacenter(self, *args): set_module_args(dict( state='enabled', password='admin', server='localhost', user='admin', name='foo' )) module = AnsibleModule( argument_spec=self.spec.argument_spec, supports_check_mode=self.spec.supports_check_mode ) mm = ModuleManager(module=module) # Override methods to force specific logic in the module to happen mm.exists = Mock(side_effect=[False, True]) mm.create_on_device = Mock(return_value=True) results = mm.exec_module() assert results['changed'] is True assert results['enabled'] is True assert results['disabled'] is False def test_idempotent_disable_datacenter(self, *args): set_module_args(dict( state='disabled', password='admin', server='localhost', user='admin', name='foo' )) module = AnsibleModule( argument_spec=self.spec.argument_spec, supports_check_mode=self.spec.supports_check_mode ) current = ApiParameters(params=load_fixture('load_gtm_datacenter_disabled.json')) mm = ModuleManager(module=module) # Override methods to force specific logic in the module to happen mm.exists = Mock(return_value=True) mm.update_on_device = Mock(return_value=True) mm.read_current_from_device = Mock(return_value=current) results = mm.exec_module() assert results['changed'] is False
import unittest, os, errno from ctypes import * from ctypes.util import find_library from test import test_support try: import threading except ImportError: threading = None class Test(unittest.TestCase): def test_open(self): libc_name = find_library("c") if libc_name is None: raise unittest.SkipTest("Unable to find C library") libc = CDLL(libc_name, use_errno=True) if os.name == "nt": libc_open = libc._open else: libc_open = libc.open libc_open.argtypes = c_char_p, c_int self.assertEqual(libc_open("", 0), -1) self.assertEqual(get_errno(), errno.ENOENT) self.assertEqual(set_errno(32), errno.ENOENT) self.assertEqual(get_errno(), 32) if threading: def _worker(): set_errno(0) libc = CDLL(libc_name, use_errno=False) if os.name == "nt": libc_open = libc._open else: libc_open = libc.open libc_open.argtypes = c_char_p, c_int self.assertEqual(libc_open("", 0), -1) self.assertEqual(get_errno(), 0) t = threading.Thread(target=_worker) t.start() t.join() self.assertEqual(get_errno(), 32) set_errno(0) @unittest.skipUnless(os.name == "nt", 'Test specific to Windows') def test_GetLastError(self): dll = WinDLL("kernel32", use_last_error=True) GetModuleHandle = dll.GetModuleHandleA GetModuleHandle.argtypes = [c_wchar_p] self.assertEqual(0, GetModuleHandle("foo")) self.assertEqual(get_last_error(), 126) self.assertEqual(set_last_error(32), 126) self.assertEqual(get_last_error(), 32) def _worker(): set_last_error(0) dll = WinDLL("kernel32", use_last_error=False) GetModuleHandle = dll.GetModuleHandleW GetModuleHandle.argtypes = [c_wchar_p] GetModuleHandle("bar") self.assertEqual(get_last_error(), 0) t = threading.Thread(target=_worker) t.start() t.join() self.assertEqual(get_last_error(), 32) set_last_error(0) if __name__ == "__main__": unittest.main()
import os import shutil import sys from django.core.exceptions import ImproperlyConfigured from django.db.backends.base.creation import BaseDatabaseCreation from django.utils.encoding import force_text from django.utils.six.moves import input class DatabaseCreation(BaseDatabaseCreation): @staticmethod def is_in_memory_db(database_name): return database_name == ':memory:' or 'mode=memory' in force_text(database_name) def _get_test_db_name(self): test_database_name = self.connection.settings_dict['TEST']['NAME'] can_share_in_memory_db = self.connection.features.can_share_in_memory_db if not test_database_name: test_database_name = ':memory:' if can_share_in_memory_db: if test_database_name == ':memory:': return 'file:memorydb_%s?mode=memory&cache=shared' % self.connection.alias elif 'mode=memory' in test_database_name: raise ImproperlyConfigured( "Using a shared memory database with `mode=memory` in the " "database name is not supported in your environment, " "use `:memory:` instead." ) return test_database_name def _create_test_db(self, verbosity, autoclobber, keepdb=False): test_database_name = self._get_test_db_name() if keepdb: return test_database_name if not self.is_in_memory_db(test_database_name): # Erase the old test database if verbosity >= 1: print("Destroying old test database for alias %s..." % ( self._get_database_display_str(verbosity, test_database_name), )) if os.access(test_database_name, os.F_OK): if not autoclobber: confirm = input( "Type 'yes' if you would like to try deleting the test " "database '%s', or 'no' to cancel: " % test_database_name ) if autoclobber or confirm == 'yes': try: os.remove(test_database_name) except Exception as e: sys.stderr.write("Got an error deleting the old test database: %s\n" % e) sys.exit(2) else: print("Tests cancelled.") sys.exit(1) return test_database_name def get_test_db_clone_settings(self, number): orig_settings_dict = self.connection.settings_dict source_database_name = orig_settings_dict['NAME'] if self.is_in_memory_db(source_database_name): return orig_settings_dict else: new_settings_dict = orig_settings_dict.copy() root, ext = os.path.splitext(orig_settings_dict['NAME']) new_settings_dict['NAME'] = '{}_{}.{}'.format(root, number, ext) return new_settings_dict def _clone_test_db(self, number, verbosity, keepdb=False): source_database_name = self.connection.settings_dict['NAME'] target_database_name = self.get_test_db_clone_settings(number)['NAME'] # Forking automatically makes a copy of an in-memory database. if not self.is_in_memory_db(source_database_name): # Erase the old test database if os.access(target_database_name, os.F_OK): if keepdb: return if verbosity >= 1: print("Destroying old test database for alias %s..." % ( self._get_database_display_str(verbosity, target_database_name), )) try: os.remove(target_database_name) except Exception as e: sys.stderr.write("Got an error deleting the old test database: %s\n" % e) sys.exit(2) try: shutil.copy(source_database_name, target_database_name) except Exception as e: sys.stderr.write("Got an error cloning the test database: %s\n" % e) sys.exit(2) def _destroy_test_db(self, test_database_name, verbosity): if test_database_name and not self.is_in_memory_db(test_database_name): # Remove the SQLite database file os.remove(test_database_name) def test_db_signature(self): """ Returns a tuple that uniquely identifies a test database. This takes into account the special cases of ":memory:" and "" for SQLite since the databases will be distinct despite having the same TEST NAME. See http://www.sqlite.org/inmemorydb.html """ test_database_name = self._get_test_db_name() sig = [self.connection.settings_dict['NAME']] if self.is_in_memory_db(test_database_name): sig.append(self.connection.alias) return tuple(sig)
from __future__ import absolute_import, unicode_literals import json import hashlib from django import forms from django.conf import settings from django.db import connections from django.utils.encoding import force_text from django.utils.functional import cached_property from django.core.exceptions import ValidationError from debug_toolbar.panels.sql.utils import reformat_sql class SQLSelectForm(forms.Form): """ Validate params sql: The sql statement with interpolated params raw_sql: The sql statement with placeholders params: JSON encoded parameter values duration: time for SQL to execute passed in from toolbar just for redisplay hash: the hash of (secret + sql + params) for tamper checking """ sql = forms.CharField() raw_sql = forms.CharField() params = forms.CharField() alias = forms.CharField(required=False, initial='default') duration = forms.FloatField() hash = forms.CharField() def __init__(self, *args, **kwargs): initial = kwargs.get('initial', None) if initial is not None: initial['hash'] = self.make_hash(initial) super(SQLSelectForm, self).__init__(*args, **kwargs) for name in self.fields: self.fields[name].widget = forms.HiddenInput() def clean_raw_sql(self): value = self.cleaned_data['raw_sql'] if not value.lower().strip().startswith('select'): raise ValidationError("Only 'select' queries are allowed.") return value def clean_params(self): value = self.cleaned_data['params'] try: return json.loads(value) except ValueError: raise ValidationError('Is not valid JSON') def clean_alias(self): value = self.cleaned_data['alias'] if value not in connections: raise ValidationError("Database alias '%s' not found" % value) return value def clean_hash(self): hash = self.cleaned_data['hash'] if hash != self.make_hash(self.data): raise ValidationError('Tamper alert') return hash def reformat_sql(self): return reformat_sql(self.cleaned_data['sql']) def make_hash(self, data): items = [settings.SECRET_KEY, data['sql'], data['params']] # Replace lines endings with spaces to preserve the hash value # even when the browser normalizes \r\n to \n in inputs. items = [' '.join(force_text(item).splitlines()) for item in items] return hashlib.sha1(''.join(items).encode('utf-8')).hexdigest() @property def connection(self): return connections[self.cleaned_data['alias']] @cached_property def cursor(self): return self.connection.cursor()
import os, sys, requests, pprint, re, json from uritemplate import URITemplate, expand from subprocess import call changelog_file = '../../changelog.txt' token_file = '../../../TelegramPrivate/github-releases-token.txt' version = '' commit = '' for arg in sys.argv: if re.match(r'\d+\.\d+', arg): version = arg elif re.match(r'^[a-f0-9]{40}$', arg): commit = arg # thanks http://stackoverflow.com/questions/13909900/progress-of-python-requests-post class upload_in_chunks(object): def __init__(self, filename, chunksize=1 << 13): self.filename = filename self.chunksize = chunksize self.totalsize = os.path.getsize(filename) self.readsofar = 0 def __iter__(self): with open(self.filename, 'rb') as file: while True: data = file.read(self.chunksize) if not data: sys.stderr.write("\n") break self.readsofar += len(data) percent = self.readsofar * 1e2 / self.totalsize sys.stderr.write("\r{percent:3.0f}%".format(percent=percent)) yield data def __len__(self): return self.totalsize class IterableToFileAdapter(object): def __init__(self, iterable): self.iterator = iter(iterable) self.length = len(iterable) def read(self, size=-1): # TBD: add buffer for `len(data) > size` case return next(self.iterator, b'') def __len__(self): return self.length def checkResponseCode(result, right_code): if (result.status_code != right_code): print('Wrong result code: ' + str(result.status_code) + ', should be ' + str(right_code)) sys.exit(1) pp = pprint.PrettyPrinter(indent=2) url = 'https://api.github.com/' version_parts = version.split('.') stable = 1 alpha = 0 dev = 0 if len(version_parts) < 2: print('Error: expected at least major version ' + version) sys.exit(1) if len(version_parts) > 4: print('Error: bad version passed ' + version) sys.exit(1) version_major = version_parts[0] + '.' + version_parts[1] if len(version_parts) == 2: version = version_major + '.0' version_full = version else: version = version_major + '.' + version_parts[2] version_full = version if len(version_parts) == 4: if version_parts[3] == 'dev': dev = 1 stable = 0 version_full = version + '.dev' elif version_parts[3] == 'alpha': alpha = 1 stable = 0 version_full = version + '.alpha' else: print('Error: unexpected version part ' + version_parts[3]) sys.exit(1) access_token = '' if os.path.isfile(token_file): with open(token_file) as f: for line in f: access_token = line.replace('\n', '') if access_token == '': print('Access token not found!') sys.exit(1) print('Version: ' + version_full); local_folder = '/Volumes/Storage/backup/' + version_major + '/' + version_full if stable == 1: if os.path.isdir(local_folder + '.dev'): dev = 1 stable = 0 version_full = version + '.dev' local_folder = local_folder + '.dev' elif os.path.isdir(local_folder + '.alpha'): alpha = 1 stable = 0 version_full = version + '.alpha' local_folder = local_folder + '.alpha' if not os.path.isdir(local_folder): print('Storage path not found!') sys.exit(1) local_folder = local_folder + '/' files = [] files.append({ 'local': 'tsetup.' + version_full + '.exe', 'remote': 'tsetup.' + version_full + '.exe', 'backup_folder': 'tsetup', 'mime': 'application/octet-stream', 'label': 'Windows: Installer', }) files.append({ 'local': 'tportable.' + version_full + '.zip', 'remote': 'tportable.' + version_full + '.zip', 'backup_folder': 'tsetup', 'mime': 'application/zip', 'label': 'Windows: Portable', }) files.append({ 'local': 'tsetup.' + version_full + '.dmg', 'remote': 'tsetup.' + version_full + '.dmg', 'backup_folder': 'tmac', 'mime': 'application/octet-stream', 'label': 'macOS and OS X 10.8+: Installer', }) files.append({ 'local': 'tsetup32.' + version_full + '.dmg', 'remote': 'tsetup32.' + version_full + '.dmg', 'backup_folder': 'tmac32', 'mime': 'application/octet-stream', 'label': 'OS X 10.6 and 10.7: Installer', }) files.append({ 'local': 'tsetup.' + version_full + '.tar.xz', 'remote': 'tsetup.' + version_full + '.tar.xz', 'backup_folder': 'tlinux', 'mime': 'application/octet-stream', 'label': 'Linux 64 bit: Binary', }) files.append({ 'local': 'tsetup32.' + version_full + '.tar.xz', 'remote': 'tsetup32.' + version_full + '.tar.xz', 'backup_folder': 'tlinux32', 'mime': 'application/octet-stream', 'label': 'Linux 32 bit: Binary', }) r = requests.get(url + 'repos/telegramdesktop/tdesktop/releases/tags/v' + version) if r.status_code == 404: print('Release not found, creating.') if commit == '': print('Error: specify the commit.') sys.exit(1) if not os.path.isfile(changelog_file): print('Error: Changelog file not found.') sys.exit(1) changelog = '' started = 0 with open(changelog_file) as f: for line in f: if started == 1: if re.match(r'^\d+\.\d+', line): break; if re.match(r'^\s+$', line): continue changelog += line else: if re.match(r'^\d+\.\d+', line): if line[0:len(version) + 1] == version + ' ': started = 1 elif line[0:len(version_major) + 1] == version_major + ' ': if version_major + '.0' == version: started = 1 if started != 1: print('Error: Changelog not found.') sys.exit(1) changelog = changelog.strip() print('Changelog: '); print(changelog); r = requests.post(url + 'repos/telegramdesktop/tdesktop/releases', headers={'Authorization': 'token ' + access_token}, data=json.dumps({ 'tag_name': 'v' + version, 'target_commitish': commit, 'name': 'v ' + version, 'body': changelog, 'prerelease': (dev == 1 or alpha == 1), })) checkResponseCode(r, 201) r = requests.get(url + 'repos/telegramdesktop/tdesktop/releases/tags/v' + version) checkResponseCode(r, 200); release_data = r.json() #pp.pprint(release_data) release_id = release_data['id'] print('Release ID: ' + str(release_id)) r = requests.get(url + 'repos/telegramdesktop/tdesktop/releases/' + str(release_id) + '/assets'); checkResponseCode(r, 200); assets = release_data['assets'] for asset in assets: name = asset['name'] found = 0 for file in files: if file['remote'] == name: print('Already uploaded: ' + name) file['already'] = 1 found = 1 break if found == 0: print('Warning: strange asset: ' + name) for file in files: if 'already' in file: continue file_path = local_folder + file['backup_folder'] + '/' + file['local'] if not os.path.isfile(file_path): print('Warning: file not found ' + file['local']) continue upload_url = expand(release_data['upload_url'], {'name': file['remote'], 'label': file['label']}) + '&access_token=' + access_token; content = upload_in_chunks(file_path, 10) print('Uploading: ' + file['remote'] + ' (' + str(round(len(content) / 10000) / 100.) + ' MB)') r = requests.post(upload_url, headers={"Content-Type": file['mime']}, data=IterableToFileAdapter(content)) checkResponseCode(r, 201) print('Success! Removing.') return_code = call(["rm", file_path]) if return_code != 0: print('Bad rm code: ' + str(return_code)) sys.exit(1) sys.exit()
# # Copyright 2010 Free Software Foundation, Inc. # # This file was generated by gr_modtool, a tool from the GNU Radio framework # This file is a part of gr-howto # # SPDX-License-Identifier: GPL-3.0-or-later # # """ Utilities for extracting text from generated classes. """ from __future__ import unicode_literals def is_string(txt): if isinstance(txt, str): return True try: if isinstance(txt, str): return True except NameError: pass return False def description(obj): if obj is None: return None return description_bit(obj).strip() def description_bit(obj): if hasattr(obj, 'content'): contents = [description_bit(item) for item in obj.content] result = ''.join(contents) elif hasattr(obj, 'content_'): contents = [description_bit(item) for item in obj.content_] result = ''.join(contents) elif hasattr(obj, 'value'): result = description_bit(obj.value) elif is_string(obj): return obj else: raise Exception('Expecting a string or something with content, content_ or value attribute') # If this bit is a paragraph then add one some line breaks. if hasattr(obj, 'name') and obj.name == 'para': result += "\n\n" return result
# test_hhfit.py --- # # Filename: test_hhfit.py # Description: # Author: # Maintainer: # Created: Tue May 21 16:34:45 2013 (+0530) # Version: # Last-Updated: Tue May 21 16:37:28 2013 (+0530) # By: subha # Update #: 9 # URL: # Keywords: # Compatibility: # # # Commentary: # # # # # Change log: # # Tue May 21 16:34:53 IST 2013 - Subha moved code from # test_converter.py to test_hhfit.py. # # # This program is free software; you can redistribute it and/or # modify it under the terms of the GNU General Public License as # published by the Free Software Foundation; either version 3, or # (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU # General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program; see the file COPYING. If not, write to # the Free Software Foundation, Inc., 51 Franklin Street, Fifth # Floor, Boston, MA 02110-1301, USA. # # # Code: import os import numpy as np import uuid import unittest import pylab import hhfit class TestFindRateFn(unittest.TestCase): def setUp(self): self.vmin = -120e-3 self.vmax = 40e-3 self.vdivs = 640 self.v_array = np.linspace(self.vmin, self.vmax, self.vdivs+1) # Parameters for sigmoid function - from traub2005, NaF->m_inf p_sigmoid = (1.0, 1/-10e-3, -38e-3, 0.0) self.sigmoid = p_sigmoid[0] / (1.0 + np.exp(p_sigmoid[1] * (self.v_array - p_sigmoid[2]))) + p_sigmoid[3] self.p_sigmoid = p_sigmoid # Parameters for exponential function - from traub2005, KC->n_inf p_exp = (2e3, 1/-27e-3, -53.5e-3, 0.0) self.exp = p_exp[0] * np.exp(p_exp[1] * (self.v_array - p_exp[2])) + p_exp[3] self.p_exp = p_exp # Parameters for linoid function: alpha_n from original Hodgkin-Huxley K channel. p_linoid = (-0.01*1e3, -1/10e-3, 10e-3, 0.0) self.linoid = p_linoid[3] + p_linoid[0] * (self.v_array - p_linoid[2]) / (np.exp(p_linoid[1] * (self.v_array - p_linoid[2])) - 1) self.p_linoid = p_linoid # This is tau_m of transient Ca2+ current (eq. 7) from # Huguenard and McCormick, J Neurophysiol, 68:1373-1383, # 1992.; #1e-3 * (0.612 + 1 / (np.exp((self.v_array*1e3 + 132)/-16.7) + np.exp((self.v_array*1e3 + 16.8)/18.2))) p_dblexp = (1e-3, -1/16.7e-3, -132e-3, 1/18.2e-3, -16.8e-3, 0.612e-3) self.dblexp = p_dblexp[5] + p_dblexp[0] / (np.exp(p_dblexp[1] * (self.v_array - p_dblexp[2])) + np.exp(p_dblexp[3] * (self.v_array - p_dblexp[4]))) self.p_dblexp = p_dblexp def test_sigmoid(self): print 'Testing sigmoid' fn, params = hhfit.find_ratefn(self.v_array, self.sigmoid) print 'Sigmoid params original:', self.p_sigmoid, 'detected:', params pylab.plot(self.v_array, self.sigmoid, 'y-', self.v_array, hhfit.sigmoid(self.v_array, *self.p_sigmoid), 'b--', self.v_array, fn(self.v_array, *params), 'r-.') pylab.legend(('original sigmoid', 'computed', 'fitted %s' % (fn))) pylab.show() self.assertEqual(hhfit.sigmoid, fn) rms_error = np.sqrt(np.mean((self.sigmoid - fn(self.v_array, *params))**2)) self.assertAlmostEqual(rms_error/max(abs(self.sigmoid)), 0.0, places=3) def test_exponential(self): print 'Testing exponential' fn, params = hhfit.find_ratefn(self.v_array, self.exp) print 'Exponential params original:', self.p_exp, 'detected:', params fnval = hhfit.exponential(self.v_array, *params) pylab.plot(self.v_array, self.exp, 'y-', self.v_array, hhfit.exponential(self.v_array, *self.p_exp), 'b--', self.v_array, fnval, 'r-.') pylab.legend(('original exp', 'computed', 'fitted %s' % (fn))) pylab.show() self.assertEqual(hhfit.exponential, fn) # The same exponential can be satisfied by an infinite number # of parameter values. Hence we cannot compare the parameters, # but only the fit rms_error = np.sqrt(np.sum((self.exp - fnval)**2)) # pylab.plot(self.v_array, self.exp, 'b-') # pylab.plot(self.v_array, fnval, 'r-.') # pylab.show() print rms_error, rms_error/max(self.exp) self.assertAlmostEqual(rms_error/max(self.exp), 0.0, places=3) def test_linoid(self): print 'Testing linoid' fn, params = hhfit.find_ratefn(self.v_array, self.linoid) print 'Linoid params original:', self.p_linoid, 'detected:', params pylab.plot(self.v_array, self.linoid, 'y-', self.v_array, hhfit.linoid(self.v_array, *self.p_linoid), 'b--', self.v_array, fn(self.v_array, *params), 'r-.') pylab.legend(('original linoid', 'computed', 'fitted %s' % (fn))) pylab.show() self.assertEqual(hhfit.linoid, fn) fnval = fn(self.v_array, *params) rms_error = np.sqrt(np.mean((self.linoid - fnval)**2)) self.assertAlmostEqual(rms_error/max(self.linoid), 0.0, places=3) # errors = params - np.array(self.p_linoid) # for orig, err in zip(self.p_linoid, errors): # self.assertAlmostEqual(abs(err/orig), 0.0, places=2) def test_dblexponential(self): print 'Testing double exponential' fn, params = hhfit.find_ratefn(self.v_array, self.dblexp) fnval = fn(self.v_array, *params) pylab.plot(self.v_array, self.dblexp, 'y-', self.v_array, hhfit.double_exp(self.v_array, *self.p_dblexp), 'b--', self.v_array, fnval, 'r-.') pylab.legend(('original dblexp', 'computed', 'fitted %s' % (fn))) pylab.show() self.assertEqual(hhfit.double_exp, fn) rms_error = np.sqrt(np.mean((self.dblexp - fnval)**2)) print params, rms_error self.assertAlmostEqual(rms_error/max(self.dblexp), 0.0, places=3) if __name__ == '__main__': unittest.main() # # test_hhfit.py ends here
"""Schur decomposition functions.""" import numpy from numpy import asarray_chkfinite, single # Local imports. import misc from misc import LinAlgError, _datacopied from lapack import get_lapack_funcs from decomp import eigvals __all__ = ['schur', 'rsf2csf'] _double_precision = ['i','l','d'] def schur(a, output='real', lwork=None, overwrite_a=False): """Compute Schur decomposition of a matrix. The Schur decomposition is A = Z T Z^H where Z is unitary and T is either upper-triangular, or for real Schur decomposition (output='real'), quasi-upper triangular. In the quasi-triangular form, 2x2 blocks describing complex-valued eigenvalue pairs may extrude from the diagonal. Parameters ---------- a : array, shape (M, M) Matrix to decompose output : {'real', 'complex'} Construct the real or complex Schur decomposition (for real matrices). lwork : integer Work array size. If None or -1, it is automatically computed. overwrite_a : boolean Whether to overwrite data in a (may improve performance) Returns ------- T : array, shape (M, M) Schur form of A. It is real-valued for the real Schur decomposition. Z : array, shape (M, M) An unitary Schur transformation matrix for A. It is real-valued for the real Schur decomposition. See also -------- rsf2csf : Convert real Schur form to complex Schur form """ if not output in ['real','complex','r','c']: raise ValueError("argument must be 'real', or 'complex'") a1 = asarray_chkfinite(a) if len(a1.shape) != 2 or (a1.shape[0] != a1.shape[1]): raise ValueError('expected square matrix') typ = a1.dtype.char if output in ['complex','c'] and typ not in ['F','D']: if typ in _double_precision: a1 = a1.astype('D') typ = 'D' else: a1 = a1.astype('F') typ = 'F' overwrite_a = overwrite_a or (_datacopied(a1, a)) gees, = get_lapack_funcs(('gees',), (a1,)) if lwork is None or lwork == -1: # get optimal work array result = gees(lambda x: None, a1, lwork=-1) lwork = result[-2][0].real.astype(numpy.int) result = gees(lambda x: None, a1, lwork=lwork, overwrite_a=overwrite_a) info = result[-1] if info < 0: raise ValueError('illegal value in %d-th argument of internal gees' % -info) elif info > 0: raise LinAlgError("Schur form not found. Possibly ill-conditioned.") return result[0], result[-3] eps = numpy.finfo(float).eps feps = numpy.finfo(single).eps _array_kind = {'b':0, 'h':0, 'B': 0, 'i':0, 'l': 0, 'f': 0, 'd': 0, 'F': 1, 'D': 1} _array_precision = {'i': 1, 'l': 1, 'f': 0, 'd': 1, 'F': 0, 'D': 1} _array_type = [['f', 'd'], ['F', 'D']] def _commonType(*arrays): kind = 0 precision = 0 for a in arrays: t = a.dtype.char kind = max(kind, _array_kind[t]) precision = max(precision, _array_precision[t]) return _array_type[kind][precision] def _castCopy(type, *arrays): cast_arrays = () for a in arrays: if a.dtype.char == type: cast_arrays = cast_arrays + (a.copy(),) else: cast_arrays = cast_arrays + (a.astype(type),) if len(cast_arrays) == 1: return cast_arrays[0] else: return cast_arrays def rsf2csf(T, Z): """Convert real Schur form to complex Schur form. Convert a quasi-diagonal real-valued Schur form to the upper triangular complex-valued Schur form. Parameters ---------- T : array, shape (M, M) Real Schur form of the original matrix Z : array, shape (M, M) Schur transformation matrix Returns ------- T : array, shape (M, M) Complex Schur form of the original matrix Z : array, shape (M, M) Schur transformation matrix corresponding to the complex form See also -------- schur : Schur decompose a matrix """ Z, T = map(asarray_chkfinite, (Z, T)) if len(Z.shape) != 2 or Z.shape[0] != Z.shape[1]: raise ValueError("matrix must be square.") if len(T.shape) != 2 or T.shape[0] != T.shape[1]: raise ValueError("matrix must be square.") if T.shape[0] != Z.shape[0]: raise ValueError("matrices must be same dimension.") N = T.shape[0] arr = numpy.array t = _commonType(Z, T, arr([3.0],'F')) Z, T = _castCopy(t, Z, T) conj = numpy.conj dot = numpy.dot r_ = numpy.r_ transp = numpy.transpose for m in range(N-1, 0, -1): if abs(T[m,m-1]) > eps*(abs(T[m-1,m-1]) + abs(T[m,m])): k = slice(m-1, m+1) mu = eigvals(T[k,k]) - T[m,m] r = misc.norm([mu[0], T[m,m-1]]) c = mu[0] / r s = T[m,m-1] / r G = r_[arr([[conj(c), s]], dtype=t), arr([[-s, c]], dtype=t)] Gc = conj(transp(G)) j = slice(m-1, N) T[k,j] = dot(G, T[k,j]) i = slice(0, m+1) T[i,k] = dot(T[i,k], Gc) i = slice(0, N) Z[i,k] = dot(Z[i,k], Gc) T[m,m-1] = 0.0; return T, Z
# vim: tabstop=4 shiftwidth=4 softtabstop=4 # Copyright 2012 OpenStack LLC # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from keystone import exception class DictKvs(dict): def get(self, key, default=None): try: return self[key] except KeyError: if default is not None: return default raise exception.NotFound(target=key) def set(self, key, value): if isinstance(value, dict): self[key] = value.copy() else: self[key] = value[:] def delete(self, key): """Deletes an item, returning True on success, False otherwise.""" try: del self[key] except KeyError: raise exception.NotFound(target=key) INMEMDB = DictKvs() class Base(object): def __init__(self, db=None): if db is None: db = INMEMDB elif isinstance(db, dict): db = DictKvs(db) self.db = db
# Copyright 2015 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== """Simple MNIST classifier example with JIT XLA and timelines. """ from __future__ import absolute_import from __future__ import division from __future__ import print_function import argparse import sys import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data from tensorflow.python.client import timeline FLAGS = None def main(_): # Import data mnist = input_data.read_data_sets(FLAGS.data_dir) # Create the model x = tf.placeholder(tf.float32, [None, 784]) w = tf.Variable(tf.zeros([784, 10])) b = tf.Variable(tf.zeros([10])) y = tf.matmul(x, w) + b # Define loss and optimizer y_ = tf.placeholder(tf.int64, [None]) # The raw formulation of cross-entropy, # # tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.nn.softmax(y)), # reduction_indices=[1])) # # can be numerically unstable. # # So here we use tf.losses.sparse_softmax_cross_entropy on the raw # logit outputs of 'y', and then average across the batch. cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y_, logits=y) train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy) config = tf.ConfigProto() jit_level = 0 if FLAGS.xla: # Turns on XLA JIT compilation. jit_level = tf.OptimizerOptions.ON_1 config.graph_options.optimizer_options.global_jit_level = jit_level run_metadata = tf.RunMetadata() sess = tf.Session(config=config) tf.global_variables_initializer().run(session=sess) # Train train_loops = 1000 for i in range(train_loops): batch_xs, batch_ys = mnist.train.next_batch(100) # Create a timeline for the last loop and export to json to view with # chrome://tracing/. if i == train_loops - 1: sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys}, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), run_metadata=run_metadata) trace = timeline.Timeline(step_stats=run_metadata.step_stats) with open('timeline.ctf.json', 'w') as trace_file: trace_file.write(trace.generate_chrome_trace_format()) else: sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys}) # Test trained model correct_prediction = tf.equal(tf.argmax(y, 1), y_) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})) sess.close() if __name__ == '__main__': parser = argparse.ArgumentParser() parser.add_argument( '--data_dir', type=str, default='/tmp/tensorflow/mnist/input_data', help='Directory for storing input data') parser.add_argument( '--xla', type=bool, default=True, help='Turn xla via JIT on') FLAGS, unparsed = parser.parse_known_args() tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
from django.forms.widgets import Textarea from django.template import loader, Context from django.templatetags.static import static from django.utils import translation from django.contrib.gis.gdal import OGRException from django.contrib.gis.geos import GEOSGeometry, GEOSException # Creating a template context that contains Django settings # values needed by admin map templates. geo_context = Context({'LANGUAGE_BIDI' : translation.get_language_bidi()}) class OpenLayersWidget(Textarea): """ Renders an OpenLayers map using the WKT of the geometry. """ def render(self, name, value, attrs=None): # Update the template parameters with any attributes passed in. if attrs: self.params.update(attrs) # Defaulting the WKT value to a blank string -- this # will be tested in the JavaScript and the appropriate # interface will be constructed. self.params['wkt'] = '' # If a string reaches here (via a validation error on another # field) then just reconstruct the Geometry. if isinstance(value, basestring): try: value = GEOSGeometry(value) except (GEOSException, ValueError): value = None if value and value.geom_type.upper() != self.geom_type: value = None # Constructing the dictionary of the map options. self.params['map_options'] = self.map_options() # Constructing the JavaScript module name using the name of # the GeometryField (passed in via the `attrs` keyword). # Use the 'name' attr for the field name (rather than 'field') self.params['name'] = name # note: we must switch out dashes for underscores since js # functions are created using the module variable js_safe_name = self.params['name'].replace('-','_') self.params['module'] = 'geodjango_%s' % js_safe_name if value: # Transforming the geometry to the projection used on the # OpenLayers map. srid = self.params['srid'] if value.srid != srid: try: ogr = value.ogr ogr.transform(srid) wkt = ogr.wkt except OGRException: wkt = '' else: wkt = value.wkt # Setting the parameter WKT with that of the transformed # geometry. self.params['wkt'] = wkt return loader.render_to_string(self.template, self.params, context_instance=geo_context) def map_options(self): "Builds the map options hash for the OpenLayers template." # JavaScript construction utilities for the Bounds and Projection. def ol_bounds(extent): return 'new OpenLayers.Bounds(%s)' % str(extent) def ol_projection(srid): return 'new OpenLayers.Projection("EPSG:%s")' % srid # An array of the parameter name, the name of their OpenLayers # counterpart, and the type of variable they are. map_types = [('srid', 'projection', 'srid'), ('display_srid', 'displayProjection', 'srid'), ('units', 'units', str), ('max_resolution', 'maxResolution', float), ('max_extent', 'maxExtent', 'bounds'), ('num_zoom', 'numZoomLevels', int), ('max_zoom', 'maxZoomLevels', int), ('min_zoom', 'minZoomLevel', int), ] # Building the map options hash. map_options = {} for param_name, js_name, option_type in map_types: if self.params.get(param_name, False): if option_type == 'srid': value = ol_projection(self.params[param_name]) elif option_type == 'bounds': value = ol_bounds(self.params[param_name]) elif option_type in (float, int): value = self.params[param_name] elif option_type in (str,): value = '"%s"' % self.params[param_name] else: raise TypeError map_options[js_name] = value return map_options
# Copyright (c) Twisted Matrix Laboratories. # See LICENSE for details. # """Module to parse ANSI escape sequences Maintainer: Jean-Paul Calderone """ import string # Twisted imports from twisted.python import log class ColorText: """ Represents an element of text along with the texts colors and additional attributes. """ # The colors to use COLORS = ('b', 'r', 'g', 'y', 'l', 'm', 'c', 'w') BOLD_COLORS = tuple([x.upper() for x in COLORS]) BLACK, RED, GREEN, YELLOW, BLUE, MAGENTA, CYAN, WHITE = range(len(COLORS)) # Color names COLOR_NAMES = ( 'Black', 'Red', 'Green', 'Yellow', 'Blue', 'Magenta', 'Cyan', 'White' ) def __init__(self, text, fg, bg, display, bold, underline, flash, reverse): self.text, self.fg, self.bg = text, fg, bg self.display = display self.bold = bold self.underline = underline self.flash = flash self.reverse = reverse if self.reverse: self.fg, self.bg = self.bg, self.fg class AnsiParser: """ Parser class for ANSI codes. """ # Terminators for cursor movement ansi controls - unsupported CURSOR_SET = ('H', 'f', 'A', 'B', 'C', 'D', 'R', 's', 'u', 'd','G') # Terminators for erasure ansi controls - unsupported ERASE_SET = ('J', 'K', 'P') # Terminators for mode change ansi controls - unsupported MODE_SET = ('h', 'l') # Terminators for keyboard assignment ansi controls - unsupported ASSIGN_SET = ('p',) # Terminators for color change ansi controls - supported COLOR_SET = ('m',) SETS = (CURSOR_SET, ERASE_SET, MODE_SET, ASSIGN_SET, COLOR_SET) def __init__(self, defaultFG, defaultBG): self.defaultFG, self.defaultBG = defaultFG, defaultBG self.currentFG, self.currentBG = self.defaultFG, self.defaultBG self.bold, self.flash, self.underline, self.reverse = 0, 0, 0, 0 self.display = 1 self.prepend = '' def stripEscapes(self, string): """ Remove all ANSI color escapes from the given string. """ result = '' show = 1 i = 0 L = len(string) while i < L: if show == 0 and string[i] in _sets: show = 1 elif show: n = string.find('\x1B', i) if n == -1: return result + string[i:] else: result = result + string[i:n] i = n show = 0 i = i + 1 return result def writeString(self, colorstr): pass def parseString(self, str): """ Turn a string input into a list of L{ColorText} elements. """ if self.prepend: str = self.prepend + str self.prepend = '' parts = str.split('\x1B') if len(parts) == 1: self.writeString(self.formatText(parts[0])) else: self.writeString(self.formatText(parts[0])) for s in parts[1:]: L = len(s) i = 0 type = None while i < L: if s[i] not in string.digits+'[;?': break i+=1 if not s: self.prepend = '\x1b' return if s[0]!='[': self.writeString(self.formatText(s[i+1:])) continue else: s=s[1:] i-=1 if i==L-1: self.prepend = '\x1b[' return type = _setmap.get(s[i], None) if type is None: continue if type == AnsiParser.COLOR_SET: self.parseColor(s[:i + 1]) s = s[i + 1:] self.writeString(self.formatText(s)) elif type == AnsiParser.CURSOR_SET: cursor, s = s[:i+1], s[i+1:] self.parseCursor(cursor) self.writeString(self.formatText(s)) elif type == AnsiParser.ERASE_SET: erase, s = s[:i+1], s[i+1:] self.parseErase(erase) self.writeString(self.formatText(s)) elif type == AnsiParser.MODE_SET: mode, s = s[:i+1], s[i+1:] #self.parseErase('2J') self.writeString(self.formatText(s)) elif i == L: self.prepend = '\x1B[' + s else: log.msg('Unhandled ANSI control type: %c' % (s[i],)) s = s[i + 1:] self.writeString(self.formatText(s)) def parseColor(self, str): """ Handle a single ANSI color sequence """ # Drop the trailing 'm' str = str[:-1] if not str: str = '0' try: parts = map(int, str.split(';')) except ValueError: log.msg('Invalid ANSI color sequence (%d): %s' % (len(str), str)) self.currentFG, self.currentBG = self.defaultFG, self.defaultBG return for x in parts: if x == 0: self.currentFG, self.currentBG = self.defaultFG, self.defaultBG self.bold, self.flash, self.underline, self.reverse = 0, 0, 0, 0 self.display = 1 elif x == 1: self.bold = 1 elif 30 <= x <= 37: self.currentFG = x - 30 elif 40 <= x <= 47: self.currentBG = x - 40 elif x == 39: self.currentFG = self.defaultFG elif x == 49: self.currentBG = self.defaultBG elif x == 4: self.underline = 1 elif x == 5: self.flash = 1 elif x == 7: self.reverse = 1 elif x == 8: self.display = 0 elif x == 22: self.bold = 0 elif x == 24: self.underline = 0 elif x == 25: self.blink = 0 elif x == 27: self.reverse = 0 elif x == 28: self.display = 1 else: log.msg('Unrecognised ANSI color command: %d' % (x,)) def parseCursor(self, cursor): pass def parseErase(self, erase): pass def pickColor(self, value, mode, BOLD = ColorText.BOLD_COLORS): if mode: return ColorText.COLORS[value] else: return self.bold and BOLD[value] or ColorText.COLORS[value] def formatText(self, text): return ColorText( text, self.pickColor(self.currentFG, 0), self.pickColor(self.currentBG, 1), self.display, self.bold, self.underline, self.flash, self.reverse ) _sets = ''.join(map(''.join, AnsiParser.SETS)) _setmap = {} for s in AnsiParser.SETS: for r in s: _setmap[r] = s del s
# Test the functions and main class method of FormatParagraph.py import unittest from idlelib import FormatParagraph as fp from idlelib.EditorWindow import EditorWindow from tkinter import Tk, Text, TclError from test.support import requires class Is_Get_Test(unittest.TestCase): """Test the is_ and get_ functions""" test_comment = '# This is a comment' test_nocomment = 'This is not a comment' trailingws_comment = '# This is a comment ' leadingws_comment = ' # This is a comment' leadingws_nocomment = ' This is not a comment' def test_is_all_white(self): self.assertTrue(fp.is_all_white('')) self.assertTrue(fp.is_all_white('\t\n\r\f\v')) self.assertFalse(fp.is_all_white(self.test_comment)) def test_get_indent(self): Equal = self.assertEqual Equal(fp.get_indent(self.test_comment), '') Equal(fp.get_indent(self.trailingws_comment), '') Equal(fp.get_indent(self.leadingws_comment), ' ') Equal(fp.get_indent(self.leadingws_nocomment), ' ') def test_get_comment_header(self): Equal = self.assertEqual # Test comment strings Equal(fp.get_comment_header(self.test_comment), '#') Equal(fp.get_comment_header(self.trailingws_comment), '#') Equal(fp.get_comment_header(self.leadingws_comment), ' #') # Test non-comment strings Equal(fp.get_comment_header(self.leadingws_nocomment), ' ') Equal(fp.get_comment_header(self.test_nocomment), '') class FindTest(unittest.TestCase): """Test the find_paragraph function in FormatParagraph. Using the runcase() function, find_paragraph() is called with 'mark' set at multiple indexes before and inside the test paragraph. It appears that code with the same indentation as a quoted string is grouped as part of the same paragraph, which is probably incorrect behavior. """ @classmethod def setUpClass(cls): from idlelib.idle_test.mock_tk import Text cls.text = Text() def runcase(self, inserttext, stopline, expected): # Check that find_paragraph returns the expected paragraph when # the mark index is set to beginning, middle, end of each line # up to but not including the stop line text = self.text text.insert('1.0', inserttext) for line in range(1, stopline): linelength = int(text.index("%d.end" % line).split('.')[1]) for col in (0, linelength//2, linelength): tempindex = "%d.%d" % (line, col) self.assertEqual(fp.find_paragraph(text, tempindex), expected) text.delete('1.0', 'end') def test_find_comment(self): comment = ( "# Comment block with no blank lines before\n" "# Comment line\n" "\n") self.runcase(comment, 3, ('1.0', '3.0', '#', comment[0:58])) comment = ( "\n" "# Comment block with whitespace line before and after\n" "# Comment line\n" "\n") self.runcase(comment, 4, ('2.0', '4.0', '#', comment[1:70])) comment = ( "\n" " # Indented comment block with whitespace before and after\n" " # Comment line\n" "\n") self.runcase(comment, 4, ('2.0', '4.0', ' #', comment[1:82])) comment = ( "\n" "# Single line comment\n" "\n") self.runcase(comment, 3, ('2.0', '3.0', '#', comment[1:23])) comment = ( "\n" " # Single line comment with leading whitespace\n" "\n") self.runcase(comment, 3, ('2.0', '3.0', ' #', comment[1:51])) comment = ( "\n" "# Comment immediately followed by code\n" "x = 42\n" "\n") self.runcase(comment, 3, ('2.0', '3.0', '#', comment[1:40])) comment = ( "\n" " # Indented comment immediately followed by code\n" "x = 42\n" "\n") self.runcase(comment, 3, ('2.0', '3.0', ' #', comment[1:53])) comment = ( "\n" "# Comment immediately followed by indented code\n" " x = 42\n" "\n") self.runcase(comment, 3, ('2.0', '3.0', '#', comment[1:49])) def test_find_paragraph(self): teststring = ( '"""String with no blank lines before\n' 'String line\n' '"""\n' '\n') self.runcase(teststring, 4, ('1.0', '4.0', '', teststring[0:53])) teststring = ( "\n" '"""String with whitespace line before and after\n' 'String line.\n' '"""\n' '\n') self.runcase(teststring, 5, ('2.0', '5.0', '', teststring[1:66])) teststring = ( '\n' ' """Indented string with whitespace before and after\n' ' Comment string.\n' ' """\n' '\n') self.runcase(teststring, 5, ('2.0', '5.0', ' ', teststring[1:85])) teststring = ( '\n' '"""Single line string."""\n' '\n') self.runcase(teststring, 3, ('2.0', '3.0', '', teststring[1:27])) teststring = ( '\n' ' """Single line string with leading whitespace."""\n' '\n') self.runcase(teststring, 3, ('2.0', '3.0', ' ', teststring[1:55])) class ReformatFunctionTest(unittest.TestCase): """Test the reformat_paragraph function without the editor window.""" def test_reformat_paragrah(self): Equal = self.assertEqual reform = fp.reformat_paragraph hw = "O hello world" Equal(reform(' ', 1), ' ') Equal(reform("Hello world", 20), "Hello world") # Test without leading newline Equal(reform(hw, 1), "O\nhello\nworld") Equal(reform(hw, 6), "O\nhello\nworld") Equal(reform(hw, 7), "O hello\nworld") Equal(reform(hw, 12), "O hello\nworld") Equal(reform(hw, 13), "O hello world") # Test with leading newline hw = "\nO hello world" Equal(reform(hw, 1), "\nO\nhello\nworld") Equal(reform(hw, 6), "\nO\nhello\nworld") Equal(reform(hw, 7), "\nO hello\nworld") Equal(reform(hw, 12), "\nO hello\nworld") Equal(reform(hw, 13), "\nO hello world") class ReformatCommentTest(unittest.TestCase): """Test the reformat_comment function without the editor window.""" def test_reformat_comment(self): Equal = self.assertEqual # reformat_comment formats to a minimum of 20 characters test_string = ( " \"\"\"this is a test of a reformat for a triple quoted string" " will it reformat to less than 70 characters for me?\"\"\"") result = fp.reformat_comment(test_string, 70, " ") expected = ( " \"\"\"this is a test of a reformat for a triple quoted string will it\n" " reformat to less than 70 characters for me?\"\"\"") Equal(result, expected) test_comment = ( "# this is a test of a reformat for a triple quoted string will " "it reformat to less than 70 characters for me?") result = fp.reformat_comment(test_comment, 70, "#") expected = ( "# this is a test of a reformat for a triple quoted string will it\n" "# reformat to less than 70 characters for me?") Equal(result, expected) class FormatClassTest(unittest.TestCase): def test_init_close(self): instance = fp.FormatParagraph('editor') self.assertEqual(instance.editwin, 'editor') instance.close() self.assertEqual(instance.editwin, None) # For testing format_paragraph_event, Initialize FormatParagraph with # a mock Editor with .text and .get_selection_indices. The text must # be a Text wrapper that adds two methods # A real EditorWindow creates unneeded, time-consuming baggage and # sometimes emits shutdown warnings like this: # "warning: callback failed in WindowList <class '_tkinter.TclError'> # : invalid command name ".55131368.windows". # Calling EditorWindow._close in tearDownClass prevents this but causes # other problems (windows left open). class TextWrapper: def __init__(self, master): self.text = Text(master=master) def __getattr__(self, name): return getattr(self.text, name) def undo_block_start(self): pass def undo_block_stop(self): pass class Editor: def __init__(self, root): self.text = TextWrapper(root) get_selection_indices = EditorWindow. get_selection_indices class FormatEventTest(unittest.TestCase): """Test the formatting of text inside a Text widget. This is done with FormatParagraph.format.paragraph_event, which calls functions in the module as appropriate. """ test_string = ( " '''this is a test of a reformat for a triple " "quoted string will it reformat to less than 70 " "characters for me?'''\n") multiline_test_string = ( " '''The first line is under the max width.\n" " The second line's length is way over the max width. It goes " "on and on until it is over 100 characters long.\n" " Same thing with the third line. It is also way over the max " "width, but FormatParagraph will fix it.\n" " '''\n") multiline_test_comment = ( "# The first line is under the max width.\n" "# The second line's length is way over the max width. It goes on " "and on until it is over 100 characters long.\n" "# Same thing with the third line. It is also way over the max " "width, but FormatParagraph will fix it.\n" "# The fourth line is short like the first line.") @classmethod def setUpClass(cls): requires('gui') cls.root = Tk() editor = Editor(root=cls.root) cls.text = editor.text.text # Test code does not need the wrapper. cls.formatter = fp.FormatParagraph(editor).format_paragraph_event # Sets the insert mark just after the re-wrapped and inserted text. @classmethod def tearDownClass(cls): cls.root.destroy() del cls.root del cls.text del cls.formatter def test_short_line(self): self.text.insert('1.0', "Short line\n") self.formatter("Dummy") self.assertEqual(self.text.get('1.0', 'insert'), "Short line\n" ) self.text.delete('1.0', 'end') def test_long_line(self): text = self.text # Set cursor ('insert' mark) to '1.0', within text. text.insert('1.0', self.test_string) text.mark_set('insert', '1.0') self.formatter('ParameterDoesNothing', limit=70) result = text.get('1.0', 'insert') # find function includes \n expected = ( " '''this is a test of a reformat for a triple quoted string will it\n" " reformat to less than 70 characters for me?'''\n") # yes self.assertEqual(result, expected) text.delete('1.0', 'end') # Select from 1.11 to line end. text.insert('1.0', self.test_string) text.tag_add('sel', '1.11', '1.end') self.formatter('ParameterDoesNothing', limit=70) result = text.get('1.0', 'insert') # selection excludes \n expected = ( " '''this is a test of a reformat for a triple quoted string will it reformat\n" " to less than 70 characters for me?'''") # no self.assertEqual(result, expected) text.delete('1.0', 'end') def test_multiple_lines(self): text = self.text # Select 2 long lines. text.insert('1.0', self.multiline_test_string) text.tag_add('sel', '2.0', '4.0') self.formatter('ParameterDoesNothing', limit=70) result = text.get('2.0', 'insert') expected = ( " The second line's length is way over the max width. It goes on and\n" " on until it is over 100 characters long. Same thing with the third\n" " line. It is also way over the max width, but FormatParagraph will\n" " fix it.\n") self.assertEqual(result, expected) text.delete('1.0', 'end') def test_comment_block(self): text = self.text # Set cursor ('insert') to '1.0', within block. text.insert('1.0', self.multiline_test_comment) self.formatter('ParameterDoesNothing', limit=70) result = text.get('1.0', 'insert') expected = ( "# The first line is under the max width. The second line's length is\n" "# way over the max width. It goes on and on until it is over 100\n" "# characters long. Same thing with the third line. It is also way over\n" "# the max width, but FormatParagraph will fix it. The fourth line is\n" "# short like the first line.\n") self.assertEqual(result, expected) text.delete('1.0', 'end') # Select line 2, verify line 1 unaffected. text.insert('1.0', self.multiline_test_comment) text.tag_add('sel', '2.0', '3.0') self.formatter('ParameterDoesNothing', limit=70) result = text.get('1.0', 'insert') expected = ( "# The first line is under the max width.\n" "# The second line's length is way over the max width. It goes on and\n" "# on until it is over 100 characters long.\n") self.assertEqual(result, expected) text.delete('1.0', 'end') # The following block worked with EditorWindow but fails with the mock. # Lines 2 and 3 get pasted together even though the previous block left # the previous line alone. More investigation is needed. ## # Select lines 3 and 4 ## text.insert('1.0', self.multiline_test_comment) ## text.tag_add('sel', '3.0', '5.0') ## self.formatter('ParameterDoesNothing') ## result = text.get('3.0', 'insert') ## expected = ( ##"# Same thing with the third line. It is also way over the max width,\n" ##"# but FormatParagraph will fix it. The fourth line is short like the\n" ##"# first line.\n") ## self.assertEqual(result, expected) ## text.delete('1.0', 'end') if __name__ == '__main__': unittest.main(verbosity=2, exit=2)
# coding: utf-8 from __future__ import unicode_literals import re from .common import InfoExtractor from ..utils import unescapeHTML class BaiduVideoIE(InfoExtractor): IE_DESC = '' _VALID_URL = r'https?://v\.baidu\.com/(?P<type>[a-z]+)/(?P<id>\d+)\.htm' _TESTS = [{ 'url': 'http://v.baidu.com/comic/1069.htm?frp=bdbrand&q=%E4%B8%AD%E5%8D%8E%E5%B0%8F%E5%BD%93%E5%AE%B6', 'info_dict': { 'id': '1069', 'title': ' TV', 'description': 'md5:51be07afe461cf99fa61231421b5397c', }, 'playlist_count': 52, }, { 'url': 'http://v.baidu.com/show/11595.htm?frp=bdbrand', 'info_dict': { 'id': '11595', 'title': 're:^', 'description': 'md5:1bf88bad6d850930f542d51547c089b8', }, 'playlist_mincount': 12, }] def _call_api(self, path, category, playlist_id, note): return self._download_json('http://app.video.baidu.com/%s/?worktype=adnative%s&id=%s' % ( path, category, playlist_id), playlist_id, note) def _real_extract(self, url): category, playlist_id = re.match(self._VALID_URL, url).groups() if category == 'show': category = 'tvshow' if category == 'tv': category = 'tvplay' playlist_detail = self._call_api( 'xqinfo', category, playlist_id, 'Download playlist JSON metadata') playlist_title = playlist_detail['title'] playlist_description = unescapeHTML(playlist_detail.get('intro')) episodes_detail = self._call_api( 'xqsingle', category, playlist_id, 'Download episodes JSON metadata') entries = [self.url_result( episode['url'], video_title=episode['title'] ) for episode in episodes_detail['videos']] return self.playlist_result( entries, playlist_id, playlist_title, playlist_description)
# Copyright 2012 United States Government as represented by the # Administrator of the National Aeronautics and Space Administration. # All Rights Reserved. # # Copyright 2012 Nebula, Inc. # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import json import logging from oslo_utils import units from django import conf from django.core.urlresolvers import reverse from django.core.urlresolvers import reverse_lazy from django.utils.translation import ugettext_lazy as _ from horizon import exceptions from horizon import forms from horizon import tables from horizon.utils import memoized from openstack_dashboard import api from openstack_dashboard.dashboards.project.images.images import views from openstack_dashboard.dashboards.admin.images import forms as project_forms from openstack_dashboard.dashboards.admin.images \ import tables as project_tables LOG = logging.getLogger(__name__) class IndexView(tables.DataTableView): table_class = project_tables.AdminImagesTable template_name = 'admin/images/index.html' page_title = _("Images") def has_prev_data(self, table): return self._prev def has_more_data(self, table): return self._more def get_data(self): images = [] filters = self.get_filters() prev_marker = self.request.GET.get( project_tables.AdminImagesTable._meta.prev_pagination_param, None) if prev_marker is not None: sort_dir = 'asc' marker = prev_marker else: sort_dir = 'desc' marker = self.request.GET.get( project_tables.AdminImagesTable._meta.pagination_param, None) try: images, self._more, self._prev = api.glance.image_list_detailed( self.request, marker=marker, paginate=True, filters=filters, sort_dir=sort_dir) if prev_marker is not None: images = sorted(images, key=lambda image: getattr(image, 'created_at'), reverse=True) except Exception: self._prev = False self._more = False msg = _('Unable to retrieve image list.') exceptions.handle(self.request, msg) return images def get_filters(self): filters = {'is_public': None} filter_field = self.table.get_filter_field() filter_string = self.table.get_filter_string() filter_action = self.table._meta._filter_action if filter_field and filter_string and ( filter_action.is_api_filter(filter_field)): if filter_field in ['size_min', 'size_max']: invalid_msg = ('API query is not valid and is ignored: %s=%s' % (filter_field, filter_string)) try: filter_string = long(float(filter_string) * (units.Mi)) if filter_string >= 0: filters[filter_field] = filter_string else: LOG.warning(invalid_msg) except ValueError: LOG.warning(invalid_msg) elif (filter_field == 'disk_format' and filter_string.lower() == 'docker'): filters['disk_format'] = 'raw' filters['container_format'] = 'docker' else: filters[filter_field] = filter_string return filters class CreateView(views.CreateView): template_name = 'admin/images/create.html' form_class = project_forms.AdminCreateImageForm submit_url = reverse_lazy('horizon:admin:images:create') success_url = reverse_lazy('horizon:admin:images:index') page_title = _("Create An Image") class UpdateView(views.UpdateView): template_name = 'admin/images/update.html' form_class = project_forms.AdminUpdateImageForm submit_url = "horizon:admin:images:update" success_url = reverse_lazy('horizon:admin:images:index') page_title = _("Update Image") class DetailView(views.DetailView): def get_context_data(self, **kwargs): context = super(DetailView, self).get_context_data(**kwargs) table = project_tables.AdminImagesTable(self.request) context["url"] = reverse('horizon:admin:images:index') context["actions"] = table.render_row_actions(context["image"]) return context class UpdateMetadataView(forms.ModalFormView): template_name = "admin/images/update_metadata.html" modal_header = _("Update Image") form_id = "update_image_form" form_class = project_forms.UpdateMetadataForm submit_url = "horizon:admin:images:update_metadata" success_url = reverse_lazy('horizon:admin:images:index') page_title = _("Update Image Metadata") def get_initial(self): image = self.get_object() return {'id': self.kwargs["id"], 'metadata': image.properties} def get_context_data(self, **kwargs): context = super(UpdateMetadataView, self).get_context_data(**kwargs) image = self.get_object() reserved_props = getattr(conf.settings, 'IMAGE_RESERVED_CUSTOM_PROPERTIES', []) image.properties = dict((k, v) for (k, v) in image.properties.iteritems() if k not in reserved_props) context['existing_metadata'] = json.dumps(image.properties) args = (self.kwargs['id'],) context['submit_url'] = reverse(self.submit_url, args=args) resource_type = 'OS::Glance::Image' namespaces = [] try: # metadefs_namespace_list() returns a tuple with list as 1st elem available_namespaces = [x.namespace for x in api.glance.metadefs_namespace_list( self.request, filters={"resource_types": [resource_type]} )[0]] for namespace in available_namespaces: details = api.glance.metadefs_namespace_get(self.request, namespace, resource_type) # Filter out reserved custom properties from namespace if reserved_props: if hasattr(details, 'properties'): details.properties = dict( (k, v) for (k, v) in details.properties.iteritems() if k not in reserved_props ) if hasattr(details, 'objects'): for obj in details.objects: obj['properties'] = dict( (k, v) for (k, v) in obj['properties'].iteritems() if k not in reserved_props ) namespaces.append(details) except Exception: msg = _('Unable to retrieve available properties for image.') exceptions.handle(self.request, msg) context['available_metadata'] = json.dumps({'namespaces': namespaces}) context['id'] = self.kwargs['id'] return context @memoized.memoized_method def get_object(self): image_id = self.kwargs['id'] try: return api.glance.image_get(self.request, image_id) except Exception: msg = _('Unable to retrieve the image to be updated.') exceptions.handle(self.request, msg, redirect=reverse('horizon:admin:images:index'))
# -*- coding: utf-8 -*- ############################################################################## # # OpenERP, Open Source Management Solution # Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>). # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## from openerp.osv import fields, osv from openerp.tools.translate import _ class account_move_line_reconcile_select(osv.osv_memory): _name = "account.move.line.reconcile.select" _description = "Move line reconcile select" _columns = { 'account_id': fields.many2one('account.account', 'Account', \ domain = [('reconcile', '=', 1)], required=True), } def action_open_window(self, cr, uid, ids, context=None): """ This function Open account move line window for reconcile on given account id @param cr: the current row, from the database cursor, @param uid: the current users ID for security checks, @param ids: account move line reconcile selects ID or list of IDs @return: dictionary of Open account move line window for reconcile on given account id """ data = self.read(cr, uid, ids, context=context)[0] return { 'domain': "[('account_id','=',%d),('reconcile_id','=',False),('state','<>','draft')]" % data['account_id'], 'name': _('Reconciliation'), 'view_type': 'form', 'view_mode': 'tree,form', 'view_id': False, 'res_model': 'account.move.line', 'type': 'ir.actions.act_window' } # vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
# # Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011 The SCons Foundation # # Permission is hereby granted, free of charge, to any person obtaining # a copy of this software and associated documentation files (the # "Software"), to deal in the Software without restriction, including # without limitation the rights to use, copy, modify, merge, publish, # distribute, sublicense, and/or sell copies of the Software, and to # permit persons to whom the Software is furnished to do so, subject to # the following conditions: # # The above copyright notice and this permission notice shall be included # in all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY # KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE # WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND # NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE # LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION # OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION # WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. # __doc__ = """ SCons compatibility package for old Python versions This subpackage holds modules that provide backwards-compatible implementations of various things that we'd like to use in SCons but which only show up in later versions of Python than the early, old version(s) we still support. Other code will not generally reference things in this package through the SCons.compat namespace. The modules included here add things to the builtins namespace or the global module list so that the rest of our code can use the objects and names imported here regardless of Python version. Simply enough, things that go in the builtins name space come from our _scons_builtins module. The rest of the things here will be in individual compatibility modules that are either: 1) suitably modified copies of the future modules that we want to use; or 2) backwards compatible re-implementations of the specific portions of a future module's API that we want to use. GENERAL WARNINGS: Implementations of functions in the SCons.compat modules are *NOT* guaranteed to be fully compliant with these functions in later versions of Python. We are only concerned with adding functionality that we actually use in SCons, so be wary if you lift this code for other uses. (That said, making these more nearly the same as later, official versions is still a desirable goal, we just don't need to be obsessive about it.) We name the compatibility modules with an initial '_scons_' (for example, _scons_subprocess.py is our compatibility module for subprocess) so that we can still try to import the real module name and fall back to our compatibility module if we get an ImportError. The import_as() function defined below loads the module as the "real" name (without the '_scons'), after which all of the "import {module}" statements in the rest of our code will find our pre-loaded compatibility module. """ __revision__ = "src/engine/SCons/compat/__init__.py 5357 2011/09/09 21:31:03 bdeegan" import os import sys import imp # Use the "imp" module to protect imports from fixers. def import_as(module, name): """ Imports the specified module (from our local directory) as the specified name, returning the loaded module object. """ dir = os.path.split(__file__)[0] return imp.load_module(name, *imp.find_module(module, [dir])) def rename_module(new, old): """ Attempts to import the old module and load it under the new name. Used for purely cosmetic name changes in Python 3.x. """ try: sys.modules[new] = imp.load_module(old, *imp.find_module(old)) return True except ImportError: return False rename_module('builtins', '__builtin__') import _scons_builtins try: import hashlib except ImportError: # Pre-2.5 Python has no hashlib module. try: import_as('_scons_hashlib', 'hashlib') except ImportError: # If we failed importing our compatibility module, it probably # means this version of Python has no md5 module. Don't do # anything and let the higher layer discover this fact, so it # can fall back to using timestamp. pass try: set except NameError: # Pre-2.4 Python has no native set type import_as('_scons_sets', 'sets') import builtins, sets builtins.set = sets.Set try: import collections except ImportError: # Pre-2.4 Python has no collections module. import_as('_scons_collections', 'collections') else: try: collections.UserDict except AttributeError: exec('from UserDict import UserDict as _UserDict') collections.UserDict = _UserDict del _UserDict try: collections.UserList except AttributeError: exec('from UserList import UserList as _UserList') collections.UserList = _UserList del _UserList try: collections.UserString except AttributeError: exec('from UserString import UserString as _UserString') collections.UserString = _UserString del _UserString try: import io except ImportError: # Pre-2.6 Python has no io module. import_as('_scons_io', 'io') try: os.devnull except AttributeError: # Pre-2.4 Python has no os.devnull attribute _names = sys.builtin_module_names if 'posix' in _names: os.devnull = '/dev/null' elif 'nt' in _names: os.devnull = 'nul' os.path.devnull = os.devnull try: os.path.lexists except AttributeError: # Pre-2.4 Python has no os.path.lexists function def lexists(path): return os.path.exists(path) or os.path.islink(path) os.path.lexists = lexists # When we're using the '-3' option during regression tests, importing # cPickle gives a warning no matter how it's done, so always use the # real profile module, whether it's fast or not. if os.environ.get('SCONS_HORRIBLE_REGRESSION_TEST_HACK') is None: # Not a regression test with '-3', so try to use faster version. # In 3.x, 'pickle' automatically loads the fast version if available. rename_module('pickle', 'cPickle') # In 3.x, 'profile' automatically loads the fast version if available. rename_module('profile', 'cProfile') # Before Python 3.0, the 'queue' module was named 'Queue'. rename_module('queue', 'Queue') # Before Python 3.0, the 'winreg' module was named '_winreg' rename_module('winreg', '_winreg') try: import subprocess except ImportError: # Pre-2.4 Python has no subprocess module. import_as('_scons_subprocess', 'subprocess') try: sys.intern except AttributeError: # Pre-2.6 Python has no sys.intern() function. import builtins try: sys.intern = builtins.intern except AttributeError: # Pre-2.x Python has no builtin intern() function. def intern(x): return x sys.intern = intern del intern try: sys.maxsize except AttributeError: # Pre-2.6 Python has no sys.maxsize attribute # Wrapping sys in () is silly, but protects it from 2to3 renames fixer sys.maxsize = (sys).maxint if os.environ.get('SCONS_HORRIBLE_REGRESSION_TEST_HACK') is not None: # We can't apply the 'callable' fixer until the floor is 2.6, but the # '-3' option to Python 2.6 and 2.7 generates almost ten thousand # warnings. This hack allows us to run regression tests with the '-3' # option by replacing the callable() built-in function with a hack # that performs the same function but doesn't generate the warning. # Note that this hack is ONLY intended to be used for regression # testing, and should NEVER be used for real runs. from types import ClassType def callable(obj): if hasattr(obj, '__call__'): return True if isinstance(obj, (ClassType, type)): return True return False import builtins builtins.callable = callable del callable # Local Variables: # tab-width:4 # indent-tabs-mode:nil # End: # vim: set expandtab tabstop=4 shiftwidth=4:
########################################################################## # # Copyright (c) 2007-2011, Image Engine Design Inc. All rights reserved. # # Redistribution and use in source and binary forms, with or without # modification, are permitted provided that the following conditions are # met: # # * Redistributions of source code must retain the above copyright # notice, this list of conditions and the following disclaimer. # # * Redistributions in binary form must reproduce the above copyright # notice, this list of conditions and the following disclaimer in the # documentation and/or other materials provided with the distribution. # # * Neither the name of Image Engine Design nor the names of any # other contributors to this software may be used to endorse or # promote products derived from this software without specific prior # written permission. # # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS # IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, # THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR # PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR # CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, # EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, # PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR # PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF # LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING # NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS # SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. # ########################################################################## import unittest from IECore import * class TestShader( unittest.TestCase ) : def test( self ) : s = Shader() self.assertEqual( s.name, "defaultsurface" ) self.assertEqual( s.type, "surface" ) self.assertEqual( len( s.parameters ), 0 ) self.assertEqual( s.parameters.typeName(), "CompoundData" ) s = Shader( "marble", "surface" ) self.assertEqual( s.name, "marble" ) self.assertEqual( s.type, "surface" ) ss = s.copy() self.assertEqual( ss.name, s.name ) self.assertEqual( ss.type, s.type ) def testConstructWithParameters( self ) : s = Shader( "test", "surface", CompoundData( { "a" : StringData( "a" ) } ) ) self.assertEqual( s.name, "test" ) self.assertEqual( s.type, "surface" ) self.assertEqual( len( s.parameters ), 1 ) self.assertEqual( s.parameters.typeName(), CompoundData.staticTypeName() ) self.assertEqual( s.parameters["a"], StringData( "a" ) ) def testCopy( self ) : s = Shader( "test", "surface", CompoundData( { "a" : StringData( "a" ) } ) ) ss = s.copy() self.assertEqual( s, ss ) def testHash( self ) : s = Shader() h = s.hash() s.name = "somethingElse" self.assertNotEqual( s.hash(), h ) h = s.hash() s.type = "somethingElse" self.assertNotEqual( s.hash(), h ) h = s.hash() s.parameters["a"] = StringData( "a" ) self.assertNotEqual( s.hash(), h ) if __name__ == "__main__": unittest.main()
# Copyright (c) 2013, Web Notes Technologies Pvt. Ltd. and Contributors # License: GNU General Public License v3. See license.txt from __future__ import unicode_literals import frappe from frappe.utils import cint, validate_email_add from frappe import throw, msgprint, _ from frappe.model.document import Document class Warehouse(Document): def autoname(self): suffix = " - " + frappe.db.get_value("Company", self.company, "abbr") if not self.warehouse_name.endswith(suffix): self.name = self.warehouse_name + suffix def validate(self): if self.email_id and not validate_email_add(self.email_id): throw(_("Please enter valid Email Id")) self.update_parent_account() def update_parent_account(self): if not getattr(self, "__islocal", None) \ and (self.create_account_under != frappe.db.get_value("Warehouse", self.name, "create_account_under")): self.validate_parent_account() warehouse_account = frappe.db.get_value("Account", {"account_type": "Warehouse", "company": self.company, "master_name": self.name}, ["name", "parent_account"]) if warehouse_account and warehouse_account[1] != self.create_account_under: acc_doc = frappe.get_doc("Account", warehouse_account[0]) acc_doc.parent_account = self.create_account_under acc_doc.save() def on_update(self): self.create_account_head() def create_account_head(self): if cint(frappe.defaults.get_global_default("auto_accounting_for_stock")): if not frappe.db.get_value("Account", {"account_type": "Warehouse", "master_name": self.name}): if self.get("__islocal") or not frappe.db.get_value( "Stock Ledger Entry", {"warehouse": self.name}): self.validate_parent_account() ac_doc = frappe.get_doc({ "doctype": "Account", 'account_name': self.warehouse_name, 'parent_account': self.create_account_under, 'group_or_ledger':'Ledger', 'company':self.company, "account_type": "Warehouse", "master_name": self.name, "freeze_account": "No" }) ac_doc.ignore_permissions = True ac_doc.insert() msgprint(_("Account head {0} created").format(ac_doc.name)) def validate_parent_account(self): if not self.company: frappe.throw(_("Warehouse {0}: Company is mandatory").format(self.name)) if not self.create_account_under: parent_account = frappe.db.get_value("Account", {"account_name": "Stock Assets", "company": self.company}) if parent_account: self.create_account_under = parent_account else: frappe.throw(_("Please enter parent account group for warehouse account")) elif frappe.db.get_value("Account", self.create_account_under, "company") != self.company: frappe.throw(_("Warehouse {0}: Parent account {1} does not bolong to the company {2}") .format(self.name, self.create_account_under, self.company)) def on_trash(self): # delete bin bins = frappe.db.sql("select * from `tabBin` where warehouse = %s", self.name, as_dict=1) for d in bins: if d['actual_qty'] or d['reserved_qty'] or d['ordered_qty'] or \ d['indented_qty'] or d['projected_qty'] or d['planned_qty']: throw(_("Warehouse {0} can not be deleted as quantity exists for Item {1}").format(self.name, d['item_code'])) else: frappe.db.sql("delete from `tabBin` where name = %s", d['name']) warehouse_account = frappe.db.get_value("Account", {"account_type": "Warehouse", "master_name": self.name}) if warehouse_account: frappe.delete_doc("Account", warehouse_account) if frappe.db.sql("""select name from `tabStock Ledger Entry` where warehouse = %s""", self.name): throw(_("Warehouse can not be deleted as stock ledger entry exists for this warehouse.")) def before_rename(self, olddn, newdn, merge=False): # Add company abbr if not provided from erpnext.setup.doctype.company.company import get_name_with_abbr new_warehouse = get_name_with_abbr(newdn, self.company) if merge: if not frappe.db.exists("Warehouse", new_warehouse): frappe.throw(_("Warehouse {0} does not exist").format(new_warehouse)) if self.company != frappe.db.get_value("Warehouse", new_warehouse, "company"): frappe.throw(_("Both Warehouse must belong to same Company")) frappe.db.sql("delete from `tabBin` where warehouse=%s", olddn) from erpnext.accounts.utils import rename_account_for rename_account_for("Warehouse", olddn, newdn, merge, self.company) return new_warehouse def after_rename(self, olddn, newdn, merge=False): if merge: self.recalculate_bin_qty(newdn) def recalculate_bin_qty(self, newdn): from erpnext.utilities.repost_stock import repost_stock frappe.db.auto_commit_on_many_writes = 1 frappe.db.set_default("allow_negative_stock", 1) for item in frappe.db.sql("""select distinct item_code from ( select name as item_code from `tabItem` where ifnull(is_stock_item, 'Yes')='Yes' union select distinct item_code from tabBin) a"""): repost_stock(item[0], newdn) frappe.db.set_default("allow_negative_stock", frappe.db.get_value("Stock Settings", None, "allow_negative_stock")) frappe.db.auto_commit_on_many_writes = 0
#!/usr/bin/env python # Copyright 2010-2012 RethinkDB, all rights reserved. from vcoptparse import * import vm_build import sys from threading import Thread, Semaphore class Builder(Thread): def __init__(self, name, branch, target, semaphore): Thread.__init__(self) self.name = name self.branch = branch self.target = target self.semaphore = semaphore def run(self): self.success = False try: semaphore.acquire() self.target.run(self.branch, self.name) self.success = True except vm_build.RunError, err: self.exception = err finally: semaphore.release() target_names = ["suse", "redhat5_1", "ubuntu", "debian", "centos5_5", "centos6"] def help(): print >>sys.stderr, "Virtual builder:" print >>sys.stderr, " --help Print this help." print >>sys.stderr, " --target target1 [target2, target3]" print >>sys.stderr, " Build just one target, options are:" print >>sys.stderr, " ", target_names print >>sys.stderr, " defaults to all of them." print >>sys.stderr, " --branch branch_name" print >>sys.stderr, " Build from a branch mutually exclusive with --tag." print >>sys.stderr, " --tag tag-name" print >>sys.stderr, " Build from a tag mutually exclusive with --branch." print >>sys.stderr, " --threads number" print >>sys.stderr, " The number of parallel threads to run." print >>sys.stderr, " --debug" print >>sys.stderr, " Whether to build the packages with debugging enabled." print >>sys.stderr, " --interact" print >>sys.stderr, " This starts a target so that you can interact with it." print >>sys.stderr, " Requires a target." print >>sys.stderr, " --clean-up" print >>sys.stderr, " Shutdown all running vms" print >>sys.stderr, " --username" print >>sys.stderr, " Starts the Virtual Machine using VirtualBox from the specified username." print >>sys.stderr, " --hostname" print >>sys.stderr, " Starts the Virtual Machine using VirtualBox from the specified host machine." o = OptParser() o["help"] = BoolFlag("--help") o["target"] = StringFlag("--target", None) o["branch"] = StringFlag("--branch", None) o["tag"] = StringFlag("--tag", None) o["threads"] = IntFlag("--threads", 3) o["clean-up"] = BoolFlag("--clean-up") o["interact"] = BoolFlag("--interact") o["debug"] = BoolFlag("--debug"); o["username"] = StringFlag("--username", "rethinkdb") # For now, these default values should always be the ones you should use o["hostname"] = StringFlag("--hostname", "deadshot") # because the UUID values below are hard-coded to correspond with rethinkdb@deadshot try: opts = o.parse(sys.argv) except OptError: print >>sys.stderr, "Argument parsing error" help() exit(-1) if opts["help"]: help() sys.exit(0) if opts["branch"] and opts["tag"]: print >>sys.stderr, "Error cannot use --tag and --branch together." help() sys.exit(1) if opts["branch"]: rspec = vm_build.Branch(opts["branch"]) elif opts["tag"]: rspec = vm_build.Tag(opts["tag"]) else: rspec = vm_build.Branch("master") # Prepare the build flags flags = "" # this will be given to the makefile if opts["debug"]: flags += " DEBUG=1 UNIT_TESTS=0" else: flags += " DEBUG=0" suse = vm_build.target('765127b8-2007-43ff-8668-fe4c60176a2b', '192.168.0.173', 'rethinkdb', 'make LEGACY_LINUX=1 LEGACY_GCC=1 NO_EVENTFD=1 rpm-suse10 ' + flags, 'rpm', vm_build.rpm_install, vm_build.rpm_uninstall, vm_build.rpm_get_binary, opts["username"], opts["hostname"]) redhat5_1 = vm_build.target('32340f79-cea9-42ca-94d5-2da13d408d02', '192.168.0.159', 'rethinkdb', 'make rpm LEGACY_GCC=1 LEGACY_LINUX=1 NO_EVENTFD=1' + flags, 'rpm', vm_build.rpm_install, vm_build.rpm_uninstall, vm_build.rpm_get_binary, opts["username"], opts["hostname"]) ubuntu = vm_build.target('1f4521a0-6e74-4d20-b4b9-9ffd8e231423', '192.168.0.172', 'rethinkdb', 'make deb' + flags, 'deb', vm_build.deb_install, vm_build.deb_uninstall, vm_build.deb_get_binary, opts["username"], opts["hostname"]) debian = vm_build.target('cc76e2a5-92c0-4208-be08-5c02429c2c50', '192.168.0.176', 'root', 'make deb NO_EVENTFD=1 LEGACY_LINUX=1 ' + flags, 'deb', vm_build.deb_install, vm_build.deb_uninstall, vm_build.deb_get_binary, opts["username"], opts["hostname"]) centos5_5 = vm_build.target('25710682-666f-4449-bd28-68b25abd8bea', '192.168.0.153', 'root', 'make rpm LEGACY_GCC=1 LEGACY_LINUX=1 ' + flags, 'rpm', vm_build.rpm_install, vm_build.rpm_uninstall, vm_build.rpm_get_binary, opts["username"], opts["hostname"]) centos6 = vm_build.target('d9058650-a45a-44a5-953f-c2402253a614', '192.168.0.178', 'rethinkdb', 'make rpm LEGACY_GCC=1 LEGACY_LINUX=1 ' + flags, 'rpm', vm_build.rpm_install, vm_build.rpm_uninstall, vm_build.rpm_get_binary, opts["username"], opts["hostname"]) targets = {"suse": suse, "redhat5_1": redhat5_1, "ubuntu": ubuntu, "debian": debian, "centos5_5": centos5_5, "centos6": centos6} if (opts["target"]): targets = {opts["target"]: targets[opts["target"]]} if opts["clean-up"]: map(lambda x: x[1].clean_up(), targets.iteritems()) exit(0) if opts["interact"]: if not opts["target"]: print >>sys.stderr, "Error must specify a --target for --interact mode." exit(1) for name, target in targets.iteritems(): target.interact(name) else: success = {} exception = {} semaphore = Semaphore(opts["threads"]) builders = map(lambda x: Builder(x[0], rspec, x[1], semaphore), targets.iteritems()) map(lambda x: x.start(), builders) map(lambda x: x.join(), builders) for b in builders: success[b.name] = b.success if not b.success: exception[b.name] = b.exception print "Build summary:" from termcolor import colored for name, val in success.iteritems(): print name, "." * (20 - len(name)), colored("[Pass]", "green") if val else colored("[Fail]", "red") if (not val): print "Failed on: ", exception[name] raise exception[name] print "Done."
""" mbed CMSIS-DAP debugger Copyright (c) 2006-2013 ARM Limited Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. """ from cortex_m import CortexM, DHCSR, DBGKEY, C_DEBUGEN, C_MASKINTS, C_STEP, DEMCR, VC_CORERESET, NVIC_AIRCR, NVIC_AIRCR_VECTKEY, NVIC_AIRCR_SYSRESETREQ from .memory_map import (FlashRegion, RamRegion, MemoryMap) from pyOCD.target.target import TARGET_RUNNING, TARGET_HALTED import logging DBGMCU_CR = 0xE0042004 #0111 1110 0011 1111 1111 1111 0000 0000 DBGMCU_VAL = 0x7E3FFF00 class STM32F103RC(CortexM): memoryMap = MemoryMap( FlashRegion( start=0x08000000, length=0x80000, blocksize=0x800, isBootMemory=True), RamRegion( start=0x20000000, length=0x10000) ) def __init__(self, transport): super(STM32F103RC, self).__init__(transport, self.memoryMap) def init(self): logging.debug('stm32f103rc init') CortexM.init(self) self.writeMemory(DBGMCU_CR, DBGMCU_VAL);
""" Various tests for synchronization primitives. """ import sys import time from thread import start_new_thread, get_ident import threading import unittest from test import test_support as support def _wait(): # A crude wait/yield function not relying on synchronization primitives. time.sleep(0.01) class Bunch(object): """ A bunch of threads. """ def __init__(self, f, n, wait_before_exit=False): """ Construct a bunch of `n` threads running the same function `f`. If `wait_before_exit` is True, the threads won't terminate until do_finish() is called. """ self.f = f self.n = n self.started = [] self.finished = [] self._can_exit = not wait_before_exit def task(): tid = get_ident() self.started.append(tid) try: f() finally: self.finished.append(tid) while not self._can_exit: _wait() for i in range(n): start_new_thread(task, ()) def wait_for_started(self): while len(self.started) < self.n: _wait() def wait_for_finished(self): while len(self.finished) < self.n: _wait() def do_finish(self): self._can_exit = True class BaseTestCase(unittest.TestCase): def setUp(self): self._threads = support.threading_setup() def tearDown(self): support.threading_cleanup(*self._threads) support.reap_children() class BaseLockTests(BaseTestCase): """ Tests for both recursive and non-recursive locks. """ def test_constructor(self): lock = self.locktype() del lock def test_acquire_destroy(self): lock = self.locktype() lock.acquire() del lock def test_acquire_release(self): lock = self.locktype() lock.acquire() lock.release() del lock def test_try_acquire(self): lock = self.locktype() self.assertTrue(lock.acquire(False)) lock.release() def test_try_acquire_contended(self): lock = self.locktype() lock.acquire() result = [] def f(): result.append(lock.acquire(False)) Bunch(f, 1).wait_for_finished() self.assertFalse(result[0]) lock.release() def test_acquire_contended(self): lock = self.locktype() lock.acquire() N = 5 def f(): lock.acquire() lock.release() b = Bunch(f, N) b.wait_for_started() _wait() self.assertEqual(len(b.finished), 0) lock.release() b.wait_for_finished() self.assertEqual(len(b.finished), N) def test_with(self): lock = self.locktype() def f(): lock.acquire() lock.release() def _with(err=None): with lock: if err is not None: raise err _with() # Check the lock is unacquired Bunch(f, 1).wait_for_finished() self.assertRaises(TypeError, _with, TypeError) # Check the lock is unacquired Bunch(f, 1).wait_for_finished() def test_thread_leak(self): # The lock shouldn't leak a Thread instance when used from a foreign # (non-threading) thread. lock = self.locktype() def f(): lock.acquire() lock.release() n = len(threading.enumerate()) # We run many threads in the hope that existing threads ids won't # be recycled. Bunch(f, 15).wait_for_finished() self.assertEqual(n, len(threading.enumerate())) class LockTests(BaseLockTests): """ Tests for non-recursive, weak locks (which can be acquired and released from different threads). """ def test_reacquire(self): # Lock needs to be released before re-acquiring. lock = self.locktype() phase = [] def f(): lock.acquire() phase.append(None) lock.acquire() phase.append(None) start_new_thread(f, ()) while len(phase) == 0: _wait() _wait() self.assertEqual(len(phase), 1) lock.release() while len(phase) == 1: _wait() self.assertEqual(len(phase), 2) def test_different_thread(self): # Lock can be released from a different thread. lock = self.locktype() lock.acquire() def f(): lock.release() b = Bunch(f, 1) b.wait_for_finished() lock.acquire() lock.release() class RLockTests(BaseLockTests): """ Tests for recursive locks. """ def test_reacquire(self): lock = self.locktype() lock.acquire() lock.acquire() lock.release() lock.acquire() lock.release() lock.release() def test_release_unacquired(self): # Cannot release an unacquired lock lock = self.locktype() self.assertRaises(RuntimeError, lock.release) lock.acquire() lock.acquire() lock.release() lock.acquire() lock.release() lock.release() self.assertRaises(RuntimeError, lock.release) def test_different_thread(self): # Cannot release from a different thread lock = self.locktype() def f(): lock.acquire() b = Bunch(f, 1, True) try: self.assertRaises(RuntimeError, lock.release) finally: b.do_finish() def test__is_owned(self): lock = self.locktype() self.assertFalse(lock._is_owned()) lock.acquire() self.assertTrue(lock._is_owned()) lock.acquire() self.assertTrue(lock._is_owned()) result = [] def f(): result.append(lock._is_owned()) Bunch(f, 1).wait_for_finished() self.assertFalse(result[0]) lock.release() self.assertTrue(lock._is_owned()) lock.release() self.assertFalse(lock._is_owned()) class EventTests(BaseTestCase): """ Tests for Event objects. """ def test_is_set(self): evt = self.eventtype() self.assertFalse(evt.is_set()) evt.set() self.assertTrue(evt.is_set()) evt.set() self.assertTrue(evt.is_set()) evt.clear() self.assertFalse(evt.is_set()) evt.clear() self.assertFalse(evt.is_set()) def _check_notify(self, evt): # All threads get notified N = 5 results1 = [] results2 = [] def f(): results1.append(evt.wait()) results2.append(evt.wait()) b = Bunch(f, N) b.wait_for_started() _wait() self.assertEqual(len(results1), 0) evt.set() b.wait_for_finished() self.assertEqual(results1, [True] * N) self.assertEqual(results2, [True] * N) def test_notify(self): evt = self.eventtype() self._check_notify(evt) # Another time, after an explicit clear() evt.set() evt.clear() self._check_notify(evt) def test_timeout(self): evt = self.eventtype() results1 = [] results2 = [] N = 5 def f(): results1.append(evt.wait(0.0)) t1 = time.time() r = evt.wait(0.2) t2 = time.time() results2.append((r, t2 - t1)) Bunch(f, N).wait_for_finished() self.assertEqual(results1, [False] * N) for r, dt in results2: self.assertFalse(r) self.assertTrue(dt >= 0.2, dt) # The event is set results1 = [] results2 = [] evt.set() Bunch(f, N).wait_for_finished() self.assertEqual(results1, [True] * N) for r, dt in results2: self.assertTrue(r) class ConditionTests(BaseTestCase): """ Tests for condition variables. """ def test_acquire(self): cond = self.condtype() # Be default we have an RLock: the condition can be acquired multiple # times. cond.acquire() cond.acquire() cond.release() cond.release() lock = threading.Lock() cond = self.condtype(lock) cond.acquire() self.assertFalse(lock.acquire(False)) cond.release() self.assertTrue(lock.acquire(False)) self.assertFalse(cond.acquire(False)) lock.release() with cond: self.assertFalse(lock.acquire(False)) def test_unacquired_wait(self): cond = self.condtype() self.assertRaises(RuntimeError, cond.wait) def test_unacquired_notify(self): cond = self.condtype() self.assertRaises(RuntimeError, cond.notify) def _check_notify(self, cond): N = 5 results1 = [] results2 = [] phase_num = 0 def f(): cond.acquire() cond.wait() cond.release() results1.append(phase_num) cond.acquire() cond.wait() cond.release() results2.append(phase_num) b = Bunch(f, N) b.wait_for_started() _wait() self.assertEqual(results1, []) # Notify 3 threads at first cond.acquire() cond.notify(3) _wait() phase_num = 1 cond.release() while len(results1) < 3: _wait() self.assertEqual(results1, [1] * 3) self.assertEqual(results2, []) # Notify 5 threads: they might be in their first or second wait cond.acquire() cond.notify(5) _wait() phase_num = 2 cond.release() while len(results1) + len(results2) < 8: _wait() self.assertEqual(results1, [1] * 3 + [2] * 2) self.assertEqual(results2, [2] * 3) # Notify all threads: they are all in their second wait cond.acquire() cond.notify_all() _wait() phase_num = 3 cond.release() while len(results2) < 5: _wait() self.assertEqual(results1, [1] * 3 + [2] * 2) self.assertEqual(results2, [2] * 3 + [3] * 2) b.wait_for_finished() def test_notify(self): cond = self.condtype() self._check_notify(cond) # A second time, to check internal state is still ok. self._check_notify(cond) def test_timeout(self): cond = self.condtype() results = [] N = 5 def f(): cond.acquire() t1 = time.time() cond.wait(0.2) t2 = time.time() cond.release() results.append(t2 - t1) Bunch(f, N).wait_for_finished() self.assertEqual(len(results), 5) for dt in results: self.assertTrue(dt >= 0.2, dt) class BaseSemaphoreTests(BaseTestCase): """ Common tests for {bounded, unbounded} semaphore objects. """ def test_constructor(self): self.assertRaises(ValueError, self.semtype, value = -1) self.assertRaises(ValueError, self.semtype, value = -sys.maxint) def test_acquire(self): sem = self.semtype(1) sem.acquire() sem.release() sem = self.semtype(2) sem.acquire() sem.acquire() sem.release() sem.release() def test_acquire_destroy(self): sem = self.semtype() sem.acquire() del sem def test_acquire_contended(self): sem = self.semtype(7) sem.acquire() N = 10 results1 = [] results2 = [] phase_num = 0 def f(): sem.acquire() results1.append(phase_num) sem.acquire() results2.append(phase_num) b = Bunch(f, 10) b.wait_for_started() while len(results1) + len(results2) < 6: _wait() self.assertEqual(results1 + results2, [0] * 6) phase_num = 1 for i in range(7): sem.release() while len(results1) + len(results2) < 13: _wait() self.assertEqual(sorted(results1 + results2), [0] * 6 + [1] * 7) phase_num = 2 for i in range(6): sem.release() while len(results1) + len(results2) < 19: _wait() self.assertEqual(sorted(results1 + results2), [0] * 6 + [1] * 7 + [2] * 6) # The semaphore is still locked self.assertFalse(sem.acquire(False)) # Final release, to let the last thread finish sem.release() b.wait_for_finished() def test_try_acquire(self): sem = self.semtype(2) self.assertTrue(sem.acquire(False)) self.assertTrue(sem.acquire(False)) self.assertFalse(sem.acquire(False)) sem.release() self.assertTrue(sem.acquire(False)) def test_try_acquire_contended(self): sem = self.semtype(4) sem.acquire() results = [] def f(): results.append(sem.acquire(False)) results.append(sem.acquire(False)) Bunch(f, 5).wait_for_finished() # There can be a thread switch between acquiring the semaphore and # appending the result, therefore results will not necessarily be # ordered. self.assertEqual(sorted(results), [False] * 7 + [True] * 3 ) def test_default_value(self): # The default initial value is 1. sem = self.semtype() sem.acquire() def f(): sem.acquire() sem.release() b = Bunch(f, 1) b.wait_for_started() _wait() self.assertFalse(b.finished) sem.release() b.wait_for_finished() def test_with(self): sem = self.semtype(2) def _with(err=None): with sem: self.assertTrue(sem.acquire(False)) sem.release() with sem: self.assertFalse(sem.acquire(False)) if err: raise err _with() self.assertTrue(sem.acquire(False)) sem.release() self.assertRaises(TypeError, _with, TypeError) self.assertTrue(sem.acquire(False)) sem.release() class SemaphoreTests(BaseSemaphoreTests): """ Tests for unbounded semaphores. """ def test_release_unacquired(self): # Unbounded releases are allowed and increment the semaphore's value sem = self.semtype(1) sem.release() sem.acquire() sem.acquire() sem.release() class BoundedSemaphoreTests(BaseSemaphoreTests): """ Tests for bounded semaphores. """ def test_release_unacquired(self): # Cannot go past the initial value sem = self.semtype() self.assertRaises(ValueError, sem.release) sem.acquire() sem.release() self.assertRaises(ValueError, sem.release)
from __future__ import unicode_literals from datetime import datetime, timedelta from django.template.defaultfilters import timesince_filter from django.test import SimpleTestCase from django.test.utils import requires_tz_support from ..utils import setup from .timezone_utils import TimezoneTestCase class TimesinceTests(TimezoneTestCase): """ #20246 - \xa0 in output avoids line-breaks between value and unit """ # Default compare with datetime.now() @setup({'timesince01': '{{ a|timesince }}'}) def test_timesince01(self): output = self.engine.render_to_string('timesince01', {'a': datetime.now() + timedelta(minutes=-1, seconds=-10)}) self.assertEqual(output, '1\xa0minute') @setup({'timesince02': '{{ a|timesince }}'}) def test_timesince02(self): output = self.engine.render_to_string('timesince02', {'a': datetime.now() - timedelta(days=1, minutes=1)}) self.assertEqual(output, '1\xa0day') @setup({'timesince03': '{{ a|timesince }}'}) def test_timesince03(self): output = self.engine.render_to_string('timesince03', {'a': datetime.now() - timedelta(hours=1, minutes=25, seconds=10)}) self.assertEqual(output, '1\xa0hour, 25\xa0minutes') # Compare to a given parameter @setup({'timesince04': '{{ a|timesince:b }}'}) def test_timesince04(self): output = self.engine.render_to_string( 'timesince04', {'a': self.now - timedelta(days=2), 'b': self.now - timedelta(days=1)}, ) self.assertEqual(output, '1\xa0day') @setup({'timesince05': '{{ a|timesince:b }}'}) def test_timesince05(self): output = self.engine.render_to_string( 'timesince05', {'a': self.now - timedelta(days=2, minutes=1), 'b': self.now - timedelta(days=2)}, ) self.assertEqual(output, '1\xa0minute') # Check that timezone is respected @setup({'timesince06': '{{ a|timesince:b }}'}) def test_timesince06(self): output = self.engine.render_to_string('timesince06', {'a': self.now_tz - timedelta(hours=8), 'b': self.now_tz}) self.assertEqual(output, '8\xa0hours') # Tests for #7443 @setup({'timesince07': '{{ earlier|timesince }}'}) def test_timesince07(self): output = self.engine.render_to_string('timesince07', {'earlier': self.now - timedelta(days=7)}) self.assertEqual(output, '1\xa0week') @setup({'timesince08': '{{ earlier|timesince:now }}'}) def test_timesince08(self): output = self.engine.render_to_string('timesince08', {'now': self.now, 'earlier': self.now - timedelta(days=7)}) self.assertEqual(output, '1\xa0week') @setup({'timesince09': '{{ later|timesince }}'}) def test_timesince09(self): output = self.engine.render_to_string('timesince09', {'later': self.now + timedelta(days=7)}) self.assertEqual(output, '0\xa0minutes') @setup({'timesince10': '{{ later|timesince:now }}'}) def test_timesince10(self): output = self.engine.render_to_string('timesince10', {'now': self.now, 'later': self.now + timedelta(days=7)}) self.assertEqual(output, '0\xa0minutes') # Ensures that differing timezones are calculated correctly. @setup({'timesince11': '{{ a|timesince }}'}) def test_timesince11(self): output = self.engine.render_to_string('timesince11', {'a': self.now}) self.assertEqual(output, '0\xa0minutes') @requires_tz_support @setup({'timesince12': '{{ a|timesince }}'}) def test_timesince12(self): output = self.engine.render_to_string('timesince12', {'a': self.now_tz}) self.assertEqual(output, '0\xa0minutes') @requires_tz_support @setup({'timesince13': '{{ a|timesince }}'}) def test_timesince13(self): output = self.engine.render_to_string('timesince13', {'a': self.now_tz_i}) self.assertEqual(output, '0\xa0minutes') @setup({'timesince14': '{{ a|timesince:b }}'}) def test_timesince14(self): output = self.engine.render_to_string('timesince14', {'a': self.now_tz, 'b': self.now_tz_i}) self.assertEqual(output, '0\xa0minutes') @setup({'timesince15': '{{ a|timesince:b }}'}) def test_timesince15(self): output = self.engine.render_to_string('timesince15', {'a': self.now, 'b': self.now_tz_i}) self.assertEqual(output, '') @setup({'timesince16': '{{ a|timesince:b }}'}) def test_timesince16(self): output = self.engine.render_to_string('timesince16', {'a': self.now_tz_i, 'b': self.now}) self.assertEqual(output, '') # Tests for #9065 (two date objects). @setup({'timesince17': '{{ a|timesince:b }}'}) def test_timesince17(self): output = self.engine.render_to_string('timesince17', {'a': self.today, 'b': self.today}) self.assertEqual(output, '0\xa0minutes') @setup({'timesince18': '{{ a|timesince:b }}'}) def test_timesince18(self): output = self.engine.render_to_string('timesince18', {'a': self.today, 'b': self.today + timedelta(hours=24)}) self.assertEqual(output, '1\xa0day') class FunctionTests(SimpleTestCase): def test_since_now(self): self.assertEqual(timesince_filter(datetime.now() - timedelta(1)), '1\xa0day') def test_explicit_date(self): self.assertEqual(timesince_filter(datetime(2005, 12, 29), datetime(2005, 12, 30)), '1\xa0day')
""" ============================================================================ Decoding in time-frequency space data using the Common Spatial Pattern (CSP) ============================================================================ The time-frequency decomposition is estimated by iterating over raw data that has been band-passed at different frequencies. This is used to compute a covariance matrix over each epoch or a rolling time-window and extract the CSP filtered signals. A linear discriminant classifier is then applied to these signals. """ # Authors: Laura Gwilliams <laura.gwilliams@nyu.edu> # Jean-Remi King <jeanremi.king@gmail.com> # Alex Barachant <alexandre.barachant@gmail.com> # Alexandre Gramfort <alexandre.gramfort@inria.fr> # # License: BSD (3-clause) import numpy as np import matplotlib.pyplot as plt from mne import Epochs, create_info, events_from_annotations from mne.io import concatenate_raws, read_raw_edf from mne.datasets import eegbci from mne.decoding import CSP from mne.time_frequency import AverageTFR from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.model_selection import StratifiedKFold, cross_val_score from sklearn.pipeline import make_pipeline from sklearn.preprocessing import LabelEncoder ############################################################################### # Set parameters and read data event_id = dict(hands=2, feet=3) # motor imagery: hands vs feet subject = 1 runs = [6, 10, 14] raw_fnames = eegbci.load_data(subject, runs) raw = concatenate_raws([read_raw_edf(f, preload=True) for f in raw_fnames]) # Extract information from the raw file sfreq = raw.info['sfreq'] events, _ = events_from_annotations(raw, event_id=dict(T1=2, T2=3)) raw.pick_types(meg=False, eeg=True, stim=False, eog=False, exclude='bads') # Assemble the classifier using scikit-learn pipeline clf = make_pipeline(CSP(n_components=4, reg=None, log=True, norm_trace=False), LinearDiscriminantAnalysis()) n_splits = 5 # how many folds to use for cross-validation cv = StratifiedKFold(n_splits=n_splits, shuffle=True) # Classification & Time-frequency parameters tmin, tmax = -.200, 2.000 n_cycles = 10. # how many complete cycles: used to define window size min_freq = 5. max_freq = 25. n_freqs = 8 # how many frequency bins to use # Assemble list of frequency range tuples freqs = np.linspace(min_freq, max_freq, n_freqs) # assemble frequencies freq_ranges = list(zip(freqs[:-1], freqs[1:])) # make freqs list of tuples # Infer window spacing from the max freq and number of cycles to avoid gaps window_spacing = (n_cycles / np.max(freqs) / 2.) centered_w_times = np.arange(tmin, tmax, window_spacing)[1:] n_windows = len(centered_w_times) # Instantiate label encoder le = LabelEncoder() ############################################################################### # Loop through frequencies, apply classifier and save scores # init scores freq_scores = np.zeros((n_freqs - 1,)) # Loop through each frequency range of interest for freq, (fmin, fmax) in enumerate(freq_ranges): # Infer window size based on the frequency being used w_size = n_cycles / ((fmax + fmin) / 2.) # in seconds # Apply band-pass filter to isolate the specified frequencies raw_filter = raw.copy().filter(fmin, fmax, n_jobs=1, fir_design='firwin', skip_by_annotation='edge') # Extract epochs from filtered data, padded by window size epochs = Epochs(raw_filter, events, event_id, tmin - w_size, tmax + w_size, proj=False, baseline=None, preload=True) epochs.drop_bad() y = le.fit_transform(epochs.events[:, 2]) X = epochs.get_data() # Save mean scores over folds for each frequency and time window freq_scores[freq] = np.mean(cross_val_score(estimator=clf, X=X, y=y, scoring='roc_auc', cv=cv, n_jobs=1), axis=0) ############################################################################### # Plot frequency results plt.bar(freqs[:-1], freq_scores, width=np.diff(freqs)[0], align='edge', edgecolor='black') plt.xticks(freqs) plt.ylim([0, 1]) plt.axhline(len(epochs['feet']) / len(epochs), color='k', linestyle='--', label='chance level') plt.legend() plt.xlabel('Frequency (Hz)') plt.ylabel('Decoding Scores') plt.title('Frequency Decoding Scores') ############################################################################### # Loop through frequencies and time, apply classifier and save scores # init scores tf_scores = np.zeros((n_freqs - 1, n_windows)) # Loop through each frequency range of interest for freq, (fmin, fmax) in enumerate(freq_ranges): # Infer window size based on the frequency being used w_size = n_cycles / ((fmax + fmin) / 2.) # in seconds # Apply band-pass filter to isolate the specified frequencies raw_filter = raw.copy().filter(fmin, fmax, n_jobs=1, fir_design='firwin', skip_by_annotation='edge') # Extract epochs from filtered data, padded by window size epochs = Epochs(raw_filter, events, event_id, tmin - w_size, tmax + w_size, proj=False, baseline=None, preload=True) epochs.drop_bad() y = le.fit_transform(epochs.events[:, 2]) # Roll covariance, csp and lda over time for t, w_time in enumerate(centered_w_times): # Center the min and max of the window w_tmin = w_time - w_size / 2. w_tmax = w_time + w_size / 2. # Crop data into time-window of interest X = epochs.copy().crop(w_tmin, w_tmax).get_data() # Save mean scores over folds for each frequency and time window tf_scores[freq, t] = np.mean(cross_val_score(estimator=clf, X=X, y=y, scoring='roc_auc', cv=cv, n_jobs=1), axis=0) ############################################################################### # Plot time-frequency results # Set up time frequency object av_tfr = AverageTFR(create_info(['freq'], sfreq), tf_scores[np.newaxis, :], centered_w_times, freqs[1:], 1) chance = np.mean(y) # set chance level to white in the plot av_tfr.plot([0], vmin=chance, title="Time-Frequency Decoding Scores", cmap=plt.cm.Reds)
from allauth.socialaccount.providers.base import ProviderAccount from allauth.socialaccount.providers.oauth2.provider import OAuth2Provider class Scope(object): USERINFO_PROFILE = "/authenticate" class OrcidAccount(ProviderAccount): def get_profile_url(self): return extract_from_dict(self.account.extra_data, ["orcid-identifier", "uri"]) def to_str(self): return self.account.uid class OrcidProvider(OAuth2Provider): id = "orcid" name = "Orcid.org" account_class = OrcidAccount def get_default_scope(self): return [Scope.USERINFO_PROFILE] def extract_uid(self, data): return extract_from_dict(data, ["orcid-identifier", "path"]) def extract_common_fields(self, data): common_fields = dict( email=extract_from_dict(data, ["person", "emails", "email", 0, "email"]), last_name=extract_from_dict( data, ["person", "name", "family-name", "value"] ), first_name=extract_from_dict( data, ["person", "name", "given-names", "value"] ), ) return dict((key, value) for (key, value) in common_fields.items() if value) provider_classes = [OrcidProvider] def extract_from_dict(data, path): """ Navigate `data`, a multidimensional array (list or dictionary), and returns the object at `path`. """ value = data try: for key in path: value = value[key] return value except (KeyError, IndexError, TypeError): return ""
import json import re import yaml import base64 import velruse import datetime from pygithub3 import Github from pyramid.httpexceptions import HTTPFound from pyramid.view import view_config from verse.utils import set_content #from pyramid.i18n import TranslationString as _ from .models import ( DBSession, User, ) flat_url = velruse.utils.flat_url SALT = 'supersecretsalt' def tree(items, root=None): if root: items = filter(lambda x: x['path'].startswith(root), items) trees = filter(lambda x: x['type'] == 'tree', items) blobs = filter(lambda x: x['type'] == 'blob', items) return trees, blobs fm_pattern = re.compile(r'^---\n([\s\S]*?)---\n([\s\S]*)') def fm(content): matches = fm_pattern.match(content) if matches: return matches.group(1), matches.group(2) return None, None _repo_name = 'testbest' def _get_credentials(request): return request.session.get('token'), request.session.get('github_user'), request.session.get('github_repo') def content_to_json(content_obj): content = content_obj.get_content() front_matter, post = fm(content or '') if front_matter: meta = yaml.safe_load(front_matter) else: meta = {} d = { 'content': post, 'front_matter': front_matter, 'raw': content, 'name': content_obj.name, 'path': content_obj.path, 'sha': content_obj.sha, 'url': content_obj.url, } for key, value in meta.items(): d['meta_%s' % key] = value if 'meta_published' not in d: d['meta_published'] = '' return d slug_re = re.compile(r"[^A-Za-z0-9.]+") def slugify(value): return slug_re.sub("-", value).lower() def normalize_front_matter(raw_front_matter): pass def dashboard(request): return {} class PostsView(object): def __init__(self, request): self.request = request @view_config(route_name='post.list', renderer='json', accept='application/json',) @view_config(route_name='post.list', renderer='posts.html', accept='text/html') def list(self): access_token, user, repo = _get_credentials(self.request) github = Github(token=access_token, user=user, repo=repo) items = github.repos.contents.get('_posts/') full_items = [] for item in items: if item.name != 'README': full_items.append(github.repos.contents.get(item.path)) return {'items': map(content_to_json, full_items)} @view_config(route_name='post.item', renderer='json', request_method="GET", accept='application/json',) def item(self): access_token, user, repo = _get_credentials(self.request) github = Github(token=access_token, user=user, repo=repo) name = self.request.matchdict['id'] f = github.repos.contents.get('_posts/%s' % name) post = content_to_json(f) return post @view_config(route_name='post.new', renderer='post_new.html') @view_config(route_name='post.new', renderer='json', accept='application/json') def new(self): access_token, user, repo = _get_credentials(self.request) github = Github(token=access_token, user=user, repo=repo) date = datetime.datetime.now().date() post = self.request.json_body front_matter = { 'layout': 'post', 'published': False, 'comments': 'true', 'date': date.strftime('%Y-%m-%d'), } front_matter['title'] = post['meta_title'] front_matter_raw = yaml.safe_dump(front_matter, default_flow_style=False) name = slugify(post['meta_title']) raw = '---\n%s---\n%s' % (front_matter_raw, post['content']) date_string = date.strftime('%Y-%m-%d-{}.md') name = date_string.format(name) path = '_posts/{}'.format(name) commit_data = { 'path': path, 'message': 'creating new post', 'content': base64.b64encode(raw) } resp = github.repos.contents.create(path, commit_data) sha = resp.content['sha'] d = { 'front_matter': front_matter_raw, 'content': post['content'], 'raw': raw, 'name': date_string.format(name), 'path': path, 'sha': sha, } for key, value in front_matter.items(): d['meta_%s' % key] = value return d @view_config(route_name='post.item', request_method='PUT', renderer='json') def update(self): """ if sha is in the model, then update else create """ access_token, user, repo = _get_credentials(self.request) github = Github(token=access_token, user=user, repo=repo) post = self.request.json_body #saving meta information fm = yaml.safe_load(post['front_matter']) fm['title'] = post['meta_title'] fm['date'] = post['meta_date'] fm['published'] = post['meta_published'] fm_raw = yaml.safe_dump(fm, default_flow_style=False) path = post['path'] raw = '---\n%s---\n%s' % (fm_raw, post['content']) commit_data = { 'path': path, 'message': 'updating post', 'content': base64.b64encode(raw), 'sha': post['sha'] } resp = github.repos.contents.update(path, commit_data) post['sha'] = resp.content['sha'] return post @view_config(route_name='post.item', request_method='DELETE', renderer='json') def delete(self): return {} from cornice.resource import resource, view @resource(collection_path='/pages', path='/pages/{id}') class PageView(object): def __init__(self, request): self.request = request access_token, user, repo = _get_credentials(self.request) self.github = Github(token=access_token, user=user, repo=repo) def _filter_pages(self, x): starts = ('_layouts', '_posts', '_drafts') ends = ('html', 'htm') #type is blob #endswith htm or html return x['type'] == 'blob' and x['path'].endswith(ends) and not x['path'].startswith(starts) def _map_pages(self, x): y = x.copy() y['id'] = base64.b64encode(y['path']) return y def collection_get(self): #access_token, user, repo = _get_credentials(self.request) #github = Github(token=access_token, user=user, repo=repo) tree = self.github.git_data.trees.get(sha='master', recursive=1) items = map(self._map_pages, filter(self._filter_pages, tree.tree)) return {'items': items} def collection_post(self): #this creates a new page content = set_content(self.github, self.request.POST['']) return {} def get(self): id = self.request.matchdict['id'] id = base64.b64decode(id) access_token, user, repo = _get_credentials(self.request) github = Github(token=access_token, user=user, repo=repo) content = github.repos.contents.get(id) return content._attrs def put(self): #updates a page print 'PUT' id = int(self.request.matchdict['id']) return {} def delete(self): #delete a page print 'DELETE' id = int(self.request.matchdict['id']) return {}
from javascript import JSObject from browser import window import urllib.request class TempMod: def __init__(self, name): self.name=name #define my custom import hook (just to see if it get called etc). class BaseHook: def __init__(self, fullname=None, path=None): self._fullname=fullname self._path=path # we don't are about this... self._modpath='' self._module='' def find_module(self, name=None, path=None): if name is None: name=self._fullname for _i in ('libs/%s.js' % name, 'Lib/%s.py' % name, 'Lib/%s/__init__.py' % name): _path="%s%s" % (__BRYTHON__.brython_path, _i) try: _fp,_,_headers=urllib.request.urlopen(_path) if _headers['status'] != 200: continue self._module=_fp.read() self._modpath=_path return self except urllib.error.HTTPError as e: print(str(e)) self._modpath='' self._module='' raise ImportError def is_package(self): return '.' in self._fullname def load_module(self, name): if name is None: name=self._fullname window.eval('__BRYTHON__.imported["%s"]={}' % name) return JSObject(__BRYTHON__.run_py)(TempMod(name), self._modpath, self._module)
# Copyright 2017 Google Inc. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== """An optimizer that switches between several methods.""" import tensorflow as tf from tensorflow.python.training import optimizer class CompositeOptimizer(optimizer.Optimizer): """Optimizer that switches between several methods. """ def __init__(self, optimizer1, optimizer2, switch, use_locking=False, name='Composite'): """Construct a new Composite optimizer. Args: optimizer1: A tf.python.training.optimizer.Optimizer object. optimizer2: A tf.python.training.optimizer.Optimizer object. switch: A tf.bool Tensor, selecting whether to use the first or the second optimizer. use_locking: Bool. If True apply use locks to prevent concurrent updates to variables. name: Optional name prefix for the operations created when applying gradients. Defaults to "Composite". """ super(CompositeOptimizer, self).__init__(use_locking, name) self._optimizer1 = optimizer1 self._optimizer2 = optimizer2 self._switch = switch def apply_gradients(self, grads_and_vars, global_step=None, name=None): return tf.cond( self._switch, lambda: self._optimizer1.apply_gradients(grads_and_vars, global_step, name), lambda: self._optimizer2.apply_gradients(grads_and_vars, global_step, name) ) def get_slot(self, var, name): slot1 = self._optimizer1.get_slot(var, name) slot2 = self._optimizer2.get_slot(var, name) if slot1 and slot2: raise LookupError('Slot named %s for variable %s populated for both ' 'optimizers' % (name, var.name)) return slot1 or slot2 def get_slot_names(self): return sorted(self._optimizer1.get_slot_names() + self._optimizer2.get_slot_names())
# -*- coding: utf-8 -*- ############################################################################## # # OpenERP, Open Source Management Solution # Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>). # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## { 'name': 'Invoice on Timesheets', 'version': '1.0', 'category': 'Sales Management', 'description': """ Generate your Invoices from Expenses, Timesheet Entries. ======================================================== Module to generate invoices based on costs (human resources, expenses, ...). You can define price lists in analytic account, make some theoretical revenue reports.""", 'author': 'OpenERP SA', 'website': 'https://www.odoo.com/page/employees', 'depends': ['account', 'hr_timesheet', 'report'], 'data': [ 'security/ir.model.access.csv', 'hr_timesheet_invoice_data.xml', 'hr_timesheet_invoice_view.xml', 'hr_timesheet_invoice_wizard.xml', 'hr_timesheet_invoice_report.xml', 'report/report_analytic_view.xml', 'report/hr_timesheet_invoice_report_view.xml', 'wizard/hr_timesheet_analytic_profit_view.xml', 'wizard/hr_timesheet_invoice_create_view.xml', 'wizard/hr_timesheet_invoice_create_final_view.xml', 'views/report_analyticprofit.xml', ], 'demo': ['hr_timesheet_invoice_demo.xml'], 'test': ['test/test_hr_timesheet_invoice.yml', 'test/test_hr_timesheet_invoice_no_prod_tax.yml', 'test/hr_timesheet_invoice_report.yml', ], 'installable': True, 'auto_install': False, } # vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
# # Newfies-Dialer License # http://www.newfies-dialer.org # # This Source Code Form is subject to the terms of the Mozilla Public # License, v. 2.0. If a copy of the MPL was not distributed with this file, # You can obtain one at http://mozilla.org/MPL/2.0/. # # Copyright (C) 2011-2014 Star2Billing S.L. # # The Initial Developer of the Original Code is # Arezqui Belaid <info@star2billing.com> # from django.conf.urls import patterns urlpatterns = patterns('survey.views', # Survey urls (r'^module/survey/$', 'survey_list'), (r'^module/survey/add/$', 'survey_add'), (r'^module/sealed_survey_view/(.+)/$', 'sealed_survey_view'), (r'^module/survey/del/(.+)/$', 'survey_del'), (r'^module/survey/(.+)/$', 'survey_change'), (r'^module/export_survey/(.+)/$', 'export_survey'), (r'^module/import_survey/$', 'import_survey'), (r'^module/sealed_survey/$', 'sealed_survey_list'), (r'^module/seal_survey/(.+)/$', 'seal_survey'), # Section urls (r'^section/add/$', 'section_add'), (r'^section/branch/add/$', 'section_branch_add'), (r'^section/delete/(?P<id>\w+)/$', 'section_delete'), (r'^section/(?P<id>\w+)/$', 'section_change'), (r'^section/script/(?P<id>\w+)/$', 'section_script_change'), (r'^section/script_play/(?P<id>\w+)/$', 'section_script_play'), (r'^section/branch/(?P<id>\w+)/$', 'section_branch_change'), # Survey Report urls (r'^survey_report/$', 'survey_report'), (r'^export_surveycall_report/$', 'export_surveycall_report'), (r'^survey_campaign_result/(?P<id>\w+)/$', 'survey_campaign_result'), )
"""Definition of WebDriverException classes.""" def create_webdriver_exception_strict(status_code, message): """Create the appropriate WebDriverException given the status_code.""" if status_code in _exceptions_strict: return _exceptions_strict[status_code](message) return UnknownStatusCodeException("[%s] %s" % (status_code, message)) def create_webdriver_exception_compatibility(status_code, message): """Create the appropriate WebDriverException given the status_code.""" if status_code in _exceptions_compatibility: return _exceptions_compatibility[status_code](message) return UnknownStatusCodeException("[%s] %s" % (status_code, message)) class WebDriverException(Exception): """Base class for all WebDriverExceptions.""" class UnableToSetCookieException(WebDriverException): """A request to set a cookie's value could not be satisfied.""" class InvalidElementStateException(WebDriverException): """An element command could not be completed because the element is in an invalid state (e.g. attempting to click an element that is no longer attached to the DOM). """ class NoSuchElementException(WebDriverException): """An element could not be located on the page using the given search parameters. """ class TimeoutException(WebDriverException): """An operation did not complete before its timeout expired.""" class ElementNotSelectableException(InvalidElementStateException): """An attempt was made to select an element that cannot be selected.""" class ElementNotVisibleException(InvalidElementStateException): """An element command could not be completed because the element is not visible on the page. """ class ImeEngineActivationFailedException(WebDriverException): """An IME engine could not be started.""" class ImeNotAvailableException(ImeEngineActivationFailedException): """IME was not available.""" class InvalidCookieDomainException(UnableToSetCookieException): """An illegal attempt was made to set a cookie under a different domain than the current page. """ class InvalidElementCoordinatesException(WebDriverException): """The coordinates provided to an interactions operation are invalid.""" class InvalidSelectorException(NoSuchElementException): """Argument was an invalid selector (e.g. XPath/CSS).""" class JavascriptErrorException(WebDriverException): """An error occurred while executing user supplied JavaScript.""" class MoveTargetOutOfBoundsException(InvalidElementStateException): """The target for mouse interaction is not in the browser's viewport and cannot be brought into that viewport. """ class NoSuchAlertException(WebDriverException): """An attempt was made to operate on a modal dialog when one was not open.""" class NoSuchFrameException(WebDriverException): """A request to switch to a frame could not be satisfied because the frame could not be found.""" class NoSuchWindowException(WebDriverException): """A request to switch to a different window could not be satisfied because the window could not be found. """ class ScriptTimeoutException(TimeoutException): """A script did not complete before its timeout expired.""" class SessionNotCreatedException(WebDriverException): """A new session could not be created.""" class StaleElementReferenceException(InvalidElementStateException): """An element command failed because the referenced element is no longer attached to the DOM. """ class UnexpectedAlertOpenException(WebDriverException): """A modal dialog was open, blocking this operation.""" class UnknownCommandException(WebDriverException): """A command could not be executed because the remote end is not aware of it. """ class UnknownErrorException(WebDriverException): """An unknown error occurred in the remote end while processing the command. """ class UnsupportedOperationException(WebDriverException): """Indicates that a command that should have executed properly cannot be supported for some reason. """ class UnknownStatusCodeException(WebDriverException): """Exception for all other status codes.""" _exceptions_strict = { "element not selectable": ElementNotSelectableException, "element not visible": ElementNotVisibleException, "ime engine activation failed": ImeEngineActivationFailedException, "ime not available": ImeNotAvailableException, "invalid cookie domain": InvalidCookieDomainException, "invalid element coordinates": InvalidElementCoordinatesException, "invalid element state": InvalidElementStateException, "invalid selector": InvalidSelectorException, "javascript error": JavascriptErrorException, "move target out of bounds": MoveTargetOutOfBoundsException, "no such alert": NoSuchAlertException, "no such element": NoSuchElementException, "no such frame": NoSuchFrameException, "no such window": NoSuchWindowException, "script timeout": ScriptTimeoutException, "session not created": SessionNotCreatedException, "stale element reference": StaleElementReferenceException, "success": None, "timeout": TimeoutException, "unable to set cookie": UnableToSetCookieException, "unexpected alert open": UnexpectedAlertOpenException, "unknown command": UnknownCommandException, "unknown error": UnknownErrorException, "unsupported operation": UnsupportedOperationException, } _exceptions_compatibility = { 15: ElementNotSelectableException, 11: ElementNotVisibleException, 31: ImeEngineActivationFailedException, 30: ImeNotAvailableException, 24: InvalidCookieDomainException, 29: InvalidElementCoordinatesException, 12: InvalidElementStateException, 19: InvalidSelectorException, 32: InvalidSelectorException, 17: JavascriptErrorException, 34: MoveTargetOutOfBoundsException, 27: NoSuchAlertException, 7: NoSuchElementException, 8: NoSuchFrameException, 23: NoSuchWindowException, 28: ScriptTimeoutException, 6: SessionNotCreatedException, 33: SessionNotCreatedException, 10: StaleElementReferenceException, 0: None, # success 21: TimeoutException, 25: UnableToSetCookieException, 26: UnexpectedAlertOpenException, 9: UnknownCommandException, 13: UnknownErrorException, # "unsupported operation": UnsupportedOperationException }
from __future__ import unicode_literals from django.core.exceptions import ImproperlyConfigured from django.db import models from django.http import Http404 from django.utils.translation import ugettext as _ from django.views.generic.base import ContextMixin, TemplateResponseMixin, View class SingleObjectMixin(ContextMixin): """ Provides the ability to retrieve a single object for further manipulation. """ model = None queryset = None slug_field = 'slug' context_object_name = None slug_url_kwarg = 'slug' pk_url_kwarg = 'pk' query_pk_and_slug = False def get_object(self, queryset=None): """ Returns the object the view is displaying. By default this requires `self.queryset` and a `pk` or `slug` argument in the URLconf, but subclasses can override this to return any object. """ # Use a custom queryset if provided; this is required for subclasses # like DateDetailView if queryset is None: queryset = self.get_queryset() # Next, try looking up by primary key. pk = self.kwargs.get(self.pk_url_kwarg) slug = self.kwargs.get(self.slug_url_kwarg) if pk is not None: queryset = queryset.filter(pk=pk) # Next, try looking up by slug. if slug is not None and (pk is None or self.query_pk_and_slug): slug_field = self.get_slug_field() queryset = queryset.filter(**{slug_field: slug}) # If none of those are defined, it's an error. if pk is None and slug is None: raise AttributeError("Generic detail view %s must be called with " "either an object pk or a slug." % self.__class__.__name__) try: # Get the single item from the filtered queryset obj = queryset.get() except queryset.model.DoesNotExist: raise Http404(_("No %(verbose_name)s found matching the query") % {'verbose_name': queryset.model._meta.verbose_name}) return obj def get_queryset(self): """ Return the `QuerySet` that will be used to look up the object. Note that this method is called by the default implementation of `get_object` and may not be called if `get_object` is overridden. """ if self.queryset is None: if self.model: return self.model._default_manager.all() else: raise ImproperlyConfigured( "%(cls)s is missing a QuerySet. Define " "%(cls)s.model, %(cls)s.queryset, or override " "%(cls)s.get_queryset()." % { 'cls': self.__class__.__name__ } ) return self.queryset.all() def get_slug_field(self): """ Get the name of a slug field to be used to look up by slug. """ return self.slug_field def get_context_object_name(self, obj): """ Get the name to use for the object. """ if self.context_object_name: return self.context_object_name elif isinstance(obj, models.Model): if self.object._deferred: obj = obj._meta.proxy_for_model return obj._meta.model_name else: return None def get_context_data(self, **kwargs): """ Insert the single object into the context dict. """ context = {} if self.object: context['object'] = self.object context_object_name = self.get_context_object_name(self.object) if context_object_name: context[context_object_name] = self.object context.update(kwargs) return super(SingleObjectMixin, self).get_context_data(**context) class BaseDetailView(SingleObjectMixin, View): """ A base view for displaying a single object """ def get(self, request, *args, **kwargs): self.object = self.get_object() context = self.get_context_data(object=self.object) return self.render_to_response(context) class SingleObjectTemplateResponseMixin(TemplateResponseMixin): template_name_field = None template_name_suffix = '_detail' def get_template_names(self): """ Return a list of template names to be used for the request. May not be called if render_to_response is overridden. Returns the following list: * the value of ``template_name`` on the view (if provided) * the contents of the ``template_name_field`` field on the object instance that the view is operating upon (if available) * ``<app_label>/<model_name><template_name_suffix>.html`` """ try: names = super(SingleObjectTemplateResponseMixin, self).get_template_names() except ImproperlyConfigured: # If template_name isn't specified, it's not a problem -- # we just start with an empty list. names = [] # If self.template_name_field is set, grab the value of the field # of that name from the object; this is the most specific template # name, if given. if self.object and self.template_name_field: name = getattr(self.object, self.template_name_field, None) if name: names.insert(0, name) # The least-specific option is the default <app>/<model>_detail.html; # only use this if the object in question is a model. if isinstance(self.object, models.Model): object_meta = self.object._meta if self.object._deferred: object_meta = self.object._meta.proxy_for_model._meta names.append("%s/%s%s.html" % ( object_meta.app_label, object_meta.model_name, self.template_name_suffix )) elif hasattr(self, 'model') and self.model is not None and issubclass(self.model, models.Model): names.append("%s/%s%s.html" % ( self.model._meta.app_label, self.model._meta.model_name, self.template_name_suffix )) # If we still haven't managed to find any template names, we should # re-raise the ImproperlyConfigured to alert the user. if not names: raise return names class DetailView(SingleObjectTemplateResponseMixin, BaseDetailView): """ Render a "detail" view of an object. By default this is a model instance looked up from `self.queryset`, but the view will support display of *any* object by overriding `self.get_object()`. """
from .utils import kwarg_decorator, last_arg_decorator from .version import version as __version__ from .version import version_info __all__ = [ '__version__', 'version_info', 'registry', 'register_model_chooser', 'register_simple_model_chooser', 'register_filter', ] class Registry(object): def __init__(self): self.choosers = {} self.filters = {} def register_chooser(self, chooser, **kwargs): """Adds a model chooser definition to the registry.""" if not issubclass(chooser, Chooser): return self.register_simple_chooser(chooser, **kwargs) self.choosers[chooser.model] = chooser(**kwargs) return chooser def register_simple_chooser(self, model, **kwargs): """ Generates a model chooser definition from a model, and adds it to the registry. """ name = '{}Chooser'.format(model._meta.object_name) attrs = {'model': model} attrs.update(kwargs) chooser = type(name, (Chooser,), attrs) self.register_chooser(chooser) return model def register_filter(self, model, name, filter): assert model in self.choosers self.filters[(model, name)] = filter return filter class Chooser(object): model = None icon = 'placeholder' # Customize the chooser content for just this model modal_template = None modal_results_template = None def get_queryset(self, request): return self.model._default_manager.all() def get_modal_template(self, request): return self.modal_template or 'wagtailmodelchooser/modal.html' def get_modal_results_template(self, request): return self.modal_results_template or 'wagtailmodelchooser/results.html' registry = Registry() register_model_chooser = kwarg_decorator(registry.register_chooser) register_simple_model_chooser = kwarg_decorator(registry.register_simple_chooser) register_filter = last_arg_decorator(registry.register_filter)
########################################################### # # Copyright (c) 2005, Southpaw Technology # All Rights Reserved # # PROPRIETARY INFORMATION. This software is proprietary to # Southpaw Technology, and is not to be reproduced, transmitted, # or disclosed in any way without written permission. # # # __all__ = ['FlashFileNaming'] import os, re from pyasm.biz import FileNaming, Project, Snapshot, File from pyasm.common import TacticException class FlashFileNaming(FileNaming): def add_ending(my, parts, auto_version=False): context = my.snapshot.get_value("context") version = my.snapshot.get_value("version") version = "v%0.3d" % version ext = my.get_ext() # it is only unique if we use both context and version parts.append(context) parts.append(version) filename = "_".join(parts) filename = "%s%s" % (filename, ext) # should I check if this filename is unique again? return filename # custom filename processing per sobject begins def _get_unique_filename(my): filename = my.file_object.get_full_file_name() # find if this filename has been used for this project file = File.get_by_filename(filename, skip_id=my.file_object.get_id()) if file: root, ext = os.path.splitext(filename) parts = [root] filename = my.add_ending(parts, auto_version=True) return filename else: return None def flash_nat_pause(my): return my._get_unique_filename() def flash_final_wave(my): return my._get_unique_filename()
# -*- coding: utf-8 -*- # # PublicKey/DSA.py : DSA signature primitive # # Written in 2008 by Dwayne C. Litzenberger <dlitz@dlitz.net> # # =================================================================== # The contents of this file are dedicated to the public domain. To # the extent that dedication to the public domain is not available, # everyone is granted a worldwide, perpetual, royalty-free, # non-exclusive license to exercise all rights associated with the # contents of this file for any purpose whatsoever. # No rights are reserved. # # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, # EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF # MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND # NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS # BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN # ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN # CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE # SOFTWARE. # =================================================================== """DSA public-key signature algorithm. DSA_ is a widespread public-key signature algorithm. Its security is based on the discrete logarithm problem (DLP_). Given a cyclic group, a generator *g*, and an element *h*, it is hard to find an integer *x* such that *g^x = h*. The problem is believed to be difficult, and it has been proved such (and therefore secure) for more than 30 years. The group is actually a sub-group over the integers modulo *p*, with *p* prime. The sub-group order is *q*, which is prime too; it always holds that *(p-1)* is a multiple of *q*. The cryptographic strength is linked to the magnitude of *p* and *q*. The signer holds a value *x* (*0<x<q-1*) as private key, and its public key (*y* where *y=g^x mod p*) is distributed. In 2012, a sufficient size is deemed to be 2048 bits for *p* and 256 bits for *q*. For more information, see the most recent ECRYPT_ report. DSA is reasonably secure for new designs. The algorithm can only be used for authentication (digital signature). DSA cannot be used for confidentiality (encryption). The values *(p,q,g)* are called *domain parameters*; they are not sensitive but must be shared by both parties (the signer and the verifier). Different signers can share the same domain parameters with no security concerns. The DSA signature is twice as big as the size of *q* (64 bytes if *q* is 256 bit long). This module provides facilities for generating new DSA keys and for constructing them from known components. DSA keys allows you to perform basic signing and verification. >>> from Crypto.Random import random >>> from Crypto.PublicKey import DSA >>> from Crypto.Hash import SHA >>> >>> message = "Hello" >>> key = DSA.generate(1024) >>> h = SHA.new(message).digest() >>> k = random.StrongRandom().randint(1,key.q-1) >>> sig = key.sign(h,k) >>> ... >>> if key.verify(h,sig): >>> print "OK" >>> else: >>> print "Incorrect signature" .. _DSA: http://en.wikipedia.org/wiki/Digital_Signature_Algorithm .. _DLP: http://www.cosic.esat.kuleuven.be/publications/talk-78.pdf .. _ECRYPT: http://www.ecrypt.eu.org/documents/D.SPA.17.pdf """ __revision__ = "$Id$" __all__ = ['generate', 'construct', 'error', 'DSAImplementation', '_DSAobj'] import sys if sys.version_info[0] == 2 and sys.version_info[1] == 1: from Crypto.Util.py21compat import * from Crypto.PublicKey import _DSA, _slowmath, pubkey from Crypto import Random try: from Crypto.PublicKey import _fastmath except ImportError: _fastmath = None class _DSAobj(pubkey.pubkey): """Class defining an actual DSA key. :undocumented: __getstate__, __setstate__, __repr__, __getattr__ """ #: Dictionary of DSA parameters. #: #: A public key will only have the following entries: #: #: - **y**, the public key. #: - **g**, the generator. #: - **p**, the modulus. #: - **q**, the order of the sub-group. #: #: A private key will also have: #: #: - **x**, the private key. keydata = ['y', 'g', 'p', 'q', 'x'] def __init__(self, implementation, key): self.implementation = implementation self.key = key def __getattr__(self, attrname): if attrname in self.keydata: # For backward compatibility, allow the user to get (not set) the # DSA key parameters directly from this object. return getattr(self.key, attrname) else: raise AttributeError("%s object has no %r attribute" % (self.__class__.__name__, attrname,)) def sign(self, M, K): """Sign a piece of data with DSA. :Parameter M: The piece of data to sign with DSA. It may not be longer in bit size than the sub-group order (*q*). :Type M: byte string or long :Parameter K: A secret number, chosen randomly in the closed range *[1,q-1]*. :Type K: long (recommended) or byte string (not recommended) :attention: selection of *K* is crucial for security. Generating a random number larger than *q* and taking the modulus by *q* is **not** secure, since smaller values will occur more frequently. Generating a random number systematically smaller than *q-1* (e.g. *floor((q-1)/8)* random bytes) is also **not** secure. In general, it shall not be possible for an attacker to know the value of `any bit of K`__. :attention: The number *K* shall not be reused for any other operation and shall be discarded immediately. :attention: M must be a digest cryptographic hash, otherwise an attacker may mount an existential forgery attack. :Return: A tuple with 2 longs. .. __: http://www.di.ens.fr/~pnguyen/pub_NgSh00.htm """ return pubkey.pubkey.sign(self, M, K) def verify(self, M, signature): """Verify the validity of a DSA signature. :Parameter M: The expected message. :Type M: byte string or long :Parameter signature: The DSA signature to verify. :Type signature: A tuple with 2 longs as return by `sign` :Return: True if the signature is correct, False otherwise. """ return pubkey.pubkey.verify(self, M, signature) def _encrypt(self, c, K): raise TypeError("DSA cannot encrypt") def _decrypt(self, c): raise TypeError("DSA cannot decrypt") def _blind(self, m, r): raise TypeError("DSA cannot blind") def _unblind(self, m, r): raise TypeError("DSA cannot unblind") def _sign(self, m, k): return self.key._sign(m, k) def _verify(self, m, sig): (r, s) = sig return self.key._verify(m, r, s) def has_private(self): return self.key.has_private() def size(self): return self.key.size() def can_blind(self): return False def can_encrypt(self): return False def can_sign(self): return True def publickey(self): return self.implementation.construct((self.key.y, self.key.g, self.key.p, self.key.q)) def __getstate__(self): d = {} for k in self.keydata: try: d[k] = getattr(self.key, k) except AttributeError: pass return d def __setstate__(self, d): if not hasattr(self, 'implementation'): self.implementation = DSAImplementation() t = [] for k in self.keydata: if not d.has_key(k): break t.append(d[k]) self.key = self.implementation._math.dsa_construct(*tuple(t)) def __repr__(self): attrs = [] for k in self.keydata: if k == 'p': attrs.append("p(%d)" % (self.size()+1,)) elif hasattr(self.key, k): attrs.append(k) if self.has_private(): attrs.append("private") # PY3K: This is meant to be text, do not change to bytes (data) return "<%s @0x%x %s>" % (self.__class__.__name__, id(self), ",".join(attrs)) class DSAImplementation(object): """ A DSA key factory. This class is only internally used to implement the methods of the `Crypto.PublicKey.DSA` module. """ def __init__(self, **kwargs): """Create a new DSA key factory. :Keywords: use_fast_math : bool Specify which mathematic library to use: - *None* (default). Use fastest math available. - *True* . Use fast math. - *False* . Use slow math. default_randfunc : callable Specify how to collect random data: - *None* (default). Use Random.new().read(). - not *None* . Use the specified function directly. :Raise RuntimeError: When **use_fast_math** =True but fast math is not available. """ use_fast_math = kwargs.get('use_fast_math', None) if use_fast_math is None: # Automatic if _fastmath is not None: self._math = _fastmath else: self._math = _slowmath elif use_fast_math: # Explicitly select fast math if _fastmath is not None: self._math = _fastmath else: raise RuntimeError("fast math module not available") else: # Explicitly select slow math self._math = _slowmath self.error = self._math.error # 'default_randfunc' parameter: # None (default) - use Random.new().read # not None - use the specified function self._default_randfunc = kwargs.get('default_randfunc', None) self._current_randfunc = None def _get_randfunc(self, randfunc): if randfunc is not None: return randfunc elif self._current_randfunc is None: self._current_randfunc = Random.new().read return self._current_randfunc def generate(self, bits, randfunc=None, progress_func=None): """Randomly generate a fresh, new DSA key. :Parameters: bits : int Key length, or size (in bits) of the DSA modulus *p*. It must be a multiple of 64, in the closed interval [512,1024]. randfunc : callable Random number generation function; it should accept a single integer N and return a string of random data N bytes long. If not specified, a new one will be instantiated from ``Crypto.Random``. progress_func : callable Optional function that will be called with a short string containing the key parameter currently being generated; it's useful for interactive applications where a user is waiting for a key to be generated. :attention: You should always use a cryptographically secure random number generator, such as the one defined in the ``Crypto.Random`` module; **don't** just use the current time and the ``random`` module. :Return: A DSA key object (`_DSAobj`). :Raise ValueError: When **bits** is too little, too big, or not a multiple of 64. """ # Check against FIPS 186-2, which says that the size of the prime p # must be a multiple of 64 bits between 512 and 1024 for i in (0, 1, 2, 3, 4, 5, 6, 7, 8): if bits == 512 + 64*i: return self._generate(bits, randfunc, progress_func) # The March 2006 draft of FIPS 186-3 also allows 2048 and 3072-bit # primes, but only with longer q values. Since the current DSA # implementation only supports a 160-bit q, we don't support larger # values. raise ValueError("Number of bits in p must be a multiple of 64 between 512 and 1024, not %d bits" % (bits,)) def _generate(self, bits, randfunc=None, progress_func=None): rf = self._get_randfunc(randfunc) obj = _DSA.generate_py(bits, rf, progress_func) # TODO: Don't use legacy _DSA module key = self._math.dsa_construct(obj.y, obj.g, obj.p, obj.q, obj.x) return _DSAobj(self, key) def construct(self, tup): """Construct a DSA key from a tuple of valid DSA components. The modulus *p* must be a prime. The following equations must apply: - p-1 = 0 mod q - g^x = y mod p - 0 < x < q - 1 < g < p :Parameters: tup : tuple A tuple of long integers, with 4 or 5 items in the following order: 1. Public key (*y*). 2. Sub-group generator (*g*). 3. Modulus, finite field order (*p*). 4. Sub-group order (*q*). 5. Private key (*x*). Optional. :Return: A DSA key object (`_DSAobj`). """ key = self._math.dsa_construct(*tup) return _DSAobj(self, key) _impl = DSAImplementation() generate = _impl.generate construct = _impl.construct error = _impl.error # vim:set ts=4 sw=4 sts=4 expandtab:
# Copyright 2013 The Chromium Authors. All rights reserved. # Use of this source code is governed by a BSD-style license that can be # found in the LICENSE file. """Setup for instrumentation host-driven tests.""" import logging import os import sys import types from pylib.host_driven import test_case from pylib.host_driven import test_info_collection from pylib.host_driven import test_runner def _GetPythonFiles(root, files): """Returns all files from |files| that end in 'Test.py'. Args: root: A directory name with python files. files: A list of file names. Returns: A list with all python files that match the testing naming scheme. """ return [os.path.join(root, f) for f in files if f.endswith('Test.py')] def _InferImportNameFromFile(python_file): """Given a file, infer the import name for that file. Example: /usr/foo/bar/baz.py -> baz. Args: python_file: Path to the Python file, ostensibly to import later. Returns: The module name for the given file. """ return os.path.splitext(os.path.basename(python_file))[0] def _GetTestModules(host_driven_test_root, is_official_build): """Retrieve a list of python modules that match the testing naming scheme. Walks the location of host-driven tests, imports them, and provides the list of imported modules to the caller. Args: host_driven_test_root: The path to walk, looking for the pythonDrivenTests or host_driven_tests directory is_official_build: Whether to run only those tests marked 'official' Returns: A list of python modules under |host_driven_test_root| which match the testing naming scheme. Each module should define one or more classes that derive from HostDrivenTestCase. """ # By default run all host-driven tests under pythonDrivenTests or # host_driven_tests. host_driven_test_file_list = [] for root, _, files in os.walk(host_driven_test_root): if (root.endswith('host_driven_tests') or root.endswith('pythonDrivenTests') or (is_official_build and (root.endswith('pythonDrivenTests/official') or root.endswith('host_driven_tests/official')))): host_driven_test_file_list += _GetPythonFiles(root, files) host_driven_test_file_list.sort() test_module_list = [_GetModuleFromFile(test_file) for test_file in host_driven_test_file_list] return test_module_list def _GetModuleFromFile(python_file): """Gets the python module associated with a file by importing it. Args: python_file: File to import. Returns: The module object. """ sys.path.append(os.path.dirname(python_file)) import_name = _InferImportNameFromFile(python_file) return __import__(import_name) def _GetTestsFromClass(test_case_class, **kwargs): """Returns one test object for each test method in |test_case_class|. Test methods are methods on the class which begin with 'test'. Args: test_case_class: Class derived from HostDrivenTestCase which contains zero or more test methods. kwargs: Keyword args to pass into the constructor of test cases. Returns: A list of test case objects, each initialized for a particular test method. """ test_names = [m for m in dir(test_case_class) if _IsTestMethod(m, test_case_class)] return [test_case_class(name, **kwargs) for name in test_names] def _GetTestsFromModule(test_module, **kwargs): """Gets a list of test objects from |test_module|. Args: test_module: Module from which to get the set of test methods. kwargs: Keyword args to pass into the constructor of test cases. Returns: A list of test case objects each initialized for a particular test method defined in |test_module|. """ tests = [] for name in dir(test_module): attr = getattr(test_module, name) if _IsTestCaseClass(attr): tests.extend(_GetTestsFromClass(attr, **kwargs)) return tests def _IsTestCaseClass(test_class): return (type(test_class) is types.TypeType and issubclass(test_class, test_case.HostDrivenTestCase) and test_class is not test_case.HostDrivenTestCase) def _IsTestMethod(attrname, test_case_class): """Checks whether this is a valid test method. Args: attrname: The method name. test_case_class: The test case class. Returns: True if test_case_class.'attrname' is callable and it starts with 'test'; False otherwise. """ attr = getattr(test_case_class, attrname) return callable(attr) and attrname.startswith('test') def _GetAllTests(test_root, is_official_build, **kwargs): """Retrieve a list of host-driven tests defined under |test_root|. Args: test_root: Path which contains host-driven test files. is_official_build: Whether this is an official build. kwargs: Keyword args to pass into the constructor of test cases. Returns: List of test case objects, one for each available test method. """ if not test_root: return [] all_tests = [] test_module_list = _GetTestModules(test_root, is_official_build) for module in test_module_list: all_tests.extend(_GetTestsFromModule(module, **kwargs)) return all_tests def InstrumentationSetup(host_driven_test_root, official_build, instrumentation_options): """Creates a list of host-driven instrumentation tests and a runner factory. Args: host_driven_test_root: Directory where the host-driven tests are. official_build: True if this is an official build. instrumentation_options: An InstrumentationOptions object. Returns: A tuple of (TestRunnerFactory, tests). """ test_collection = test_info_collection.TestInfoCollection() all_tests = _GetAllTests( host_driven_test_root, official_build, instrumentation_options=instrumentation_options) test_collection.AddTests(all_tests) available_tests = test_collection.GetAvailableTests( instrumentation_options.annotations, instrumentation_options.exclude_annotations, instrumentation_options.test_filter) logging.debug('All available tests: ' + str( [t.tagged_name for t in available_tests])) def TestRunnerFactory(device, shard_index): return test_runner.HostDrivenTestRunner( device, shard_index, instrumentation_options.tool) return (TestRunnerFactory, available_tests)
# # Licensed to the Apache Software Foundation (ASF) under one # or more contributor license agreements. See the NOTICE file # distributed with this work for additional information # regarding copyright ownership. The ASF licenses this file # to you under the Apache License, Version 2.0 (the # "License"); you may not use this file except in compliance # with the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, # software distributed under the License is distributed on an # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY # KIND, either express or implied. See the License for the # specific language governing permissions and limitations # under the License. # """ Simple DOM for both SGML and XML documents. """ from __future__ import division from __future__ import generators from __future__ import nested_scopes import transforms class Container: def __init__(self): self.children = [] def add(self, child): child.parent = self self.children.append(child) def extend(self, children): for child in children: child.parent = self self.children.append(child) class Component: def __init__(self): self.parent = None def index(self): if self.parent: return self.parent.children.index(self) else: return 0 def _line(self, file, line, column): self.file = file self.line = line self.column = column class DispatchError(Exception): def __init__(self, scope, f): msg = "no such attribtue" class Dispatcher: def is_type(self, type): cls = self while cls != None: if cls.type == type: return True cls = cls.base return False def dispatch(self, f, attrs = ""): cls = self while cls != None: if hasattr(f, cls.type): return getattr(f, cls.type)(self) else: cls = cls.base cls = self while cls != None: if attrs: sep = ", " if cls.base == None: sep += "or " else: sep = "" attrs += "%s'%s'" % (sep, cls.type) cls = cls.base raise AttributeError("'%s' object has no attribute %s" % (f.__class__.__name__, attrs)) class Node(Container, Component, Dispatcher): type = "node" base = None def __init__(self): Container.__init__(self) Component.__init__(self) self.query = Query([self]) def __getitem__(self, name): for nd in self.query[name]: return nd def text(self): return self.dispatch(transforms.Text()) def tag(self, name, *attrs, **kwargs): t = Tag(name, *attrs, **kwargs) self.add(t) return t def data(self, s): d = Data(s) self.add(d) return d def entity(self, s): e = Entity(s) self.add(e) return e class Tree(Node): type = "tree" base = Node class Tag(Node): type = "tag" base = Node def __init__(self, _name, *attrs, **kwargs): Node.__init__(self) self.name = _name self.attrs = list(attrs) self.attrs.extend(kwargs.items()) self.singleton = False def get_attr(self, name): for k, v in self.attrs: if name == k: return v def _idx(self, attr): idx = 0 for k, v in self.attrs: if k == attr: return idx idx += 1 return None def set_attr(self, name, value): idx = self._idx(name) if idx is None: self.attrs.append((name, value)) else: self.attrs[idx] = (name, value) def dispatch(self, f): try: attr = "do_" + self.name method = getattr(f, attr) except AttributeError: return Dispatcher.dispatch(self, f, "'%s'" % attr) return method(self) class Leaf(Component, Dispatcher): type = "leaf" base = None def __init__(self, data): assert isinstance(data, basestring) self.data = data class Data(Leaf): type = "data" base = Leaf class Entity(Leaf): type = "entity" base = Leaf class Character(Leaf): type = "character" base = Leaf class Comment(Leaf): type = "comment" base = Leaf ################### ## Query Classes ## ########################################################################### class Adder: def __add__(self, other): return Sum(self, other) class Sum(Adder): def __init__(self, left, right): self.left = left self.right = right def __iter__(self): for x in self.left: yield x for x in self.right: yield x class View(Adder): def __init__(self, source): self.source = source class Filter(View): def __init__(self, predicate, source): View.__init__(self, source) self.predicate = predicate def __iter__(self): for nd in self.source: if self.predicate(nd): yield nd class Flatten(View): def __iter__(self): sources = [iter(self.source)] while sources: try: nd = sources[-1].next() if isinstance(nd, Tree): sources.append(iter(nd.children)) else: yield nd except StopIteration: sources.pop() class Children(View): def __iter__(self): for nd in self.source: for child in nd.children: yield child class Attributes(View): def __iter__(self): for nd in self.source: for a in nd.attrs: yield a class Values(View): def __iter__(self): for name, value in self.source: yield value def flatten_path(path): if isinstance(path, basestring): for part in path.split("/"): yield part elif callable(path): yield path else: for p in path: for fp in flatten_path(p): yield fp class Query(View): def __iter__(self): for nd in self.source: yield nd def __getitem__(self, path): query = self.source for p in flatten_path(path): if callable(p): select = Query pred = p source = query elif isinstance(p, basestring): if p[0] == "@": select = Values pred = lambda x, n=p[1:]: x[0] == n source = Attributes(query) elif p[0] == "#": select = Query pred = lambda x, t=p[1:]: x.is_type(t) source = Children(query) else: select = Query pred = lambda x, n=p: isinstance(x, Tag) and x.name == n source = Flatten(Children(query)) else: raise ValueError(p) query = select(Filter(pred, source)) return query
""" Support for MyQ-Enabled Garage Doors. For more details about this platform, please refer to the documentation https://home-assistant.io/components/cover.myq/ """ import logging import voluptuous as vol from homeassistant.components.cover import CoverDevice from homeassistant.const import ( CONF_USERNAME, CONF_PASSWORD, CONF_TYPE, STATE_CLOSED) import homeassistant.helpers.config_validation as cv import homeassistant.loader as loader REQUIREMENTS = ['pymyq==0.0.8'] _LOGGER = logging.getLogger(__name__) DEFAULT_NAME = 'myq' NOTIFICATION_ID = 'myq_notification' NOTIFICATION_TITLE = 'MyQ Cover Setup' COVER_SCHEMA = vol.Schema({ vol.Required(CONF_TYPE): cv.string, vol.Required(CONF_USERNAME): cv.string, vol.Required(CONF_PASSWORD): cv.string }) def setup_platform(hass, config, add_devices, discovery_info=None): """Set up the MyQ component.""" from pymyq import MyQAPI as pymyq username = config.get(CONF_USERNAME) password = config.get(CONF_PASSWORD) brand = config.get(CONF_TYPE) persistent_notification = loader.get_component('persistent_notification') myq = pymyq(username, password, brand) try: if not myq.is_supported_brand(): raise ValueError("Unsupported type. See documentation") if not myq.is_login_valid(): raise ValueError("Username or Password is incorrect") add_devices(MyQDevice(myq, door) for door in myq.get_garage_doors()) return True except (TypeError, KeyError, NameError, ValueError) as ex: _LOGGER.error("%s", ex) persistent_notification.create( hass, 'Error: {}<br />' 'You will need to restart hass after fixing.' ''.format(ex), title=NOTIFICATION_TITLE, notification_id=NOTIFICATION_ID) return False class MyQDevice(CoverDevice): """Representation of a MyQ cover.""" def __init__(self, myq, device): """Initialize with API object, device id.""" self.myq = myq self.device_id = device['deviceid'] self._name = device['name'] self._status = STATE_CLOSED @property def should_poll(self): """Poll for state.""" return True @property def name(self): """Return the name of the garage door if any.""" return self._name if self._name else DEFAULT_NAME @property def is_closed(self): """Return true if cover is closed, else False.""" return self._status == STATE_CLOSED def close_cover(self): """Issue close command to cover.""" self.myq.close_device(self.device_id) def open_cover(self): """Issue open command to cover.""" self.myq.open_device(self.device_id) def update(self): """Update status of cover.""" self._status = self.myq.get_status(self.device_id)
#!/usr/bin/env python """This file abstracts the loading of the private key.""" from cryptography import x509 from cryptography.hazmat.backends import openssl from cryptography.hazmat.primitives import hashes from cryptography.x509 import oid from grr.lib import rdfvalue from grr.lib.rdfvalues import crypto as rdf_crypto def MakeCASignedCert(common_name, private_key, ca_cert, ca_private_key, serial_number=2): """Make a cert and sign it with the CA's private key.""" public_key = private_key.GetPublicKey() builder = x509.CertificateBuilder() builder = builder.issuer_name(ca_cert.GetIssuer()) subject = x509.Name( [x509.NameAttribute(oid.NameOID.COMMON_NAME, common_name)]) builder = builder.subject_name(subject) valid_from = rdfvalue.RDFDatetime.Now() - rdfvalue.Duration("1d") valid_until = rdfvalue.RDFDatetime.Now() + rdfvalue.Duration("3650d") builder = builder.not_valid_before(valid_from.AsDatetime()) builder = builder.not_valid_after(valid_until.AsDatetime()) builder = builder.serial_number(serial_number) builder = builder.public_key(public_key.GetRawPublicKey()) builder = builder.add_extension( x509.BasicConstraints( ca=False, path_length=None), critical=True) certificate = builder.sign( private_key=ca_private_key.GetRawPrivateKey(), algorithm=hashes.SHA256(), backend=openssl.backend) return rdf_crypto.RDFX509Cert(certificate) def MakeCACert(private_key, common_name=u"grr", issuer_cn=u"grr_test", issuer_c=u"US"): """Generate a CA certificate. Args: private_key: The private key to use. common_name: Name for cert. issuer_cn: Name for issuer. issuer_c: Country for issuer. Returns: The certificate. """ public_key = private_key.GetPublicKey() builder = x509.CertificateBuilder() issuer = x509.Name([ x509.NameAttribute(oid.NameOID.COMMON_NAME, issuer_cn), x509.NameAttribute(oid.NameOID.COUNTRY_NAME, issuer_c) ]) subject = x509.Name( [x509.NameAttribute(oid.NameOID.COMMON_NAME, common_name)]) builder = builder.subject_name(subject) builder = builder.issuer_name(issuer) valid_from = rdfvalue.RDFDatetime.Now() - rdfvalue.Duration("1d") valid_until = rdfvalue.RDFDatetime.Now() + rdfvalue.Duration("3650d") builder = builder.not_valid_before(valid_from.AsDatetime()) builder = builder.not_valid_after(valid_until.AsDatetime()) builder = builder.serial_number(1) builder = builder.public_key(public_key.GetRawPublicKey()) builder = builder.add_extension( x509.BasicConstraints( ca=True, path_length=None), critical=True) builder = builder.add_extension( x509.SubjectKeyIdentifier.from_public_key(public_key.GetRawPublicKey()), critical=False) certificate = builder.sign( private_key=private_key.GetRawPrivateKey(), algorithm=hashes.SHA256(), backend=openssl.backend) return rdf_crypto.RDFX509Cert(certificate)
# # gdb helper commands and functions for Linux kernel debugging # # load kernel and module symbols # # Copyright (c) Siemens AG, 2011-2013 # # Authors: # Jan Kiszka <jan.kiszka@siemens.com> # # This work is licensed under the terms of the GNU GPL version 2. # import gdb import os import re from linux import modules if hasattr(gdb, 'Breakpoint'): class LoadModuleBreakpoint(gdb.Breakpoint): def __init__(self, spec, gdb_command): super(LoadModuleBreakpoint, self).__init__(spec, internal=True) self.silent = True self.gdb_command = gdb_command def stop(self): module = gdb.parse_and_eval("mod") module_name = module['name'].string() cmd = self.gdb_command # enforce update if object file is not found cmd.module_files_updated = False # Disable pagination while reporting symbol (re-)loading. # The console input is blocked in this context so that we would # get stuck waiting for the user to acknowledge paged output. show_pagination = gdb.execute("show pagination", to_string=True) pagination = show_pagination.endswith("on.\n") gdb.execute("set pagination off") if module_name in cmd.loaded_modules: gdb.write("refreshing all symbols to reload module " "'{0}'\n".format(module_name)) cmd.load_all_symbols() else: cmd.load_module_symbols(module) # restore pagination state gdb.execute("set pagination %s" % ("on" if pagination else "off")) return False class LxSymbols(gdb.Command): """(Re-)load symbols of Linux kernel and currently loaded modules. The kernel (vmlinux) is taken from the current working directly. Modules (.ko) are scanned recursively, starting in the same directory. Optionally, the module search path can be extended by a space separated list of paths passed to the lx-symbols command.""" module_paths = [] module_files = [] module_files_updated = False loaded_modules = [] breakpoint = None def __init__(self): super(LxSymbols, self).__init__("lx-symbols", gdb.COMMAND_FILES, gdb.COMPLETE_FILENAME) def _update_module_files(self): self.module_files = [] for path in self.module_paths: gdb.write("scanning for modules in {0}\n".format(path)) for root, dirs, files in os.walk(path): for name in files: if name.endswith(".ko"): self.module_files.append(root + "/" + name) self.module_files_updated = True def _get_module_file(self, module_name): module_pattern = ".*/{0}\.ko$".format( module_name.replace("_", r"[_\-]")) for name in self.module_files: if re.match(module_pattern, name) and os.path.exists(name): return name return None def _section_arguments(self, module): try: sect_attrs = module['sect_attrs'].dereference() except gdb.error: return "" attrs = sect_attrs['attrs'] section_name_to_address = { attrs[n]['name'].string(): attrs[n]['address'] for n in range(int(sect_attrs['nsections']))} args = [] for section_name in [".data", ".data..read_mostly", ".rodata", ".bss"]: address = section_name_to_address.get(section_name) if address: args.append(" -s {name} {addr}".format( name=section_name, addr=str(address))) return "".join(args) def load_module_symbols(self, module): module_name = module['name'].string() module_addr = str(module['core_layout']['base']).split()[0] module_file = self._get_module_file(module_name) if not module_file and not self.module_files_updated: self._update_module_files() module_file = self._get_module_file(module_name) if module_file: gdb.write("loading @{addr}: {filename}\n".format( addr=module_addr, filename=module_file)) cmdline = "add-symbol-file {filename} {addr}{sections}".format( filename=module_file, addr=module_addr, sections=self._section_arguments(module)) gdb.execute(cmdline, to_string=True) if module_name not in self.loaded_modules: self.loaded_modules.append(module_name) else: gdb.write("no module object found for '{0}'\n".format(module_name)) def load_all_symbols(self): gdb.write("loading vmlinux\n") # Dropping symbols will disable all breakpoints. So save their states # and restore them afterward. saved_states = [] if hasattr(gdb, 'breakpoints') and not gdb.breakpoints() is None: for bp in gdb.breakpoints(): saved_states.append({'breakpoint': bp, 'enabled': bp.enabled}) # drop all current symbols and reload vmlinux gdb.execute("symbol-file", to_string=True) gdb.execute("symbol-file vmlinux") self.loaded_modules = [] module_list = modules.module_list() if not module_list: gdb.write("no modules found\n") else: [self.load_module_symbols(module) for module in module_list] for saved_state in saved_states: saved_state['breakpoint'].enabled = saved_state['enabled'] def invoke(self, arg, from_tty): self.module_paths = arg.split() self.module_paths.append(os.getcwd()) # enforce update self.module_files = [] self.module_files_updated = False self.load_all_symbols() if hasattr(gdb, 'Breakpoint'): if self.breakpoint is not None: self.breakpoint.delete() self.breakpoint = None self.breakpoint = LoadModuleBreakpoint( "kernel/module.c:do_init_module", self) else: gdb.write("Note: symbol update on module loading not supported " "with this gdb version\n") LxSymbols()
# -*- encoding: utf-8 -*- # This file is distributed under the same license as the Django package. # from __future__ import unicode_literals # The *_FORMAT strings use the Django date format syntax, # see http://docs.djangoproject.com/en/dev/ref/templates/builtins/#date DATE_FORMAT = 'N j, Y' TIME_FORMAT = 'P' DATETIME_FORMAT = 'N j, Y, P' YEAR_MONTH_FORMAT = 'F Y' MONTH_DAY_FORMAT = 'F j' SHORT_DATE_FORMAT = 'm/d/Y' SHORT_DATETIME_FORMAT = 'm/d/Y P' FIRST_DAY_OF_WEEK = 0 # Sunday # The *_INPUT_FORMATS strings use the Python strftime format syntax, # see http://docs.python.org/library/datetime.html#strftime-strptime-behavior # Kept ISO formats as they are in first position DATE_INPUT_FORMATS = [ '%Y-%m-%d', '%m/%d/%Y', '%m/%d/%y', # '2006-10-25', '10/25/2006', '10/25/06' # '%b %d %Y', '%b %d, %Y', # 'Oct 25 2006', 'Oct 25, 2006' # '%d %b %Y', '%d %b, %Y', # '25 Oct 2006', '25 Oct, 2006' # '%B %d %Y', '%B %d, %Y', # 'October 25 2006', 'October 25, 2006' # '%d %B %Y', '%d %B, %Y', # '25 October 2006', '25 October, 2006' ] DATETIME_INPUT_FORMATS = [ '%Y-%m-%d %H:%M:%S', # '2006-10-25 14:30:59' '%Y-%m-%d %H:%M:%S.%f', # '2006-10-25 14:30:59.000200' '%Y-%m-%d %H:%M', # '2006-10-25 14:30' '%Y-%m-%d', # '2006-10-25' '%m/%d/%Y %H:%M:%S', # '10/25/2006 14:30:59' '%m/%d/%Y %H:%M:%S.%f', # '10/25/2006 14:30:59.000200' '%m/%d/%Y %H:%M', # '10/25/2006 14:30' '%m/%d/%Y', # '10/25/2006' '%m/%d/%y %H:%M:%S', # '10/25/06 14:30:59' '%m/%d/%y %H:%M:%S.%f', # '10/25/06 14:30:59.000200' '%m/%d/%y %H:%M', # '10/25/06 14:30' '%m/%d/%y', # '10/25/06' ] DECIMAL_SEPARATOR = '.' THOUSAND_SEPARATOR = ',' NUMBER_GROUPING = 3
# Copyright: (c) 2017, Ansible Project # GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt) from __future__ import (absolute_import, division, print_function) __metaclass__ = type from ansible import constants as C from ansible.plugins.action import ActionBase from ansible.utils.vars import merge_hash class ActionModule(ActionBase): def run(self, tmp=None, task_vars=None): self._supports_async = True results = super(ActionModule, self).run(tmp, task_vars) del tmp # tmp no longer has any effect # Command module has a special config option to turn off the command nanny warnings if 'warn' not in self._task.args: self._task.args['warn'] = C.COMMAND_WARNINGS wrap_async = self._task.async_val and not self._connection.has_native_async results = merge_hash(results, self._execute_module(task_vars=task_vars, wrap_async=wrap_async)) if not wrap_async: # remove a temporary path we created self._remove_tmp_path(self._connection._shell.tmpdir) return results
# HTTM: A transformation library for RAW and Electron Flux TESS Images # Copyright (C) 2016, 2017 John Doty and Matthew Wampler-Doty of Noqsi Aerospace, Ltd. # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. """ ``httm.transformations.metadata`` ================================= This module contains metadata related to transformation functions. - ``electron_flux_transformations`` is metadata describing transformation functions from images in electron counts to simulated raw images in *Analogue to Digital Converter Units* (ADU). - ``raw_transformations`` is metadata describing transformation functions from raw images in *Analogue to Digital Converter Units* (ADU) to calibrated images in electron counts. """ from collections import OrderedDict from .raw_converters_to_calibrated import remove_pattern_noise, convert_adu_to_electrons, remove_baseline, \ remove_start_of_line_ringing, remove_undershoot, remove_smear from .electron_flux_converters_to_raw import introduce_smear_rows, add_shot_noise, simulate_blooming, \ add_readout_noise, simulate_undershoot, simulate_start_of_line_ringing, add_baseline, convert_electrons_to_adu, \ add_pattern_noise electron_flux_transformations = OrderedDict([ ('introduce_smear_rows', { 'default': True, 'documentation': 'Introduce *smear rows* to each slice of the image.', 'function': introduce_smear_rows, }), ('add_shot_noise', { 'default': True, 'documentation': 'Add *shot noise* to each pixel in each slice of the image.', 'function': add_shot_noise, }), ('simulate_blooming', { 'default': True, 'documentation': 'Simulate *blooming* on for each column for each slice of the image.', 'function': simulate_blooming, }), ('add_readout_noise', { 'default': True, 'documentation': 'Add *readout noise* to each pixel in each slice of the image.', 'function': add_readout_noise, }), ('simulate_undershoot', { 'default': True, 'documentation': 'Simulate *undershoot* on each row of each slice in the image.', 'function': simulate_undershoot, }), ('simulate_start_of_line_ringing', { 'default': True, 'documentation': 'Simulate *start of line ringing* on each row of each slice in the image.', 'function': simulate_start_of_line_ringing, }), ('add_baseline', { 'default': True, 'documentation': 'Add a *baseline electron count* to each slice in the image.', 'function': add_baseline, }), ('convert_electrons_to_adu', { 'default': True, 'documentation': 'Convert the image from having pixel units in electron counts to ' '*Analogue to Digital Converter Units* (ADU).', 'function': convert_electrons_to_adu, }), ('add_pattern_noise', { 'default': True, 'documentation': 'Add a fixed *pattern noise* to each slice in the image.', 'function': add_pattern_noise, }), ]) raw_transformations = OrderedDict([ ('remove_pattern_noise', { 'default': True, 'documentation': 'Compensate for a fixed *pattern noise* on each slice of the image.', 'function': remove_pattern_noise, }), ('convert_adu_to_electrons', { 'default': True, 'documentation': 'Convert the image from having units in ' '*Analogue to Digital Converter Units* (ADU) ' 'to electron counts.', 'function': convert_adu_to_electrons, }), ('remove_baseline', { 'default': True, 'documentation': 'Average the pixels in the dark columns and subtract ' 'the result from each pixel in the image.', 'function': remove_baseline, }), ('remove_start_of_line_ringing', { 'default': True, 'documentation': 'Compensate for *start of line ringing* on each row of each slice of the image.', 'function': remove_start_of_line_ringing, }), ('remove_undershoot', { 'default': True, 'documentation': 'Compensate for *undershoot* for each row of each slice of the image.', 'function': remove_undershoot, }), ('remove_smear', { 'default': True, 'documentation': 'Compensate for *smear* in the image by reading it from the ' '*smear rows* each slice and removing it from the rest of the slice.', 'function': remove_smear, }), ])
# This file is part of Indico. # Copyright (C) 2002 - 2019 CERN # # Indico is free software; you can redistribute it and/or # modify it under the terms of the MIT License; see the # LICENSE file for more details. from __future__ import unicode_literals import os from flask_webpackext import FlaskWebpackExt from flask_webpackext.manifest import JinjaManifestLoader from pywebpack import ManifestLoader from indico.web.assets.util import get_custom_assets class IndicoManifestLoader(JinjaManifestLoader): cache = {} def __init__(self, *args, **kwargs): self.custom = kwargs.pop('custom', True) super(IndicoManifestLoader, self).__init__(*args, **kwargs) def load(self, filepath): key = (filepath, os.path.getmtime(filepath)) if key not in IndicoManifestLoader.cache: IndicoManifestLoader.cache[key] = manifest = ManifestLoader.load(self, filepath) if self.custom: self._add_custom_assets(manifest) return IndicoManifestLoader.cache[key] def _add_custom_assets(self, manifest): # custom assets (from CUSTOMIZATION_DIR) are not part of the webpack manifest # since they are not build with webpack (it's generally not available on the # machine running indico), but we add them here anyway so they can be handled # without too much extra code, e.g. when building a static site. manifest.add(self.entry_cls('__custom.css', get_custom_assets('css'))) manifest.add(self.entry_cls('__custom.js', get_custom_assets('js'))) webpack = FlaskWebpackExt()
""" Views and functions for serving static files. These are only to be used during development, and SHOULD NOT be used in a production setting. """ from __future__ import unicode_literals import mimetypes import os import posixpath import re import stat from django.http import ( FileResponse, Http404, HttpResponse, HttpResponseNotModified, HttpResponseRedirect, ) from django.template import Context, Engine, TemplateDoesNotExist, loader from django.utils.http import http_date, parse_http_date from django.utils.six.moves.urllib.parse import unquote from django.utils.translation import ugettext as _, ugettext_lazy def serve(request, path, document_root=None, show_indexes=False): """ Serve static files below a given point in the directory structure. To use, put a URL pattern such as:: from django.views.static import serve url(r'^(?P<path>.*)$', serve, {'document_root': '/path/to/my/files/'}) in your URLconf. You must provide the ``document_root`` param. You may also set ``show_indexes`` to ``True`` if you'd like to serve a basic index of the directory. This index view will use the template hardcoded below, but if you'd like to override it, you can create a template called ``static/directory_index.html``. """ path = posixpath.normpath(unquote(path)) path = path.lstrip('/') newpath = '' for part in path.split('/'): if not part: # Strip empty path components. continue drive, part = os.path.splitdrive(part) head, part = os.path.split(part) if part in (os.curdir, os.pardir): # Strip '.' and '..' in path. continue newpath = os.path.join(newpath, part).replace('\\', '/') if newpath and path != newpath: return HttpResponseRedirect(newpath) fullpath = os.path.join(document_root, newpath) if os.path.isdir(fullpath): if show_indexes: return directory_index(newpath, fullpath) raise Http404(_("Directory indexes are not allowed here.")) if not os.path.exists(fullpath): raise Http404(_('"%(path)s" does not exist') % {'path': fullpath}) # Respect the If-Modified-Since header. statobj = os.stat(fullpath) if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'), statobj.st_mtime, statobj.st_size): return HttpResponseNotModified() content_type, encoding = mimetypes.guess_type(fullpath) content_type = content_type or 'application/octet-stream' response = FileResponse(open(fullpath, 'rb'), content_type=content_type) response["Last-Modified"] = http_date(statobj.st_mtime) if stat.S_ISREG(statobj.st_mode): response["Content-Length"] = statobj.st_size if encoding: response["Content-Encoding"] = encoding return response DEFAULT_DIRECTORY_INDEX_TEMPLATE = """ {% load i18n %} <!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-type" content="text/html; charset=utf-8" /> <meta http-equiv="Content-Language" content="en-us" /> <meta name="robots" content="NONE,NOARCHIVE" /> <title>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</title> </head> <body> <h1>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</h1> <ul> {% ifnotequal directory "/" %} <li><a href="../">../</a></li> {% endifnotequal %} {% for f in file_list %} <li><a href="{{ f|urlencode }}">{{ f }}</a></li> {% endfor %} </ul> </body> </html> """ template_translatable = ugettext_lazy("Index of %(directory)s") def directory_index(path, fullpath): try: t = loader.select_template([ 'static/directory_index.html', 'static/directory_index', ]) except TemplateDoesNotExist: t = Engine().from_string(DEFAULT_DIRECTORY_INDEX_TEMPLATE) files = [] for f in os.listdir(fullpath): if not f.startswith('.'): if os.path.isdir(os.path.join(fullpath, f)): f += '/' files.append(f) c = Context({ 'directory': path + '/', 'file_list': files, }) return HttpResponse(t.render(c)) def was_modified_since(header=None, mtime=0, size=0): """ Was something modified since the user last downloaded it? header This is the value of the If-Modified-Since header. If this is None, I'll just return True. mtime This is the modification time of the item we're talking about. size This is the size of the item we're talking about. """ try: if header is None: raise ValueError matches = re.match(r"^([^;]+)(; length=([0-9]+))?$", header, re.IGNORECASE) header_mtime = parse_http_date(matches.group(1)) header_len = matches.group(3) if header_len and int(header_len) != size: raise ValueError if int(mtime) > header_mtime: raise ValueError except (AttributeError, ValueError, OverflowError): return True return False
#!/usr/bin/env python # Copyright (c) 2012 The Chromium Authors. All rights reserved. # Use of this source code is governed by a BSD-style license that can be # found in the LICENSE file. '''Unit tests for grit.tclib''' import sys import os.path if __name__ == '__main__': sys.path.append(os.path.join(os.path.dirname(__file__), '..')) import types import unittest from grit import tclib from grit import exception import grit.extern.tclib class TclibUnittest(unittest.TestCase): def testInit(self): msg = tclib.Message(text=u'Hello Earthlings', description='Greetings\n\t message') self.failUnlessEqual(msg.GetPresentableContent(), 'Hello Earthlings') self.failUnless(isinstance(msg.GetPresentableContent(), types.StringTypes)) self.failUnlessEqual(msg.GetDescription(), 'Greetings message') def testGetAttr(self): msg = tclib.Message() msg.AppendText(u'Hello') # Tests __getattr__ self.failUnless(msg.GetPresentableContent() == 'Hello') self.failUnless(isinstance(msg.GetPresentableContent(), types.StringTypes)) def testAll(self): text = u'Howdie USERNAME' phs = [tclib.Placeholder(u'USERNAME', u'%s', 'Joi')] msg = tclib.Message(text=text, placeholders=phs) self.failUnless(msg.GetPresentableContent() == 'Howdie USERNAME') trans = tclib.Translation(text=text, placeholders=phs) self.failUnless(trans.GetPresentableContent() == 'Howdie USERNAME') self.failUnless(isinstance(trans.GetPresentableContent(), types.StringTypes)) def testUnicodeReturn(self): text = u'\u00fe' msg = tclib.Message(text=text) self.failUnless(msg.GetPresentableContent() == text) from_list = msg.GetContent()[0] self.failUnless(from_list == text) def testRegressionTranslationInherited(self): '''Regression tests a bug that was caused by grit.tclib.Translation inheriting from the translation console's Translation object instead of only owning an instance of it. ''' msg = tclib.Message(text=u"BLA1\r\nFrom: BLA2 \u00fe BLA3", placeholders=[ tclib.Placeholder('BLA1', '%s', '%s'), tclib.Placeholder('BLA2', '%s', '%s'), tclib.Placeholder('BLA3', '%s', '%s')]) transl = tclib.Translation(text=msg.GetPresentableContent(), placeholders=msg.GetPlaceholders()) content = transl.GetContent() self.failUnless(isinstance(content[3], types.UnicodeType)) def testFingerprint(self): # This has Windows line endings. That is on purpose. id = grit.extern.tclib.GenerateMessageId( 'Google Desktop for Enterprise\r\n' 'Copyright (C) 2006 Google Inc.\r\n' 'All Rights Reserved\r\n' '\r\n' '---------\r\n' 'Contents\r\n' '---------\r\n' 'This distribution contains the following files:\r\n' '\r\n' 'GoogleDesktopSetup.msi - Installation and setup program\r\n' 'GoogleDesktop.adm - Group Policy administrative template file\r\n' 'AdminGuide.pdf - Google Desktop for Enterprise administrative guide\r\n' '\r\n' '\r\n' '--------------\r\n' 'Documentation\r\n' '--------------\r\n' 'Full documentation and installation instructions are in the \r\n' 'administrative guide, and also online at \r\n' 'http://desktop.google.com/enterprise/adminguide.html.\r\n' '\r\n' '\r\n' '------------------------\r\n' 'IBM Lotus Notes Plug-In\r\n' '------------------------\r\n' 'The Lotus Notes plug-in is included in the release of Google \r\n' 'Desktop for Enterprise. The IBM Lotus Notes Plug-in for Google \r\n' 'Desktop indexes mail, calendar, task, contact and journal \r\n' 'documents from Notes. Discussion documents including those from \r\n' 'the discussion and team room templates can also be indexed by \r\n' 'selecting an option from the preferences. Once indexed, this data\r\n' 'will be returned in Google Desktop searches. The corresponding\r\n' 'document can be opened in Lotus Notes from the Google Desktop \r\n' 'results page.\r\n' '\r\n' 'Install: The plug-in will install automatically during the Google \r\n' 'Desktop setup process if Lotus Notes is already installed. Lotus \r\n' 'Notes must not be running in order for the install to occur. \r\n' '\r\n' 'Preferences: Preferences and selection of databases to index are\r\n' 'set in the \'Google Desktop for Notes\' dialog reached through the \r\n' '\'Actions\' menu.\r\n' '\r\n' 'Reindexing: Selecting \'Reindex all databases\' will index all the \r\n' 'documents in each database again.\r\n' '\r\n' '\r\n' 'Notes Plug-in Known Issues\r\n' '---------------------------\r\n' '\r\n' 'If the \'Google Desktop for Notes\' item is not available from the \r\n' 'Lotus Notes Actions menu, then installation was not successful. \r\n' 'Installation consists of writing one file, notesgdsplugin.dll, to \r\n' 'the Notes application directory and a setting to the notes.ini \r\n' 'configuration file. The most likely cause of an unsuccessful \r\n' 'installation is that the installer was not able to locate the \r\n' 'notes.ini file. Installation will complete if the user closes Notes\r\n' 'and manually adds the following setting to this file on a new line:\r\n' 'AddinMenus=notegdsplugin.dll\r\n' '\r\n' 'If the notesgdsplugin.dll file is not in the application directory\r\n' '(e.g., C:\Program Files\Lotus\Notes) after Google Desktop \r\n' 'installation, it is likely that Notes was not installed correctly. \r\n' '\r\n' 'Only local databases can be indexed. If they can be determined, \r\n' 'the user\'s local mail file and address book will be included in the\r\n' 'list automatically. Mail archives and other databases must be \r\n' 'added with the \'Add\' button.\r\n' '\r\n' 'Some users may experience performance issues during the initial \r\n' 'indexing of a database. The \'Perform the initial index of a \r\n' 'database only when I\'m idle\' option will limit the indexing process\r\n' 'to times when the user is not using the machine. If this does not \r\n' 'alleviate the problem or the user would like to continually index \r\n' 'but just do so more slowly or quickly, the GoogleWaitTime notes.ini\r\n' 'value can be set. Increasing the GoogleWaitTime value will slow \r\n' 'down the indexing process, and lowering the value will speed it up.\r\n' 'A value of zero causes the fastest possible indexing. Removing the\r\n' 'ini parameter altogether returns it to the default (20).\r\n' '\r\n' 'Crashes have been known to occur with certain types of history \r\n' 'bookmarks. If the Notes client seems to crash randomly, try \r\n' 'disabling the \'Index note history\' option. If it crashes before,\r\n' 'you can get to the preferences, add the following line to your \r\n' 'notes.ini file:\r\n' 'GDSNoIndexHistory=1\r\n') self.failUnless(id == '8961534701379422820') def testPlaceholderNameChecking(self): try: ph = tclib.Placeholder('BINGO BONGO', 'bla', 'bla') raise Exception("We shouldn't get here") except exception.InvalidPlaceholderName: pass # Expect exception to be thrown because presentation contained space def testTagsWithCommonSubstring(self): word = 'ABCDEFGHIJ' text = ' '.join([word[:i] for i in range(1, 11)]) phs = [tclib.Placeholder(word[:i], str(i), str(i)) for i in range(1, 11)] try: msg = tclib.Message(text=text, placeholders=phs) self.failUnless(msg.GetRealContent() == '1 2 3 4 5 6 7 8 9 10') except: self.fail('tclib.Message() should handle placeholders that are ' 'substrings of each other') if __name__ == '__main__': unittest.main()
import terminal import curses import time from curses import panel class InfoContainer(object): def __init__(self, stdscreen, title, debug_console): self.debug_console = debug_console self.height = int(terminal.height/2) self.width = terminal.width - 2 self.title = title self.window = stdscreen.subwin(self.height,self.width,1,1) self.window.border(0) self.window.addstr(0,1,title) self.panel = panel.new_panel(self.window) self.panel.hide() panel.update_panels() # Add the Border self.second = time.time() self.writebuffer = [] def display(self): self.panel.top() self.panel.show() #self.window.clear() def hide(self): self.window.clear() self.panel.hide() panel.update_panels() curses.doupdate() def refresh(self): self.window.clear() self.window.border(0) self.window.addstr(0,1,self.title) #draw the last 20 log items #foreach i from 0 till (self.height-2) #draw string on i place #self.writebuffer[-(self.height-2):] maxlength = (self.height-3) lengthofbuffer = len(self.writebuffer) if(lengthofbuffer>maxlength): startindex = (lengthofbuffer-1)-maxlength else: startindex = 0 maxlength = lengthofbuffer for i in range(0, maxlength): #self.window.addstr(i,1,str(i)) self.window.addstr(i+1,1,self.writebuffer[i+startindex]) self.window.refresh() curses.doupdate() def addPacket(self, packet): #1 refresh per second// or 2? if(time.time() - self.second >= 1): self.writebuffer.append(packet) self.second = time.time() else: self.writebuffer.append(packet)
#!/usr/bin/env python """Node Server Example This example demonstrates how to create a very simple node server that supports bi-diractional messaging between server and connected clients forming a cluster of nodes. """ from __future__ import print_function from os import getpid from optparse import OptionParser from circuits.node import Node from circuits import Component, Debugger __version__ = "0.0.1" USAGE = "%prog [options]" VERSION = "%prog v" + __version__ def parse_options(): parser = OptionParser(usage=USAGE, version=VERSION) parser.add_option( "-b", "--bind", action="store", type="string", default="0.0.0.0:8000", dest="bind", help="Bind to address:[port]" ) parser.add_option( "-d", "--debug", action="store_true", default=False, dest="debug", help="Enable debug mode" ) opts, args = parser.parse_args() return opts, args class NodeServer(Component): def init(self, args, opts): """Initialize our ``ChatServer`` Component. This uses the convenience ``init`` method which is called after the component is proeprly constructed and initialized and passed the same args and kwargs that were passed during construction. """ self.args = args self.opts = opts self.clients = {} if opts.debug: Debugger().register(self) if ":" in opts.bind: address, port = opts.bind.split(":") port = int(port) else: address, port = opts.bind, 8000 Node(port=port, server_ip=address).register(self) def connect(self, sock, host, port): """Connect Event -- Triggered for new connecting clients""" self.clients[sock] = { "host": sock, "port": port, } def disconnect(self, sock): """Disconnect Event -- Triggered for disconnecting clients""" if sock not in self.clients: return del self.clients[sock] def ready(self, server, bind): print("Ready! Listening on {}:{}".format(*bind)) print("Waiting for remote events...") def hello(self): return "Hello World! ({0:d})".format(getpid()) def main(): opts, args = parse_options() # Configure and "run" the System. NodeServer(args, opts).run() if __name__ == "__main__": main()
#!/usr/bin/env python # Copyright (c) 2012 The Chromium Authors. All rights reserved. # Use of this source code is governed by a BSD-style license that can be # found in the LICENSE file. # This script is used to copy all dependencies into the local directory. # The package of files can then be uploaded to App Engine. import os import shutil import stat import sys SRC_DIR = os.path.join(sys.path[0], os.pardir, os.pardir, os.pardir, os.pardir, os.pardir) THIRD_PARTY_DIR = os.path.join(SRC_DIR, 'third_party') LOCAL_THIRD_PARTY_DIR = os.path.join(sys.path[0], 'third_party') TOOLS_DIR = os.path.join(SRC_DIR, 'tools') SCHEMA_COMPILER_FILES = ['memoize.py', 'model.py', 'idl_schema.py', 'schema_util.py', 'json_parse.py', 'json_schema.py'] def MakeInit(path): path = os.path.join(path, '__init__.py') with open(os.path.join(path), 'w') as f: os.utime(os.path.join(path), None) def OnError(function, path, excinfo): os.chmod(path, stat.S_IWUSR) function(path) def CopyThirdParty(src, dest, files=None, make_init=True): dest_path = os.path.join(LOCAL_THIRD_PARTY_DIR, dest) if not files: shutil.copytree(src, dest_path) if make_init: MakeInit(dest_path) return try: os.makedirs(dest_path) except Exception: pass if make_init: MakeInit(dest_path) for filename in files: shutil.copy(os.path.join(src, filename), os.path.join(dest_path, filename)) def main(): if os.path.isdir(LOCAL_THIRD_PARTY_DIR): try: shutil.rmtree(LOCAL_THIRD_PARTY_DIR, False, OnError) except OSError: print('*-------------------------------------------------------------*\n' '| If you are receiving an upload error, try removing |\n' '| chrome/common/extensions/docs/server2/third_party manually. |\n' '*-------------------------------------------------------------*\n') CopyThirdParty(os.path.join(THIRD_PARTY_DIR, 'motemplate'), 'motemplate') CopyThirdParty(os.path.join(THIRD_PARTY_DIR, 'markdown'), 'markdown', make_init=False) CopyThirdParty(os.path.join(SRC_DIR, 'ppapi', 'generators'), 'json_schema_compiler') CopyThirdParty(os.path.join(THIRD_PARTY_DIR, 'ply'), os.path.join('json_schema_compiler', 'ply')) CopyThirdParty(os.path.join(TOOLS_DIR, 'json_schema_compiler'), 'json_schema_compiler', SCHEMA_COMPILER_FILES) CopyThirdParty(os.path.join(TOOLS_DIR, 'json_comment_eater'), 'json_schema_compiler', ['json_comment_eater.py']) CopyThirdParty(os.path.join(THIRD_PARTY_DIR, 'simplejson'), os.path.join('json_schema_compiler', 'simplejson'), make_init=False) MakeInit(LOCAL_THIRD_PARTY_DIR) CopyThirdParty(os.path.join(THIRD_PARTY_DIR, 'google_appengine_cloudstorage', 'cloudstorage'), 'cloudstorage') # To be able to use the Motemplate class we need this import in __init__.py. with open(os.path.join(LOCAL_THIRD_PARTY_DIR, 'motemplate', '__init__.py'), 'a') as f: f.write('from motemplate import Motemplate\n') if __name__ == '__main__': main()
# -*- coding: utf-8 -*- ############################################################################## # # Copyright (c) 2008 JAILLET Simon - CrysaLEAD - www.crysalead.fr # # WARNING: This program as such is intended to be used by professional # programmers who take the whole responsability of assessing all potential # consequences resulting from its eventual inadequacies and bugs # End users who are looking for a ready-to-use solution with commercial # garantees and support are strongly adviced to contract a Free Software # Service Company # # This program is Free Software; you can redistribute it and/or # modify it under the terms of the GNU General Public License # as published by the Free Software Foundation; either version 2 # of the License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program; if not, write to the Free Software # Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA # ############################################################################## import l10n_fr import report import wizard # vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
# Program to show the maps of RMSE averaged over time import matplotlib.pyplot as plt from sklearn.metrics import mean_squared_error import os from netCDF4 import Dataset as NetCDFFile import numpy as np from CCLM_OUTS import Plot_CCLM # option == 1 -> shift 4 with default cclm domain and nboundlines = 3 # option == 2 -> shift 4 with smaller cclm domain and nboundlines = 3 # option == 3 -> shift 4 with smaller cclm domain and nboundlines = 6 # option == 4 -> shift 4 with corrected smaller cclm domain and nboundlines = 3 # option == 5 -> shift 4 with corrected smaller cclm domain and nboundlines = 4 # option == 6 -> shift 4 with corrected smaller cclm domain and nboundlines = 6 # option == 7 -> shift 4 with corrected smaller cclm domain and nboundlines = 9 # option == 8 -> shift 4 with corrected bigger cclm domain and nboundlines = 3 from CCLM_OUTS import Plot_CCLM #def f(x): # if x==-9999: # return float('NaN') # else: # return x def read_data_from_mistral(dir='/work/bb1029/b324045/work1/work/member/post/',name='member_T_2M_ts_seasmean.nc',var='T_2M'): # type: (object, object, object) -> object #a function to read the data from mistral work """ :rtype: object """ #CMD = 'scp $mistral:' + dir + name + ' ./' CMD = 'wget users.met.fu-berlin.de/~BijanFallah/' + dir + name os.system(CMD) nc = NetCDFFile(name) # for name2, variable in nc.variables.items(): # for attrname in variable.ncattrs(): # print(name2, variable, '-----------------',attrname) # #print("{} -- {}".format(attrname, getattr(variable, attrname))) os.remove(name) lats = nc.variables['lat'][:] lons = nc.variables['lon'][:] t = nc.variables[var][:].squeeze() rlats = nc.variables['rlat'][:] # extract/copy the data rlons = nc.variables['rlon'][:] #f2 = np.vectorize(f) #t= f2(t) #t=t.data t=t.squeeze() #print() nc.close() return(t, lats, lons, rlats, rlons)
#!/usr/bin/env python # Copyright 2013 Brett Slatkin # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. """App Engine configuration file. See: https://developers.google.com/appengine/docs/python/tools/appengineconfig """ import os import logging import os import sys # Log to disk for managed VMs: # https://cloud.google.com/appengine/docs/managed-vms/custom-runtimes#logging if os.environ.get('LOG_TO_DISK'): log_dir = '/var/log/app_engine/custom_logs' try: os.makedirs(log_dir) except OSError: pass # Directory already exists log_path = os.path.join(log_dir, 'app.log') handler = logging.FileHandler(log_path) handler.setLevel(logging.DEBUG) handler.setFormatter(logging.Formatter( '%(levelname)s %(filename)s:%(lineno)s] %(message)s')) logging.getLogger().addHandler(handler) # Load up our app and all its dependencies. Make the environment sane. from dpxdt.tools import run_server # Initialize flags from flags file or enviornment. import gflags gflags.FLAGS(['dpxdt_server', '--flagfile=flags.cfg']) logging.info('BEGIN Flags') for key, flag in gflags.FLAGS.FlagDict().iteritems(): logging.info('%s = %s', key, flag.value) logging.info('END Flags') # When in production use precompiled templates. Sometimes templates break # in production. To debug templates there, comment this out entirely. if os.environ.get('SERVER_SOFTWARE', '').startswith('Google App Engine'): import jinja2 from dpxdt.server import app app.jinja_env.auto_reload = False app.jinja_env.loader = jinja2.ModuleLoader('templates_compiled.zip') # Install dpxdt.server override hooks. from dpxdt.server import api import hooks api._artifact_created = hooks._artifact_created api._get_artifact_response = hooks._get_artifact_response # Don't log when appstats is active. appstats_DUMP_LEVEL = -1 # SQLAlchemy stacks are really deep. appstats_MAX_STACK = 20 # Use very shallow local variable reprs to reduce noise. appstats_MAX_DEPTH = 2 # Enable the remote shell, since the old admin interactive console doesn't # work with managed VMs. appstats_SHELL_OK = True # These are only used if gae_mini_profiler was properly installed def gae_mini_profiler_should_profile_production(): from google.appengine.api import users return users.is_current_user_admin() def gae_mini_profiler_should_profile_development(): return True # Fix the appstats module's formatting helper function. import appstats_monkey_patch
# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors # License: GNU General Public License v3. See license.txt from __future__ import unicode_literals import frappe from frappe import _ def execute(filters=None): columns = get_columns() proj_details = get_project_details() pr_item_map = get_purchased_items_cost() se_item_map = get_issued_items_cost() dn_item_map = get_delivered_items_cost() data = [] for project in proj_details: data.append([project.name, pr_item_map.get(project.name, 0), se_item_map.get(project.name, 0), dn_item_map.get(project.name, 0), project.project_name, project.status, project.company, project.customer, project.estimated_costing, project.expected_start_date, project.expected_end_date]) return columns, data def get_columns(): return [_("Project Id") + ":Link/Project:140", _("Cost of Purchased Items") + ":Currency:160", _("Cost of Issued Items") + ":Currency:160", _("Cost of Delivered Items") + ":Currency:160", _("Project Name") + "::120", _("Project Status") + "::120", _("Company") + ":Link/Company:100", _("Customer") + ":Link/Customer:140", _("Project Value") + ":Currency:120", _("Project Start Date") + ":Date:120", _("Completion Date") + ":Date:120"] def get_project_details(): return frappe.db.sql(""" select name, project_name, status, company, customer, estimated_costing, expected_start_date, expected_end_date from tabProject where docstatus < 2""", as_dict=1) def get_purchased_items_cost(): pr_items = frappe.db.sql("""select project_name, sum(base_net_amount) as amount from `tabPurchase Receipt Item` where ifnull(project_name, '') != '' and docstatus = 1 group by project_name""", as_dict=1) pr_item_map = {} for item in pr_items: pr_item_map.setdefault(item.project_name, item.amount) return pr_item_map def get_issued_items_cost(): se_items = frappe.db.sql("""select se.project_name, sum(se_item.amount) as amount from `tabStock Entry` se, `tabStock Entry Detail` se_item where se.name = se_item.parent and se.docstatus = 1 and ifnull(se_item.t_warehouse, '') = '' and ifnull(se.project_name, '') != '' group by se.project_name""", as_dict=1) se_item_map = {} for item in se_items: se_item_map.setdefault(item.project_name, item.amount) return se_item_map def get_delivered_items_cost(): dn_items = frappe.db.sql("""select dn.project_name, sum(dn_item.base_net_amount) as amount from `tabDelivery Note` dn, `tabDelivery Note Item` dn_item where dn.name = dn_item.parent and dn.docstatus = 1 and ifnull(dn.project_name, '') != '' group by dn.project_name""", as_dict=1) si_items = frappe.db.sql("""select si.project_name, sum(si_item.base_net_amount) as amount from `tabSales Invoice` si, `tabSales Invoice Item` si_item where si.name = si_item.parent and si.docstatus = 1 and ifnull(si.update_stock, 0) = 1 and ifnull(si.is_pos, 0) = 1 and ifnull(si.project_name, '') != '' group by si.project_name""", as_dict=1) dn_item_map = {} for item in dn_items: dn_item_map.setdefault(item.project_name, item.amount) for item in si_items: dn_item_map.setdefault(item.project_name, item.amount) return dn_item_map
""" 13. Adding hooks before/after saving and deleting To execute arbitrary code around ``save()`` and ``delete()``, just subclass the methods. """ from __future__ import unicode_literals from django.db import models from django.utils.encoding import python_2_unicode_compatible @python_2_unicode_compatible class Person(models.Model): first_name = models.CharField(max_length=20) last_name = models.CharField(max_length=20) def __init__(self, *args, **kwargs): super(Person, self).__init__(*args, **kwargs) self.data = [] def __str__(self): return "%s %s" % (self.first_name, self.last_name) def save(self, *args, **kwargs): self.data.append("Before save") # Call the "real" save() method super(Person, self).save(*args, **kwargs) self.data.append("After save") def delete(self): self.data.append("Before deletion") # Call the "real" delete() method super(Person, self).delete() self.data.append("After deletion")
from copy import deepcopy from django.core.checks.templates import E001 from django.test import SimpleTestCase from django.test.utils import override_settings class CheckTemplateSettingsAppDirsTest(SimpleTestCase): TEMPLATES_APP_DIRS_AND_LOADERS = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'APP_DIRS': True, 'OPTIONS': { 'loaders': ['django.template.loaders.filesystem.Loader'], }, }, ] @property def func(self): from django.core.checks.templates import check_setting_app_dirs_loaders return check_setting_app_dirs_loaders @override_settings(TEMPLATES=TEMPLATES_APP_DIRS_AND_LOADERS) def test_app_dirs_and_loaders(self): """ Error if template loaders are specified and APP_DIRS is True. """ self.assertEqual(self.func(None), [E001]) def test_app_dirs_removed(self): TEMPLATES = deepcopy(self.TEMPLATES_APP_DIRS_AND_LOADERS) del TEMPLATES[0]['APP_DIRS'] with self.settings(TEMPLATES=TEMPLATES): self.assertEqual(self.func(None), []) def test_loaders_removed(self): TEMPLATES = deepcopy(self.TEMPLATES_APP_DIRS_AND_LOADERS) del TEMPLATES[0]['OPTIONS']['loaders'] with self.settings(TEMPLATES=TEMPLATES): self.assertEqual(self.func(None), [])
# -*- coding: utf-8 -*- # # This file is part of Invenio. # Copyright (C) 2013, 2014 CERN. # # Invenio is free software; you can redistribute it and/or # modify it under the terms of the GNU General Public License as # published by the Free Software Foundation; either version 2 of the # License, or (at your option) any later version. # # Invenio is distributed in the hope that it will be useful, but # WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU # General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Invenio; if not, write to the Free Software Foundation, Inc., # 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA. """Documentation Flask Blueprint.""" import os from flask import render_template, current_app, abort, url_for, Blueprint from flask.helpers import send_from_directory from werkzeug.utils import cached_property, import_string from sphinx.websupport import WebSupport from sphinx.websupport.errors import DocumentNotFoundError from invenio.base.globals import cfg from invenio.base.i18n import _ from flask.ext.breadcrumbs import (default_breadcrumb_root, register_breadcrumb, current_breadcrumbs) from flask.ext.menu import register_menu class DocsBlueprint(Blueprint): """Wrap blueprint with Sphinx ``WebSupport``.""" @cached_property def documentation_package(self): """Return documentation package.""" try: invenio_docs = import_string(cfg['DOCUMENTATION_PACKAGE']) except ImportError: import docs as invenio_docs return invenio_docs @cached_property def support(self): """Return an instance of Sphinx ``WebSupport``.""" builddir = os.path.abspath(os.path.join( current_app.instance_path, 'docs')) return WebSupport( srcdir=self.documentation_package.__path__[0], builddir=builddir, staticroot=os.path.join(blueprint.url_prefix, 'static'), docroot=blueprint.url_prefix ) def send_static_file(self, filename): """Return static file.""" try: return super(self.__class__, self).send_static_file(filename) except: cache_timeout = self.get_send_file_max_age(filename) return send_from_directory( os.path.join(current_app.instance_path, "docs", "static"), filename, cache_timeout=cache_timeout) blueprint = DocsBlueprint('documentation', __name__, url_prefix="/documentation", template_folder='templates', static_folder='static') default_breadcrumb_root(blueprint, '.documentation') @blueprint.route('/', strict_slashes=True) @blueprint.route('/<path:docname>') @register_menu(blueprint, 'main.documentation', _('Help'), order=99) @register_breadcrumb(blueprint, '.', _('Help')) def index(docname=None): """Render documentation page.""" try: document = blueprint.support.get_document( docname or cfg["DOCUMENTATION_INDEX"]) except DocumentNotFoundError: abort(404) additional_breadcrumbs = [{'text': document['title'], 'url': url_for('.index', docname=docname)}] return render_template( 'documentation/index.html', document=document, breadcrumbs=current_breadcrumbs + additional_breadcrumbs)
#!/usr/bin/env python # # Copyright 2014 The Chromium Authors. All rights reserved. # Use of this source code is governed by a BSD-style license that can be # found in the LICENSE file. """ Concatenates module scripts based on the module.json descriptor. Optionally, minifies the result using rjsmin. """ from cStringIO import StringIO from os import path import os import re import sys try: import simplejson as json except ImportError: import json rjsmin_path = path.abspath(path.join( path.dirname(__file__), '..', '..', 'build', 'scripts')) sys.path.append(rjsmin_path) import rjsmin def read_file(filename): with open(path.normpath(filename), 'rt') as file: return file.read() def write_file(filename, content): # This is here to avoid overwriting source tree files due to hard links. if path.exists(filename): os.remove(filename) with open(filename, 'wt') as file: file.write(content) def concatenate_scripts(file_names, module_dir, output_dir, output): for file_name in file_names: output.write('/* %s */\n' % file_name) file_path = path.join(module_dir, file_name) # This allows to also concatenate generated files found in output_dir. if not path.isfile(file_path): file_path = path.join(output_dir, path.basename(module_dir), file_name) output.write(read_file(file_path)) output.write(';') def main(argv): if len(argv) < 3: print('Usage: %s module_json output_file no_minify' % argv[0]) return 1 module_json_file_name = argv[1] output_file_name = argv[2] no_minify = len(argv) > 3 and argv[3] module_dir = path.dirname(module_json_file_name) output = StringIO() descriptor = None try: descriptor = json.loads(read_file(module_json_file_name)) except: print 'ERROR: Failed to load JSON from ' + module_json_file_name raise # pylint: disable=E1103 scripts = descriptor.get('scripts') assert(scripts) output_root_dir = path.join(path.dirname(output_file_name), '..') concatenate_scripts(scripts, module_dir, output_root_dir, output) output_script = output.getvalue() output.close() write_file(output_file_name, output_script if no_minify else rjsmin.jsmin(output_script)) if __name__ == '__main__': sys.exit(main(sys.argv))
# -*- coding: utf-8 -*- """ Pelican Mathjax Markdown Extension ================================== An extension for the Python Markdown module that enables the Pelican python blog to process mathjax. This extension gives Pelican the ability to use Mathjax as a "first class citizen" of the blog """ import markdown from markdown.util import etree from markdown.util import AtomicString class PelicanMathJaxPattern(markdown.inlinepatterns.Pattern): """Inline markdown processing that matches mathjax""" def __init__(self, pelican_mathjax_extension, tag, pattern): super(PelicanMathJaxPattern,self).__init__(pattern) self.math_tag_class = pelican_mathjax_extension.getConfig('math_tag_class') self.pelican_mathjax_extension = pelican_mathjax_extension self.tag = tag def handleMatch(self, m): node = markdown.util.etree.Element(self.tag) node.set('class', self.math_tag_class) prefix = '\\(' if m.group('prefix') == '$' else m.group('prefix') suffix = '\\)' if m.group('suffix') == '$' else m.group('suffix') node.text = markdown.util.AtomicString(prefix + m.group('math') + suffix) # If mathjax was successfully matched, then JavaScript needs to be added # for rendering. The boolean below indicates this self.pelican_mathjax_extension.mathjax_needed = True return node class PelicanMathJaxCorrectDisplayMath(markdown.treeprocessors.Treeprocessor): """Corrects invalid html that results from a <div> being put inside a <p> for displayed math""" def __init__(self, pelican_mathjax_extension): self.pelican_mathjax_extension = pelican_mathjax_extension def correct_html(self, root, children, div_math, insert_idx, text): """Separates out <div class="math"> from the parent tag <p>. Anything in between is put into its own parent tag of <p>""" current_idx = 0 for idx in div_math: el = markdown.util.etree.Element('p') el.text = text el.extend(children[current_idx:idx]) # Test to ensure that empty <p> is not inserted if len(el) != 0 or (el.text and not el.text.isspace()): root.insert(insert_idx, el) insert_idx += 1 text = children[idx].tail children[idx].tail = None root.insert(insert_idx, children[idx]) insert_idx += 1 current_idx = idx+1 el = markdown.util.etree.Element('p') el.text = text el.extend(children[current_idx:]) if len(el) != 0 or (el.text and not el.text.isspace()): root.insert(insert_idx, el) def run(self, root): """Searches for <div class="math"> that are children in <p> tags and corrects the invalid HTML that results""" math_tag_class = self.pelican_mathjax_extension.getConfig('math_tag_class') for parent in root: div_math = [] children = list(parent) for div in parent.findall('div'): if div.get('class') == math_tag_class: div_math.append(children.index(div)) # Do not process further if no displayed math has been found if not div_math: continue insert_idx = list(root).index(parent) self.correct_html(root, children, div_math, insert_idx, parent.text) root.remove(parent) # Parent must be removed last for correct insertion index return root class PelicanMathJaxAddJavaScript(markdown.treeprocessors.Treeprocessor): """Tree Processor for adding Mathjax JavaScript to the blog""" def __init__(self, pelican_mathjax_extension): self.pelican_mathjax_extension = pelican_mathjax_extension def run(self, root): # If no mathjax was present, then exit if (not self.pelican_mathjax_extension.mathjax_needed): return root # Add the mathjax script to the html document mathjax_script = etree.Element('script') mathjax_script.set('type','text/javascript') mathjax_script.text = AtomicString(self.pelican_mathjax_extension.getConfig('mathjax_script')) root.append(mathjax_script) # Reset the boolean switch to false so that script is only added # to other pages if needed self.pelican_mathjax_extension.mathjax_needed = False return root class PelicanMathJaxExtension(markdown.Extension): """A markdown extension enabling mathjax processing in Markdown for Pelican""" def __init__(self, config): try: # Needed for markdown versions >= 2.5 self.config['mathjax_script'] = ['', 'Mathjax JavaScript script'] self.config['math_tag_class'] = ['math', 'The class of the tag in which mathematics is wrapped'] self.config['auto_insert'] = [True, 'Determines if mathjax script is automatically inserted into content'] super(PelicanMathJaxExtension,self).__init__(**config) except AttributeError: # Markdown versions < 2.5 config['mathjax_script'] = [config['mathjax_script'], 'Mathjax JavaScript script'] config['math_tag_class'] = [config['math_tag_class'], 'The class of the tag in which mathematic is wrapped'] config['auto_insert'] = [config['auto_insert'], 'Determines if mathjax script is automatically inserted into content'] super(PelicanMathJaxExtension,self).__init__(config) # Used as a flag to determine if javascript # needs to be injected into a document self.mathjax_needed = False def extendMarkdown(self, md, md_globals): # Regex to detect mathjax mathjax_inline_regex = r'(?P<prefix>\$)(?P<math>.+?)(?P<suffix>(?<!\s)\2)' mathjax_display_regex = r'(?P<prefix>\$\$|\\begin\{(.+?)\})(?P<math>.+?)(?P<suffix>\2|\\end\{\3\})' # Process mathjax before escapes are processed since escape processing will # intefer with mathjax. The order in which the displayed and inlined math # is registered below matters md.inlinePatterns.add('mathjax_displayed', PelicanMathJaxPattern(self, 'div', mathjax_display_regex), '<escape') md.inlinePatterns.add('mathjax_inlined', PelicanMathJaxPattern(self, 'span', mathjax_inline_regex), '<escape') # Correct the invalid HTML that results from teh displayed math (<div> tag within a <p> tag) md.treeprocessors.add('mathjax_correctdisplayedmath', PelicanMathJaxCorrectDisplayMath(self), '>inline') # If necessary, add the JavaScript Mathjax library to the document. This must # be last in the ordered dict (hence it is given the position '_end') if self.getConfig('auto_insert'): md.treeprocessors.add('mathjax_addjavascript', PelicanMathJaxAddJavaScript(self), '_end')
""" ========================================================= Multi-dimensional image processing (:mod:`scipy.ndimage`) ========================================================= .. currentmodule:: scipy.ndimage This package contains various functions for multi-dimensional image processing. Filters :mod:`scipy.ndimage.filters` ==================================== .. module:: scipy.ndimage.filters .. autosummary:: :toctree: generated/ convolve - Multi-dimensional convolution convolve1d - 1-D convolution along the given axis correlate - Multi-dimensional correlation correlate1d - 1-D correlation along the given axis gaussian_filter gaussian_filter1d gaussian_gradient_magnitude gaussian_laplace generic_filter - Multi-dimensional filter using a given function generic_filter1d - 1-D generic filter along the given axis generic_gradient_magnitude generic_laplace laplace - n-D Laplace filter based on approximate second derivatives maximum_filter maximum_filter1d median_filter - Calculates a multi-dimensional median filter minimum_filter minimum_filter1d percentile_filter - Calculates a multi-dimensional percentile filter prewitt rank_filter - Calculates a multi-dimensional rank filter sobel uniform_filter - Multi-dimensional uniform filter uniform_filter1d - 1-D uniform filter along the given axis Fourier filters :mod:`scipy.ndimage.fourier` ============================================ .. module:: scipy.ndimage.fourier .. autosummary:: :toctree: generated/ fourier_ellipsoid fourier_gaussian fourier_shift fourier_uniform Interpolation :mod:`scipy.ndimage.interpolation` ================================================ .. module:: scipy.ndimage.interpolation .. autosummary:: :toctree: generated/ affine_transform - Apply an affine transformation geometric_transform - Apply an arbritrary geometric transform map_coordinates - Map input array to new coordinates by interpolation rotate - Rotate an array shift - Shift an array spline_filter spline_filter1d zoom - Zoom an array Measurements :mod:`scipy.ndimage.measurements` ============================================== .. module:: scipy.ndimage.measurements .. autosummary:: :toctree: generated/ center_of_mass - The center of mass of the values of an array at labels extrema - Min's and max's of an array at labels, with their positions find_objects - Find objects in a labeled array histogram - Histogram of the values of an array, optionally at labels label - Label features in an array labeled_comprehension maximum maximum_position mean - Mean of the values of an array at labels minimum minimum_position standard_deviation - Standard deviation of an n-D image array sum - Sum of the values of the array variance - Variance of the values of an n-D image array watershed_ift Morphology :mod:`scipy.ndimage.morphology` ========================================== .. module:: scipy.ndimage.morphology .. autosummary:: :toctree: generated/ binary_closing binary_dilation binary_erosion binary_fill_holes binary_hit_or_miss binary_opening binary_propagation black_tophat distance_transform_bf distance_transform_cdt distance_transform_edt generate_binary_structure grey_closing grey_dilation grey_erosion grey_opening iterate_structure morphological_gradient morphological_laplace white_tophat Utility ======= .. currentmodule:: scipy.ndimage .. autosummary:: :toctree: generated/ imread - Load an image from a file """ # Copyright (C) 2003-2005 Peter J. Verveer # # Redistribution and use in source and binary forms, with or without # modification, are permitted provided that the following conditions # are met: # # 1. Redistributions of source code must retain the above copyright # notice, this list of conditions and the following disclaimer. # # 2. Redistributions in binary form must reproduce the above # copyright notice, this list of conditions and the following # disclaimer in the documentation and/or other materials provided # with the distribution. # # 3. The name of the author may not be used to endorse or promote # products derived from this software without specific prior # written permission. # # THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS # OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED # WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE # ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY # DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL # DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE # GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, # WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING # NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS # SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. from __future__ import division, print_function, absolute_import from .filters import * from .fourier import * from .interpolation import * from .measurements import * from .morphology import * from .io import * __version__ = '2.0' __all__ = [s for s in dir() if not s.startswith('_')] from numpy.testing import Tester test = Tester().test
#!/usr/bin/env python2 # -*- coding: utf-8 -*- # # GuessIt - A library for guessing information from filenames # Copyright (c) 2012 Nicolas Wack <wackou@gmail.com> # # GuessIt is free software; you can redistribute it and/or modify it under # the terms of the Lesser GNU General Public License as published by # the Free Software Foundation; either version 3 of the License, or # (at your option) any later version. # # GuessIt is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # Lesser GNU General Public License for more details. # # You should have received a copy of the Lesser GNU General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # from __future__ import unicode_literals from guessit import Guess from guessit.transfo import SingleNodeGuesser from guessit.patterns import weak_episode_rexps import re import logging log = logging.getLogger(__name__) def guess_weak_episodes_rexps(string, node): if 'episodeNumber' in node.root.info: return None, None for rexp, span_adjust in weak_episode_rexps: match = re.search(rexp, string, re.IGNORECASE) if match: metadata = match.groupdict() span = (match.start() + span_adjust[0], match.end() + span_adjust[1]) epnum = int(metadata['episodeNumber']) if epnum > 100: season, epnum = epnum // 100, epnum % 100 # episodes which have a season > 25 are most likely errors # (Simpsons is at 23!) if season > 25: continue return Guess({ 'season': season, 'episodeNumber': epnum }, confidence=0.6), span else: return Guess(metadata, confidence=0.3), span return None, None guess_weak_episodes_rexps.use_node = True def process(mtree): SingleNodeGuesser(guess_weak_episodes_rexps, 0.6, log).process(mtree)
# -*- coding: utf-8 -*- """ Created on Wed May 06 11:00:53 2015 @author: newJustin """ import ChronoTrack_pandas as CT import pylab as py if __name__ == '__main__': # logger import logging as lg lg.basicConfig(fileName = 'logFile.log', level=lg.WARN, format='%(message)s') # default font size import matplotlib font = {'size' : 14} matplotlib.rc('font', **font) # ********************************************************************** # =============== USER INPUT ======================================= # data dir, end w/ '/' # data_dir = 'D:/Chrono_github_Build/bin/outdata_M113/' data_dir = 'E:/Chrono_github_Build/bin/outdata_M113/' ''' # list of data files to plot chassis = 'M113_chassis.csv' gearSubsys = 'M113_Side0_gear.csv' idlerSubsys = 'M113_Side0_idler.csv' # ptrainSubsys = 'test_driveChain_ptrain.csv' shoe0 = 'M113_Side0_shoe0.csv' ''' chassis = 'M113_400_200__chassis.csv' gearSubsys = 'M113_400_200__Side0_gear.csv' idlerSubsys = 'M113_400_200__Side0_idler.csv' # ptrainSubsys = 'test_driveChain_ptrain.csv' shoe0 = 'M113_400_200__Side0_shoe0.csv' data_files = [data_dir + chassis, data_dir + gearSubsys, data_dir + idlerSubsys, data_dir + shoe0] handle_list = ['chassis','gear','idler','shoe0'] # handle_list = ['Gear','idler','ptrain','shoe0','gearCV','idlerCV','rollerCV','gearContact','shoeGearContact'] ''' gearCV = 'test_driveChain_GearCV.csv' idlerCV = 'test_driveChain_idler0CV.csv' rollerCV = 'test_driveChain_roller0CV.csv' gearContact = 'test_driveChain_gearContact.csv' shoeGearContact = 'test_driveChain_shoe0GearContact.csv' ''' # data_files = [data_dir + gearSubsys, data_dir + idlerSubsys, data_dir + ptrainSubsys, data_dir + shoe0, data_dir + gearCV, data_dir + idlerCV, data_dir + rollerCV, data_dir + gearContact, data_dir+shoeGearContact] # handle_list = ['Gear','idler','ptrain','shoe0','gearCV','idlerCV','rollerCV','gearContact','shoeGearContact'] # list of data files for gear/pin comparison plots # Primitive gear geometry ''' gear = 'driveChain_P_gear.csv' gearContact = 'driveChain_P_gearContact.csv' shoe = 'driveChain_P_shoe0.csv' shoeContact = 'driveChain_P_shoe0GearContact.csv' ptrain = 'driveChain_P_ptrain.csv' # Collision Callback gear geometry gear = 'driveChain_CC_gear.csv' gearContact = 'driveChain_CC_gearContact.csv' shoe = 'driveChain_CC_shoe0.csv' shoeContact = 'driveChain_CC_shoe0GearContact.csv' ptrain = 'driveChain_CC_ptrain.csv' data_files = [data_dir+gear, data_dir+gearContact, data_dir+shoe, data_dir+shoeContact, data_dir+ptrain] handle_list = ['Gear','gearContact','shoe0','shoeGearContact','ptrain'] ''' # construct the panda class for the DriveChain, file list and list of legend M113_Chain0 = CT.ChronoTrack_pandas(data_files, handle_list) # set the time limits. tmin = -1 will plot the entire time range tmin = 1.0 tmax = 8.0 #0) plot the chassis M113_Chain0.plot_chassis(tmin, tmax) # 1) plot the gear body info M113_Chain0.plot_gear(tmin, tmax) # 2) plot idler body info, tensioner force M113_Chain0.plot_idler(tmin,tmax) ''' # 3) plot powertrain info M113_Chain0.plot_ptrain() ''' # 4) plot shoe 0 body info, and pin 0 force/torque M113_Chain0.plot_shoe(tmin,tmax) ''' # 5) plot gear Constraint Violations M113_Chain0.plot_gearCV(tmin,tmax) # 6) plot idler Constraint Violations M113_Chain0.plot_idlerCV(tmin,tmax) # 7) plot roller Constraint Violations M113_Chain0.plot_rollerCV(tmin,tmax) # 8) from the contact report callback function, gear contact info M113_Chain0.plot_gearContactInfo(tmin,tmax) # 9) from shoe-gear report callback function, contact info M113_Chain0.plot_shoeGearContactInfo(tmin,tmax) ''' # 10) track shoe trajectory: rel-X vs. rel-Y M113_Chain0.plot_trajectory(tmin,tmax) py.show()
# -*- coding: utf-8 -*- ''' FanFilm Add-on Copyright (C) 2015 lambda This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see <http://www.gnu.org/licenses/>. ''' import re from resources.lib.libraries import client def resolve(url): try: url = url.replace('/embed-', '/') url = re.compile('//.+?/([\w]+)').findall(url)[0] url = 'http://putstream.com/embed-%s.html' % url result = client.request(url) url = re.compile('file *: *"(http.+?)"').findall(result)[-1] return url except: return
import os import shutil import zipfile import fnmatch import uuid def main(): kits = findAll(".") for kit in kits: print("* ", kit, " -> ", kits[kit]) print() print() print("Starting extraction:") print("------------------------------------------") extractKits(kits) def findAll(dir): print() print("All zip files:") print("---------------------------") kits = {} files = os.listdir(".") for file in files: if file.endswith(".zip"): kits[file] = getType(file) return kits def getType(file): if "-pp" in file: return "paper" if "-ap" in file: return "alpha" if "-ep" in file: return "embellishment" options = {1: "embellishment", 2: "alpha", 3: "paper", 4:"other"} #DEBUG: return options[1]; goodInput = False while not goodInput: print() print("File: ", file) print(" 1) Embellishment") print(" 2) Alpha") print(" 3) Paper") print(" 4) Other") action = input("Please Enter the Number (default = 1):") if action is "": return options[1]; if action.isdigit(): actionNum = int(action) if actionNum > 0 and actionNum < len(options)+1: return options[actionNum] def extractKits(kits): tmpDir = "./tmp"; kitNames = {} x = 0 for kit in kits: # kit = next(iter(kits.keys())) x = x + 1 print() print() print() print("Extracting: ", kit, " ( ", x, " of ", len(kits), ")") kitStr = kit.rsplit("-", 1)[0] print("Kit Name: ", kitStr) if kitStr in kitNames: name = input("Please Enter Kit Name (default = "+kitNames[kitStr]+"): ") name = name or kitNames[kitStr] else: name = input("Please Enter Kit Name: ") kitNames[kitStr] =name if os.path.exists(tmpDir): shutil.rmtree(tmpDir) else: os.makedirs(tmpDir) if not os.path.exists("./" + name): os.makedirs("./" + name) kitzip = zipfile.ZipFile("./" + kit) kitzip.extractall(tmpDir) images = copyExtractedFiles("./" + name +"/") createManifest(kit, name, images, kits[kit]) def copyExtractedFiles(dest): matches = [] filenames = [".png", ".jpg"] for rootpath, subdirs, files in os.walk("./tmp"): for filename in files: if os.path.splitext(filename)[1].lower() in filenames: # print(os.path.join(rootpath, filename).replace('\\','/')) shutil.move(os.path.join(rootpath, filename).replace('\\','/'), dest+filename) matches.append(dest + filename) return matches def createManifest(kit, name, images, type): manifest = [] manifest.append('<Manifest vendorid="0" vendorpackageid="0" maintaincopyright="True" dpi="300">') manifest.append('<Groups />') manifest.append('<Entries>') for image in images: manifest.append('<Image ID="'+str(uuid.uuid4())+'" Name="'+image+'" Group="Embellishment" />') manifest.append('</Entries>') manifest.append('</Manifest>') with open('./'+name+'/package.manifestx', 'w') as f: for line in manifest: f.write(line + os.linesep) if __name__ == "__main__": main()
# Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import abc import collections from oslo_log import log as logging import six from neutron.api.rpc.callbacks import exceptions as rpc_exc from neutron.api.rpc.callbacks import resources from neutron.callbacks import exceptions LOG = logging.getLogger(__name__) # TODO(QoS): split the registry/resources_rpc modules into two separate things: # one for pull and one for push APIs def _validate_resource_type(resource_type): if not resources.is_valid_resource_type(resource_type): raise exceptions.Invalid(element='resource', value=resource_type) @six.add_metaclass(abc.ABCMeta) class ResourceCallbacksManager(object): """A callback system that allows information providers in a loose manner. """ # This hook is to allow tests to get new objects for the class _singleton = True def __new__(cls, *args, **kwargs): if not cls._singleton: return super(ResourceCallbacksManager, cls).__new__(cls) if not hasattr(cls, '_instance'): cls._instance = super(ResourceCallbacksManager, cls).__new__(cls) return cls._instance @abc.abstractmethod def _add_callback(self, callback, resource_type): pass @abc.abstractmethod def _delete_callback(self, callback, resource_type): pass def register(self, callback, resource_type): """Register a callback for a resource type. :param callback: the callback. It must raise or return NeutronObject. :param resource_type: must be a valid resource type. """ LOG.debug("Registering callback for %s", resource_type) _validate_resource_type(resource_type) self._add_callback(callback, resource_type) def unregister(self, callback, resource_type): """Unregister callback from the registry. :param callback: the callback. :param resource_type: must be a valid resource type. """ LOG.debug("Unregistering callback for %s", resource_type) _validate_resource_type(resource_type) self._delete_callback(callback, resource_type) @abc.abstractmethod def clear(self): """Brings the manager to a clean state.""" def get_subscribed_types(self): return list(self._callbacks.keys()) class ProducerResourceCallbacksManager(ResourceCallbacksManager): _callbacks = dict() def _add_callback(self, callback, resource_type): if resource_type in self._callbacks: raise rpc_exc.CallbacksMaxLimitReached(resource_type=resource_type) self._callbacks[resource_type] = callback def _delete_callback(self, callback, resource_type): try: del self._callbacks[resource_type] except KeyError: raise rpc_exc.CallbackNotFound(resource_type=resource_type) def clear(self): self._callbacks = dict() def get_callback(self, resource_type): _validate_resource_type(resource_type) try: return self._callbacks[resource_type] except KeyError: raise rpc_exc.CallbackNotFound(resource_type=resource_type) class ConsumerResourceCallbacksManager(ResourceCallbacksManager): _callbacks = collections.defaultdict(set) def _add_callback(self, callback, resource_type): self._callbacks[resource_type].add(callback) def _delete_callback(self, callback, resource_type): try: self._callbacks[resource_type].remove(callback) if not self._callbacks[resource_type]: del self._callbacks[resource_type] except KeyError: raise rpc_exc.CallbackNotFound(resource_type=resource_type) def clear(self): self._callbacks = collections.defaultdict(set) def get_callbacks(self, resource_type): """Return the callback if found, None otherwise. :param resource_type: must be a valid resource type. """ _validate_resource_type(resource_type) callbacks = self._callbacks[resource_type] if not callbacks: raise rpc_exc.CallbackNotFound(resource_type=resource_type) return callbacks
# coding: utf-8 from __future__ import unicode_literals import os.path import re from .common import InfoExtractor from ..utils import ( ExtractorError, remove_start, sanitized_Request, urlencode_postdata, ) class MonikerIE(InfoExtractor): IE_DESC = 'allmyvideos.net and vidspot.net' _VALID_URL = r'https?://(?:www\.)?(?:allmyvideos|vidspot)\.net/(?:(?:2|v)/v-)?(?P<id>[a-zA-Z0-9_-]+)' _TESTS = [{ 'url': 'http://allmyvideos.net/jih3nce3x6wn', 'md5': '710883dee1bfc370ecf9fa6a89307c88', 'info_dict': { 'id': 'jih3nce3x6wn', 'ext': 'mp4', 'title': 'youtube-dl test video', }, }, { 'url': 'http://allmyvideos.net/embed-jih3nce3x6wn', 'md5': '710883dee1bfc370ecf9fa6a89307c88', 'info_dict': { 'id': 'jih3nce3x6wn', 'ext': 'mp4', 'title': 'youtube-dl test video', }, }, { 'url': 'http://vidspot.net/l2ngsmhs8ci5', 'md5': '710883dee1bfc370ecf9fa6a89307c88', 'info_dict': { 'id': 'l2ngsmhs8ci5', 'ext': 'mp4', 'title': 'youtube-dl test video', }, }, { 'url': 'https://www.vidspot.net/l2ngsmhs8ci5', 'only_matching': True, }, { 'url': 'http://vidspot.net/2/v-ywDf99', 'md5': '5f8254ce12df30479428b0152fb8e7ba', 'info_dict': { 'id': 'ywDf99', 'ext': 'mp4', 'title': 'IL FAIT LE MALIN EN PORSHE CAYENNE ( mais pas pour longtemps)', 'description': 'IL FAIT LE MALIN EN PORSHE CAYENNE.', }, }, { 'url': 'http://allmyvideos.net/v/v-HXZm5t', 'only_matching': True, }] def _real_extract(self, url): orig_video_id = self._match_id(url) video_id = remove_start(orig_video_id, 'embed-') url = url.replace(orig_video_id, video_id) assert re.match(self._VALID_URL, url) is not None orig_webpage = self._download_webpage(url, video_id) if '>File Not Found<' in orig_webpage: raise ExtractorError('Video %s does not exist' % video_id, expected=True) error = self._search_regex( r'class="err">([^<]+)<', orig_webpage, 'error', default=None) if error: raise ExtractorError( '%s returned error: %s' % (self.IE_NAME, error), expected=True) builtin_url = self._search_regex( r'<iframe[^>]+src=(["\'])(?P<url>.+?/builtin-.+?)\1', orig_webpage, 'builtin URL', default=None, group='url') if builtin_url: req = sanitized_Request(builtin_url) req.add_header('Referer', url) webpage = self._download_webpage(req, video_id, 'Downloading builtin page') title = self._og_search_title(orig_webpage).strip() description = self._og_search_description(orig_webpage).strip() else: fields = re.findall(r'type="hidden" name="(.+?)"\s* value="?(.+?)">', orig_webpage) data = dict(fields) post = urlencode_postdata(data) headers = { b'Content-Type': b'application/x-www-form-urlencoded', } req = sanitized_Request(url, post, headers) webpage = self._download_webpage( req, video_id, note='Downloading video page ...') title = os.path.splitext(data['fname'])[0] description = None # Could be several links with different quality links = re.findall(r'"file" : "?(.+?)",', webpage) # Assume the links are ordered in quality formats = [{ 'url': l, 'quality': i, } for i, l in enumerate(links)] self._sort_formats(formats) return { 'id': video_id, 'title': title, 'description': description, 'formats': formats, }
""" Management utility to create superusers. """ from __future__ import unicode_literals import getpass import sys from django.contrib.auth import get_user_model from django.contrib.auth.management import get_default_username from django.core import exceptions from django.core.management.base import BaseCommand, CommandError from django.db import DEFAULT_DB_ALIAS from django.utils.encoding import force_str from django.utils.six.moves import input from django.utils.text import capfirst class NotRunningInTTYException(Exception): pass class Command(BaseCommand): help = 'Used to create a superuser.' def __init__(self, *args, **kwargs): super(Command, self).__init__(*args, **kwargs) self.UserModel = get_user_model() self.username_field = self.UserModel._meta.get_field(self.UserModel.USERNAME_FIELD) def add_arguments(self, parser): parser.add_argument('--%s' % self.UserModel.USERNAME_FIELD, dest=self.UserModel.USERNAME_FIELD, default=None, help='Specifies the login for the superuser.') parser.add_argument('--noinput', action='store_false', dest='interactive', default=True, help=('Tells Django to NOT prompt the user for input of any kind. ' 'You must use --%s with --noinput, along with an option for ' 'any other required field. Superusers created with --noinput will ' ' not be able to log in until they\'re given a valid password.' % self.UserModel.USERNAME_FIELD)) parser.add_argument('--database', action='store', dest='database', default=DEFAULT_DB_ALIAS, help='Specifies the database to use. Default is "default".') for field in self.UserModel.REQUIRED_FIELDS: parser.add_argument('--%s' % field, dest=field, default=None, help='Specifies the %s for the superuser.' % field) def execute(self, *args, **options): self.stdin = options.get('stdin', sys.stdin) # Used for testing return super(Command, self).execute(*args, **options) def handle(self, *args, **options): username = options.get(self.UserModel.USERNAME_FIELD) database = options.get('database') # If not provided, create the user with an unusable password password = None user_data = {} # Do quick and dirty validation if --noinput if not options['interactive']: try: if not username: raise CommandError("You must use --%s with --noinput." % self.UserModel.USERNAME_FIELD) username = self.username_field.clean(username, None) for field_name in self.UserModel.REQUIRED_FIELDS: if options.get(field_name): field = self.UserModel._meta.get_field(field_name) user_data[field_name] = field.clean(options[field_name], None) else: raise CommandError("You must use --%s with --noinput." % field_name) except exceptions.ValidationError as e: raise CommandError('; '.join(e.messages)) else: # Prompt for username/password, and any other required fields. # Enclose this whole thing in a try/except to catch # KeyboardInterrupt and exit gracefully. default_username = get_default_username() try: if hasattr(self.stdin, 'isatty') and not self.stdin.isatty(): raise NotRunningInTTYException("Not running in a TTY") # Get a username verbose_field_name = self.username_field.verbose_name while username is None: input_msg = capfirst(verbose_field_name) if default_username: input_msg += " (leave blank to use '%s')" % default_username username_rel = self.username_field.remote_field input_msg = force_str('%s%s: ' % ( input_msg, ' (%s.%s)' % ( username_rel.model._meta.object_name, username_rel.field_name ) if username_rel else '') ) username = self.get_input_data(self.username_field, input_msg, default_username) if not username: continue if self.username_field.unique: try: self.UserModel._default_manager.db_manager(database).get_by_natural_key(username) except self.UserModel.DoesNotExist: pass else: self.stderr.write("Error: That %s is already taken." % verbose_field_name) username = None for field_name in self.UserModel.REQUIRED_FIELDS: field = self.UserModel._meta.get_field(field_name) user_data[field_name] = options.get(field_name) while user_data[field_name] is None: message = force_str('%s%s: ' % ( capfirst(field.verbose_name), ' (%s.%s)' % ( field.remote_field.model._meta.object_name, field.remote_field.field_name, ) if field.remote_field else '', )) user_data[field_name] = self.get_input_data(field, message) # Get a password while password is None: if not password: password = getpass.getpass() password2 = getpass.getpass(force_str('Password (again): ')) if password != password2: self.stderr.write("Error: Your passwords didn't match.") password = None continue if password.strip() == '': self.stderr.write("Error: Blank passwords aren't allowed.") password = None continue except KeyboardInterrupt: self.stderr.write("\nOperation cancelled.") sys.exit(1) except NotRunningInTTYException: self.stdout.write( "Superuser creation skipped due to not running in a TTY. " "You can run `manage.py createsuperuser` in your project " "to create one manually." ) if username: user_data[self.UserModel.USERNAME_FIELD] = username user_data['password'] = password self.UserModel._default_manager.db_manager(database).create_superuser(**user_data) if options['verbosity'] >= 1: self.stdout.write("Superuser created successfully.") def get_input_data(self, field, message, default=None): """ Override this method if you want to customize data inputs or validation exceptions. """ raw_value = input(message) if default and raw_value == '': raw_value = default try: val = field.clean(raw_value, None) except exceptions.ValidationError as e: self.stderr.write("Error: %s" % '; '.join(e.messages)) val = None return val
# This file is part of pybootchartgui. # pybootchartgui is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # pybootchartgui is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # You should have received a copy of the GNU General Public License # along with pybootchartgui. If not, see <http://www.gnu.org/licenses/>. class DiskStatSample: def __init__(self, time): self.time = time self.diskdata = [0, 0, 0] def add_diskdata(self, new_diskdata): self.diskdata = [ a + b for a, b in zip(self.diskdata, new_diskdata) ] class CPUSample: def __init__(self, time, user, sys, io = 0.0, swap = 0.0): self.time = time self.user = user self.sys = sys self.io = io self.swap = swap @property def cpu(self): return self.user + self.sys def __str__(self): return str(self.time) + "\t" + str(self.user) + "\t" + \ str(self.sys) + "\t" + str(self.io) + "\t" + str (self.swap) class MemSample: used_values = ('MemTotal', 'MemFree', 'Buffers', 'Cached', 'SwapTotal', 'SwapFree',) def __init__(self, time): self.time = time self.records = {} def add_value(self, name, value): if name in MemSample.used_values: self.records[name] = value def valid(self): keys = self.records.keys() # discard incomplete samples return [v for v in MemSample.used_values if v not in keys] == [] class ProcessSample: def __init__(self, time, state, cpu_sample): self.time = time self.state = state self.cpu_sample = cpu_sample def __str__(self): return str(self.time) + "\t" + str(self.state) + "\t" + str(self.cpu_sample) class ProcessStats: def __init__(self, writer, process_map, sample_count, sample_period, start_time, end_time): self.process_map = process_map self.sample_count = sample_count self.sample_period = sample_period self.start_time = start_time self.end_time = end_time writer.info ("%d samples, avg. sample length %f" % (self.sample_count, self.sample_period)) writer.info ("process list size: %d" % len (self.process_map.values())) class Process: def __init__(self, writer, pid, cmd, ppid, start_time): self.writer = writer self.pid = pid self.cmd = cmd self.exe = cmd self.args = [] self.ppid = ppid self.start_time = start_time self.duration = 0 self.samples = [] self.parent = None self.child_list = [] self.active = None self.last_user_cpu_time = None self.last_sys_cpu_time = None self.last_cpu_ns = 0 self.last_blkio_delay_ns = 0 self.last_swapin_delay_ns = 0 # split this process' run - triggered by a name change def split(self, writer, pid, cmd, ppid, start_time): split = Process (writer, pid, cmd, ppid, start_time) split.last_cpu_ns = self.last_cpu_ns split.last_blkio_delay_ns = self.last_blkio_delay_ns split.last_swapin_delay_ns = self.last_swapin_delay_ns return split def __str__(self): return " ".join([str(self.pid), self.cmd, str(self.ppid), '[ ' + str(len(self.samples)) + ' samples ]' ]) def calc_stats(self, samplePeriod): if self.samples: firstSample = self.samples[0] lastSample = self.samples[-1] self.start_time = min(firstSample.time, self.start_time) self.duration = lastSample.time - self.start_time + samplePeriod activeCount = sum( [1 for sample in self.samples if sample.cpu_sample and sample.cpu_sample.sys + sample.cpu_sample.user + sample.cpu_sample.io > 0.0] ) activeCount = activeCount + sum( [1 for sample in self.samples if sample.state == 'D'] ) self.active = (activeCount>2) def calc_load(self, userCpu, sysCpu, interval): userCpuLoad = float(userCpu - self.last_user_cpu_time) / interval sysCpuLoad = float(sysCpu - self.last_sys_cpu_time) / interval cpuLoad = userCpuLoad + sysCpuLoad # normalize if cpuLoad > 1.0: userCpuLoad = userCpuLoad / cpuLoad sysCpuLoad = sysCpuLoad / cpuLoad return (userCpuLoad, sysCpuLoad) def set_parent(self, processMap): if self.ppid != None: self.parent = processMap.get (self.ppid) if self.parent == None and self.pid // 1000 > 1 and \ not (self.ppid == 2000 or self.pid == 2000): # kernel threads: ppid=2 self.writer.warn("Missing CONFIG_PROC_EVENTS: no parent for pid '%i' ('%s') with ppid '%i'" \ % (self.pid,self.cmd,self.ppid)) def get_end_time(self): return self.start_time + self.duration class DiskSample: def __init__(self, time, read, write, util): self.time = time self.read = read self.write = write self.util = util self.tput = read + write def __str__(self): return "\t".join([str(self.time), str(self.read), str(self.write), str(self.util)])
# vim: tabstop=4 shiftwidth=4 softtabstop=4 # # Copyright 2012 NEC Corporation. All rights reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. # @author: Ryota MIBU # @author: Akihiro MOTOKI import netaddr from neutron.common import utils from neutron.openstack.common import log as logging from neutron.plugins.nec.common import config from neutron.plugins.nec.common import exceptions as nexc from neutron.plugins.nec.db import api as ndb from neutron.plugins.nec import drivers LOG = logging.getLogger(__name__) class OFCManager(object): """This class manages an OpenFlow Controller and map resources. This class manage an OpenFlow Controller (OFC) with a driver specified in a configuration of this plugin. This keeps mappings between IDs on Neutron and OFC for various entities such as Tenant, Network and Filter. A Port on OFC is identified by a switch ID 'datapath_id' and a port number 'port_no' of the switch. An ID named as 'ofc_*' is used to identify resource on OFC. """ def __init__(self): self.driver = drivers.get_driver(config.OFC.driver)(config.OFC) def _get_ofc_id(self, context, resource, neutron_id): return ndb.get_ofc_id_lookup_both(context.session, resource, neutron_id) def _exists_ofc_item(self, context, resource, neutron_id): return ndb.exists_ofc_item_lookup_both(context.session, resource, neutron_id) def _add_ofc_item(self, context, resource, neutron_id, ofc_id): # Ensure a new item is added to the new mapping table ndb.add_ofc_item(context.session, resource, neutron_id, ofc_id) def _del_ofc_item(self, context, resource, neutron_id): ndb.del_ofc_item_lookup_both(context.session, resource, neutron_id) def ensure_ofc_tenant(self, context, tenant_id): if not self.exists_ofc_tenant(context, tenant_id): self.create_ofc_tenant(context, tenant_id) def create_ofc_tenant(self, context, tenant_id): desc = "ID=%s at OpenStack." % tenant_id ofc_tenant_id = self.driver.create_tenant(desc, tenant_id) self._add_ofc_item(context, "ofc_tenant", tenant_id, ofc_tenant_id) def exists_ofc_tenant(self, context, tenant_id): return self._exists_ofc_item(context, "ofc_tenant", tenant_id) def delete_ofc_tenant(self, context, tenant_id): ofc_tenant_id = self._get_ofc_id(context, "ofc_tenant", tenant_id) ofc_tenant_id = self.driver.convert_ofc_tenant_id( context, ofc_tenant_id) self.driver.delete_tenant(ofc_tenant_id) self._del_ofc_item(context, "ofc_tenant", tenant_id) def create_ofc_network(self, context, tenant_id, network_id, network_name=None): ofc_tenant_id = self._get_ofc_id(context, "ofc_tenant", tenant_id) ofc_tenant_id = self.driver.convert_ofc_tenant_id( context, ofc_tenant_id) desc = "ID=%s Name=%s at Neutron." % (network_id, network_name) ofc_net_id = self.driver.create_network(ofc_tenant_id, desc, network_id) self._add_ofc_item(context, "ofc_network", network_id, ofc_net_id) def exists_ofc_network(self, context, network_id): return self._exists_ofc_item(context, "ofc_network", network_id) def delete_ofc_network(self, context, network_id, network): ofc_net_id = self._get_ofc_id(context, "ofc_network", network_id) ofc_net_id = self.driver.convert_ofc_network_id( context, ofc_net_id, network['tenant_id']) self.driver.delete_network(ofc_net_id) self._del_ofc_item(context, "ofc_network", network_id) def create_ofc_port(self, context, port_id, port): ofc_net_id = self._get_ofc_id(context, "ofc_network", port['network_id']) ofc_net_id = self.driver.convert_ofc_network_id( context, ofc_net_id, port['tenant_id']) portinfo = ndb.get_portinfo(context.session, port_id) if not portinfo: raise nexc.PortInfoNotFound(id=port_id) ofc_port_id = self.driver.create_port(ofc_net_id, portinfo, port_id) self._add_ofc_item(context, "ofc_port", port_id, ofc_port_id) def exists_ofc_port(self, context, port_id): return self._exists_ofc_item(context, "ofc_port", port_id) def delete_ofc_port(self, context, port_id, port): ofc_port_id = self._get_ofc_id(context, "ofc_port", port_id) ofc_port_id = self.driver.convert_ofc_port_id( context, ofc_port_id, port['tenant_id'], port['network_id']) self.driver.delete_port(ofc_port_id) self._del_ofc_item(context, "ofc_port", port_id) def create_ofc_packet_filter(self, context, filter_id, filter_dict): ofc_net_id = self._get_ofc_id(context, "ofc_network", filter_dict['network_id']) ofc_net_id = self.driver.convert_ofc_network_id( context, ofc_net_id, filter_dict['tenant_id']) in_port_id = filter_dict.get('in_port') portinfo = None if in_port_id: portinfo = ndb.get_portinfo(context.session, in_port_id) if not portinfo: raise nexc.PortInfoNotFound(id=in_port_id) ofc_pf_id = self.driver.create_filter(ofc_net_id, filter_dict, portinfo, filter_id) self._add_ofc_item(context, "ofc_packet_filter", filter_id, ofc_pf_id) def exists_ofc_packet_filter(self, context, filter_id): return self._exists_ofc_item(context, "ofc_packet_filter", filter_id) def delete_ofc_packet_filter(self, context, filter_id): ofc_pf_id = self._get_ofc_id(context, "ofc_packet_filter", filter_id) ofc_pf_id = self.driver.convert_ofc_filter_id(context, ofc_pf_id) self.driver.delete_filter(ofc_pf_id) self._del_ofc_item(context, "ofc_packet_filter", filter_id) def create_ofc_router(self, context, tenant_id, router_id, name=None): ofc_tenant_id = self._get_ofc_id(context, "ofc_tenant", tenant_id) ofc_tenant_id = self.driver.convert_ofc_tenant_id( context, ofc_tenant_id) desc = "ID=%s Name=%s at Neutron." % (router_id, name) ofc_router_id = self.driver.create_router(ofc_tenant_id, router_id, desc) self._add_ofc_item(context, "ofc_router", router_id, ofc_router_id) def exists_ofc_router(self, context, router_id): return self._exists_ofc_item(context, "ofc_router", router_id) def delete_ofc_router(self, context, router_id, router): ofc_router_id = self._get_ofc_id(context, "ofc_router", router_id) self.driver.delete_router(ofc_router_id) self._del_ofc_item(context, "ofc_router", router_id) def add_ofc_router_interface(self, context, router_id, port_id, port): # port must have the following fields: # network_id, cidr, ip_address, mac_address ofc_router_id = self._get_ofc_id(context, "ofc_router", router_id) ofc_net_id = self._get_ofc_id(context, "ofc_network", port['network_id']) ip_address = '%s/%s' % (port['ip_address'], netaddr.IPNetwork(port['cidr']).prefixlen) mac_address = port['mac_address'] ofc_inf_id = self.driver.add_router_interface( ofc_router_id, ofc_net_id, ip_address, mac_address) # Use port mapping table to maintain an interface of OFC router self._add_ofc_item(context, "ofc_port", port_id, ofc_inf_id) def delete_ofc_router_interface(self, context, router_id, port_id): # Use port mapping table to maintain an interface of OFC router ofc_inf_id = self._get_ofc_id(context, "ofc_port", port_id) self.driver.delete_router_interface(ofc_inf_id) self._del_ofc_item(context, "ofc_port", port_id) def update_ofc_router_route(self, context, router_id, new_routes): ofc_router_id = self._get_ofc_id(context, "ofc_router", router_id) ofc_routes = self.driver.list_router_routes(ofc_router_id) route_dict = {} cur_routes = [] for r in ofc_routes: key = ','.join((r['destination'], r['nexthop'])) route_dict[key] = r['id'] del r['id'] cur_routes.append(r) added, removed = utils.diff_list_of_dict(cur_routes, new_routes) for r in removed: key = ','.join((r['destination'], r['nexthop'])) route_id = route_dict[key] self.driver.delete_router_route(route_id) for r in added: self.driver.add_router_route(ofc_router_id, r['destination'], r['nexthop'])
#!/usr/bin/env python # Copyright 2009 Simon Arlott # # This program is free software; you can redistribute it and/or modify it # under the terms of the GNU General Public License as published by the Free # Software Foundation; either version 2 of the License, or (at your option) # any later version. # # This program is distributed in the hope that it will be useful, but WITHOUT # ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or # FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for # more details. # # You should have received a copy of the GNU General Public License along with # this program; if not, write to the Free Software Foundation, Inc., 59 # Temple Place - Suite 330, Boston, MA 02111-1307, USA. # # Usage: cxacru-cf.py < cxacru-cf.bin # Output: values string suitable for the sysfs adsl_config attribute # # Warning: cxacru-cf.bin with MD5 hash cdbac2689969d5ed5d4850f117702110 # contains mis-aligned values which will stop the modem from being able # to make a connection. If the first and last two bytes are removed then # the values become valid, but the modulation will be forced to ANSI # T1.413 only which may not be appropriate. # # The original binary format is a packed list of le32 values. import sys import struct i = 0 while True: buf = sys.stdin.read(4) if len(buf) == 0: break elif len(buf) != 4: sys.stdout.write("\n") sys.stderr.write("Error: read {0} not 4 bytes\n".format(len(buf))) sys.exit(1) if i > 0: sys.stdout.write(" ") sys.stdout.write("{0:x}={1}".format(i, struct.unpack("<I", buf)[0])) i += 1 sys.stdout.write("\n")
import random from utils import constrained_sum_sample_pos, arr_str scenario_description = ( "During the fueling period of the DAO, send enough ether from all " "accounts to create tokens and then assert that the user's balance is " "indeed correct and that the minimum fueling goal has been reached" ) def run(ctx): ctx.assert_scenario_ran('deploy') creation_secs = ctx.remaining_time() ctx.total_supply = ( ctx.args.deploy_min_tokens_to_create + random.randint(1, 100) ) ctx.token_amounts = constrained_sum_sample_pos( len(ctx.accounts), ctx.total_supply ) ctx.create_js_file(substitutions={ "dao_abi": ctx.dao_abi, "dao_address": ctx.dao_address, "wait_ms": (creation_secs-3)*1000, "amounts": arr_str(ctx.token_amounts) } ) print( "Notice: Fueling period is {} seconds so the test will wait " "as much".format(creation_secs) ) adjusted_amounts = ( [x/1.5 for x in ctx.token_amounts] if ctx.scenario_uses_extrabalance() else ctx.token_amounts ) adjusted_supply = ( ctx.total_supply / 1.5 if ctx.scenario_uses_extrabalance() else ctx.total_supply ) ctx.execute(expected={ "dao_fueled": True, "total_supply": adjusted_supply, "balances": adjusted_amounts, "user0_after": adjusted_amounts[0] })
# -*- coding: utf-8 -*- # taken from http://code.activestate.com/recipes/252524-length-limited-o1-lru-cache-implementation/ import threading from func import synchronized __all__ = ['LRU'] class LRUNode(object): __slots__ = ['prev', 'next', 'me'] def __init__(self, prev, me): self.prev = prev self.me = me self.next = None class LRU(object): """ Implementation of a length-limited O(1) LRU queue. Built for and used by PyPE: http://pype.sourceforge.net Copyright 2003 Josiah Carlson. """ def __init__(self, count, pairs=[]): self._lock = threading.RLock() self.count = max(count, 1) self.d = {} self.first = None self.last = None for key, value in pairs: self[key] = value @synchronized() def __contains__(self, obj): return obj in self.d @synchronized() def __getitem__(self, obj): a = self.d[obj].me self[a[0]] = a[1] return a[1] @synchronized() def __setitem__(self, obj, val): if obj in self.d: del self[obj] nobj = LRUNode(self.last, (obj, val)) if self.first is None: self.first = nobj if self.last: self.last.next = nobj self.last = nobj self.d[obj] = nobj if len(self.d) > self.count: if self.first == self.last: self.first = None self.last = None return a = self.first a.next.prev = None self.first = a.next a.next = None del self.d[a.me[0]] del a @synchronized() def __delitem__(self, obj): nobj = self.d[obj] if nobj.prev: nobj.prev.next = nobj.next else: self.first = nobj.next if nobj.next: nobj.next.prev = nobj.prev else: self.last = nobj.prev del self.d[obj] @synchronized() def __iter__(self): cur = self.first while cur is not None: cur2 = cur.next yield cur.me[1] cur = cur2 @synchronized() def __len__(self): return len(self.d) @synchronized() def iteritems(self): cur = self.first while cur is not None: cur2 = cur.next yield cur.me cur = cur2 @synchronized() def iterkeys(self): return iter(self.d) @synchronized() def itervalues(self): for i,j in self.iteritems(): yield j @synchronized() def keys(self): return self.d.keys() @synchronized() def pop(self,key): v=self[key] del self[key] return v @synchronized() def clear(self): self.d = {} self.first = None self.last = None @synchronized() def clear_prefix(self, prefix): """ Remove from `self` all the items with the given `prefix`. """ n = len(prefix) for key in self.keys(): if key[:n] == prefix: del self[key] # vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
# -*- coding: utf-8 -*- # Generated by Django 1.10.8 on 2019-06-19 18:23 from __future__ import unicode_literals from django.conf import settings from django.db import migrations, models import django.db.models.deletion class Migration(migrations.Migration): dependencies = [ migrations.swappable_dependency(settings.AUTH_USER_MODEL), ('organization-projects', '0084_auto_20190304_2221'), ] operations = [ migrations.AlterModelOptions( name='projectpage', options={'permissions': (('user_edit', 'Mezzo - User can edit its own content'), ('user_delete', 'Mezzo - User can delete its own content'), ('team_edit', "Mezzo - User can edit his team's content"), ('team_delete', "Mezzo - User can delete his team's content"))}, ), migrations.AddField( model_name='projectpage', name='user', field=models.ForeignKey(default=4, on_delete=django.db.models.deletion.CASCADE, related_name='projectpages', to=settings.AUTH_USER_MODEL, verbose_name='Author'), preserve_default=False, ), ]
# -*- coding: utf-8 -*- ## ## This file is part of Invenio. ## Copyright (C) 2011, 2012, 2013 CERN. ## ## Invenio is free software; you can redistribute it and/or ## modify it under the terms of the GNU General Public License as ## published by the Free Software Foundation; either version 2 of the ## License, or (at your option) any later version. ## ## Invenio is distributed in the hope that it will be useful, but ## WITHOUT ANY WARRANTY; without even the implied warranty of ## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU ## General Public License for more details. ## ## You should have received a copy of the GNU General Public License ## along with Invenio; if not, write to the Free Software Foundation, Inc., ## 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA. """Unit Tests for BibAuthority""" from invenio.testsuite import InvenioTestCase from invenio.testsuite import make_test_suite, run_test_suite class TestBibAuthorityEngine(InvenioTestCase): """Unit tests for bibauthority_engine""" def test_split_name_parts(self): """bibauthority - test get_type_from_authority_id""" from invenio.legacy.bibauthority.config import CFG_BIBAUTHORITY_PREFIX_SEP from invenio.legacy.bibauthority.engine import get_type_from_control_no prefix = "JOURNAL" control_no = "(CERN)abcd1234" # must start with a '(' self.assertEqual(get_type_from_control_no( prefix + CFG_BIBAUTHORITY_PREFIX_SEP + control_no), prefix) TEST_SUITE = make_test_suite(TestBibAuthorityEngine) if __name__ == "__main__": run_test_suite(TEST_SUITE)
"""Support for Dyson Pure Cool Link Sensors.""" import logging from libpurecool.dyson_pure_cool import DysonPureCool from libpurecool.dyson_pure_cool_link import DysonPureCoolLink from homeassistant.const import PERCENTAGE, STATE_OFF, TEMP_CELSIUS, TIME_HOURS from homeassistant.helpers.entity import Entity from . import DYSON_DEVICES SENSOR_UNITS = { "air_quality": None, "dust": None, "filter_life": TIME_HOURS, "humidity": PERCENTAGE, } SENSOR_ICONS = { "air_quality": "mdi:fan", "dust": "mdi:cloud", "filter_life": "mdi:filter-outline", "humidity": "mdi:water-percent", "temperature": "mdi:thermometer", } DYSON_SENSOR_DEVICES = "dyson_sensor_devices" _LOGGER = logging.getLogger(__name__) def setup_platform(hass, config, add_entities, discovery_info=None): """Set up the Dyson Sensors.""" if discovery_info is None: return hass.data.setdefault(DYSON_SENSOR_DEVICES, []) unit = hass.config.units.temperature_unit devices = hass.data[DYSON_SENSOR_DEVICES] # Get Dyson Devices from parent component device_ids = [device.unique_id for device in hass.data[DYSON_SENSOR_DEVICES]] new_entities = [] for device in hass.data[DYSON_DEVICES]: if isinstance(device, DysonPureCool): if f"{device.serial}-temperature" not in device_ids: new_entities.append(DysonTemperatureSensor(device, unit)) if f"{device.serial}-humidity" not in device_ids: new_entities.append(DysonHumiditySensor(device)) elif isinstance(device, DysonPureCoolLink): new_entities.append(DysonFilterLifeSensor(device)) new_entities.append(DysonDustSensor(device)) new_entities.append(DysonHumiditySensor(device)) new_entities.append(DysonTemperatureSensor(device, unit)) new_entities.append(DysonAirQualitySensor(device)) if not new_entities: return devices.extend(new_entities) add_entities(devices) class DysonSensor(Entity): """Representation of a generic Dyson sensor.""" def __init__(self, device, sensor_type): """Create a new generic Dyson sensor.""" self._device = device self._old_value = None self._name = None self._sensor_type = sensor_type async def async_added_to_hass(self): """Call when entity is added to hass.""" self._device.add_message_listener(self.on_message) def on_message(self, message): """Handle new messages which are received from the fan.""" # Prevent refreshing if not needed if self._old_value is None or self._old_value != self.state: _LOGGER.debug("Message received for %s device: %s", self.name, message) self._old_value = self.state self.schedule_update_ha_state() @property def should_poll(self): """No polling needed.""" return False @property def name(self): """Return the name of the Dyson sensor name.""" return self._name @property def unit_of_measurement(self): """Return the unit the value is expressed in.""" return SENSOR_UNITS[self._sensor_type] @property def icon(self): """Return the icon for this sensor.""" return SENSOR_ICONS[self._sensor_type] @property def unique_id(self): """Return the sensor's unique id.""" return f"{self._device.serial}-{self._sensor_type}" class DysonFilterLifeSensor(DysonSensor): """Representation of Dyson Filter Life sensor (in hours).""" def __init__(self, device): """Create a new Dyson Filter Life sensor.""" super().__init__(device, "filter_life") self._name = f"{self._device.name} Filter Life" @property def state(self): """Return filter life in hours.""" if self._device.state: return int(self._device.state.filter_life) return None class DysonDustSensor(DysonSensor): """Representation of Dyson Dust sensor (lower is better).""" def __init__(self, device): """Create a new Dyson Dust sensor.""" super().__init__(device, "dust") self._name = f"{self._device.name} Dust" @property def state(self): """Return Dust value.""" if self._device.environmental_state: return self._device.environmental_state.dust return None class DysonHumiditySensor(DysonSensor): """Representation of Dyson Humidity sensor.""" def __init__(self, device): """Create a new Dyson Humidity sensor.""" super().__init__(device, "humidity") self._name = f"{self._device.name} Humidity" @property def state(self): """Return Humidity value.""" if self._device.environmental_state: if self._device.environmental_state.humidity == 0: return STATE_OFF return self._device.environmental_state.humidity return None class DysonTemperatureSensor(DysonSensor): """Representation of Dyson Temperature sensor.""" def __init__(self, device, unit): """Create a new Dyson Temperature sensor.""" super().__init__(device, "temperature") self._name = f"{self._device.name} Temperature" self._unit = unit @property def state(self): """Return Temperature value.""" if self._device.environmental_state: temperature_kelvin = self._device.environmental_state.temperature if temperature_kelvin == 0: return STATE_OFF if self._unit == TEMP_CELSIUS: return float(f"{(temperature_kelvin - 273.15):.1f}") return float(f"{(temperature_kelvin * 9 / 5 - 459.67):.1f}") return None @property def unit_of_measurement(self): """Return the unit the value is expressed in.""" return self._unit class DysonAirQualitySensor(DysonSensor): """Representation of Dyson Air Quality sensor (lower is better).""" def __init__(self, device): """Create a new Dyson Air Quality sensor.""" super().__init__(device, "air_quality") self._name = f"{self._device.name} AQI" @property def state(self): """Return Air Quality value.""" if self._device.environmental_state: return int(self._device.environmental_state.volatil_organic_compounds) return None
#!/usr/bin/env python # # Copyright 2011 Free Software Foundation, Inc. # # This file is part of GNU Radio # # GNU Radio is free software; you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation; either version 3, or (at your option) # any later version. # # GNU Radio is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with GNU Radio; see the file COPYING. If not, write to # the Free Software Foundation, Inc., 51 Franklin Street, # Boston, MA 02110-1301, USA. # from gnuradio import gr, gr_unittest, digital class test_digital(gr_unittest.TestCase): def setUp(self): self.tb = gr.top_block() def tearDown(self): self.tb = None if __name__ == '__main__': gr_unittest.run(test_digital, "test_digital.xml")
from django.test import TestCase from django.test.utils import override_settings from template_preprocess.processor import process_template_content from template_preprocess.test import get_test_template_settings template_settings = get_test_template_settings() @override_settings(**template_settings) class TestExtendBlock(TestCase): def test_basic_block(self): content = '{% include "extends/sub_template1.html" %}' result = process_template_content(content) correct = ('Before {% block inserted_content %}The Block' '{%endblock inserted_content%} {% block block2 %}' 'Block 2{%endblock block2 %} {% block notreplaced %}' 'In wrapper{%endblock%} After ') self.assertEquals(result, correct) def test_extends_missing_template(self): content = '{% include "extends/parent_is_missing.html" %}' result = process_template_content(content) self.assertEquals(result, content) def test_recursive_extends(self): content = '{% include "extends/recursive.html" %}' result = process_template_content(content) self.assertEquals(result, content) def test_nested_blocks(self): content = '{% include "extends/nested.html" %}' result = process_template_content(content) self.assertEquals( result, '{% block a %}{% block b %}{% endblock b %}{% endblock %} ') def test_load_tag_outside_of_block(self): content = '{% include "extends/load_tag_out_of_block.html" %}' result = process_template_content(content) correct = ('{% load another more from app.templatetags %}' '{% load i18n %}Before {% block content %}' 'The content{% endblock %} After ') self.assertEquals(result, correct) def test_multiline_block(self): content = '{% include "extends/multiline.html" %}' result = process_template_content(content) correct = 'Before {%block ok%}Line 1 Line 2{%endblock%} ' self.assertEquals(result, correct)
from __future__ import print_function import argparse import collections import json import logging import sys from grocsvs import options as svoptions from grocsvs import log from grocsvs import pipeline from grocsvs import utilities from grocsvs import stages as svstages logging.basicConfig(format='%(message)s', level=logging.DEBUG) def ready_output_dir(options): utilities.ensure_dir(options.working_dir) utilities.ensure_dir(options.results_dir) utilities.ensure_dir(options.log_dir) def run(options): """ 1. create output directories 2. collect args for each stage 3. check which stages need to run 4. iterate through stages and submit jobs 5. validate that we're done running """ svoptions.validate_options(options) ready_output_dir(options) stages = get_stages() runner = pipeline.Runner(options) for stage_name, stage in stages.items(): runner.run_stage(stage, stage_name) def clean(options, clean_stage_name): stages = get_stages() if not clean_stage_name in stages: print('*'*20, "ERROR", '*'*20) print('Error: unknown stage "{}". Stage must be one of the '.format(clean_stage_name)) print('following (remember to include surrounding quotation marks):') for i, stage_name in enumerate(stages): print('{:>3} - "{}"'.format(i+1, stage_name)) sys.exit(1) doclean = False for stage_name, stage in stages.items(): if doclean or stage_name == clean_stage_name: doclean = True stage.clean_all_steps(options) def get_stages(): stages = collections.OrderedDict() # Pre-processing stages["Preflight"] = svstages.preflight.PreflightStep stages["Constants"] = svstages.constants.ConstantsStep stages["Estimate Read Cloud Parameters"] = svstages.call_readclouds.EstimateReadCloudParamsStep stages["Call Read Clouds"] = svstages.call_readclouds.CallReadcloudsStep stages["Combine Read Clouds"] = svstages.call_readclouds.CombineReadcloudsStep # stages["Filter Fragments"] = svstages.filter_fragments.FilterFragmentsStep stages["Sample Info"] = svstages.sample_info.SampleInfoStep stages["QC"] = svstages.qc.QCStep # Find SV candidates stages["Window Barcodes"] = svstages.window_barcodes.WindowBarcodesStep stages["Barcode Overlaps"] = svstages.barcode_overlaps.BarcodeOverlapsStep stages["SV Candidate Regions"] = \ svstages.sv_candidate_regions.SVCandidateRegionsStep stages["SV Candidates From Regions"] = \ svstages.sv_candidates.SVCandidatesStep ### Initial clustering ### stages["Refine Breakpoints"] = \ svstages.refine_grid_search_breakpoints.RefineGridSearchBreakpointsStep stages["Combine Refined Breakpoints"] = \ svstages.refine_grid_search_breakpoints.CombineRefinedBreakpointsStep stages["Cluster SVs"] = svstages.cluster_svs.ClusterSVsStep ### Assembly ### stages["Barcodes From Graphs"] = \ svstages.barcodes_from_graphs.BarcodesFromGraphsStep stages["Collect Reads for Barcodes"] = \ svstages.collect_reads_for_barcodes.CollectReadsForBarcodesStep stages["Perform Assembly"] = svstages.assembly.AssemblyStep stages["Walk Assemblies"] = svstages.walk_assemblies.WalkAssembliesStep stages["Postassembly Merge"] = \ svstages.postassembly_merge.PostAssemblyMergeStep ### Final clustering ### stages["Supporting Barcodes"] = \ svstages.supporting_barcodes.SupportingBarcodesStep stages["Pair Evidence"] = svstages.pair_evidence.PairEvidenceStep stages["Final Refine"] = \ svstages.refine_breakpoints.RefineBreakpointsWithAssembliesStep stages["Final Cluster SVs"] = svstages.final_clustering.FinalClusterSVsStep ### Multi-sample genotyping and postprocessing ### stages["Genotyping"] = svstages.genotyping.GenotypingStep stages["Merge Genotypes"] = svstages.genotyping.MergeGenotypesStep stages["Postprocessing"] = svstages.postprocessing.PostprocessingStep stages["Visualize"] = svstages.visualize.VisualizeStep return stages def load_config(config_path): try: config = json.load(open(config_path)) except ValueError as err: print("Error parsing configuration file '{}': '{}'\n Check that this is a properly formatted JSON file!".format(config_path, err)) sys.exit(1) options = svoptions.Options.deserialize(config, config_path) return options def parse_arguments(args): parser = argparse.ArgumentParser(description="Genome-wide Reconstruction of Complex Structural Variants") parser.add_argument("config", help="Path to configuration.json file") parser.add_argument("--restart", metavar="FROM-STAGE", help="restart from this stage") parser.add_argument("--local", action="store_true", help="run locally in single processor mode") parser.add_argument("--multiprocessing", action="store_true", help="run locally using multiprocessing") parser.add_argument("--debug", action="store_true", help="run in debug mode") if len(args) < 1: parser.print_help() sys.exit(1) args = parser.parse_args(args) options = load_config(args.config) options.debug = args.debug print(options) if args.local: options.cluster_settings = svoptions.ClusterSettings() if args.multiprocessing: options.cluster_settings = svoptions.ClusterSettings() options.cluster_settings.cluster_type = "multiprocessing" if args.restart is not None: clean(options, args.restart) log.log_command(options, sys.argv) return options def main(): options = parse_arguments(sys.argv[1:]) run(options) if __name__ == '__main__': main()
#!/usr/bin/python # -*- coding: utf-8 -*- # This file is part of Ansible # # Ansible is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Ansible is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Ansible. If not, see <http://www.gnu.org/licenses/>. ANSIBLE_METADATA = {'status': ['preview'], 'supported_by': 'community', 'version': '1.0'} DOCUMENTATION = """ --- module: vertica_facts version_added: '2.0' short_description: Gathers Vertica database facts. description: - Gathers Vertica database facts. options: cluster: description: - Name of the cluster running the schema. required: false default: localhost port: description: Database port to connect to. required: false default: 5433 db: description: - Name of the database running the schema. required: false default: null login_user: description: - The username used to authenticate with. required: false default: dbadmin login_password: description: - The password used to authenticate with. required: false default: null notes: - The default authentication assumes that you are either logging in as or sudo'ing to the C(dbadmin) account on the host. - This module uses C(pyodbc), a Python ODBC database adapter. You must ensure that C(unixODBC) and C(pyodbc) is installed on the host and properly configured. - Configuring C(unixODBC) for Vertica requires C(Driver = /opt/vertica/lib64/libverticaodbc.so) to be added to the C(Vertica) section of either C(/etc/odbcinst.ini) or C($HOME/.odbcinst.ini) and both C(ErrorMessagesPath = /opt/vertica/lib64) and C(DriverManagerEncoding = UTF-16) to be added to the C(Driver) section of either C(/etc/vertica.ini) or C($HOME/.vertica.ini). requirements: [ 'unixODBC', 'pyodbc' ] author: "Dariusz Owczarek (@dareko)" """ EXAMPLES = """ - name: gathering vertica facts vertica_facts: db=db_name """ try: import pyodbc except ImportError: pyodbc_found = False else: pyodbc_found = True from ansible.module_utils.basic import AnsibleModule from ansible.module_utils.pycompat24 import get_exception class NotSupportedError(Exception): pass # module specific functions def get_schema_facts(cursor, schema=''): facts = {} cursor.execute(""" select schema_name, schema_owner, create_time from schemata where not is_system_schema and schema_name not in ('public') and (? = '' or schema_name ilike ?) """, schema, schema) while True: rows = cursor.fetchmany(100) if not rows: break for row in rows: facts[row.schema_name.lower()] = { 'name': row.schema_name, 'owner': row.schema_owner, 'create_time': str(row.create_time), 'usage_roles': [], 'create_roles': []} cursor.execute(""" select g.object_name as schema_name, r.name as role_name, lower(g.privileges_description) privileges_description from roles r join grants g on g.grantee = r.name and g.object_type='SCHEMA' and g.privileges_description like '%USAGE%' and g.grantee not in ('public', 'dbadmin') and (? = '' or g.object_name ilike ?) """, schema, schema) while True: rows = cursor.fetchmany(100) if not rows: break for row in rows: schema_key = row.schema_name.lower() if 'create' in row.privileges_description: facts[schema_key]['create_roles'].append(row.role_name) else: facts[schema_key]['usage_roles'].append(row.role_name) return facts def get_user_facts(cursor, user=''): facts = {} cursor.execute(""" select u.user_name, u.is_locked, u.lock_time, p.password, p.acctexpired as is_expired, u.profile_name, u.resource_pool, u.all_roles, u.default_roles from users u join password_auditor p on p.user_id = u.user_id where not u.is_super_user and (? = '' or u.user_name ilike ?) """, user, user) while True: rows = cursor.fetchmany(100) if not rows: break for row in rows: user_key = row.user_name.lower() facts[user_key] = { 'name': row.user_name, 'locked': str(row.is_locked), 'password': row.password, 'expired': str(row.is_expired), 'profile': row.profile_name, 'resource_pool': row.resource_pool, 'roles': [], 'default_roles': []} if row.is_locked: facts[user_key]['locked_time'] = str(row.lock_time) if row.all_roles: facts[user_key]['roles'] = row.all_roles.replace(' ', '').split(',') if row.default_roles: facts[user_key]['default_roles'] = row.default_roles.replace(' ', '').split(',') return facts def get_role_facts(cursor, role=''): facts = {} cursor.execute(""" select r.name, r.assigned_roles from roles r where (? = '' or r.name ilike ?) """, role, role) while True: rows = cursor.fetchmany(100) if not rows: break for row in rows: role_key = row.name.lower() facts[role_key] = { 'name': row.name, 'assigned_roles': []} if row.assigned_roles: facts[role_key]['assigned_roles'] = row.assigned_roles.replace(' ', '').split(',') return facts def get_configuration_facts(cursor, parameter=''): facts = {} cursor.execute(""" select c.parameter_name, c.current_value, c.default_value from configuration_parameters c where c.node_name = 'ALL' and (? = '' or c.parameter_name ilike ?) """, parameter, parameter) while True: rows = cursor.fetchmany(100) if not rows: break for row in rows: facts[row.parameter_name.lower()] = { 'parameter_name': row.parameter_name, 'current_value': row.current_value, 'default_value': row.default_value} return facts def get_node_facts(cursor, schema=''): facts = {} cursor.execute(""" select node_name, node_address, export_address, node_state, node_type, catalog_path from nodes """) while True: rows = cursor.fetchmany(100) if not rows: break for row in rows: facts[row.node_address] = { 'node_name': row.node_name, 'export_address': row.export_address, 'node_state': row.node_state, 'node_type': row.node_type, 'catalog_path': row.catalog_path} return facts # module logic def main(): module = AnsibleModule( argument_spec=dict( cluster=dict(default='localhost'), port=dict(default='5433'), db=dict(default=None), login_user=dict(default='dbadmin'), login_password=dict(default=None), ), supports_check_mode = True) if not pyodbc_found: module.fail_json(msg="The python pyodbc module is required.") db = '' if module.params['db']: db = module.params['db'] try: dsn = ( "Driver=Vertica;" "Server=%s;" "Port=%s;" "Database=%s;" "User=%s;" "Password=%s;" "ConnectionLoadBalance=%s" ) % (module.params['cluster'], module.params['port'], db, module.params['login_user'], module.params['login_password'], 'true') db_conn = pyodbc.connect(dsn, autocommit=True) cursor = db_conn.cursor() except Exception: e = get_exception() module.fail_json(msg="Unable to connect to database: %s." % str(e)) try: schema_facts = get_schema_facts(cursor) user_facts = get_user_facts(cursor) role_facts = get_role_facts(cursor) configuration_facts = get_configuration_facts(cursor) node_facts = get_node_facts(cursor) module.exit_json(changed=False, ansible_facts={'vertica_schemas': schema_facts, 'vertica_users': user_facts, 'vertica_roles': role_facts, 'vertica_configuration': configuration_facts, 'vertica_nodes': node_facts}) except NotSupportedError: e = get_exception() module.fail_json(msg=str(e)) except SystemExit: # avoid catching this on python 2.4 raise except Exception: e = get_exception() module.fail_json(msg=e) if __name__ == '__main__': main()
# -*- coding: utf-8 -*- """ markupsafe ~~~~~~~~~~ Implements a Markup string. :copyright: (c) 2010 by Armin Ronacher. :license: BSD, see LICENSE for more details. """ import re import string from collections import Mapping from markupsafe._compat import text_type, string_types, int_types, \ unichr, iteritems, PY2 __version__ = "1.0" __all__ = ['Markup', 'soft_unicode', 'escape', 'escape_silent'] _striptags_re = re.compile(r'(<!--.*?-->|<[^>]*>)') _entity_re = re.compile(r'&([^& ;]+);') class Markup(text_type): r"""Marks a string as being safe for inclusion in HTML/XML output without needing to be escaped. This implements the `__html__` interface a couple of frameworks and web applications use. :class:`Markup` is a direct subclass of `unicode` and provides all the methods of `unicode` just that it escapes arguments passed and always returns `Markup`. The `escape` function returns markup objects so that double escaping can't happen. The constructor of the :class:`Markup` class can be used for three different things: When passed an unicode object it's assumed to be safe, when passed an object with an HTML representation (has an `__html__` method) that representation is used, otherwise the object passed is converted into a unicode string and then assumed to be safe: >>> Markup("Hello <em>World</em>!") Markup(u'Hello <em>World</em>!') >>> class Foo(object): ... def __html__(self): ... return '<a href="#">foo</a>' ... >>> Markup(Foo()) Markup(u'<a href="#">foo</a>') If you want object passed being always treated as unsafe you can use the :meth:`escape` classmethod to create a :class:`Markup` object: >>> Markup.escape("Hello <em>World</em>!") Markup(u'Hello &lt;em&gt;World&lt;/em&gt;!') Operations on a markup string are markup aware which means that all arguments are passed through the :func:`escape` function: >>> em = Markup("<em>%s</em>") >>> em % "foo & bar" Markup(u'<em>foo &amp; bar</em>') >>> strong = Markup("<strong>%(text)s</strong>") >>> strong % {'text': '<blink>hacker here</blink>'} Markup(u'<strong>&lt;blink&gt;hacker here&lt;/blink&gt;</strong>') >>> Markup("<em>Hello</em> ") + "<foo>" Markup(u'<em>Hello</em> &lt;foo&gt;') """ __slots__ = () def __new__(cls, base=u'', encoding=None, errors='strict'): if hasattr(base, '__html__'): base = base.__html__() if encoding is None: return text_type.__new__(cls, base) return text_type.__new__(cls, base, encoding, errors) def __html__(self): return self def __add__(self, other): if isinstance(other, string_types) or hasattr(other, '__html__'): return self.__class__(super(Markup, self).__add__(self.escape(other))) return NotImplemented def __radd__(self, other): if hasattr(other, '__html__') or isinstance(other, string_types): return self.escape(other).__add__(self) return NotImplemented def __mul__(self, num): if isinstance(num, int_types): return self.__class__(text_type.__mul__(self, num)) return NotImplemented __rmul__ = __mul__ def __mod__(self, arg): if isinstance(arg, tuple): arg = tuple(_MarkupEscapeHelper(x, self.escape) for x in arg) else: arg = _MarkupEscapeHelper(arg, self.escape) return self.__class__(text_type.__mod__(self, arg)) def __repr__(self): return '%s(%s)' % ( self.__class__.__name__, text_type.__repr__(self) ) def join(self, seq): return self.__class__(text_type.join(self, map(self.escape, seq))) join.__doc__ = text_type.join.__doc__ def split(self, *args, **kwargs): return list(map(self.__class__, text_type.split(self, *args, **kwargs))) split.__doc__ = text_type.split.__doc__ def rsplit(self, *args, **kwargs): return list(map(self.__class__, text_type.rsplit(self, *args, **kwargs))) rsplit.__doc__ = text_type.rsplit.__doc__ def splitlines(self, *args, **kwargs): return list(map(self.__class__, text_type.splitlines( self, *args, **kwargs))) splitlines.__doc__ = text_type.splitlines.__doc__ def unescape(self): r"""Unescape markup again into an text_type string. This also resolves known HTML4 and XHTML entities: >>> Markup("Main &raquo; <em>About</em>").unescape() u'Main \xbb <em>About</em>' """ from markupsafe._constants import HTML_ENTITIES def handle_match(m): name = m.group(1) if name in HTML_ENTITIES: return unichr(HTML_ENTITIES[name]) try: if name[:2] in ('#x', '#X'): return unichr(int(name[2:], 16)) elif name.startswith('#'): return unichr(int(name[1:])) except ValueError: pass # Don't modify unexpected input. return m.group() return _entity_re.sub(handle_match, text_type(self)) def striptags(self): r"""Unescape markup into an text_type string and strip all tags. This also resolves known HTML4 and XHTML entities. Whitespace is normalized to one: >>> Markup("Main &raquo; <em>About</em>").striptags() u'Main \xbb About' """ stripped = u' '.join(_striptags_re.sub('', self).split()) return Markup(stripped).unescape() @classmethod def escape(cls, s): """Escape the string. Works like :func:`escape` with the difference that for subclasses of :class:`Markup` this function would return the correct subclass. """ rv = escape(s) if rv.__class__ is not cls: return cls(rv) return rv def make_simple_escaping_wrapper(name): orig = getattr(text_type, name) def func(self, *args, **kwargs): args = _escape_argspec(list(args), enumerate(args), self.escape) _escape_argspec(kwargs, iteritems(kwargs), self.escape) return self.__class__(orig(self, *args, **kwargs)) func.__name__ = orig.__name__ func.__doc__ = orig.__doc__ return func for method in '__getitem__', 'capitalize', \ 'title', 'lower', 'upper', 'replace', 'ljust', \ 'rjust', 'lstrip', 'rstrip', 'center', 'strip', \ 'translate', 'expandtabs', 'swapcase', 'zfill': locals()[method] = make_simple_escaping_wrapper(method) # new in python 2.5 if hasattr(text_type, 'partition'): def partition(self, sep): return tuple(map(self.__class__, text_type.partition(self, self.escape(sep)))) def rpartition(self, sep): return tuple(map(self.__class__, text_type.rpartition(self, self.escape(sep)))) # new in python 2.6 if hasattr(text_type, 'format'): def format(*args, **kwargs): self, args = args[0], args[1:] formatter = EscapeFormatter(self.escape) kwargs = _MagicFormatMapping(args, kwargs) return self.__class__(formatter.vformat(self, args, kwargs)) def __html_format__(self, format_spec): if format_spec: raise ValueError('Unsupported format specification ' 'for Markup.') return self # not in python 3 if hasattr(text_type, '__getslice__'): __getslice__ = make_simple_escaping_wrapper('__getslice__') del method, make_simple_escaping_wrapper class _MagicFormatMapping(Mapping): """This class implements a dummy wrapper to fix a bug in the Python standard library for string formatting. See http://bugs.python.org/issue13598 for information about why this is necessary. """ def __init__(self, args, kwargs): self._args = args self._kwargs = kwargs self._last_index = 0 def __getitem__(self, key): if key == '': idx = self._last_index self._last_index += 1 try: return self._args[idx] except LookupError: pass key = str(idx) return self._kwargs[key] def __iter__(self): return iter(self._kwargs) def __len__(self): return len(self._kwargs) if hasattr(text_type, 'format'): class EscapeFormatter(string.Formatter): def __init__(self, escape): self.escape = escape def format_field(self, value, format_spec): if hasattr(value, '__html_format__'): rv = value.__html_format__(format_spec) elif hasattr(value, '__html__'): if format_spec: raise ValueError('No format specification allowed ' 'when formatting an object with ' 'its __html__ method.') rv = value.__html__() else: # We need to make sure the format spec is unicode here as # otherwise the wrong callback methods are invoked. For # instance a byte string there would invoke __str__ and # not __unicode__. rv = string.Formatter.format_field( self, value, text_type(format_spec)) return text_type(self.escape(rv)) def _escape_argspec(obj, iterable, escape): """Helper for various string-wrapped functions.""" for key, value in iterable: if hasattr(value, '__html__') or isinstance(value, string_types): obj[key] = escape(value) return obj class _MarkupEscapeHelper(object): """Helper for Markup.__mod__""" def __init__(self, obj, escape): self.obj = obj self.escape = escape __getitem__ = lambda s, x: _MarkupEscapeHelper(s.obj[x], s.escape) __unicode__ = __str__ = lambda s: text_type(s.escape(s.obj)) __repr__ = lambda s: str(s.escape(repr(s.obj))) __int__ = lambda s: int(s.obj) __float__ = lambda s: float(s.obj) # we have to import it down here as the speedups and native # modules imports the markup type which is define above. try: from markupsafe._speedups import escape, escape_silent, soft_unicode except ImportError: from markupsafe._native import escape, escape_silent, soft_unicode if not PY2: soft_str = soft_unicode __all__.append('soft_str')
""" Views for groups info API """ from rest_framework import generics, status, mixins from rest_framework.response import Response from django.conf import settings import facebook from ...utils import mobile_view from . import serializers @mobile_view() class Groups(generics.CreateAPIView, mixins.DestroyModelMixin): """ **Use Case** An API to Create or Delete course groups. Note: The Delete is not invoked from the current version of the app and is used only for testing with facebook dependencies. **Creation Example request**: POST /api/mobile/v0.5/social/facebook/groups/ Parameters: name : string, description : string, privacy : open/closed **Creation Response Values** {"id": group_id} **Deletion Example request**: DELETE /api/mobile/v0.5/social/facebook/groups/<group_id> **Deletion Response Values** {"success" : "true"} """ serializer_class = serializers.GroupSerializer def create(self, request, *args, **kwargs): serializer = self.get_serializer(data=request.DATA, files=request.FILES) if not serializer.is_valid(): return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST) try: app_groups_response = facebook_graph_api().request( settings.FACEBOOK_API_VERSION + '/' + settings.FACEBOOK_APP_ID + "/groups", post_args=request.POST.dict() ) return Response(app_groups_response) except facebook.GraphAPIError, ex: return Response({'error': ex.result['error']['message']}, status=status.HTTP_400_BAD_REQUEST) def delete(self, request, *args, **kwargs): # pylint: disable=unused-argument """ Deletes the course group. """ try: return Response( facebook_graph_api().request( settings.FACEBOOK_API_VERSION + '/' + settings.FACEBOOK_APP_ID + "/groups/" + kwargs['group_id'], post_args={'method': 'delete'} ) ) except facebook.GraphAPIError, ex: return Response({'error': ex.result['error']['message']}, status=status.HTTP_400_BAD_REQUEST) @mobile_view() class GroupsMembers(generics.CreateAPIView, mixins.DestroyModelMixin): """ **Use Case** An API to Invite and Remove members to a group Note: The Remove is not invoked from the current version of the app and is used only for testing with facebook dependencies. **Invite Example request**: POST /api/mobile/v0.5/social/facebook/groups/<group_id>/member/ Parameters: members : int,int,int... **Invite Response Values** {"member_id" : success/error_message} A response with each member_id and whether or not the member was added successfully. If the member was not added successfully the Facebook error message is provided. **Remove Example request**: DELETE /api/mobile/v0.5/social/facebook/groups/<group_id>/member/<member_id> **Remove Response Values** {"success" : "true"} """ serializer_class = serializers.GroupsMembersSerializer def create(self, request, *args, **kwargs): serializer = self.get_serializer(data=request.DATA, files=request.FILES) if not serializer.is_valid(): return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST) graph = facebook_graph_api() url = settings.FACEBOOK_API_VERSION + '/' + kwargs['group_id'] + "/members" member_ids = serializer.object['member_ids'].split(',') response = {} for member_id in member_ids: try: if 'success' in graph.request(url, post_args={'member': member_id}): response[member_id] = 'success' except facebook.GraphAPIError, ex: response[member_id] = ex.result['error']['message'] return Response(response, status=status.HTTP_200_OK) def delete(self, request, *args, **kwargs): # pylint: disable=unused-argument """ Deletes the member from the course group. """ try: return Response( facebook_graph_api().request( settings.FACEBOOK_API_VERSION + '/' + kwargs['group_id'] + "/members", post_args={'method': 'delete', 'member': kwargs['member_id']} ) ) except facebook.GraphAPIError, ex: return Response({'error': ex.result['error']['message']}, status=status.HTTP_400_BAD_REQUEST) def facebook_graph_api(): """ Returns the result from calling Facebook's Graph API with the app's access token. """ return facebook.GraphAPI(facebook.get_app_access_token(settings.FACEBOOK_APP_ID, settings.FACEBOOK_APP_SECRET))
# -*- coding: utf-8 -*- from __future__ import print_function from __future__ import unicode_literals from __future__ import division from time import sleep from django.core.urlresolvers import reverse from django.core import mail from registration.models import RegistrationProfile from treemap.tests.ui import UITestCase from treemap.tests import make_user, create_mock_system_user class LoginLogoutTest(UITestCase): def setUp(self): create_mock_system_user() super(LoginLogoutTest, self).setUp() self.user = make_user(username='username', password='password') self.profile = RegistrationProfile.objects.create_profile(self.user) def test_invalid_login(self): self.browse_to_url(reverse('auth_login')) login_url = self.driver.current_url self.process_login_form( self.user.username, 'passwordinvalid') # There should be an error list with at least one element self.wait_until_present('.errorlist li') # We should be on the same page self.assertEqual(login_url, self.driver.current_url) def test_valid_login(self): self.browse_to_url(reverse('auth_login')) login_url = self.driver.current_url self.process_login_form(self.user.username, 'password') email_element = self.wait_until_present( '[data-field="user.email"][data-class="display"]') # We should not be on the same page self.assertNotEqual(login_url, self.driver.current_url) # We should expect our username in the url self.assertIn(self.user.username, self.driver.current_url) value = email_element.get_attribute('data-value') self.assertEqual(self.user.email, value) sleep(1) # prevent hang class ForgotUsernameTest(UITestCase): def setUp(self): create_mock_system_user() super(ForgotUsernameTest, self).setUp() self.user = make_user(username='username', password='password') def tearDown(self): mail.outbox = [] super(ForgotUsernameTest, self).tearDown() def test_can_get_to_page(self): self.browse_to_url(reverse('auth_login')) forgot_username_url = reverse('forgot_username') link = self.find_anchor_by_url(forgot_username_url) link.click() self.wait_until_present('input[name="email"]') self.assertEqual(self.live_server_url + forgot_username_url, self.driver.current_url) def test_can_retrieve_username(self): self.browse_to_url(reverse('forgot_username')) email_elem = self.driver.find_element_by_name('email') email_elem.send_keys(self.user.email) self.click('form input[type="submit"]') self.wait_until_text_present('Email Sent') self.assertEqual(len(mail.outbox), 1)
#!/usr/bin/env python3 import builtins from pmlr import pmlr debug_write = pmlr.util.debug_write ERR_DATA = { ZeroDivisionError: {"IS_FATAL": False, "TYPE": "DEBUG"}, LookupError: {"IS_FATAL": False, "TYPE": "RANGE"}, IndexError: {"IS_FATAL": False, "TYPE": "RANGE"}, TypeError: {"IS_FATAL": True, "TYPE": "ERROR"}, NameError: {"IS_FATAL": True, "TYPE": "FATAL"}, ValueError: {"IS_FATAL": True, "TYPE": "FATAL"}, AssertionError: {"IS_FATAL": True, "TYPE": "FATAL"}, } def is_none(*args): return None in args def cmp_all(val, *tests): return builtins.all([val == test for test in tests]) def all(*args): return builtins.all(args) def any(*args): return builtins.any(args) class Forth(object): def __init__(self): (self._stk, self._lopstk, self._retstk, self._sftstk) = [Stack() for i in range(4)] self.dict = { "": () } self.funcdict = { "": () } def run(self, prog, sandbox=False): pass def define(self, name, defn): defn = " ".join(defn).strip() try: self.run(defn, sandbox=True) except MalformedExpressionException as err: debug_write(err.msg, level=err.level) return None # {"name": "None", "desc": "debug"} class OpCore(): """bare stack operator mixin""" def peek(self, from_idx=0, to_idx=-1): return self._stk[:] def pop(self, count=1, idx=-1): """( x -- ) take something and return it""" if count > len(self._stk): pmlr.util.debug_write( "popping more items than exist on stack!\n", level="WARN" ) # http://stackoverflow.com/a/34633242/4532996 # from testing it seems that pop(x) is slower than pop() # pop(-1) doesn't seem to be optimised to pop(), # so avoid it if possible x = [] if -1 == idx: for i in range(count): try: x.append(self._stk.pop()) except LookupError as err: self.err(err, errtype="RANGE") break else: for i in range(count): try: x.append(self._stk.pop(idx)) except LookupError as err: self.err(err, errtype="RANGE") break return x[0] if len(x) == 1 else list(reversed(x)) def push(self, *args, idx=-1): """( -- x ... ) put somethings at idx""" if idx == -1: self._stk.extend(args) else: if idx < 0: for arg in args: self._stk.insert(idx, arg) idx -= 1 else: for arg in args: self._stk.insert(idx, arg) idx += 1 def clear(self): """( z y x -- ) clear the stack completely""" y = self._stk.copy() self._stk.clear() return y def pick(self, idx=-3, drop=False): """( x -- x ) pick somethings from a range of indicies""" s = self._stk[idx] if drop: self._stk[idx] = [] return s def drop(self, count=1, idx=-1): """( x -- ) drop items without returning (cheaper pop)""" [self.pop(idx=idx) for i in range(count)] def dup(self, count=1, from_idx=-1): """( y -- y y ) duplicate something and push""" try: y = self._stk[from_idx] * count except LookupError as err: self.err(err, errtype="RANGE") self.push(*y, idx=idx) def dupn(self, count=2, idx=-1): """( x y -- x y x y ) dup count items from an idx""" y = [] for i in range(count): try: y.append(self._stk[idx - i]) except LookupError as err: if idx == 1: continue else: self.err(err, errtype="RANGE") return None self.push(*y, idx=idx) def swap(self, idx=-1): """( x y -- y x ) swap two things at an index""" self.push(*reversed([self.pop(idx=idx) for i in range(2)]), idx=idx) def rot(self, idx=-1, count=3): """( w x y z -- x y z w ) rotate things left, at an index""" l = [self.pop(idx=idx) for i in range(count)] l.insert(0, l.pop()) self.push(*l, idx=idx) def urot(self, idx=-1, count=3): """( w x y z -- z w x y ) rotate things right, at an index""" l = [self.pop(idx=idx) for i in range(count)] l.append(l.pop(0)) self.push(*l, idx=idx) class OpLogik(): pass class OpString(): pass class Stack(OpCore, OpLogik, OpString): "the mixin mixer of the above mixins" def __init__(self): self._stk = [] def err(self, err, errtype=None, framelevel=3): if errtype is None: errtype = ERR_DATA.get(err.__class__, {"TYPE": "FATAL"})["TYPE"] errtype = errtype.upper() debug_write(*err.args, "\n", level=errtype, framelevel=framelevel) if ERR_DATA.get(err.__class__, {"IS_FATAL": True})["IS_FATAL"]: raise err.__class__( pmlr.util.debug_fmt( errtype, framelevel=framelevel ) + " " + "".join([str(i) for i in err.args]) ) def __repr__(self): return "<{}> {}".format(len(self._stk), _fmt_collection(self._stk)) is_collection = lambda c: any(issubclass(c.__class__, (list, tuple, dict, set)), isinstance(c, (list, tuple, dict, set))) def _fmt_collection(col): "format a collection literal" t_super = col.__class__ try: t_mro = t_super.mro() t_meta = t_mro[1] if cmp_all(type(t_meta), object, type, type(object), type(type)): raise TypeError except (NameError, TypeError, IndexError, AttributeError) as err: if cmp_all(err.__class__, NameError, AttributeError) and not hasattr(t_super, "mro"): raise else: raise TypeError("need object instance but found {} (class constructor, type or object object)".format(type(col))) is_iter = hasattr(col, "__iter__") is_meta_iter = hasattr(col.__class__, "__iter__") if not any(is_iter, is_meta_iter): raise TypeError("({}) {} object is not iterable".format(col, col.__class__)) orderedary = (list, tuple, set) if any(isinstance(col, orderedary), issubclass(col.__class__, orderedary)): return "[ {} ]".format(" ".join(repr(i) if not is_collection(i) else _fmt_collection(i) for i in col)) elif any(isinstance(col, dict), issubclass(col, dict)): return " ".join("{}:{}".format(str(key), str(value)) for key, value in col.items()) else: raise TypeError("don't know how to format that container") return locals() if __name__ == "__main__": from tests import main as test_main test_main()
#!/usr/bin/env python """blinky.py: A small library that uses wiriping pi access to raspbery pi GPIO ports,aimed at providing a simple notification interface""" __author__ = "minos197@gmail.com" __license__ = "LGPL" __version__ = "0.0.1" __email__ = "Minos Galanakis" __project__ = "smartpi" __date__ = "01-06-2015" import io import time import fcntl import serial import struct from subprocess import Popen, PIPE from colorlogger import CLogger from functools import wraps from pidaemon import start_daemon, kill_daemon, normal_start def blinker(color, period=0.2, times=3): """ Decorator that allows modular output formating for PiLogger """ def blinker_decorator(func): @wraps(func) def func_wrapper(class_obj, message): # Blinke the LED before printing sdout class_obj.blink(color, times, period) return func(class_obj, color, message) return func_wrapper return blinker_decorator class PiBlinkerError(Exception): __module__ = 'exceptions' class PiBlinker(): def __init__(self): raise ValueError('PiBlinker is not meant to be instantiated') @classmethod def setup(self, log_level="ver_debug", log_label="PiBlinker", log_path=None, log_colors=None): """ Module Init.""" # Map a color to GPIO.BCM PIN self.LEDS = {"RED": [17], "GREEN": [18], "BLUE": [27], "PURPLE": [17, 27], "YELLOW": [17, 18], "CYAN": [18, 27], "WHITE": [17, 18, 27]} self.last_mode = 0 # Configure the GPIO ports in hardware map(self.run, [(x % v) for n in self.LEDS.values() for v in n for x in ["gpio export %d out", "gpio -g mode %d out"]]) self.i2c_devices = {} # Assosiate log levels with colors if not log_colors: log_colors = {"base_color": "CYAN", "info": "HBLUE", "warning": "YELLOW", "error": "RED", "debug": "GREEN", "ver_debug": "GREEN"} # Initalise the logging module CLogger.setup(log_label, log_level, log_path, log_colors) return self @staticmethod def run(cmd): """ Execute shell command in detached mdoe.""" proc = Popen([cmd], stdout=PIPE, stderr=PIPE, shell=True) ret, err = proc.communicate() if err: # ignore warnings in error stream if "Warning" in err: CLogger.warning(err.strip()) return err raise PiBlinkerError(err) else: return ret @classmethod def set_led(self, led, mode): """ Set an LED to one of the supported states.""" if led not in self.LEDS.keys(): return mlist = {"ON": 1, "OFF": 0, "Toggle": -1} # convert input to a numerical mode try: md = mode if mode not in mlist\ else {k: v for k, v in mlist.iteritems()}[mode] except KeyError: raise PiBlinkerError("Mode %s is not reognised" % mode) # Toggle the led if required led_state = md if md >= 0 else (self.last_mode + 1) % 2 # Toggle the GPIO map(self.run, ["gpio -g write %d %d" % (led_no, led_state) for led_no in self.LEDS[led]]) self.last_mode = led_state @classmethod def blink(self, led, times, delay=1): """ Blink an LED n number of times.""" # Make sure led is uppercase led = led.upper() if led not in self.LEDS.keys(): return mode = 0 count = 1 while (count <= times * 2): self.set_led(led, mode) time.sleep(delay) mode = (mode + 1) % 2 count += 1 self.set_led(led, mode) @classmethod def led_print(self, color, text): """ Print a debug message and notify the user with the LED.""" eval("self.%s" % color.lower())(text) @classmethod def led_bcast(self, data): """ Broadcast a number through led brings """ import re # separate the numbers in the string ie 192.168.3.1 will become array data = map(int, filter(lambda x: x, re.split(r'\D', data))) # Separate the digits to a three color tuple data = map(lambda x: (x/100, (x % 100)/10, (x % 10)), data) for red_cnt, green_cnt, blue_cnt in data: self.blink("GREEN", 1, 1) time.sleep(0.5) self.blink("RED", red_cnt, 0.2) time.sleep(0.5) self.blink("GREEN", green_cnt, 0.2) time.sleep(0.5) self.blink("BLUE", blue_cnt, 0.2) time.sleep(0.5) self.blink("RED", 1, 1) @classmethod @blinker("RED") def red(self, *args): """ Print a debug message and notify the user with the LED.""" color, message = args print"|%s|> %s" % (color, message) @classmethod @blinker("GREEN") def green(self, *args): """ Print a debug message and notify the user with the LED.""" color, message = args print"|%s|> %s" % (color, message) @classmethod @blinker("BLUE") def blue(self, *args): """ Print a debug message and notify the user with the LED.""" color, message = args print"|%s|> %s" % (color, message) @classmethod @blinker("RED") def error(self, *args): """ Print a debug message and notify the user with the LED.""" CLogger.error(args[-1]) @classmethod @blinker("BLUE") def info(self, *args): """ Print a debug message and notify the user with the LED.""" CLogger.info(args[-1]) @classmethod @blinker("RED") def warning(self, *args): """ Print a debug message and notify the user with the LED.""" CLogger.warning(args[-1]) @classmethod @blinker("GREEN") def debug(self, *args): """ Print a debug message and notify the user with the LED.""" CLogger.debug(args[-1]) @classmethod def uart_open(self, port="/dev/ttyAMA0", baud=9600, time_out=None): """Open the Serial Channel""" try: self.uart = serial.Serial(port, baud, timeout=time_out) except serial.SerialException: print "** Failed to initialize serial, check your port.** " raise ValueError @classmethod def uart_activate(self): """ Spam UART port untill it receives an ACK """ self.uart_open() countr = 0 # Test with a not supported command t_char = "O" while True: self.uart.write(t_char) if self.uart.inWaiting(): repl = self.uart.read(2) if repl == "OK": print "UART Activated" else: print "UART was already enabled" break elif countr == 99: # Test with a supported command to see if activated t_char = "2" elif countr > 100: break time.sleep(0.05) countr += 1 @classmethod def uart_read(self, target="ADC"): """Read the register through uart""" cmd = {"ADC": "2", "PIN": "1"} if target in cmd.keys(): self.uart.write(cmd[target]) return self.uart.readline()[:-1] @classmethod def uart_close(self): """Close the serial channel""" self.uart.close() @classmethod def i2c_open_file(self, slave_id, bus=1): """Open the I2C channel for raw byte comms""" if slave_id in self.i2c_devices.keys(): print "Device %d already open" % slave_id return # Open the file descriptors read_ch = io.open("/dev/i2c-"+str(bus), "rb", buffering=0) write_ch = io.open("/dev/i2c-"+str(bus), "wb", buffering=0) # Set the register fcntl.ioctl(read_ch, 0x0703, slave_id) fcntl.ioctl(write_ch, 0x0703, slave_id) # store it to an internal dict self.i2c_devices[slave_id] = (read_ch, write_ch) # return the file descriptors if the user wants to manually drive them return (read_ch, write_ch) @classmethod def i2c_write_as(self, slave_id, format, data): """Write the data formatted using struct pack, Format needs to be specified""" try: wb_file = self.i2c_devices[slave_id][1] wb_file.write(struct.pack(format, data)) except KeyError: print "Device %d does not exist" % slave_id except struct.error: print "Pack Error make sure the data fits the format structure" except: raise IOError @classmethod def i2c_read_as(self, slave_id, format, byte_no): try: rb_file = self.i2c_devices[slave_id][0] return struct.unpack(format, rb_file.read(byte_no)) except KeyError: print "Device %d does not exit" % slave_id except struct.error: print "Pack Error make sure the data fits the format structure" except: raise IOError @classmethod def i2c_close(self, slave_id): """Close the file descriptors associated to the slave channel""" try: self.i2c_devices.pop(slave_id) except KeyError: print "Device %d does not exit" % slave_id @classmethod def demux(self, data): """ For efficiency purposes 10Bit ADC are muxed GPIO state.""" adc_val = data & 0x3FF pin_val = (data >> 15) return (adc_val, pin_val) @classmethod def i2c_read_adc(self, slave_id): """Reads data as returned from a 10Bit ADC sampling operation""" return self.demux(self.i2c_read_as(slave_id, '>H', 2)[0])[0] @classmethod def i2c_read_pin(self, slave_id): """Reads data as returned from a 10Bit ADC sampling operation""" return self.demux(self.i2c_read_as(slave_id, '>H', 2)[0])[1] @classmethod def test_hardware(self): """ Detect hardware shield's presense """ detected = False try: self.uart_open(time_out=2) reading = self.uart_read("ADC") if len(reading): detected = True except: pass try: readf, writef = self.i2c_open_file(0x04, 1) self.i2c_read_as(04, ">H", 2) self.i2c_close(0x04) detected = True except: pass return detected if __name__ == "__main__": import argparse parser = argparse.ArgumentParser() parser.add_argument("-t", "--test", help="Test Hardware, select from [all,\ i2c, led, log, poll, uart]", dest='test') parser.add_argument("-a", "--activate", help="Activate UART mode\ after a reset", action="store_true") parser.add_argument("-d", "--daemon", help="Start a button monitor daemon", action="store_true") parser.add_argument("-nd", "--nodaemon", help="Start monitor without\ daemon context used in conjuction with wrappers", action="store_true") parser.add_argument("-b1", "--button1", help="Bind script to button1", dest='button1') parser.add_argument("-b2", "--button2", help="Bind script to button2", dest='button2') parser.add_argument("-u", "--user", help="Select different user\ to run script as") parser.add_argument("-s", "--sudopass", help="Set optional sudo password\ for elevated priviledges") parser.add_argument("-k", "--kill", help="increase output verbosity", action="store_true") parser.add_argument("-i", "--blinkip", help="increase output verbosity", action="store_true") args = parser.parse_args() mode = 0 pb = PiBlinker.setup() if args.daemon or args.nodaemon: arguments = [args.button1, args.button2, args.user, args.sudopass] if args.nodaemon: normal_start(*arguments) else: start_daemon(*arguments) elif args.kill: kill_daemon() elif args.activate: pb.uart_activate() elif args.blinkip: pb.led_bcast(pb.run("hostname -I")) elif args.test: if args.test == "all": pb.red("This is important") pb.green("This worked") pb.blue("This you should know") readf, writef = pb.i2c_open_file(0x04, 1) # read two bytes using the direct file descriptor print "|RAW ADC|>", repr(readf.read(2)) # read a 2byte uint8_t variable print "|DEC ADC|>", pb.i2c_read_as(04, ">H", 2)[0] pb.i2c_close(0x04) pb.info("This is an info") pb.warning("This is a warning") pb.error("This is an error") pb.debug("This is debug") elif args.test == "i2c": readf, writef = pb.i2c_open_file(0x04, 1) # read two bytes using the direct file descriptor print "|RAW ADC|>", repr(readf.read(2)) # read a 2byte uint8_t variable print "|DEC ADC|>", pb.i2c_read_as(04, ">H", 2)[0] pb.i2c_close(0x04) elif args.test == "poll": readf, writef = pb.i2c_open_file(0x04, 1) try: while True: # Read using read ADC print "| ADC:", pb.i2c_read_adc(0x04), "| PIN: ",\ pb.i2c_read_pin(0x04), "|" time.sleep(0.2) except KeyboardInterrupt: pass pb.i2c_close(0x04) elif args.test == "uart": pb.uart_open() print "ADC:", pb.uart_read("ADC") print "PIN:", pb.uart_read("PIN") pb.uart_close() elif args.test == "led": pb.led_print("RED", "This is RED") pb.led_print("GREEN", "This is GREEN") pb.led_print("BLUE", "This is BLUE") elif args.test == "log": pb.info("This is an info") pb.warning("This is a warning") pb.error("This is an error") pb.debug("This is debug") else: parser.print_help()
#!/usr/bin/env python from __future__ import print_function from ImageD11.grain import read_grain_file import sys, os gf = read_grain_file(sys.argv[1]) mapfile=open(sys.argv[2],"w") def dodot(xyz,k): mapfile.write("%f %f %f %d\n"%(xyz[0],xyz[1],xyz[2],k)) def getmedian(s): items=s.split() j = -1 for i in range(len(items)): if items[i] == "median": j = i if j == -1: return 0 return abs(float(items[j+2])) try: outersf = float(sys.argv[3]) except: outersf = 1.0 print("Scale factor is",outersf) for g in gf: #print g.translation, g.ubi mapfile.write("\n\n") o = g.translation try: sf = pow(getmedian(g.intensity_info),0.3333)*outersf except: sf = outersf try: k = int(g.npks) except: k = 1 for u in g.ubi: dodot(o,k) dodot(o+u*sf,int(g.npks)) for u in g.ubi: dodot(o,k) dodot(o-u*sf,int(g.npks)) # dodot(o,k) # dodot(o+sf*(-g.ubi[0]-g.ubi[1]),k) # dodot(o,k) # dodot(o+sf*(g.ubi[0]+g.ubi[1]),k) mapfile.close() term = " " if "linux" in sys.platform: term = "set term x11" if "win32" in sys.platform: term = "set term windows" open("gnuplot.in","w").write(""" %s set ticslevel 0 set title "Color proportional to number of peaks" set palette model RGB set palette defined ( 0 "violet", 1 "blue", 2 "green", 3 "yellow", 4 "orange", 5 "red" ) set view equal xyz set view 75,0,1,1 #set terminal gif animate delay 10 loop 1 optimize size 1024,768 set nokey set hidden3d #set output "ImageD11map.gif" splot "%s" u 1:2:3:4 w l lw 2 lc pal z """%(term, sys.argv[2]) # "".join(["set view 75,%d\n replot\n"%(i) for i in range(1,360,1)]) ) os.system("gnuplot -background white gnuplot.in -")
# -*- coding: utf-8 -*- ############################################################################## # # Copyright (C) 2016 Compassion CH (http://www.compassion.ch) # Releasing children from poverty in Jesus' name # @author: Emanuel Cino <ecino@compassion.ch> # # The licence is in the file __manifest__.py # ############################################################################## from odoo.addons.message_center_compassion.mappings.base_mapping import \ OnrampMapping class HouseHoldMapping(OnrampMapping): ODOO_MODEL = 'compassion.household' CONNECT_MAPPING = { "BeneficiaryHouseholdMemberList": ('member_ids', 'compassion.household.member'), "BeneficiaryHouseholdMemberDetails": ('member_ids', 'compassion.household.member'), "FemaleGuardianEmploymentStatus": 'female_guardian_job_type', "FemaleGuardianOccupation": 'female_guardian_job', "Household_ID": "household_id", "Household_Name": "name", "IsNaturalFatherLivingWithChild": 'father_living_with_child', "IsNaturalMotherLivingWithChild": 'mother_living_with_child', "MaleGuardianEmploymentStatus": 'male_guardian_job_type', "MaleGuardianOccupation": "male_guardian_job", "NaturalFatherAlive": "father_alive", "NaturalMotherAlive": "mother_alive", "NumberOfSiblingBeneficiaries": "number_beneficiaries", "ParentsMaritalStatus": "marital_status", "ParentsTogether": "parents_together", 'RevisedValues': 'revised_value_ids', # Not define "SourceKitName": None, } def _process_odoo_data(self, odoo_data): # Unlink old revised values and create new ones if isinstance(odoo_data.get('revised_value_ids'), list): household = self.env[self.ODOO_MODEL].search( [('household_id', '=', odoo_data['household_id'])]) household.revised_value_ids.unlink() for value in odoo_data['revised_value_ids']: self.env['compassion.major.revision'].create({ 'name': value, 'household_id': household.id, }) del odoo_data['revised_value_ids'] # Replace dict by a tuple for the ORM update/create if 'member_ids' in odoo_data: # Remove all members household = self.env[self.ODOO_MODEL].search( [('household_id', '=', odoo_data['household_id'])]) household.member_ids.unlink() member_list = list() for member in odoo_data['member_ids']: orm_tuple = (0, 0, member) member_list.append(orm_tuple) odoo_data['member_ids'] = member_list or False for key in odoo_data.iterkeys(): val = odoo_data[key] if isinstance(val, basestring) and val.lower() in ( 'null', 'false', 'none', 'other', 'unknown'): odoo_data[key] = False class HouseholdMemberMapping(OnrampMapping): ODOO_MODEL = 'compassion.household.member' CONNECT_MAPPING = { "Beneficiary_GlobalID": ('child_id.global_id', 'compassion.child'), "Beneficiary_LocalID": 'beneficiary_local_id', "FullName": None, "HouseholdMemberRole": 'role', "HouseholdMember_Name": 'name', "IsCaregiver": 'is_caregiver', "IsPrimaryCaregiver": 'is_primary_caregiver', }
# ---------------------------------------------------------------------- # Numenta Platform for Intelligent Computing (NuPIC) # Copyright (C) 2013, Numenta, Inc. Unless you have an agreement # with Numenta, Inc., for a separate license for this software code, the # following terms and conditions apply: # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero Public License version 3 as # published by the Free Software Foundation. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. # See the GNU Affero Public License for more details. # # You should have received a copy of the GNU Affero Public License # along with this program. If not, see http://www.gnu.org/licenses. # # http://numenta.org/licenses/ # ---------------------------------------------------------------------- import numpy from nupic.data.fieldmeta import FieldMetaType from nupic.data import SENTINEL_VALUE_FOR_MISSING_DATA from nupic.encoders.base import Encoder, EncoderResult from nupic.encoders.scalar import ScalarEncoder UNKNOWN = "<UNKNOWN>" class CategoryEncoder(Encoder): """Encodes a list of discrete categories (described by strings), that aren't related to each other, so we never emit a mixture of categories. The value of zero is reserved for "unknown category" Internally we use a ScalarEncoder with a radius of 1, but since we only encode integers, we never get mixture outputs. The SDRCategoryEncoder uses a different method to encode categories""" def __init__(self, w, categoryList, name="category", verbosity=0, forced=False): """params: forced (default False) : if True, skip checks for parameters' settings; see encoders/scalar.py for details """ self.encoders = None self.verbosity = verbosity # number of categories includes "unknown" self.ncategories = len(categoryList) + 1 self.categoryToIndex = dict() self.indexToCategory = dict() self.indexToCategory[0] = UNKNOWN for i in xrange(len(categoryList)): self.categoryToIndex[categoryList[i]] = i+1 self.indexToCategory[i+1] = categoryList[i] self.encoder = ScalarEncoder(w, minval=0, maxval=self.ncategories - 1, radius=1, periodic=False, forced=forced) self.width = w * self.ncategories assert self.encoder.getWidth() == self.width self.description = [(name, 0)] self.name = name # These are used to support the topDownCompute method self._topDownMappingM = None # This gets filled in by getBucketValues self._bucketValues = None def getDecoderOutputFieldTypes(self): """ [Encoder class virtual method override] """ # TODO: change back to string meta-type after the decoding logic is fixed # to output strings instead of internal index values. #return (FieldMetaType.string,) return (FieldMetaType.integer,) def getWidth(self): return self.width def getDescription(self): return self.description def getScalars(self, input): """ See method description in base.py """ if input == SENTINEL_VALUE_FOR_MISSING_DATA: return numpy.array([None]) else: return numpy.array([self.categoryToIndex.get(input, 0)]) def getBucketIndices(self, input): """ See method description in base.py """ # Get the bucket index from the underlying scalar encoder if input == SENTINEL_VALUE_FOR_MISSING_DATA: return [None] else: return self.encoder.getBucketIndices(self.categoryToIndex.get(input, 0)) def encodeIntoArray(self, input, output): # if not found, we encode category 0 if input == SENTINEL_VALUE_FOR_MISSING_DATA: output[0:] = 0 val = "<missing>" else: val = self.categoryToIndex.get(input, 0) self.encoder.encodeIntoArray(val, output) if self.verbosity >= 2: print "input:", input, "va:", val, "output:", output print "decoded:", self.decodedToStr(self.decode(output)) def decode(self, encoded, parentFieldName=''): """ See the function description in base.py """ # Get the scalar values from the underlying scalar encoder (fieldsDict, fieldNames) = self.encoder.decode(encoded) if len(fieldsDict) == 0: return (fieldsDict, fieldNames) # Expect only 1 field assert(len(fieldsDict) == 1) # Get the list of categories the scalar values correspond to and # generate the description from the category name(s). (inRanges, inDesc) = fieldsDict.values()[0] outRanges = [] desc = "" for (minV, maxV) in inRanges: minV = int(round(minV)) maxV = int(round(maxV)) outRanges.append((minV, maxV)) while minV <= maxV: if len(desc) > 0: desc += ", " desc += self.indexToCategory[minV] minV += 1 # Return result if parentFieldName != '': fieldName = "%s.%s" % (parentFieldName, self.name) else: fieldName = self.name return ({fieldName: (outRanges, desc)}, [fieldName]) def closenessScores(self, expValues, actValues, fractional=True,): """ See the function description in base.py kwargs will have the keyword "fractional", which is ignored by this encoder """ expValue = expValues[0] actValue = actValues[0] if expValue == actValue: closeness = 1.0 else: closeness = 0.0 if not fractional: closeness = 1.0 - closeness return numpy.array([closeness]) def getBucketValues(self): """ See the function description in base.py """ if self._bucketValues is None: numBuckets = len(self.encoder.getBucketValues()) self._bucketValues = [] for bucketIndex in range(numBuckets): self._bucketValues.append(self.getBucketInfo([bucketIndex])[0].value) return self._bucketValues def getBucketInfo(self, buckets): """ See the function description in base.py """ # For the category encoder, the bucket index is the category index bucketInfo = self.encoder.getBucketInfo(buckets)[0] categoryIndex = int(round(bucketInfo.value)) category = self.indexToCategory[categoryIndex] return [EncoderResult(value=category, scalar=categoryIndex, encoding=bucketInfo.encoding)] def topDownCompute(self, encoded): """ See the function description in base.py """ encoderResult = self.encoder.topDownCompute(encoded)[0] value = encoderResult.value categoryIndex = int(round(value)) category = self.indexToCategory[categoryIndex] return EncoderResult(value=category, scalar=categoryIndex, encoding=encoderResult.encoding) @classmethod def read(cls, proto): encoder = object.__new__(cls) encoder.verbosity = proto.verbosity encoder.encoder = ScalarEncoder.read(proto.encoder) encoder.width = proto.width encoder.description = [(proto.name, 0)] encoder.name = proto.name encoder.indexToCategory = {x.index: x.category for x in proto.indexToCategory} encoder.categoryToIndex = {category: index for index, category in encoder.indexToCategory.items() if category != UNKNOWN} encoder._topDownMappingM = None encoder._bucketValues = None return encoder def write(self, proto): proto.width = self.width proto.indexToCategory = [ {"index": index, "category": category} for index, category in self.indexToCategory.items() ] proto.name = self.name proto.verbosity = self.verbosity self.encoder.write(proto.encoder)
from __future__ import unicode_literals import copy import datetime from django.contrib.auth.models import User from django.db import models from django.utils.encoding import python_2_unicode_compatible @python_2_unicode_compatible class RevisionableModel(models.Model): base = models.ForeignKey('self', null=True) title = models.CharField(blank=True, max_length=255) when = models.DateTimeField(default=datetime.datetime.now) def __str__(self): return "%s (%s, %s)" % (self.title, self.id, self.base.id) def save(self, *args, **kwargs): super(RevisionableModel, self).save(*args, **kwargs) if not self.base: self.base = self kwargs.pop('force_insert', None) kwargs.pop('force_update', None) super(RevisionableModel, self).save(*args, **kwargs) def new_revision(self): new_revision = copy.copy(self) new_revision.pk = None return new_revision class Order(models.Model): created_by = models.ForeignKey(User) text = models.TextField() @python_2_unicode_compatible class TestObject(models.Model): first = models.CharField(max_length=20) second = models.CharField(max_length=20) third = models.CharField(max_length=20) def __str__(self): return 'TestObject: %s,%s,%s' % (self.first,self.second,self.third)
# mysql/pyodbc.py # Copyright (C) 2005-2014 the SQLAlchemy authors and contributors # <see AUTHORS file> # # This module is part of SQLAlchemy and is released under # the MIT License: http://www.opensource.org/licenses/mit-license.php """ .. dialect:: mysql+pyodbc :name: PyODBC :dbapi: pyodbc :connectstring: mysql+pyodbc://<username>:<password>@<dsnname> :url: http://pypi.python.org/pypi/pyodbc/ Limitations ----------- The mysql-pyodbc dialect is subject to unresolved character encoding issues which exist within the current ODBC drivers available. (see http://code.google.com/p/pyodbc/issues/detail?id=25). Consider usage of OurSQL, MySQLdb, or MySQL-connector/Python. """ from .base import MySQLDialect, MySQLExecutionContext from ...connectors.pyodbc import PyODBCConnector from ... import util import re class MySQLExecutionContext_pyodbc(MySQLExecutionContext): def get_lastrowid(self): cursor = self.create_cursor() cursor.execute("SELECT LAST_INSERT_ID()") lastrowid = cursor.fetchone()[0] cursor.close() return lastrowid class MySQLDialect_pyodbc(PyODBCConnector, MySQLDialect): supports_unicode_statements = False execution_ctx_cls = MySQLExecutionContext_pyodbc pyodbc_driver_name = "MySQL" def __init__(self, **kw): # deal with http://code.google.com/p/pyodbc/issues/detail?id=25 kw.setdefault('convert_unicode', True) super(MySQLDialect_pyodbc, self).__init__(**kw) def _detect_charset(self, connection): """Sniff out the character set in use for connection results.""" # Prefer 'character_set_results' for the current connection over the # value in the driver. SET NAMES or individual variable SETs will # change the charset without updating the driver's view of the world. # # If it's decided that issuing that sort of SQL leaves you SOL, then # this can prefer the driver value. rs = connection.execute("SHOW VARIABLES LIKE 'character_set%%'") opts = dict([(row[0], row[1]) for row in self._compat_fetchall(rs)]) for key in ('character_set_connection', 'character_set'): if opts.get(key, None): return opts[key] util.warn("Could not detect the connection character set. " "Assuming latin1.") return 'latin1' def _extract_error_code(self, exception): m = re.compile(r"\((\d+)\)").search(str(exception.args)) c = m.group(1) if c: return int(c) else: return None dialect = MySQLDialect_pyodbc
# -*- coding: utf-8 -*- import babel.dates import re import werkzeug from datetime import datetime, timedelta from dateutil.relativedelta import relativedelta from odoo import fields, http, _ from odoo.addons.website.models.website import slug from odoo.http import request class WebsiteEventController(http.Controller): @http.route(['/event', '/event/page/<int:page>', '/events', '/events/page/<int:page>'], type='http', auth="public", website=True) def events(self, page=1, **searches): Event = request.env['event.event'] EventType = request.env['event.type'] searches.setdefault('date', 'all') searches.setdefault('type', 'all') searches.setdefault('country', 'all') domain_search = {} def sdn(date): return fields.Datetime.to_string(date.replace(hour=23, minute=59, second=59)) def sd(date): return fields.Datetime.to_string(date) today = datetime.today() dates = [ ['all', _('Next Events'), [("date_end", ">", sd(today))], 0], ['today', _('Today'), [ ("date_end", ">", sd(today)), ("date_begin", "<", sdn(today))], 0], ['week', _('This Week'), [ ("date_end", ">=", sd(today + relativedelta(days=-today.weekday()))), ("date_begin", "<", sdn(today + relativedelta(days=6-today.weekday())))], 0], ['nextweek', _('Next Week'), [ ("date_end", ">=", sd(today + relativedelta(days=7-today.weekday()))), ("date_begin", "<", sdn(today + relativedelta(days=13-today.weekday())))], 0], ['month', _('This month'), [ ("date_end", ">=", sd(today.replace(day=1))), ("date_begin", "<", (today.replace(day=1) + relativedelta(months=1)).strftime('%Y-%m-%d 00:00:00'))], 0], ['nextmonth', _('Next month'), [ ("date_end", ">=", sd(today.replace(day=1) + relativedelta(months=1))), ("date_begin", "<", (today.replace(day=1) + relativedelta(months=2)).strftime('%Y-%m-%d 00:00:00'))], 0], ['old', _('Old Events'), [ ("date_end", "<", today.strftime('%Y-%m-%d 00:00:00'))], 0], ] # search domains # TDE note: WTF ??? current_date = None current_type = None current_country = None for date in dates: if searches["date"] == date[0]: domain_search["date"] = date[2] if date[0] != 'all': current_date = date[1] if searches["type"] != 'all': current_type = EventType.browse(int(searches['type'])) domain_search["type"] = [("event_type_id", "=", int(searches["type"]))] if searches["country"] != 'all' and searches["country"] != 'online': current_country = request.env['res.country'].browse(int(searches['country'])) domain_search["country"] = ['|', ("country_id", "=", int(searches["country"])), ("country_id", "=", False)] elif searches["country"] == 'online': domain_search["country"] = [("country_id", "=", False)] def dom_without(without): domain = [('state', "in", ['draft', 'confirm', 'done'])] for key, search in domain_search.items(): if key != without: domain += search return domain # count by domains without self search for date in dates: if date[0] != 'old': date[3] = Event.search_count(dom_without('date') + date[2]) domain = dom_without('type') types = Event.read_group(domain, ["id", "event_type_id"], groupby=["event_type_id"], orderby="event_type_id") types.insert(0, { 'event_type_id_count': sum([int(type['event_type_id_count']) for type in types]), 'event_type_id': ("all", _("All Categories")) }) domain = dom_without('country') countries = Event.read_group(domain, ["id", "country_id"], groupby="country_id", orderby="country_id") countries.insert(0, { 'country_id_count': sum([int(country['country_id_count']) for country in countries]), 'country_id': ("all", _("All Countries")) }) step = 10 # Number of events per page event_count = Event.search_count(dom_without("none")) pager = request.website.pager( url="/event", url_args={'date': searches.get('date'), 'type': searches.get('type'), 'country': searches.get('country')}, total=event_count, page=page, step=step, scope=5) order = 'website_published desc, date_begin' if searches.get('date', 'all') == 'old': order = 'website_published desc, date_begin desc' events = Event.search(dom_without("none"), limit=step, offset=pager['offset'], order=order) values = { 'current_date': current_date, 'current_country': current_country, 'current_type': current_type, 'event_ids': events, # event_ids used in website_event_track so we keep name as it is 'dates': dates, 'types': types, 'countries': countries, 'pager': pager, 'searches': searches, 'search_path': "?%s" % werkzeug.url_encode(searches), } return request.render("website_event.index", values) @http.route(['/event/<model("event.event"):event>/page/<path:page>'], type='http', auth="public", website=True) def event_page(self, event, page, **post): values = { 'event': event, 'main_object': event } if '.' not in page: page = 'website_event.%s' % page try: request.website.get_template(page) except ValueError: # page not found values['path'] = re.sub(r"^website_event\.", '', page) values['from_template'] = 'website_event.default_page' # .strip('website_event.') page = 'website.page_404' return request.render(page, values) @http.route(['/event/<model("event.event"):event>'], type='http', auth="public", website=True) def event(self, event, **post): if event.menu_id and event.menu_id.child_id: target_url = event.menu_id.child_id[0].url else: target_url = '/event/%s/register' % str(event.id) if post.get('enable_editor') == '1': target_url += '?enable_editor=1' return request.redirect(target_url) @http.route(['/event/<model("event.event"):event>/register'], type='http', auth="public", website=True) def event_register(self, event, **post): values = { 'event': event, 'main_object': event, 'range': range, } return request.render("website_event.event_description_full", values) @http.route('/event/add_event', type='http', auth="user", methods=['POST'], website=True) def add_event(self, event_name="New Event", **kwargs): event = self._add_event(event_name, request.context) return request.redirect("/event/%s/register?enable_editor=1" % slug(event)) def _add_event(self, event_name=None, context=None, **kwargs): if not event_name: event_name = _("New Event") date_begin = datetime.today() + timedelta(days=(14)) vals = { 'name': event_name, 'date_begin': fields.Date.to_string(date_begin), 'date_end': fields.Date.to_string((date_begin + timedelta(days=(1)))), 'seats_available': 1000, } return request.env['event.event'].with_context(context or {}).create(vals) def get_formated_date(self, event): start_date = fields.Datetime.from_string(event.date_begin).date() end_date = fields.Datetime.from_string(event.date_end).date() month = babel.dates.get_month_names('abbreviated', locale=event.env.context.get('lang', 'en_US'))[start_date.month] return ('%s %s%s') % (month, start_date.strftime("%e"), (end_date != start_date and ("-" + end_date.strftime("%e")) or "")) @http.route('/event/get_country_event_list', type='http', auth='public', website=True) def get_country_events(self, **post): Event = request.env['event.event'] country_code = request.session['geoip'].get('country_code') result = {'events': [], 'country': False} events = None if country_code: country = request.env['res.country'].search([('code', '=', country_code)], limit=1) events = Event.search(['|', ('address_id', '=', None), ('country_id.code', '=', country_code), ('date_begin', '>=', '%s 00:00:00' % fields.Date.today()), ('state', '=', 'confirm')], order="date_begin") if not events: events = Event.search([('date_begin', '>=', '%s 00:00:00' % fields.Date.today()), ('state', '=', 'confirm')], order="date_begin") for event in events: if country_code and event.country_id.code == country_code: result['country'] = country result['events'].append({ "date": self.get_formated_date(event), "event": event, "url": event.website_url}) return request.render("website_event.country_events_list", result) def _process_tickets_details(self, data): nb_register = int(data.get('nb_register-0', 0)) if nb_register: return [{'id': 0, 'name': 'Registration', 'quantity': nb_register, 'price': 0}] return [] @http.route(['/event/<model("event.event"):event>/registration/new'], type='json', auth="public", methods=['POST'], website=True) def registration_new(self, event, **post): tickets = self._process_tickets_details(post) if not tickets: return request.redirect("/event/%s" % slug(event)) return request.env['ir.ui.view'].render_template("website_event.registration_attendee_details", {'tickets': tickets, 'event': event}) def _process_registration_details(self, details): ''' Process data posted from the attendee details form. ''' registrations = {} global_values = {} for key, value in details.iteritems(): counter, field_name = key.split('-', 1) if counter == '0': global_values[field_name] = value else: registrations.setdefault(counter, dict())[field_name] = value for key, value in global_values.iteritems(): for registration in registrations.values(): registration[key] = value return registrations.values() @http.route(['/event/<model("event.event"):event>/registration/confirm'], type='http', auth="public", methods=['POST'], website=True) def registration_confirm(self, event, **post): Attendees = request.env['event.registration'] registrations = self._process_registration_details(post) for registration in registrations: registration['event_id'] = event Attendees += Attendees.sudo().create( Attendees._prepare_attendee_values(registration)) return request.render("website_event.registration_complete", { 'attendees': Attendees, 'event': event, })
# Copyright 2018 The TensorFlow Authors All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== """Cell structure used by NAS.""" from __future__ import absolute_import from __future__ import division from __future__ import print_function import tensorflow as tf from deeplab.core.utils import resize_bilinear from deeplab.core.utils import scale_dimension arg_scope = tf.contrib.framework.arg_scope slim = tf.contrib.slim class NASBaseCell(object): """NASNet Cell class that is used as a 'layer' in image architectures. See https://arxiv.org/abs/1707.07012 and https://arxiv.org/abs/1712.00559. Args: num_conv_filters: The number of filters for each convolution operation. operations: List of operations that are performed in the NASNet Cell in order. used_hiddenstates: Binary array that signals if the hiddenstate was used within the cell. This is used to determine what outputs of the cell should be concatenated together. hiddenstate_indices: Determines what hiddenstates should be combined together with the specified operations to create the NASNet cell. """ def __init__(self, num_conv_filters, operations, used_hiddenstates, hiddenstate_indices, drop_path_keep_prob, total_num_cells, total_training_steps): if len(hiddenstate_indices) != len(operations): raise ValueError( 'Number of hiddenstate_indices and operations should be the same.') if len(operations) % 2: raise ValueError('Number of operations should be even.') self._num_conv_filters = num_conv_filters self._operations = operations self._used_hiddenstates = used_hiddenstates self._hiddenstate_indices = hiddenstate_indices self._drop_path_keep_prob = drop_path_keep_prob self._total_num_cells = total_num_cells self._total_training_steps = total_training_steps def __call__(self, net, scope, filter_scaling, stride, prev_layer, cell_num): """Runs the conv cell.""" self._cell_num = cell_num self._filter_scaling = filter_scaling self._filter_size = int(self._num_conv_filters * filter_scaling) with tf.variable_scope(scope): net = self._cell_base(net, prev_layer) for i in range(len(self._operations) // 2): with tf.variable_scope('comb_iter_{}'.format(i)): h1 = net[self._hiddenstate_indices[i * 2]] h2 = net[self._hiddenstate_indices[i * 2 + 1]] with tf.variable_scope('left'): h1 = self._apply_conv_operation( h1, self._operations[i * 2], stride, self._hiddenstate_indices[i * 2] < 2) with tf.variable_scope('right'): h2 = self._apply_conv_operation( h2, self._operations[i * 2 + 1], stride, self._hiddenstate_indices[i * 2 + 1] < 2) with tf.variable_scope('combine'): h = h1 + h2 net.append(h) with tf.variable_scope('cell_output'): net = self._combine_unused_states(net) return net def _cell_base(self, net, prev_layer): """Runs the beginning of the conv cell before the chosen ops are run.""" filter_size = self._filter_size if prev_layer is None: prev_layer = net else: if net.shape[2] != prev_layer.shape[2]: prev_layer = resize_bilinear( prev_layer, tf.shape(net)[1:3], prev_layer.dtype) if filter_size != prev_layer.shape[3]: prev_layer = tf.nn.relu(prev_layer) prev_layer = slim.conv2d(prev_layer, filter_size, 1, scope='prev_1x1') prev_layer = slim.batch_norm(prev_layer, scope='prev_bn') net = tf.nn.relu(net) net = slim.conv2d(net, filter_size, 1, scope='1x1') net = slim.batch_norm(net, scope='beginning_bn') net = tf.split(axis=3, num_or_size_splits=1, value=net) net.append(prev_layer) return net def _apply_conv_operation(self, net, operation, stride, is_from_original_input): """Applies the predicted conv operation to net.""" if stride > 1 and not is_from_original_input: stride = 1 input_filters = net.shape[3] filter_size = self._filter_size if 'separable' in operation: num_layers = int(operation.split('_')[-1]) kernel_size = int(operation.split('x')[0][-1]) for layer_num in range(num_layers): net = tf.nn.relu(net) net = slim.separable_conv2d( net, filter_size, kernel_size, depth_multiplier=1, scope='separable_{0}x{0}_{1}'.format(kernel_size, layer_num + 1), stride=stride) net = slim.batch_norm( net, scope='bn_sep_{0}x{0}_{1}'.format(kernel_size, layer_num + 1)) stride = 1 elif 'atrous' in operation: kernel_size = int(operation.split('x')[0][-1]) net = tf.nn.relu(net) if stride == 2: scaled_height = scale_dimension(tf.shape(net)[1], 0.5) scaled_width = scale_dimension(tf.shape(net)[2], 0.5) net = resize_bilinear(net, [scaled_height, scaled_width], net.dtype) net = slim.conv2d(net, filter_size, kernel_size, rate=1, scope='atrous_{0}x{0}'.format(kernel_size)) else: net = slim.conv2d(net, filter_size, kernel_size, rate=2, scope='atrous_{0}x{0}'.format(kernel_size)) net = slim.batch_norm(net, scope='bn_atr_{0}x{0}'.format(kernel_size)) elif operation in ['none']: if stride > 1 or (input_filters != filter_size): net = tf.nn.relu(net) net = slim.conv2d(net, filter_size, 1, stride=stride, scope='1x1') net = slim.batch_norm(net, scope='bn_1') elif 'pool' in operation: pooling_type = operation.split('_')[0] pooling_shape = int(operation.split('_')[-1].split('x')[0]) if pooling_type == 'avg': net = slim.avg_pool2d(net, pooling_shape, stride=stride, padding='SAME') elif pooling_type == 'max': net = slim.max_pool2d(net, pooling_shape, stride=stride, padding='SAME') else: raise ValueError('Unimplemented pooling type: ', pooling_type) if input_filters != filter_size: net = slim.conv2d(net, filter_size, 1, stride=1, scope='1x1') net = slim.batch_norm(net, scope='bn_1') else: raise ValueError('Unimplemented operation', operation) if operation != 'none': net = self._apply_drop_path(net) return net def _combine_unused_states(self, net): """Concatenates the unused hidden states of the cell.""" used_hiddenstates = self._used_hiddenstates states_to_combine = ([ h for h, is_used in zip(net, used_hiddenstates) if not is_used]) net = tf.concat(values=states_to_combine, axis=3) return net @tf.contrib.framework.add_arg_scope def _apply_drop_path(self, net): """Apply drop_path regularization.""" drop_path_keep_prob = self._drop_path_keep_prob if drop_path_keep_prob < 1.0: # Scale keep prob by layer number. assert self._cell_num != -1 layer_ratio = (self._cell_num + 1) / float(self._total_num_cells) drop_path_keep_prob = 1 - layer_ratio * (1 - drop_path_keep_prob) # Decrease keep prob over time. current_step = tf.cast(tf.train.get_or_create_global_step(), tf.float32) current_ratio = tf.minimum(1.0, current_step / self._total_training_steps) drop_path_keep_prob = (1 - current_ratio * (1 - drop_path_keep_prob)) # Drop path. noise_shape = [tf.shape(net)[0], 1, 1, 1] random_tensor = drop_path_keep_prob random_tensor += tf.random_uniform(noise_shape, dtype=tf.float32) binary_tensor = tf.cast(tf.floor(random_tensor), net.dtype) keep_prob_inv = tf.cast(1.0 / drop_path_keep_prob, net.dtype) net = net * keep_prob_inv * binary_tensor return net
# This Source Code Form is subject to the terms of the Mozilla Public # License, v. 2.0. If a copy of the MPL was not distributed with this # file, You can obtain one at http://mozilla.org/MPL/2.0/. from pathlib import Path import textwrap from django.core.management.base import BaseCommand class Command(BaseCommand): help = 'Convert a template to use Fluent for l10n' requires_system_checks = False def add_arguments(self, parser): subparsers = parser.add_subparsers( title='subcommand', dest='subcommand' ) subparsers.add_parser('help') recipe_parser = subparsers.add_parser( 'recipe', description='Create migration recipe from template' ) recipe_parser.add_argument('template', type=Path) ftl_parser = subparsers.add_parser( 'ftl', description='Create Fluent file with existing recipe' ) ftl_parser.add_argument( 'recipe_or_template', type=Path, help='Path to the recipe or the template from which the recipe was generated' ) ftl_parser.add_argument( 'locales', nargs='*', default=['en'], metavar='ab-CD', help='Locale codes to create ftl files for' ) template_parser = subparsers.add_parser( 'template', description='Create template_ftl.html file with existing recipe' ) template_parser.add_argument('template', type=Path) activation_parser = subparsers.add_parser( 'activation', description='Port activation data from .lang for a recipe/template' ) activation_parser.add_argument( 'recipe_or_template', type=Path, help='Path to the recipe or the template from which the recipe was generated' ) def handle(self, subcommand, **kwargs): if subcommand == 'recipe': return self.create_recipe(**kwargs) if subcommand == 'ftl': return self.create_ftl(**kwargs) if subcommand == 'template': return self.create_template(**kwargs) if subcommand == 'activation': return self.activation(**kwargs) return self.handle_help(**kwargs) def handle_help(self, **kwargs): self.stdout.write(textwrap.dedent('''\ To migrate a template from .lang to Fluent, use the subcommands like so ./manage.py fluent recipe bedrock/app/templates/app/some.html # edit IDs in lib/fluent_migrations/app/some.py ./manage.py fluent template bedrock/app/templates/app/some.html ./manage.py fluent ftl bedrock/app/templates/app/some.html More documentation on https://bedrock.readthedocs.io/en/latest/fluent-conversion.html. ''')) def create_recipe(self, template, **kwargs): from ._fluent_recipe import Recipe recipe = Recipe(self) recipe.handle(template) def create_template(self, template, **kwargs): from ._fluent_templater import Templater templater = Templater(self) templater.handle(template) def create_ftl(self, recipe_or_template, locales, **kwargs): from ._fluent_ftl import FTLCreator ftl_creator = FTLCreator(self) for locale in locales: ftl_creator.handle(recipe_or_template, locale) def activation(self, recipe_or_template, **kwargs): from ._fluent_activation import Activation activation = Activation(self) activation.handle(recipe_or_template)
from test.support import run_unittest from test.script_helper import assert_python_failure, temp_dir import unittest import sys import cgitb class TestCgitb(unittest.TestCase): def test_fonts(self): text = "Hello Robbie!" self.assertEqual(cgitb.small(text), "<small>{}</small>".format(text)) self.assertEqual(cgitb.strong(text), "<strong>{}</strong>".format(text)) self.assertEqual(cgitb.grey(text), '<font color="#909090">{}</font>'.format(text)) def test_blanks(self): self.assertEqual(cgitb.small(""), "") self.assertEqual(cgitb.strong(""), "") self.assertEqual(cgitb.grey(""), "") def test_html(self): try: raise ValueError("Hello World") except ValueError as err: # If the html was templated we could do a bit more here. # At least check that we get details on what we just raised. html = cgitb.html(sys.exc_info()) self.assertIn("ValueError", html) self.assertIn(str(err), html) def test_text(self): try: raise ValueError("Hello World") except ValueError as err: text = cgitb.text(sys.exc_info()) self.assertIn("ValueError", text) self.assertIn("Hello World", text) def test_syshook_no_logdir_default_format(self): with temp_dir() as tracedir: rc, out, err = assert_python_failure( '-c', ('import cgitb; cgitb.enable(logdir=%s); ' 'raise ValueError("Hello World")') % repr(tracedir)) out = out.decode(sys.getfilesystemencoding()) self.assertIn("ValueError", out) self.assertIn("Hello World", out) # By default we emit HTML markup. self.assertIn('<p>', out) self.assertIn('</p>', out) def test_syshook_no_logdir_text_format(self): # Issue 12890: we were emitting the <p> tag in text mode. with temp_dir() as tracedir: rc, out, err = assert_python_failure( '-c', ('import cgitb; cgitb.enable(format="text", logdir=%s); ' 'raise ValueError("Hello World")') % repr(tracedir)) out = out.decode(sys.getfilesystemencoding()) self.assertIn("ValueError", out) self.assertIn("Hello World", out) self.assertNotIn('<p>', out) self.assertNotIn('</p>', out) def test_main(): run_unittest(TestCgitb) if __name__ == "__main__": test_main()
""" This module contains helper classes and methods for the facebook integration module .. module:: application.facebook.facebook .. moduleauthor:: Devin Schwab <dts34@case.edu> """ import facebooksdk as fb import models from flask import flash class AlbumList(object): def __init__(self, token): """ Given an an access token this class will get all albums for the object associated with the token (i.e. a page or a user) It will lazily construct an Album instance for each of the album ids returned """ self.graph = fb.GraphAPI(token.access_token) albums_data = self.graph.get_connections('me', 'albums')['data'] self.album_ids = {} self.album_names = {} for data in albums_data: self.album_ids[data['id']] = data self.album_names[data['name']] = data def get_albums_by_name(self, names): """ Given a list of names this method will return album objects for each matching name. If a name is not found then it is silently ignored. This method returns a dictionary mapping name to Album object. """ albums = {} for name in names: if name in self.album_names: if isinstance(self.album_names[name], Album): albums[name] = self.album_names[name] else: self.album_names[name] = Album(graph=self.graph, album_data=self.album_names[name]) self.album_ids[self.album_names[name].me] = self.album_names[name] albums[name] = self.album_names[name] return albums def get_albums_by_id(self, ids): """ Given a list of ids this method will return album objects for each matching id. If an id is not found then it is silently ignored. This method returns a dictionary mapping id to Album object """ albums = {} for album_id in ids: if album_id in self.album_ids: if isinstance(self.album_ids[album_id], Album): albums[album_id] = self.album_ids[album_id] else: self.album_ids[album_id] = Album(graph=self.graph, album_data=self.album_ids[album_id]) self.album_names[self.album_ids[album_id].name] = self.album_ids[album_id] albums[album_id] = self.album_ids[album_id] return albums def get_all_albums_by_id(self): """ This method returns a dictionary of all albums with album ids as the keys """ for album_id in self.album_ids: if not isinstance(self.album_ids[album_id], Album): self.album_ids[album_id] = Album(graph=self.graph, album_data=self.album_ids[album_id]) self.album_names[self.album_ids[album_id].name] = self.album_ids[album_id] return self.album_ids def get_all_albums_by_name(self): """ This method returns a dictionary of all albums with album names as the keys """ for name in self.album_names: if not isinstance(self.album_names[name], Album): self.album_names[name] = Album(graph=self.graph, album_data=self.album_names[name]) self.album_ids[self.album_names[name].me] = self.album_names[name] return self.album_names class Album(object): def __init__(self, graph=None, token=None, album_id=None, album_data=None): """ Initializes a new Album object. If graph is provided then the graph object is saved to this instance. If the token is provided then the graph object for this token is created and saved to this instance. If both are none then an error is raised. If album_id is provided then the graph object is queried for the id and the album object populates itself with this data If album_data is provided then the graph object is populated with the data in the json derived object If both are None then an error is raised """ if graph is None and token is None: raise TypeError("Either a graph object must be provided or a token must be provided") if graph is not None: self.graph = graph query = models.AccessTokenModel.all() query.filter('access_token =', graph.access_token) try: self.token = query.fetch(1)[0] except IndexError: raise TypeError('The token object provided was not an AccessTokenModel instance') else: self.graph = fb.GraphAPI(token.access_token) self.token = token if album_id is None and album_data is None: raise TypeError("Either an album id or a album data must be provided") if album_id is not None: album_data = self.graph.get_object(album_id) self.me = album_data['id'] self.name = album_data['name'] self.desc = album_data.get('description', None) self.count = album_data.get('count', 0) if 'cover_photo' in album_data: self.cover_photo = Photo(self.me, graph=self.graph, photo_id=album_data['cover_photo']).thumbnail else: self.cover_photo = None def get_model(self): query = models.AlbumModel.all() query.filter('me =', self.me) try: return query.fetch(1)[0] except IndexError: cover_thumb = None if self.cover_photo is not None: cover_thumb = self.cover_photo entity = models.AlbumModel(me=self.me, token=self.token, name=self.name, desc=self.desc, cover_photo=cover_thumb) entity.put() return entity def get_photos(self): """ Get a list of Photo objects """ photos_data = self.graph.get_connections(self.me, 'photos')['data'] photos = [] for photo_data in photos_data: query = models.PhotoModel.all() query.filter('me =', photo_data['id']) try: photos.append(query.fetch(1)[0]) except IndexError: name = None if 'name' in photo_data: name = photo_data['name'] orig = photo_data['images'][0]['source'] entity = models.PhotoModel(me=photo_data['id'], album_id=self.me, name=name, thumbnail=photo_data['picture'], original=orig) entity.put() photos.append(entity) return photos class Photo(object): def __init__(self, album_id, graph=None, token=None, photo_id=None, photo_data=None): if graph is None and token is None: raise TypeError("Either a graph object must be provided or a token must be provided") if graph is not None: self.graph = graph else: self.graph = fb.GraphAPI(token.access_token) if photo_id is None and photo_data is None: raise TypeError("Either an album id or a album data must be provided") if photo_id is not None: photo_data = self.graph.get_object(photo_id) self.me = photo_data['id'] self.name = photo_data.get('name', None) self.thumbnail = photo_data['picture'] self.original = photo_data['images'][0]['source'] self.album_id = album_id def get_model(self): query = models.PhotoModel.all() query.filter('me =', self.me) try: return query.fetch(1)[0] except IndexError: entity = models.PhotoModel(me=self.me, album_id=self.album_id, name=self.name, thumbnail=self.thumbnail, original=self.original) entity.put() return entity
# Copyright 2014 Cloudera Inc. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. from ibis.common import IbisError import ibis.expr.operations as ops import ibis.expr.types as ir import ibis.expr.temporal as T from ibis.expr.tests.mocks import MockConnection from ibis.compat import unittest class TestFixedOffsets(unittest.TestCase): def setUp(self): self.con = MockConnection() self.table = self.con.table('alltypes') def test_upconvert(self): cases = [ (T.day(14), 'w', T.week(2)), (T.hour(72), 'd', T.day(3)), (T.minute(240), 'h', T.hour(4)), (T.second(360), 'm', T.minute(6)), (T.second(3 * 86400), 'd', T.day(3)), (T.millisecond(5000), 's', T.second(5)), (T.microsecond(5000000), 's', T.second(5)), (T.nanosecond(5000000000), 's', T.second(5)), ] for offset, unit, expected in cases: result = offset.to_unit(unit) assert result.equals(expected) def test_multiply(self): offset = T.day(2) assert (offset * 2).equals(T.day(4)) assert (offset * (-2)).equals(T.day(-4)) assert (3 * offset).equals(T.day(6)) assert ((-3) * offset).equals(T.day(-6)) def test_repr(self): assert repr(T.day()) == '<Timedelta: 1 day>' assert repr(T.day(2)) == '<Timedelta: 2 days>' assert repr(T.year()) == '<Timedelta: 1 year>' assert repr(T.month(2)) == '<Timedelta: 2 months>' assert repr(T.second(40)) == '<Timedelta: 40 seconds>' def test_cannot_upconvert(self): cases = [ (T.day(), 'w'), (T.hour(), 'd'), (T.minute(), 'h'), (T.second(), 'm'), (T.second(), 'd'), (T.millisecond(), 's'), (T.microsecond(), 's'), (T.nanosecond(), 's'), ] for delta, target in cases: self.assertRaises(IbisError, delta.to_unit, target) def test_downconvert_second_parts(self): K = 2 sec = T.second(K) milli = T.millisecond(K) micro = T.microsecond(K) nano = T.nanosecond(K) cases = [ (sec.to_unit('s'), T.second(K)), (sec.to_unit('ms'), T.millisecond(K * 1000)), (sec.to_unit('us'), T.microsecond(K * 1000000)), (sec.to_unit('ns'), T.nanosecond(K * 1000000000)), (milli.to_unit('ms'), T.millisecond(K)), (milli.to_unit('us'), T.microsecond(K * 1000)), (milli.to_unit('ns'), T.nanosecond(K * 1000000)), (micro.to_unit('us'), T.microsecond(K)), (micro.to_unit('ns'), T.nanosecond(K * 1000)), (nano.to_unit('ns'), T.nanosecond(K)) ] self._check_cases(cases) def test_downconvert_hours(self): K = 2 offset = T.hour(K) cases = [ (offset.to_unit('h'), T.hour(K)), (offset.to_unit('m'), T.minute(K * 60)), (offset.to_unit('s'), T.second(K * 3600)), (offset.to_unit('ms'), T.millisecond(K * 3600000)), (offset.to_unit('us'), T.microsecond(K * 3600000000)), (offset.to_unit('ns'), T.nanosecond(K * 3600000000000)) ] self._check_cases(cases) def test_downconvert_day(self): K = 2 week = T.week(K) day = T.day(K) cases = [ (week.to_unit('d'), T.day(K * 7)), (week.to_unit('h'), T.hour(K * 7 * 24)), (day.to_unit('d'), T.day(K)), (day.to_unit('h'), T.hour(K * 24)), (day.to_unit('m'), T.minute(K * 1440)), (day.to_unit('s'), T.second(K * 86400)), (day.to_unit('ms'), T.millisecond(K * 86400000)), (day.to_unit('us'), T.microsecond(K * 86400000000)), (day.to_unit('ns'), T.nanosecond(K * 86400000000000)) ] self._check_cases(cases) def test_combine_with_different_kinds(self): cases = [ (T.day() + T.minute(), T.minute(1441)), (T.second() + T.millisecond(10), T.millisecond(1010)), (T.hour() + T.minute(5) + T.second(10), T.second(3910)) ] self._check_cases(cases) def test_timedelta_generic_api(self): cases = [ (T.timedelta(weeks=2), T.week(2)), (T.timedelta(days=3), T.day(3)), (T.timedelta(hours=4), T.hour(4)), (T.timedelta(minutes=5), T.minute(5)), (T.timedelta(seconds=6), T.second(6)), (T.timedelta(milliseconds=7), T.millisecond(7)), (T.timedelta(microseconds=8), T.microsecond(8)), (T.timedelta(nanoseconds=9), T.nanosecond(9)), ] self._check_cases(cases) def _check_cases(self, cases): for x, y in cases: assert x.equals(y) def test_offset_timestamp_expr(self): c = self.table.i x = T.timedelta(days=1) expr = x + c assert isinstance(expr, ir.TimestampArray) assert isinstance(expr.op(), ops.TimestampDelta) # test radd expr = c + x assert isinstance(expr, ir.TimestampArray) assert isinstance(expr.op(), ops.TimestampDelta) class TestTimedelta(unittest.TestCase): def test_compound_offset(self): # These are not yet allowed (e.g. 1 month + 1 hour) pass def test_offset_months(self): pass
import sys from django import http from django.core import signals from django.utils.encoding import force_unicode from django.utils.importlib import import_module class BaseHandler(object): # Changes that are always applied to a response (in this order). response_fixes = [ http.fix_location_header, http.conditional_content_removal, http.fix_IE_for_attach, http.fix_IE_for_vary, ] def __init__(self): self._request_middleware = self._view_middleware = self._response_middleware = self._exception_middleware = None def load_middleware(self): """ Populate middleware lists from settings.MIDDLEWARE_CLASSES. Must be called after the environment is fixed (see __call__). """ from django.conf import settings from django.core import exceptions self._view_middleware = [] self._response_middleware = [] self._exception_middleware = [] request_middleware = [] for middleware_path in settings.MIDDLEWARE_CLASSES: try: dot = middleware_path.rindex('.') except ValueError: raise exceptions.ImproperlyConfigured('%s isn\'t a middleware module' % middleware_path) mw_module, mw_classname = middleware_path[:dot], middleware_path[dot+1:] try: mod = import_module(mw_module) except ImportError, e: raise exceptions.ImproperlyConfigured('Error importing middleware %s: "%s"' % (mw_module, e)) try: mw_class = getattr(mod, mw_classname) except AttributeError: raise exceptions.ImproperlyConfigured('Middleware module "%s" does not define a "%s" class' % (mw_module, mw_classname)) try: mw_instance = mw_class() except exceptions.MiddlewareNotUsed: continue if hasattr(mw_instance, 'process_request'): request_middleware.append(mw_instance.process_request) if hasattr(mw_instance, 'process_view'): self._view_middleware.append(mw_instance.process_view) if hasattr(mw_instance, 'process_response'): self._response_middleware.insert(0, mw_instance.process_response) if hasattr(mw_instance, 'process_exception'): self._exception_middleware.insert(0, mw_instance.process_exception) # We only assign to this when initialization is complete as it is used # as a flag for initialization being complete. self._request_middleware = request_middleware def get_response(self, request): "Returns an HttpResponse object for the given HttpRequest" from django.core import exceptions, urlresolvers from django.conf import settings try: try: # Setup default url resolver for this thread. urlconf = settings.ROOT_URLCONF urlresolvers.set_urlconf(urlconf) resolver = urlresolvers.RegexURLResolver(r'^/', urlconf) # Apply request middleware for middleware_method in self._request_middleware: response = middleware_method(request) if response: return response if hasattr(request, "urlconf"): # Reset url resolver with a custom urlconf. urlconf = request.urlconf urlresolvers.set_urlconf(urlconf) resolver = urlresolvers.RegexURLResolver(r'^/', urlconf) callback, callback_args, callback_kwargs = resolver.resolve( request.path_info) # Apply view middleware for middleware_method in self._view_middleware: response = middleware_method(request, callback, callback_args, callback_kwargs) if response: return response try: response = callback(request, *callback_args, **callback_kwargs) except Exception, e: # If the view raised an exception, run it through exception # middleware, and if the exception middleware returns a # response, use that. Otherwise, reraise the exception. for middleware_method in self._exception_middleware: response = middleware_method(request, e) if response: return response raise # Complain if the view returned None (a common error). if response is None: try: view_name = callback.func_name # If it's a function except AttributeError: view_name = callback.__class__.__name__ + '.__call__' # If it's a class raise ValueError("The view %s.%s didn't return an HttpResponse object." % (callback.__module__, view_name)) return response except http.Http404, e: if settings.DEBUG: from django.views import debug return debug.technical_404_response(request, e) else: try: callback, param_dict = resolver.resolve404() return callback(request, **param_dict) except: try: return self.handle_uncaught_exception(request, resolver, sys.exc_info()) finally: receivers = signals.got_request_exception.send(sender=self.__class__, request=request) except exceptions.PermissionDenied: return http.HttpResponseForbidden('<h1>Permission denied</h1>') except SystemExit: # Allow sys.exit() to actually exit. See tickets #1023 and #4701 raise except: # Handle everything else, including SuspiciousOperation, etc. # Get the exception info now, in case another exception is thrown later. receivers = signals.got_request_exception.send(sender=self.__class__, request=request) return self.handle_uncaught_exception(request, resolver, sys.exc_info()) finally: # Reset URLconf for this thread on the way out for complete # isolation of request.urlconf urlresolvers.set_urlconf(None) def handle_uncaught_exception(self, request, resolver, exc_info): """ Processing for any otherwise uncaught exceptions (those that will generate HTTP 500 responses). Can be overridden by subclasses who want customised 500 handling. Be *very* careful when overriding this because the error could be caused by anything, so assuming something like the database is always available would be an error. """ from django.conf import settings from django.core.mail import mail_admins if settings.DEBUG_PROPAGATE_EXCEPTIONS: raise if settings.DEBUG: from django.views import debug return debug.technical_500_response(request, *exc_info) # When DEBUG is False, send an error message to the admins. subject = 'Error (%s IP): %s' % ((request.META.get('REMOTE_ADDR') in settings.INTERNAL_IPS and 'internal' or 'EXTERNAL'), request.path) try: request_repr = repr(request) except: request_repr = "Request repr() unavailable" message = "%s\n\n%s" % (self._get_traceback(exc_info), request_repr) mail_admins(subject, message, fail_silently=True) # If Http500 handler is not installed, re-raise last exception if resolver.urlconf_module is None: raise exc_info[1], None, exc_info[2] # Return an HttpResponse that displays a friendly error message. callback, param_dict = resolver.resolve500() return callback(request, **param_dict) def _get_traceback(self, exc_info=None): "Helper function to return the traceback as a string" import traceback return '\n'.join(traceback.format_exception(*(exc_info or sys.exc_info()))) def apply_response_fixes(self, request, response): """ Applies each of the functions in self.response_fixes to the request and response, modifying the response in the process. Returns the new response. """ for func in self.response_fixes: response = func(request, response) return response def get_script_name(environ): """ Returns the equivalent of the HTTP request's SCRIPT_NAME environment variable. If Apache mod_rewrite has been used, returns what would have been the script name prior to any rewriting (so it's the script name as seen from the client's perspective), unless DJANGO_USE_POST_REWRITE is set (to anything). """ from django.conf import settings if settings.FORCE_SCRIPT_NAME is not None: return force_unicode(settings.FORCE_SCRIPT_NAME) # If Apache's mod_rewrite had a whack at the URL, Apache set either # SCRIPT_URL or REDIRECT_URL to the full resource URL before applying any # rewrites. Unfortunately not every Web server (lighttpd!) passes this # information through all the time, so FORCE_SCRIPT_NAME, above, is still # needed. script_url = environ.get('SCRIPT_URL', u'') if not script_url: script_url = environ.get('REDIRECT_URL', u'') if script_url: return force_unicode(script_url[:-len(environ.get('PATH_INFO', ''))]) return force_unicode(environ.get('SCRIPT_NAME', u''))
""" mbed SDK Copyright (c) 2011-2013 ARM Limited Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. """ import os import sys from os.path import join, basename, exists, abspath, dirname from time import sleep from host_test_plugins import HostTestPluginBase sys.path.append(abspath(join(dirname(__file__), "../../../"))) from tools.test_api import get_autodetected_MUTS_list class HostTestPluginCopyMethod_Smart(HostTestPluginBase): # Plugin interface name = 'HostTestPluginCopyMethod_Smart' type = 'CopyMethod' stable = True capabilities = ['smart'] required_parameters = ['image_path', 'destination_disk', 'target_mcu'] def setup(self, *args, **kwargs): """ Configure plugin, this function should be called before plugin execute() method is used. """ return True def execute(self, capability, *args, **kwargs): """ Executes capability by name. Each capability may directly just call some command line program or execute building pythonic function """ result = False if self.check_parameters(capability, *args, **kwargs) is True: image_path = kwargs['image_path'] destination_disk = kwargs['destination_disk'] target_mcu = kwargs['target_mcu'] # Wait for mount point to be ready self.check_mount_point_ready(destination_disk) # Blocking # Prepare correct command line parameter values image_base_name = basename(image_path) destination_path = join(destination_disk, image_base_name) if capability == 'smart': if os.name == 'posix': cmd = ['cp', image_path, destination_path] result = self.run_command(cmd, shell=False) cmd = ['sync'] result = self.run_command(cmd, shell=False) elif os.name == 'nt': cmd = ['copy', image_path, destination_path] result = self.run_command(cmd, shell=True) # Give the OS and filesystem time to settle down sleep(3) platform_name_filter = [target_mcu] muts_list = {} remount_complete = False for i in range(0, 60): print('Looking for %s with MBEDLS' % target_mcu) muts_list = get_autodetected_MUTS_list(platform_name_filter=platform_name_filter) if 1 in muts_list: mut = muts_list[1] destination_disk = mut['disk'] destination_path = join(destination_disk, image_base_name) if mut['mcu'] == 'LPC1768' or mut['mcu'] == 'LPC11U24': if exists(destination_disk) and exists(destination_path): remount_complete = True break; else: if exists(destination_disk) and not exists(destination_path): remount_complete = True break; sleep(1) if remount_complete: print('Remount complete') else: print('Remount FAILED') if exists(destination_disk): print('Disk exists') else: print('Disk does not exist') if exists(destination_path): print('Image exists') else: print('Image does not exist') result = None return result def load_plugin(): """ Returns plugin available in this module """ return HostTestPluginCopyMethod_Smart()
""" This file contains implementation override of SearchFilterGenerator which will allow * Filter by all courses in which the user is enrolled in """ from microsite_configuration import microsite from student.models import CourseEnrollment from opaque_keys import InvalidKeyError from opaque_keys.edx.keys import CourseKey from opaque_keys.edx.locations import SlashSeparatedCourseKey from xmodule.modulestore.django import modulestore from search.filter_generator import SearchFilterGenerator from openedx.core.djangoapps.user_api.partition_schemes import RandomUserPartitionScheme from openedx.core.djangoapps.course_groups.partition_scheme import CohortPartitionScheme from courseware.access import get_user_role INCLUDE_SCHEMES = [CohortPartitionScheme, RandomUserPartitionScheme, ] SCHEME_SUPPORTS_ASSIGNMENT = [RandomUserPartitionScheme, ] class LmsSearchFilterGenerator(SearchFilterGenerator): """ SearchFilterGenerator for LMS Search """ _user_enrollments = {} def _enrollments_for_user(self, user): """ Return the specified user's course enrollments """ if user not in self._user_enrollments: self._user_enrollments[user] = CourseEnrollment.enrollments_for_user(user) return self._user_enrollments[user] def filter_dictionary(self, **kwargs): """ LMS implementation, adds filtering by user partition, course id and user """ def get_group_for_user_partition(user_partition, course_key, user): """ Returns the specified user's group for user partition """ if user_partition.scheme in SCHEME_SUPPORTS_ASSIGNMENT: return user_partition.scheme.get_group_for_user( course_key, user, user_partition, assign=False, ) else: return user_partition.scheme.get_group_for_user( course_key, user, user_partition, ) def get_group_ids_for_user(course, user): """ Collect user partition group ids for user for this course """ partition_groups = [] for user_partition in course.user_partitions: if user_partition.scheme in INCLUDE_SCHEMES: group = get_group_for_user_partition(user_partition, course.id, user) if group: partition_groups.append(group) partition_group_ids = [unicode(partition_group.id) for partition_group in partition_groups] return partition_group_ids if partition_group_ids else None filter_dictionary = super(LmsSearchFilterGenerator, self).filter_dictionary(**kwargs) if 'user' in kwargs: user = kwargs['user'] if 'course_id' in kwargs and kwargs['course_id']: try: course_key = CourseKey.from_string(kwargs['course_id']) except InvalidKeyError: course_key = SlashSeparatedCourseKey.from_deprecated_string(kwargs['course_id']) # Staff user looking at course as staff user if get_user_role(user, course_key) in ('instructor', 'staff'): return filter_dictionary # Need to check course exist (if course gets deleted enrollments don't get cleaned up) course = modulestore().get_course(course_key) if course: filter_dictionary['content_groups'] = get_group_ids_for_user(course, user) else: user_enrollments = self._enrollments_for_user(user) content_groups = [] for enrollment in user_enrollments: course = modulestore().get_course(enrollment.course_id) if course: enrollment_group_ids = get_group_ids_for_user(course, user) if enrollment_group_ids: content_groups.extend(enrollment_group_ids) filter_dictionary['content_groups'] = content_groups if content_groups else None return filter_dictionary def field_dictionary(self, **kwargs): """ add course if provided otherwise add courses in which the user is enrolled in """ field_dictionary = super(LmsSearchFilterGenerator, self).field_dictionary(**kwargs) if not kwargs.get('user'): field_dictionary['course'] = [] elif not kwargs.get('course_id'): user_enrollments = self._enrollments_for_user(kwargs['user']) field_dictionary['course'] = [unicode(enrollment.course_id) for enrollment in user_enrollments] # if we have an org filter, only include results for this org filter course_org_filter = microsite.get_value('course_org_filter') if course_org_filter: field_dictionary['org'] = course_org_filter return field_dictionary def exclude_dictionary(self, **kwargs): """ If we are not on a microsite, then exclude any microsites that are defined """ exclude_dictionary = super(LmsSearchFilterGenerator, self).exclude_dictionary(**kwargs) course_org_filter = microsite.get_value('course_org_filter') # If we have a course filter we are ensuring that we only get those courses above if not course_org_filter: org_filter_out_set = microsite.get_all_orgs() if org_filter_out_set: exclude_dictionary['org'] = list(org_filter_out_set) return exclude_dictionary
# coding: utf-8 from __future__ import unicode_literals from .common import InfoExtractor from ..utils import ( parse_iso8601, int_or_none, ) class TwentyFourVideoIE(InfoExtractor): IE_NAME = '24video' _VALID_URL = r'https?://(?:www\.)?24video\.net/(?:video/(?:view|xml)/|player/new24_play\.swf\?id=)(?P<id>\d+)' _TESTS = [ { 'url': 'http://www.24video.net/video/view/1044982', 'md5': '48dd7646775690a80447a8dca6a2df76', 'info_dict': { 'id': '1044982', 'ext': 'mp4', 'title': '  ', 'description': '     .', 'thumbnail': 're:^https?://.*\.jpg$', 'uploader': 'SUPERTELO', 'duration': 31, 'timestamp': 1275937857, 'upload_date': '20100607', 'age_limit': 18, 'like_count': int, 'dislike_count': int, }, }, { 'url': 'http://www.24video.net/player/new24_play.swf?id=1044982', 'only_matching': True, } ] def _real_extract(self, url): video_id = self._match_id(url) webpage = self._download_webpage( 'http://www.24video.net/video/view/%s' % video_id, video_id) title = self._og_search_title(webpage) description = self._html_search_regex( r'<span itemprop="description">([^<]+)</span>', webpage, 'description', fatal=False) thumbnail = self._og_search_thumbnail(webpage) duration = int_or_none(self._og_search_property( 'duration', webpage, 'duration', fatal=False)) timestamp = parse_iso8601(self._search_regex( r'<time id="video-timeago" datetime="([^"]+)" itemprop="uploadDate">', webpage, 'upload date')) uploader = self._html_search_regex( r'\s*<a href="/jsecUser/movies/[^"]+" class="link">([^<]+)</a>', webpage, 'uploader', fatal=False) view_count = int_or_none(self._html_search_regex( r'<span class="video-views">(\d+) ', webpage, 'view count', fatal=False)) comment_count = int_or_none(self._html_search_regex( r'<div class="comments-title" id="comments-count">(\d+) ', webpage, 'comment count', fatal=False)) formats = [] pc_video = self._download_xml( 'http://www.24video.net/video/xml/%s?mode=play' % video_id, video_id, 'Downloading PC video URL').find('.//video') formats.append({ 'url': pc_video.attrib['url'], 'format_id': 'pc', 'quality': 1, }) like_count = int_or_none(pc_video.get('ratingPlus')) dislike_count = int_or_none(pc_video.get('ratingMinus')) age_limit = 18 if pc_video.get('adult') == 'true' else 0 mobile_video = self._download_xml( 'http://www.24video.net/video/xml/%s' % video_id, video_id, 'Downloading mobile video URL').find('.//video') formats.append({ 'url': mobile_video.attrib['url'], 'format_id': 'mobile', 'quality': 0, }) self._sort_formats(formats) return { 'id': video_id, 'title': title, 'description': description, 'thumbnail': thumbnail, 'uploader': uploader, 'duration': duration, 'timestamp': timestamp, 'view_count': view_count, 'comment_count': comment_count, 'like_count': like_count, 'dislike_count': dislike_count, 'age_limit': age_limit, 'formats': formats, }
# coding: utf-8 from __future__ import unicode_literals import re from .common import InfoExtractor from ..compat import compat_str from ..utils import ( int_or_none, js_to_json, strip_or_none, try_get, unified_timestamp, ) class WatchBoxIE(InfoExtractor): _VALID_URL = r'https?://(?:www\.)?watchbox\.de/(?P<kind>serien|filme)/(?:[^/]+/)*[^/]+-(?P<id>\d+)' _TESTS = [{ # film 'url': 'https://www.watchbox.de/filme/free-jimmy-12325.html', 'info_dict': { 'id': '341368', 'ext': 'mp4', 'title': 'Free Jimmy', 'description': 'md5:bcd8bafbbf9dc0ef98063d344d7cc5f6', 'thumbnail': r're:^https?://.*\.jpg$', 'duration': 4890, 'age_limit': 16, 'release_year': 2009, }, 'params': { 'format': 'bestvideo', 'skip_download': True, }, 'expected_warnings': ['Failed to download m3u8 information'], }, { # episode 'url': 'https://www.watchbox.de/serien/ugly-americans-12231/staffel-1/date-in-der-hoelle-328286.html', 'info_dict': { 'id': '328286', 'ext': 'mp4', 'title': 'S01 E01 - Date in der Hlle', 'description': 'md5:2f31c74a8186899f33cb5114491dae2b', 'thumbnail': r're:^https?://.*\.jpg$', 'duration': 1291, 'age_limit': 12, 'release_year': 2010, 'series': 'Ugly Americans', 'season_number': 1, 'episode': 'Date in der Hlle', 'episode_number': 1, }, 'params': { 'format': 'bestvideo', 'skip_download': True, }, 'expected_warnings': ['Failed to download m3u8 information'], }, { 'url': 'https://www.watchbox.de/serien/ugly-americans-12231/staffel-2/der-ring-des-powers-328270', 'only_matching': True, }] def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) kind, video_id = mobj.group('kind', 'id') webpage = self._download_webpage(url, video_id) source = self._parse_json( self._search_regex( r'(?s)source\s*:\s*({.+?})\s*,\s*\n', webpage, 'source', default='{}'), video_id, transform_source=js_to_json, fatal=False) or {} video_id = compat_str(source.get('videoId') or video_id) devapi = self._download_json( 'http://api.watchbox.de/devapi/id/%s' % video_id, video_id, query={ 'format': 'json', 'apikey': 'hbbtv', }, fatal=False) item = try_get(devapi, lambda x: x['items'][0], dict) or {} title = item.get('title') or try_get( item, lambda x: x['movie']['headline_movie'], compat_str) or source['title'] formats = [] hls_url = item.get('media_videourl_hls') or source.get('hls') if hls_url: formats.extend(self._extract_m3u8_formats( hls_url, video_id, 'mp4', entry_protocol='m3u8_native', m3u8_id='hls', fatal=False)) dash_url = item.get('media_videourl_wv') or source.get('dash') if dash_url: formats.extend(self._extract_mpd_formats( dash_url, video_id, mpd_id='dash', fatal=False)) mp4_url = item.get('media_videourl') if mp4_url: formats.append({ 'url': mp4_url, 'format_id': 'mp4', 'width': int_or_none(item.get('width')), 'height': int_or_none(item.get('height')), 'tbr': int_or_none(item.get('bitrate')), }) self._sort_formats(formats) description = strip_or_none(item.get('descr')) thumbnail = item.get('media_content_thumbnail_large') or source.get('poster') or item.get('media_thumbnail') duration = int_or_none(item.get('media_length') or source.get('length')) timestamp = unified_timestamp(item.get('pubDate')) view_count = int_or_none(item.get('media_views')) age_limit = int_or_none(try_get(item, lambda x: x['movie']['fsk'])) release_year = int_or_none(try_get(item, lambda x: x['movie']['rel_year'])) info = { 'id': video_id, 'title': title, 'description': description, 'thumbnail': thumbnail, 'duration': duration, 'timestamp': timestamp, 'view_count': view_count, 'age_limit': age_limit, 'release_year': release_year, 'formats': formats, } if kind.lower() == 'serien': series = try_get( item, lambda x: x['special']['title'], compat_str) or source.get('format') season_number = int_or_none(self._search_regex( r'^S(\d{1,2})\s*E\d{1,2}', title, 'season number', default=None) or self._search_regex( r'/staffel-(\d+)/', url, 'season number', default=None)) episode = source.get('title') episode_number = int_or_none(self._search_regex( r'^S\d{1,2}\s*E(\d{1,2})', title, 'episode number', default=None)) info.update({ 'series': series, 'season_number': season_number, 'episode': episode, 'episode_number': episode_number, }) return info
## # Copyright 2009-2014 Ghent University # # This file is part of EasyBuild, # originally created by the HPC team of Ghent University (http://ugent.be/hpc/en), # with support of Ghent University (http://ugent.be/hpc), # the Flemish Supercomputer Centre (VSC) (https://vscentrum.be/nl/en), # the Hercules foundation (http://www.herculesstichting.be/in_English) # and the Department of Economy, Science and Innovation (EWI) (http://www.ewi-vlaanderen.be/en). # # http://github.com/hpcugent/easybuild # # EasyBuild is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation v2. # # EasyBuild is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with EasyBuild. If not, see <http://www.gnu.org/licenses/>. ## """ Module that takes control of versioning. @author: Stijn De Weirdt (Ghent University) @author: Dries Verdegem (Ghent University) @author: Kenneth Hoste (Ghent University) @author: Pieter De Baets (Ghent University) @author: Jens Timmerman (Ghent University) """ import os from distutils.version import LooseVersion from socket import gethostname # note: release candidates should be versioned as a pre-release, e.g. "1.1rc1" # 1.1-rc1 would indicate a post-release, i.e., and update of 1.1, so beware! VERSION = LooseVersion("1.14.0") UNKNOWN = "UNKNOWN" def get_git_revision(): """ Returns the git revision (e.g. aab4afc016b742c6d4b157427e192942d0e131fe), or UNKNOWN is getting the git revision fails relies on GitPython (see http://gitorious.org/git-python) """ try: import git except ImportError: return UNKNOWN try: path = os.path.dirname(__file__) gitrepo = git.Git(path) return gitrepo.rev_list("HEAD").splitlines()[0] except git.GitCommandError: return UNKNOWN git_rev = get_git_revision() if git_rev == UNKNOWN: VERBOSE_VERSION = VERSION else: VERBOSE_VERSION = LooseVersion("%s-r%s" % (VERSION, get_git_revision())) # alias FRAMEWORK_VERSION = VERBOSE_VERSION # EasyBlock version try: from easybuild.easyblocks import VERBOSE_VERSION as EASYBLOCKS_VERSION except: EASYBLOCKS_VERSION = '0.0.UNKNOWN.EASYBLOCKS' # make sure it is smaller then anything def this_is_easybuild(): """Standard starting message""" top_version = max(FRAMEWORK_VERSION, EASYBLOCKS_VERSION) # !!! bootstrap_eb.py script checks hard on the string below, so adjust with sufficient care !!! msg = "This is EasyBuild %s (framework: %s, easyblocks: %s) on host %s." \ % (top_version, FRAMEWORK_VERSION, EASYBLOCKS_VERSION, gethostname()) return msg
#!/usr/bin/env python # # rt-mutex tester # # (C) 2006 Thomas Gleixner <tglx@linutronix.de> # # This program is free software; you can redistribute it and/or modify # it under the terms of the GNU General Public License version 2 as # published by the Free Software Foundation. # import os import sys import getopt import shutil import string # Globals quiet = 0 test = 0 comments = 0 sysfsprefix = "/sys/devices/system/rttest/rttest" statusfile = "/status" commandfile = "/command" # Command opcodes cmd_opcodes = { "schedother" : "1", "schedfifo" : "2", "lock" : "3", "locknowait" : "4", "lockint" : "5", "lockintnowait" : "6", "lockcont" : "7", "unlock" : "8", "lockbkl" : "9", "unlockbkl" : "10", "signal" : "11", "resetevent" : "98", "reset" : "99", } test_opcodes = { "prioeq" : ["P" , "eq" , None], "priolt" : ["P" , "lt" , None], "priogt" : ["P" , "gt" , None], "nprioeq" : ["N" , "eq" , None], "npriolt" : ["N" , "lt" , None], "npriogt" : ["N" , "gt" , None], "unlocked" : ["M" , "eq" , 0], "trylock" : ["M" , "eq" , 1], "blocked" : ["M" , "eq" , 2], "blockedwake" : ["M" , "eq" , 3], "locked" : ["M" , "eq" , 4], "opcodeeq" : ["O" , "eq" , None], "opcodelt" : ["O" , "lt" , None], "opcodegt" : ["O" , "gt" , None], "eventeq" : ["E" , "eq" , None], "eventlt" : ["E" , "lt" , None], "eventgt" : ["E" , "gt" , None], } # Print usage information def usage(): print "rt-tester.py <-c -h -q -t> <testfile>" print " -c display comments after first command" print " -h help" print " -q quiet mode" print " -t test mode (syntax check)" print " testfile: read test specification from testfile" print " otherwise from stdin" return # Print progress when not in quiet mode def progress(str): if not quiet: print str # Analyse a status value def analyse(val, top, arg): intval = int(val) if top[0] == "M": intval = intval / (10 ** int(arg)) intval = intval % 10 argval = top[2] elif top[0] == "O": argval = int(cmd_opcodes.get(arg, arg)) else: argval = int(arg) # progress("%d %s %d" %(intval, top[1], argval)) if top[1] == "eq" and intval == argval: return 1 if top[1] == "lt" and intval < argval: return 1 if top[1] == "gt" and intval > argval: return 1 return 0 # Parse the commandline try: (options, arguments) = getopt.getopt(sys.argv[1:],'chqt') except getopt.GetoptError, ex: usage() sys.exit(1) # Parse commandline options for option, value in options: if option == "-c": comments = 1 elif option == "-q": quiet = 1 elif option == "-t": test = 1 elif option == '-h': usage() sys.exit(0) # Select the input source if arguments: try: fd = open(arguments[0]) except Exception,ex: sys.stderr.write("File not found %s\n" %(arguments[0])) sys.exit(1) else: fd = sys.stdin linenr = 0 # Read the test patterns while 1: linenr = linenr + 1 line = fd.readline() if not len(line): break line = line.strip() parts = line.split(":") if not parts or len(parts) < 1: continue if len(parts[0]) == 0: continue if parts[0].startswith("#"): if comments > 1: progress(line) continue if comments == 1: comments = 2 progress(line) cmd = parts[0].strip().lower() opc = parts[1].strip().lower() tid = parts[2].strip() dat = parts[3].strip() try: # Test or wait for a status value if cmd == "t" or cmd == "w": testop = test_opcodes[opc] fname = "%s%s%s" %(sysfsprefix, tid, statusfile) if test: print fname continue while 1: query = 1 fsta = open(fname, 'r') status = fsta.readline().strip() fsta.close() stat = status.split(",") for s in stat: s = s.strip() if s.startswith(testop[0]): # Seperate status value val = s[2:].strip() query = analyse(val, testop, dat) break if query or cmd == "t": break progress(" " + status) if not query: sys.stderr.write("Test failed in line %d\n" %(linenr)) sys.exit(1) # Issue a command to the tester elif cmd == "c": cmdnr = cmd_opcodes[opc] # Build command string and sys filename cmdstr = "%s:%s" %(cmdnr, dat) fname = "%s%s%s" %(sysfsprefix, tid, commandfile) if test: print fname continue fcmd = open(fname, 'w') fcmd.write(cmdstr) fcmd.close() except Exception,ex: sys.stderr.write(str(ex)) sys.stderr.write("\nSyntax error in line %d\n" %(linenr)) if not test: fd.close() sys.exit(1) # Normal exit pass print "Pass" sys.exit(0)
# -*- coding: utf-8 -*- # # Copyright: (c) 2017, F5 Networks Inc. # GNU General Public License v3.0 (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt) from __future__ import (absolute_import, division, print_function) __metaclass__ = type import os import json import pytest import sys if sys.version_info < (2, 7): pytestmark = pytest.mark.skip("F5 Ansible modules require Python >= 2.7") from ansible.module_utils.basic import AnsibleModule try: from library.modules.bigip_service_policy import ApiParameters from library.modules.bigip_service_policy import ModuleParameters from library.modules.bigip_service_policy import ModuleManager from library.modules.bigip_service_policy import ArgumentSpec # In Ansible 2.8, Ansible changed import paths. from test.units.compat import unittest from test.units.compat.mock import Mock from test.units.compat.mock import patch from test.units.modules.utils import set_module_args except ImportError: from ansible.modules.network.f5.bigip_service_policy import ApiParameters from ansible.modules.network.f5.bigip_service_policy import ModuleParameters from ansible.modules.network.f5.bigip_service_policy import ModuleManager from ansible.modules.network.f5.bigip_service_policy import ArgumentSpec # Ansible 2.8 imports from units.compat import unittest from units.compat.mock import Mock from units.compat.mock import patch from units.modules.utils import set_module_args fixture_path = os.path.join(os.path.dirname(__file__), 'fixtures') fixture_data = {} def load_fixture(name): path = os.path.join(fixture_path, name) if path in fixture_data: return fixture_data[path] with open(path) as f: data = f.read() try: data = json.loads(data) except Exception: pass fixture_data[path] = data return data class TestParameters(unittest.TestCase): def test_module_parameters(self): args = dict( name='foo', description='my description', timer_policy='timer1', port_misuse_policy='misuse1', ) p = ModuleParameters(params=args) assert p.name == 'foo' assert p.description == 'my description' assert p.timer_policy == '/Common/timer1' assert p.port_misuse_policy == '/Common/misuse1' def test_api_parameters(self): args = load_fixture('load_net_service_policy_1.json') p = ApiParameters(params=args) assert p.name == 'baz' assert p.description == 'my description' assert p.timer_policy == '/Common/foo' assert p.port_misuse_policy == '/Common/bar' class TestManager(unittest.TestCase): def setUp(self): self.spec = ArgumentSpec() try: self.p1 = patch('library.modules.bigip_service_policy.module_provisioned') self.m1 = self.p1.start() self.m1.return_value = True except Exception: self.p1 = patch('ansible.modules.network.f5.bigip_service_policy.module_provisioned') self.m1 = self.p1.start() self.m1.return_value = True def test_create_selfip(self, *args): set_module_args(dict( name='foo', description='my description', timer_policy='timer1', port_misuse_policy='misuse1', partition='Common', state='present', provider=dict( server='localhost', password='password', user='admin' ) )) module = AnsibleModule( argument_spec=self.spec.argument_spec, supports_check_mode=self.spec.supports_check_mode ) mm = ModuleManager(module=module) # Override methods to force specific logic in the module to happen mm.exists = Mock(side_effect=[False, True]) mm.create_on_device = Mock(return_value=True) mm.module_provisioned = Mock(return_value=True) results = mm.exec_module() assert results['changed'] is True
#!/usr/bin/env python # # Copyright 2013 Quantopian, Inc. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. import matplotlib.pyplot as plt from zipline.algorithm import TradingAlgorithm from zipline.utils.factory import load_from_yahoo # Import exponential moving average from talib wrapper from zipline.transforms.ta import EMA from datetime import datetime import pytz class DualEMATaLib(TradingAlgorithm): """Dual Moving Average Crossover algorithm. This algorithm buys apple once its short moving average crosses its long moving average (indicating upwards momentum) and sells its shares once the averages cross again (indicating downwards momentum). """ def initialize(self, short_window=20, long_window=40): # Add 2 mavg transforms, one with a long window, one # with a short window. self.short_ema_trans = EMA(timeperiod=short_window) self.long_ema_trans = EMA(timeperiod=long_window) # To keep track of whether we invested in the stock or not self.invested = False def handle_data(self, data): self.short_ema = self.short_ema_trans.handle_data(data) self.long_ema = self.long_ema_trans.handle_data(data) if self.short_ema is None or self.long_ema is None: return self.buy = False self.sell = False if (self.short_ema > self.long_ema).all() and not self.invested: self.order('AAPL', 100) self.invested = True self.buy = True elif (self.short_ema < self.long_ema).all() and self.invested: self.order('AAPL', -100) self.invested = False self.sell = True self.record(AAPL=data['AAPL'].price, short_ema=self.short_ema['AAPL'], long_ema=self.long_ema['AAPL'], buy=self.buy, sell=self.sell) if __name__ == '__main__': start = datetime(1990, 1, 1, 0, 0, 0, 0, pytz.utc) end = datetime(1991, 1, 1, 0, 0, 0, 0, pytz.utc) data = load_from_yahoo(stocks=['AAPL'], indexes={}, start=start, end=end) dma = DualEMATaLib() results = dma.run(data).dropna() fig = plt.figure() ax1 = fig.add_subplot(211, ylabel='portfolio value') results.portfolio_value.plot(ax=ax1) ax2 = fig.add_subplot(212) results[['AAPL', 'short_ema', 'long_ema']].plot(ax=ax2) ax2.plot(results.ix[results.buy].index, results.short_ema[results.buy], '^', markersize=10, color='m') ax2.plot(results.ix[results.sell].index, results.short_ema[results.sell], 'v', markersize=10, color='k') plt.legend(loc=0) plt.gcf().set_size_inches(18, 8)
# -*- encoding: utf-8 -*- ############################################################################## # # OpenERP, Open Source Management Solution # # Copyright (c) 2011 Noviat nv/sa (www.noviat.be). All rights reserved. # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## import time from osv import osv, fields import decimal_precision as dp import netsvc from tools.translate import _ class account_bank_statement(osv.osv): _inherit = 'account.bank.statement' def write(self, cr, uid, ids, vals, context=None): if context is None: context = {} # bypass obsolete statement line resequencing if vals.get('line_ids', False) or context.get('ebanking_import', False): res = super(osv.osv, self).write(cr, uid, ids, vals, context=context) else: res = super(account_bank_statement, self).write(cr, uid, ids, vals, context=context) return res def button_confirm_bank(self, cr, uid, ids, context=None): super(account_bank_statement, self).button_confirm_bank(cr, uid, ids, context=context) for st in self.browse(cr, uid, ids, context=context): cr.execute("UPDATE account_bank_statement_line \ SET state='confirm' WHERE id in %s ", (tuple([x.id for x in st.line_ids]),)) return True def button_cancel(self, cr, uid, ids, context=None): super(account_bank_statement, self).button_cancel(cr, uid, ids, context=context) for st in self.browse(cr, uid, ids, context=context): if st.line_ids: cr.execute("UPDATE account_bank_statement_line \ SET state='draft' WHERE id in %s ", (tuple([x.id for x in st.line_ids]),)) return True account_bank_statement() class account_bank_statement_line_global(osv.osv): _name = 'account.bank.statement.line.global' _description = 'Batch Payment Info' _columns = { 'name': fields.char('Communication', size=128, required=True), 'code': fields.char('Code', size=64, required=True), 'parent_id': fields.many2one('account.bank.statement.line.global', 'Parent Code', ondelete='cascade'), 'child_ids': fields.one2many('account.bank.statement.line.global', 'parent_id', 'Child Codes'), 'type': fields.selection([ ('iso20022', 'ISO 20022'), ('coda', 'CODA'), ('manual', 'Manual'), ], 'Type', required=True), 'amount': fields.float('Amount', digits_compute=dp.get_precision('Account')), 'bank_statement_line_ids': fields.one2many('account.bank.statement.line', 'globalisation_id', 'Bank Statement Lines'), } _rec_name = 'code' _defaults = { 'code': lambda s,c,u,ctx={}: s.pool.get('ir.sequence').get(c, u, 'account.bank.statement.line.global'), 'name': '/', } _sql_constraints = [ ('code_uniq', 'unique (code)', 'The code must be unique !'), ] def name_search(self, cr, user, name, args=None, operator='ilike', context=None, limit=100): if not args: args = [] ids = [] if name: ids = self.search(cr, user, [('code', 'ilike', name)] + args, limit=limit) if not ids: ids = self.search(cr, user, [('name', operator, name)] + args, limit=limit) if not ids and len(name.split()) >= 2: #Separating code and name for searching operand1, operand2 = name.split(' ', 1) #name can contain spaces ids = self.search(cr, user, [('code', 'like', operand1), ('name', operator, operand2)] + args, limit=limit) else: ids = self.search(cr, user, args, context=context, limit=limit) return self.name_get(cr, user, ids, context=context) account_bank_statement_line_global() class account_bank_statement_line(osv.osv): _inherit = 'account.bank.statement.line' _columns = { 'date': fields.date('Entry Date', required=True, states={'confirm': [('readonly', True)]}), 'val_date': fields.date('Valuta Date', states={'confirm': [('readonly', True)]}), 'globalisation_id': fields.many2one('account.bank.statement.line.global', 'Globalisation ID', states={'confirm': [('readonly', True)]}, help="Code to identify transactions belonging to the same globalisation level within a batch payment"), 'globalisation_amount': fields.related('globalisation_id', 'amount', type='float', relation='account.bank.statement.line.global', string='Glob. Amount', readonly=True), 'journal_id': fields.related('statement_id', 'journal_id', type='many2one', relation='account.journal', string='Journal', store=True, readonly=True), 'state': fields.selection([('draft', 'Draft'), ('confirm', 'Confirmed')], 'State', required=True, readonly=True), 'counterparty_name': fields.char('Counterparty Name', size=35), 'counterparty_bic': fields.char('Counterparty BIC', size=11), 'counterparty_number': fields.char('Counterparty Number', size=34), 'counterparty_currency': fields.char('Counterparty Currency', size=3), } _defaults = { 'state': 'draft', } def unlink(self, cr, uid, ids, context=None): if context is None: context = {} if context.get('block_statement_line_delete', False): raise osv.except_osv(_('Warning'), _('Delete operation not allowed ! \ Please go to the associated bank statement in order to delete and/or modify this bank statement line')) return super(account_bank_statement_line, self).unlink(cr, uid, ids, context=context) account_bank_statement_line() # vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
""" Useful auxilliary data structures for query construction. Not useful outside the SQL domain. """ class EmptyResultSet(Exception): pass class FullResultSet(Exception): pass class MultiJoin(Exception): """ Used by join construction code to indicate the point at which a multi-valued join was attempted (if the caller wants to treat that exceptionally). """ def __init__(self, level): self.level = level class Empty(object): pass class RawValue(object): def __init__(self, value): self.value = value class Date(object): """ Add a date selection column. """ def __init__(self, col, lookup_type): self.col = col self.lookup_type = lookup_type def relabel_aliases(self, change_map): c = self.col if isinstance(c, (list, tuple)): self.col = (change_map.get(c[0], c[0]), c[1]) def as_sql(self, qn, connection): if isinstance(self.col, (list, tuple)): col = '%s.%s' % tuple([qn(c) for c in self.col]) else: col = self.col return connection.ops.date_trunc_sql(self.lookup_type, col)
# -*- coding: utf-8 -*- # # Copyright (C) 2012-2013 Vinay Sajip. # Licensed to the Python Software Foundation under a contributor agreement. # See LICENSE.txt and CONTRIBUTORS.txt. # """Parser for the environment markers micro-language defined in PEP 345.""" import ast import os import sys import platform from .compat import python_implementation, string_types from .util import in_venv __all__ = ['interpret'] class Evaluator(object): """ A limited evaluator for Python expressions. """ operators = { 'eq': lambda x, y: x == y, 'gt': lambda x, y: x > y, 'gte': lambda x, y: x >= y, 'in': lambda x, y: x in y, 'lt': lambda x, y: x < y, 'lte': lambda x, y: x <= y, 'not': lambda x: not x, 'noteq': lambda x, y: x != y, 'notin': lambda x, y: x not in y, } allowed_values = { 'sys_platform': sys.platform, 'python_version': '%s.%s' % sys.version_info[:2], # parsing sys.platform is not reliable, but there is no other # way to get e.g. 2.7.2+, and the PEP is defined with sys.version 'python_full_version': sys.version.split(' ', 1)[0], 'os_name': os.name, 'platform_in_venv': str(in_venv()), 'platform_release': platform.release(), 'platform_version': platform.version(), 'platform_machine': platform.machine(), 'platform_python_implementation': python_implementation(), } def __init__(self, context=None): """ Initialise an instance. :param context: If specified, names are looked up in this mapping. """ self.context = context or {} self.source = None def get_fragment(self, offset): """ Get the part of the source which is causing a problem. """ fragment_len = 10 s = '%r' % (self.source[offset:offset + fragment_len]) if offset + fragment_len < len(self.source): s += '...' return s def get_handler(self, node_type): """ Get a handler for the specified AST node type. """ return getattr(self, 'do_%s' % node_type, None) def evaluate(self, node, filename=None): """ Evaluate a source string or node, using ``filename`` when displaying errors. """ if isinstance(node, string_types): self.source = node kwargs = {'mode': 'eval'} if filename: kwargs['filename'] = filename try: node = ast.parse(node, **kwargs) except SyntaxError as e: s = self.get_fragment(e.offset) raise SyntaxError('syntax error %s' % s) node_type = node.__class__.__name__.lower() handler = self.get_handler(node_type) if handler is None: if self.source is None: s = '(source not available)' else: s = self.get_fragment(node.col_offset) raise SyntaxError("don't know how to evaluate %r %s" % ( node_type, s)) return handler(node) def get_attr_key(self, node): assert isinstance(node, ast.Attribute), 'attribute node expected' return '%s.%s' % (node.value.id, node.attr) def do_attribute(self, node): if not isinstance(node.value, ast.Name): valid = False else: key = self.get_attr_key(node) valid = key in self.context or key in self.allowed_values if not valid: raise SyntaxError('invalid expression: %s' % key) if key in self.context: result = self.context[key] else: result = self.allowed_values[key] return result def do_boolop(self, node): result = self.evaluate(node.values[0]) is_or = node.op.__class__ is ast.Or is_and = node.op.__class__ is ast.And assert is_or or is_and if (is_and and result) or (is_or and not result): for n in node.values[1:]: result = self.evaluate(n) if (is_or and result) or (is_and and not result): break return result def do_compare(self, node): def sanity_check(lhsnode, rhsnode): valid = True if isinstance(lhsnode, ast.Str) and isinstance(rhsnode, ast.Str): valid = False #elif (isinstance(lhsnode, ast.Attribute) # and isinstance(rhsnode, ast.Attribute)): # klhs = self.get_attr_key(lhsnode) # krhs = self.get_attr_key(rhsnode) # valid = klhs != krhs if not valid: s = self.get_fragment(node.col_offset) raise SyntaxError('Invalid comparison: %s' % s) lhsnode = node.left lhs = self.evaluate(lhsnode) result = True for op, rhsnode in zip(node.ops, node.comparators): sanity_check(lhsnode, rhsnode) op = op.__class__.__name__.lower() if op not in self.operators: raise SyntaxError('unsupported operation: %r' % op) rhs = self.evaluate(rhsnode) result = self.operators[op](lhs, rhs) if not result: break lhs = rhs lhsnode = rhsnode return result def do_expression(self, node): return self.evaluate(node.body) def do_name(self, node): valid = False if node.id in self.context: valid = True result = self.context[node.id] elif node.id in self.allowed_values: valid = True result = self.allowed_values[node.id] if not valid: raise SyntaxError('invalid expression: %s' % node.id) return result def do_str(self, node): return node.s def interpret(marker, execution_context=None): """ Interpret a marker and return a result depending on environment. :param marker: The marker to interpret. :type marker: str :param execution_context: The context used for name lookup. :type execution_context: mapping """ return Evaluator(execution_context).evaluate(marker.strip())
import zmq import array import time import struct import numpy as np import pmt import sys class zmq_pull_socket(): def __init__(self, tcp_str, verbose=0): self.context = zmq.Context() self.receiver = self.context.socket(zmq.PULL) self.receiver.connect(tcp_str) def poll(self, type_str='f', verbose=0): raw_data = self.receiver.recv() a = array.array(type_str, raw_data) return a def poll_message(self): msg = self.receiver.recv() # this is a binary string, convert it to a list of ints byte_list = [] for byte in msg: byte_list.append(ord(byte)) return byte_list # incomplete attempt to optimize data flow by # sending bytes instead of floats; flowgraph # changes needed to support this, as well # as all downstream code reworked to use # bytes def poll_short(self, type_str='h', verbose=0): raw_data = self.receiver.recv() a = array.array(type_str, raw_data) npa_s = np.asarray(a) npa_f = npa_s.astype(float) npa_f *= (1.0/10000.0) #fmt = "<%dI" % (len(raw_data) //4) #a = list(struct.unpack(fmt, raw_data)) return list(npa_f)
""" Many-to-one relationships To define a many-to-one relationship, use ``ForeignKey()``. """ from __future__ import unicode_literals from django.db import models from django.utils.encoding import python_2_unicode_compatible @python_2_unicode_compatible class Reporter(models.Model): first_name = models.CharField(max_length=30) last_name = models.CharField(max_length=30) email = models.EmailField() def __str__(self): return "%s %s" % (self.first_name, self.last_name) @python_2_unicode_compatible class Article(models.Model): headline = models.CharField(max_length=100) pub_date = models.DateField() reporter = models.ForeignKey(Reporter, models.CASCADE) def __str__(self): return self.headline class Meta: ordering = ('headline',) # If ticket #1578 ever slips back in, these models will not be able to be # created (the field names being lower-cased versions of their opposite # classes is important here). class First(models.Model): second = models.IntegerField() class Second(models.Model): first = models.ForeignKey(First, models.CASCADE, related_name='the_first') # Protect against repetition of #1839, #2415 and #2536. class Third(models.Model): name = models.CharField(max_length=20) third = models.ForeignKey('self', models.SET_NULL, null=True, related_name='child_set') class Parent(models.Model): name = models.CharField(max_length=20, unique=True) bestchild = models.ForeignKey('Child', models.SET_NULL, null=True, related_name='favored_by') class Child(models.Model): name = models.CharField(max_length=20) parent = models.ForeignKey(Parent, models.CASCADE) class ToFieldChild(models.Model): parent = models.ForeignKey(Parent, models.CASCADE, to_field='name') # Multiple paths to the same model (#7110, #7125) @python_2_unicode_compatible class Category(models.Model): name = models.CharField(max_length=20) def __str__(self): return self.name class Record(models.Model): category = models.ForeignKey(Category, models.CASCADE) @python_2_unicode_compatible class Relation(models.Model): left = models.ForeignKey(Record, models.CASCADE, related_name='left_set') right = models.ForeignKey(Record, models.CASCADE, related_name='right_set') def __str__(self): return "%s - %s" % (self.left.category.name, self.right.category.name) # Test related objects visibility. class SchoolManager(models.Manager): def get_queryset(self): return super(SchoolManager, self).get_queryset().filter(is_public=True) class School(models.Model): is_public = models.BooleanField(default=False) objects = SchoolManager() class Student(models.Model): school = models.ForeignKey(School, models.CASCADE)
from __future__ import (absolute_import, division, print_function, unicode_literals) import numpy as np from ggplot.utils import make_iterable_ntimes from .geom import geom # Note when documenting # slope and intercept can be functions that compute the slope # and intercept using the data. If that is the case then the # x and y aesthetics must be mapped class geom_abline(geom): DEFAULT_AES = {'color': 'black', 'linetype': 'solid', 'alpha': None, 'size': 1.0, 'x': None, 'y': None} REQUIRED_AES = {'slope', 'intercept'} DEFAULT_PARAMS = {'stat': 'abline', 'position': 'identity'} _aes_renames = {'linetype': 'linestyle', 'size': 'linewidth'} def _plot_unit(self, pinfo, ax): slope = pinfo['slope'] intercept = pinfo['intercept'] n = len(slope) linewidth = make_iterable_ntimes(pinfo['linewidth'], n) linestyle = make_iterable_ntimes(pinfo['linestyle'], n) alpha = make_iterable_ntimes(pinfo['alpha'], n) color = make_iterable_ntimes(pinfo['color'], n) ax.set_autoscale_on(False) xlim = ax.get_xlim() _x = np.array([np.min(xlim), np.max(xlim)]) for i in range(len(slope)): _y = _x * slope[i] + intercept[i] ax.plot(_x, _y, linewidth=linewidth[i], linestyle=linestyle[i], alpha=alpha[i], color=color[i])
import uuid from django.contrib.auth import get_permission_codename from django.contrib.sites.models import Site from django.core.exceptions import ValidationError from django.db import models from django.utils.encoding import python_2_unicode_compatible from django.utils.translation import ugettext_lazy as _ from cms.models.fields import PlaceholderField from cms.utils.copy_plugins import copy_plugins_to def static_slotname(instance): """ Returns a string to be used as the slot for the static placeholder field. """ return instance.code @python_2_unicode_compatible class StaticPlaceholder(models.Model): CREATION_BY_TEMPLATE = 'template' CREATION_BY_CODE = 'code' CREATION_METHODS = ( (CREATION_BY_TEMPLATE, _('by template')), (CREATION_BY_CODE, _('by code')), ) name = models.CharField( verbose_name=_(u'static placeholder name'), max_length=255, blank=True, default='', help_text=_(u'Descriptive name to identify this static placeholder. Not displayed to users.')) code = models.CharField( verbose_name=_(u'placeholder code'), max_length=255, blank=True, help_text=_(u'To render the static placeholder in templates.')) draft = PlaceholderField(static_slotname, verbose_name=_(u'placeholder content'), related_name='static_draft') public = PlaceholderField(static_slotname, editable=False, related_name='static_public') dirty = models.BooleanField(default=False, editable=False) creation_method = models.CharField( verbose_name=_('creation_method'), choices=CREATION_METHODS, default=CREATION_BY_CODE, max_length=20, blank=True, ) site = models.ForeignKey(Site, null=True, blank=True) class Meta: verbose_name = _(u'static placeholder') verbose_name_plural = _(u'static placeholders') app_label = 'cms' unique_together = (('code', 'site'),) def __str__(self): return self.name def clean(self): # TODO: check for clashes if the random code is already taken if not self.code: self.code = u'static-%s' % uuid.uuid4() if not self.site: placeholders = StaticPlaceholder.objects.filter(code=self.code, site__isnull=True) if self.pk: placeholders = placeholders.exclude(pk=self.pk) if placeholders.exists(): raise ValidationError(_("A static placeholder with the same site and code already exists")) def publish(self, request, language, force=False): if force or self.has_publish_permission(request): self.public.clear(language=language) plugins = self.draft.get_plugins_list(language=language) copy_plugins_to(plugins, self.public, no_signals=True) self.dirty = False self.save() return True return False def has_change_permission(self, request): if request.user.is_superuser: return True opts = self._meta return request.user.has_perm(opts.app_label + '.' + get_permission_codename('change', opts)) def has_publish_permission(self, request): if request.user.is_superuser: return True opts = self._meta return request.user.has_perm(opts.app_label + '.' + get_permission_codename('change', opts)) and \ request.user.has_perm(opts.app_label + '.' + 'publish_page')
"""Tests for the Hisense AEH-W4A1 init file.""" from unittest.mock import patch from pyaehw4a1 import exceptions from homeassistant import config_entries, data_entry_flow from homeassistant.components import hisense_aehw4a1 from homeassistant.setup import async_setup_component async def test_creating_entry_sets_up_climate_discovery(hass): """Test setting up Hisense AEH-W4A1 loads the climate component.""" with patch( "homeassistant.components.hisense_aehw4a1.config_flow.AehW4a1.discovery", return_value=["1.2.3.4"], ), patch( "homeassistant.components.hisense_aehw4a1.climate.async_setup_entry", return_value=True, ) as mock_setup: result = await hass.config_entries.flow.async_init( hisense_aehw4a1.DOMAIN, context={"source": config_entries.SOURCE_USER} ) # Confirmation form assert result["type"] == data_entry_flow.RESULT_TYPE_FORM result = await hass.config_entries.flow.async_configure(result["flow_id"], {}) assert result["type"] == data_entry_flow.RESULT_TYPE_CREATE_ENTRY await hass.async_block_till_done() assert len(mock_setup.mock_calls) == 1 async def test_configuring_hisense_w4a1_create_entry(hass): """Test that specifying config will create an entry.""" with patch( "homeassistant.components.hisense_aehw4a1.config_flow.AehW4a1.check", return_value=True, ), patch( "homeassistant.components.hisense_aehw4a1.async_setup_entry", return_value=True, ) as mock_setup: await async_setup_component( hass, hisense_aehw4a1.DOMAIN, {"hisense_aehw4a1": {"ip_address": ["1.2.3.4"]}}, ) await hass.async_block_till_done() assert len(mock_setup.mock_calls) == 1 async def test_configuring_hisense_w4a1_not_creates_entry_for_device_not_found(hass): """Test that specifying config will not create an entry.""" with patch( "homeassistant.components.hisense_aehw4a1.config_flow.AehW4a1.check", side_effect=exceptions.ConnectionError, ), patch( "homeassistant.components.hisense_aehw4a1.async_setup_entry", return_value=True, ) as mock_setup: await async_setup_component( hass, hisense_aehw4a1.DOMAIN, {"hisense_aehw4a1": {"ip_address": ["1.2.3.4"]}}, ) await hass.async_block_till_done() assert len(mock_setup.mock_calls) == 0 async def test_configuring_hisense_w4a1_not_creates_entry_for_empty_import(hass): """Test that specifying config will not create an entry.""" with patch( "homeassistant.components.hisense_aehw4a1.async_setup_entry", return_value=True, ) as mock_setup: await async_setup_component(hass, hisense_aehw4a1.DOMAIN, {}) await hass.async_block_till_done() assert len(mock_setup.mock_calls) == 0
""" ======================================= Receiver Operating Characteristic (ROC) ======================================= Example of Receiver Operating Characteristic (ROC) metric to evaluate classifier output quality. ROC curves typically feature true positive rate on the Y axis, and false positive rate on the X axis. This means that the top left corner of the plot is the "ideal" point - a false positive rate of zero, and a true positive rate of one. This is not very realistic, but it does mean that a larger area under the curve (AUC) is usually better. The "steepness" of ROC curves is also important, since it is ideal to maximize the true positive rate while minimizing the false positive rate. ROC curves are typically used in binary classification to study the output of a classifier. In order to extend ROC curve and ROC area to multi-class or multi-label classification, it is necessary to binarize the output. One ROC curve can be drawn per label, but one can also draw a ROC curve by considering each element of the label indicator matrix as a binary prediction (micro-averaging). .. note:: See also :func:`sklearn.metrics.roc_auc_score`, :ref:`example_model_selection_plot_roc_crossval.py`. """ print(__doc__) import numpy as np import matplotlib.pyplot as plt from sklearn import svm, datasets from sklearn.metrics import roc_curve, auc from sklearn.cross_validation import train_test_split from sklearn.preprocessing import label_binarize from sklearn.multiclass import OneVsRestClassifier # Import some data to play with iris = datasets.load_iris() X = iris.data y = iris.target # Binarize the output y = label_binarize(y, classes=[0, 1, 2]) n_classes = y.shape[1] # Add noisy features to make the problem harder random_state = np.random.RandomState(0) n_samples, n_features = X.shape X = np.c_[X, random_state.randn(n_samples, 200 * n_features)] # shuffle and split training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=0) # Learn to predict each class against the other classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True, random_state=random_state)) y_score = classifier.fit(X_train, y_train).decision_function(X_test) # Compute ROC curve and ROC area for each class fpr = dict() tpr = dict() roc_auc = dict() for i in range(n_classes): fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i]) roc_auc[i] = auc(fpr[i], tpr[i]) # Compute micro-average ROC curve and ROC area fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_score.ravel()) roc_auc["micro"] = auc(fpr["micro"], tpr["micro"]) # Plot of a ROC curve for a specific class plt.figure() plt.plot(fpr[2], tpr[2], label='ROC curve (area = %0.2f)' % roc_auc[2]) plt.plot([0, 1], [0, 1], 'k--') plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('Receiver operating characteristic example') plt.legend(loc="lower right") plt.show() # Plot ROC curve plt.figure() plt.plot(fpr["micro"], tpr["micro"], label='micro-average ROC curve (area = {0:0.2f})' ''.format(roc_auc["micro"])) for i in range(n_classes): plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})' ''.format(i, roc_auc[i])) plt.plot([0, 1], [0, 1], 'k--') plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('Some extension of Receiver operating characteristic to multi-class') plt.legend(loc="lower right") plt.show()
#!/usr/bin/env python # Copyright 2015 The Kubernetes Authors All rights reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer PORT_NUMBER = 8000 # This class will handles any incoming request. class HTTPHandler(BaseHTTPRequestHandler): # Handler for the GET requests def do_GET(self): self.send_response(200) self.send_header('Content-type','text/html') self.end_headers() self.wfile.write("Hello World!") try: # Create a web server and define the handler to manage the incoming request. server = HTTPServer(('', PORT_NUMBER), HTTPHandler) print 'Started httpserver on port ' , PORT_NUMBER server.serve_forever() except KeyboardInterrupt: print '^C received, shutting down the web server' server.socket.close()
""" Clickjacking Protection Middleware. This module provides a middleware that implements protection against a malicious site loading resources from your site in a hidden frame. """ from django.conf import settings class XFrameOptionsMiddleware(object): """ Middleware that sets the X-Frame-Options HTTP header in HTTP responses. Does not set the header if it's already set or if the response contains a xframe_options_exempt value set to True. By default, sets the X-Frame-Options header to 'SAMEORIGIN', meaning the response can only be loaded on a frame within the same site. To prevent the response from being loaded in a frame in any site, set X_FRAME_OPTIONS in your project's Django settings to 'DENY'. Note: older browsers will quietly ignore this header, thus other clickjacking protection techniques should be used if protection in those browsers is required. https://en.wikipedia.org/wiki/Clickjacking#Server_and_client """ def process_response(self, request, response): # Don't set it if it's already in the response if response.get('X-Frame-Options') is not None: return response # Don't set it if they used @xframe_options_exempt if getattr(response, 'xframe_options_exempt', False): return response response['X-Frame-Options'] = self.get_xframe_options_value(request, response) return response def get_xframe_options_value(self, request, response): """ Gets the value to set for the X_FRAME_OPTIONS header. By default this uses the value from the X_FRAME_OPTIONS Django settings. If not found in settings, defaults to 'SAMEORIGIN'. This method can be overridden if needed, allowing it to vary based on the request or response. """ return getattr(settings, 'X_FRAME_OPTIONS', 'SAMEORIGIN').upper()
import requests import json import numpy as np import pandas as pd import CoHouseToken from difflib import SequenceMatcher # In[3]: def exactMatch(line1, line2): line1=line1.upper().rstrip() line2=line2.upper().rstrip() #print("|"+line1+"|"+line2+"|",line1==line2) return line1==line2 def aStopWord(word): return word.upper().replace("COMPANY","CO").replace("LIMITED","LTD").replace("&","AND").rstrip() def spaces(word): w = word.upper().replace("/"," ") w = w.replace("."," ").replace(","," ").replace("-"," ").rstrip() return w def removeAStopWord(word): w = word.upper().replace("LTD"," ").replace("CO"," ").replace("AND"," ").replace("("," ").replace("/"," ") w = w.replace(")"," ").replace("."," ").replace(","," ").replace("-"," ").rstrip() return w def removeABlank(word): w = word.replace(" ","") return w def removeABracket (line): flag = False word="" for a in line: if a=="(": flag = True a="" if a==")": a="" flag = False if flag: a="" word+=a return word def stopWord(line1, line2): line1=aStopWord(line1) line2=aStopWord(line2) #print("|"+line1+"|"+line2+"|",line1==line2) return line1==line2 def removeStopWord(line1, line2): line1=spaces(line1) line2=spaces(line2) line1=aStopWord(line1) line2=aStopWord(line2) line1=removeAStopWord(line1) line2=removeAStopWord(line2) #print("|"+line1+"|"+line2+"|",line1==line2) return line1==line2 def removeBlanks(line1, line2): line1=spaces(line1) line2=spaces(line2) line1=aStopWord(line1) line2=aStopWord(line2) line1=removeAStopWord(line1) line2=removeAStopWord(line2) line1=removeABlank(line1) line2=removeABlank(line2) return line1==line2 def removeBrackets(line1, line2): line1=removeABracket(line1) line2=removeABracket(line2) line1=spaces(line1) line2=spaces(line2) line1=aStopWord(line1) line2=aStopWord(line2) line1=removeAStopWord(line1) line2=removeAStopWord(line2) line1=removeABlank(line1) line2=removeABlank(line2) #print("|"+line1+"|"+line2+"|",line1==line2) return line1==line2 def strip(line1, line2): line1=removeABracket(line1) line2=removeABracket(line2) line1=spaces(line1) line2=spaces(line2) line1=aStopWord(line1) line2=aStopWord(line2) line1=removeAStopWord(line1) line2=removeAStopWord(line2) line1=removeABlank(line1) line2=removeABlank(line2) return line1,line2 def match(company,results): for i in results['items']: line = i['title'] number = i['company_number'] if(exactMatch(company,line)): return True,line,number for i in results['items']: line = i['title'] number = i['company_number'] if(stopWord(company,line)): return True,line,number for i in results['items']: line = i['title'] number = i['company_number'] if(removeStopWord(company,line)): return True,line,number for i in results['items']: line = i['title'] number = i['company_number'] if(removeBlanks(company,line)): return True,line,number for i in results['items']: line = i['title'] number = i['company_number'] if(removeBrackets(company,line)): return True,line,number #old_match(company,results) return False,"","" def main(args): print(args[0]) search_url ="https://api.companieshouse.gov.uk/search/companies?q=" token = CoHouseToken.getToken() pw = '' base_url = 'https://api.companieshouse.gov.uk' file = args[1] print(file) df = pd.read_csv(file,names=['Organisation']) companies = df.Organisation count=0 found = open("found2.csv",'w') missing = open("missing2.csv",'w') for c in companies: c =c.upper().replace("&","AND") c = c.split(" T/A ")[0] c = c.split("WAS ")[0] c= spaces(c) url=search_url+c results = json.loads(requests.get(url, auth=(token,pw)).text) for i , key in enumerate(results['items']): a,b = strip(c, key['title']) r = SequenceMatcher(None, a, b).ratio() print("%s \t %s\t %.2f \t %s \t %s"%(i,c,r,key['company_number'],key['title'])) v = input('type number or return to reject: ') if v =="": print("reject") missing.write("%s\n"%(c)) else: key = results['items'][int(v)] print("%s \t %s\t %.2f \t %s \t %s"%(v,c,r,key['company_number'],key['title'])) print("*************************") found.write("%s,%s,%s,\n"%(c,key['title'],key['company_number'])) print() #print(count/len(companies)) return 0 if __name__ == '__main__': import sys sys.exit(main(sys.argv))
#!/usr/bin/python # Copyright (c) 2014, Intel Corporation All rights reserved. # # Redistribution and use in source and binary forms, with or without # modification, are permitted provided that the following conditions are # met: # # 1. Redistributions of source code must retain the above copyright # notice, this list of conditions and the following disclaimer. # # 2. Redistributions in binary form must reproduce the above copyright # notice, this list of conditions and the following disclaimer in the # documentation and/or other materials provided with the distribution. # # 3. Neither the name of the copyright holder nor the names of its # contributors may be used to endorse or promote products derived from # this software without specific prior written permission. # # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS # IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED # TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A # PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT # HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, # SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED # TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR # PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF # LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING # NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS # SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. import pyMIC as mic import numpy as np device = mic.devices[0] a = device.zeros((8,8))
#!/usr/bin/env python """html2text: Turn HTML into equivalent Markdown-structured text.""" __version__ = "2.36" __author__ = "Aaron Swartz (me@aaronsw.com)" __copyright__ = "(C) 2004-2008 Aaron Swartz. GNU GPL 3." __contributors__ = ["Martin 'Joey' Schulze", "Ricardo Reyes", "Kevin Jay North"] # TODO: # Support decoded entities with unifiable. if not hasattr(__builtins__, 'True'): True, False = 1, 0 import re, sys, urllib, htmlentitydefs, codecs import sgmllib import urlparse sgmllib.charref = re.compile('&#([xX]?[0-9a-fA-F]+)[^0-9a-fA-F]') try: from textwrap import wrap except: pass # Use Unicode characters instead of their ascii psuedo-replacements UNICODE_SNOB = 0 # Put the links after each paragraph instead of at the end. LINKS_EACH_PARAGRAPH = 0 # Wrap long lines at position. 0 for no wrapping. (Requires Python 2.3.) BODY_WIDTH = 78 # Don't show internal links (href="#local-anchor") -- corresponding link targets # won't be visible in the plain text file anyway. SKIP_INTERNAL_LINKS = False ### Entity Nonsense ### def name2cp(k): if k == 'apos': return ord("'") if hasattr(htmlentitydefs, "name2codepoint"): # requires Python 2.3 return htmlentitydefs.name2codepoint[k] else: k = htmlentitydefs.entitydefs[k] if k.startswith("&#") and k.endswith(";"): return int(k[2:-1]) # not in latin-1 return ord(codecs.latin_1_decode(k)[0]) unifiable = {'rsquo':"'", 'lsquo':"'", 'rdquo':'"', 'ldquo':'"', 'copy':'(C)', 'mdash':'--', 'nbsp':' ', 'rarr':'->', 'larr':'<-', 'middot':'*', 'ndash':'-', 'oelig':'oe', 'aelig':'ae', 'agrave':'a', 'aacute':'a', 'acirc':'a', 'atilde':'a', 'auml':'a', 'aring':'a', 'egrave':'e', 'eacute':'e', 'ecirc':'e', 'euml':'e', 'igrave':'i', 'iacute':'i', 'icirc':'i', 'iuml':'i', 'ograve':'o', 'oacute':'o', 'ocirc':'o', 'otilde':'o', 'ouml':'o', 'ugrave':'u', 'uacute':'u', 'ucirc':'u', 'uuml':'u'} unifiable_n = {} for k in unifiable.keys(): unifiable_n[name2cp(k)] = unifiable[k] def charref(name): if name[0] in ['x','X']: c = int(name[1:], 16) else: c = int(name) if not UNICODE_SNOB and c in unifiable_n.keys(): return unifiable_n[c] else: return unichr(c) def entityref(c): if not UNICODE_SNOB and c in unifiable.keys(): return unifiable[c] else: try: name2cp(c) except KeyError: return "&" + c else: return unichr(name2cp(c)) def replaceEntities(s): s = s.group(1) if s[0] == "#": return charref(s[1:]) else: return entityref(s) r_unescape = re.compile(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));") def unescape(s): return r_unescape.sub(replaceEntities, s) def fixattrs(attrs): # Fix bug in sgmllib.py if not attrs: return attrs newattrs = [] for attr in attrs: newattrs.append((attr[0], unescape(attr[1]))) return newattrs ### End Entity Nonsense ### def onlywhite(line): """Return true if the line does only consist of whitespace characters.""" for c in line: if c is not ' ' and c is not ' ': return c is ' ' return line def optwrap(text): """Wrap all paragraphs in the provided text.""" if not BODY_WIDTH: return text assert wrap, "Requires Python 2.3." result = '' newlines = 0 for para in text.split("\n"): if len(para) > 0: if para[0] is not ' ' and para[0] is not '-' and para[0] is not '*': for line in wrap(para, BODY_WIDTH): result += line + "\n" result += "\n" newlines = 2 else: if not onlywhite(para): result += para + "\n" newlines = 1 else: if newlines < 2: result += "\n" newlines += 1 return result def hn(tag): if tag[0] == 'h' and len(tag) == 2: try: n = int(tag[1]) if n in range(1, 10): return n except ValueError: return 0 class _html2text(sgmllib.SGMLParser): def __init__(self, out=sys.stdout.write, baseurl=''): sgmllib.SGMLParser.__init__(self) if out is None: self.out = self.outtextf else: self.out = out self.outtext = u'' self.quiet = 0 self.p_p = 0 self.outcount = 0 self.start = 1 self.space = 0 self.a = [] self.astack = [] self.acount = 0 self.list = [] self.blockquote = 0 self.pre = 0 self.startpre = 0 self.lastWasNL = 0 self.abbr_title = None # current abbreviation definition self.abbr_data = None # last inner HTML (for abbr being defined) self.abbr_list = {} # stack of abbreviations to write later self.baseurl = baseurl def outtextf(self, s): self.outtext += s def close(self): sgmllib.SGMLParser.close(self) self.pbr() self.o('', 0, 'end') return self.outtext def handle_charref(self, c): self.o(charref(c)) def handle_entityref(self, c): self.o(entityref(c)) def unknown_starttag(self, tag, attrs): self.handle_tag(tag, attrs, 1) def unknown_endtag(self, tag): self.handle_tag(tag, None, 0) def previousIndex(self, attrs): """ returns the index of certain set of attributes (of a link) in the self.a list If the set of attributes is not found, returns None """ if not attrs.has_key('href'): return None i = -1 for a in self.a: i += 1 match = 0 if a.has_key('href') and a['href'] == attrs['href']: if a.has_key('title') or attrs.has_key('title'): if (a.has_key('title') and attrs.has_key('title') and a['title'] == attrs['title']): match = True else: match = True if match: return i def handle_tag(self, tag, attrs, start): attrs = fixattrs(attrs) if hn(tag): self.p() if start: self.o(hn(tag)*"#" + ' ') if tag in ['p', 'div']: self.p() if tag == "br" and start: self.o(" \n") if tag == "hr" and start: self.p() self.o("* * *") self.p() if tag in ["head", "style", 'script']: if start: self.quiet += 1 else: self.quiet -= 1 if tag in ["body"]: self.quiet = 0 # sites like 9rules.com never close <head> if tag == "blockquote": if start: self.p(); self.o('> ', 0, 1); self.start = 1 self.blockquote += 1 else: self.blockquote -= 1 self.p() if tag in ['em', 'i', 'u']: self.o("_") if tag in ['strong', 'b']: self.o("**") if tag == "code" and not self.pre: self.o('`') #TODO: `` `this` `` if tag == "abbr": if start: attrsD = {} for (x, y) in attrs: attrsD[x] = y attrs = attrsD self.abbr_title = None self.abbr_data = '' if attrs.has_key('title'): self.abbr_title = attrs['title'] else: if self.abbr_title != None: self.abbr_list[self.abbr_data] = self.abbr_title self.abbr_title = None self.abbr_data = '' if tag == "a": if start: attrsD = {} for (x, y) in attrs: attrsD[x] = y attrs = attrsD if attrs.has_key('href') and not (SKIP_INTERNAL_LINKS and attrs['href'].startswith('#')): self.astack.append(attrs) self.o("[") else: self.astack.append(None) else: if self.astack: a = self.astack.pop() if a: i = self.previousIndex(a) if i is not None: a = self.a[i] else: self.acount += 1 a['count'] = self.acount a['outcount'] = self.outcount self.a.append(a) self.o("][" + `a['count']` + "]") if tag == "img" and start: attrsD = {} for (x, y) in attrs: attrsD[x] = y attrs = attrsD if attrs.has_key('src'): attrs['href'] = attrs['src'] alt = attrs.get('alt', '') i = self.previousIndex(attrs) if i is not None: attrs = self.a[i] else: self.acount += 1 attrs['count'] = self.acount attrs['outcount'] = self.outcount self.a.append(attrs) self.o("![") self.o(alt) self.o("]["+`attrs['count']`+"]") if tag == 'dl' and start: self.p() if tag == 'dt' and not start: self.pbr() if tag == 'dd' and start: self.o(' ') if tag == 'dd' and not start: self.pbr() if tag in ["ol", "ul"]: if start: self.list.append({'name':tag, 'num':0}) else: if self.list: self.list.pop() self.p() if tag == 'li': if start: self.pbr() if self.list: li = self.list[-1] else: li = {'name':'ul', 'num':0} self.o(" "*len(self.list)) #TODO: line up <ol><li>s > 9 correctly. if li['name'] == "ul": self.o("* ") elif li['name'] == "ol": li['num'] += 1 self.o(`li['num']`+". ") self.start = 1 else: self.pbr() if tag in ["table", "tr"] and start: self.p() if tag == 'td': self.pbr() if tag == "pre": if start: self.startpre = 1 self.pre = 1 else: self.pre = 0 self.p() def pbr(self): if self.p_p == 0: self.p_p = 1 def p(self): self.p_p = 2 def o(self, data, puredata=0, force=0): if self.abbr_data is not None: self.abbr_data += data if not self.quiet: if puredata and not self.pre: data = re.sub('\s+', ' ', data) if data and data[0] == ' ': self.space = 1 data = data[1:] if not data and not force: return if self.startpre: #self.out(" :") #TODO: not output when already one there self.startpre = 0 bq = (">" * self.blockquote) if not (force and data and data[0] == ">") and self.blockquote: bq += " " if self.pre: bq += " " data = data.replace("\n", "\n"+bq) if self.start: self.space = 0 self.p_p = 0 self.start = 0 if force == 'end': # It's the end. self.p_p = 0 self.out("\n") self.space = 0 if self.p_p: self.out(('\n'+bq)*self.p_p) self.space = 0 if self.space: if not self.lastWasNL: self.out(' ') self.space = 0 if self.a and ((self.p_p == 2 and LINKS_EACH_PARAGRAPH) or force == "end"): if force == "end": self.out("\n") newa = [] for link in self.a: if self.outcount > link['outcount']: self.out(" ["+`link['count']`+"]: " + urlparse.urljoin(self.baseurl, link['href'])) if link.has_key('title'): self.out(" ("+link['title']+")") self.out("\n") else: newa.append(link) if self.a != newa: self.out("\n") # Don't need an extra line when nothing was done. self.a = newa if self.abbr_list and force == "end": for abbr, definition in self.abbr_list.items(): self.out(" *[" + abbr + "]: " + definition + "\n") self.p_p = 0 self.out(data) self.lastWasNL = data and data[-1] == '\n' self.outcount += 1 def handle_data(self, data): if r'\/script>' in data: self.quiet -= 1 self.o(data, 1) def unknown_decl(self, data): pass def wrapwrite(text): sys.stdout.write(text.encode('utf8')) def html2text_file(html, out=wrapwrite, baseurl=''): h = _html2text(out, baseurl) h.feed(html) h.feed("") return h.close() def html2text(html, baseurl=''): return optwrap(html2text_file(html, None, baseurl)) if __name__ == "__main__": baseurl = '' if sys.argv[1:]: arg = sys.argv[1] if arg.startswith('http://'): baseurl = arg j = urllib.urlopen(baseurl) try: from feedparser import _getCharacterEncoding as enc except ImportError: enc = lambda x, y: ('utf-8', 1) text = j.read() encoding = enc(j.headers, text)[0] if encoding == 'us-ascii': encoding = 'utf-8' data = text.decode(encoding) else: encoding = 'utf8' if len(sys.argv) > 2: encoding = sys.argv[2] f = open(arg, 'r') try: data = f.read().decode(encoding) finally: f.close() else: data = sys.stdin.read().decode('utf8') wrapwrite(html2text(data, baseurl)) # vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
DROP_TBL_PERFDATA = "DROP TABLE perfdata" CREATE_TBL_PERFDATA = """CREATE TABLE `perfdata` ( `pk` int(11) NOT NULL AUTO_INCREMENT, `id` varchar(16) NOT NULL, `fn` varchar(256) NOT NULL, `wt` int(11) NOT NULL, `ct` int(11) NOT NULL, `pmu` int(11) NOT NULL, `mu` int(11) NOT NULL, `cpu` int(11) NOT NULL, `rec_on` datetime NOT NULL, PRIMARY KEY (`pk`) )""" CREATE_TBL_PARENT_CHILD = """CREATE TABLE IF NOT EXISTS `parent_child` ( `id` int(11) NOT NULL AUTO_INCREMENT, `run` varchar(32) NOT NULL, `parent` varchar(128) DEFAULT NULL, `child` varchar(128) NOT NULL, `wt` int(11) NOT NULL, `pmu` int(11) NOT NULL, `mu` int(11) NOT NULL, `cpu` int(11) NOT NULL, `ct` int(11) NOT NULL, `rec_on` datetime NOT NULL, PRIMARY KEY (`id`) )"""; SELECT_DETAILS = "select id, get, post, cookie, perfdata, `timestamp` from details" INSERT_INTO_PC = """insert into parent_child (run, parent, child, ct, wt, cpu, mu, pmu, rec_on) values ( %(run_id)s, %(parent)s, %(child)s, %(callnum)s, %(walltime)s, %(proc)s, %(mem)s, %(peakmem)s, %(rec_on)s )""";
# -*- coding: utf-8 -*- # Copyright(C) 2012 Romain Bignon # # This file is part of weboob. # # weboob is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # weboob is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with weboob. If not, see <http://www.gnu.org/licenses/>. import datetime from decimal import Decimal import re from weboob.deprecated.browser import Page from weboob.capabilities.bank import Account from weboob.tools.capabilities.bank.transactions import FrenchTransaction class LoginPage(Page): def login(self, login, passwd): self.browser.select_form(name='frmLogin') self.browser['username'] = login.encode(self.browser.ENCODING) self.browser['password'] = passwd.encode(self.browser.ENCODING) self.browser.submit(nologin=True) def has_redirect(self): if len(self.document.getroot().xpath('//form')) > 0: return False else: return True class Login2Page(Page): def login(self, secret): label = self.document.xpath('//span[@class="PF_LABEL"]')[0].text.strip() letters = '' for n in re.findall('(\d+)', label): letters += secret[int(n) - 1] self.browser.select_form(name='frmControl') self.browser['word'] = letters self.browser.submit(name='valider', nologin=True) class IndexPage(Page): pass class AccountsPage(Page): ACCOUNT_TYPES = {u'Epargne': Account.TYPE_SAVINGS, u'Liquidits': Account.TYPE_CHECKING, u'Titres': Account.TYPE_MARKET, u'Prts': Account.TYPE_LOAN, } def get_list(self): accounts = [] for block in self.document.xpath('//div[@class="pave"]/div'): head_type = block.xpath('./div/span[@class="accGroupLabel"]')[0].text.strip() account_type = self.ACCOUNT_TYPES.get(head_type, Account.TYPE_UNKNOWN) for tr in block.cssselect('ul li.tbord_account'): id = tr.attrib.get('id', '') if id.find('contratId') != 0: self.logger.warning('Unable to parse contract ID: %r' % id) continue id = id[id.find('contratId')+len('contratId'):] link = tr.cssselect('span.accountLabel a')[0] balance = Decimal(FrenchTransaction.clean_amount(tr.cssselect('span.accountTotal')[0].text)) if id.endswith('CRT'): account = accounts[-1] account._card_links.append(link.attrib['href']) if not account.coming: account.coming = Decimal('0.0') account.coming += balance continue account = Account() account.id = id account.label = unicode(link.text.strip()) account.type = account_type account.balance = balance account.currency = account.get_currency(tr.cssselect('span.accountDev')[0].text) account._link = link.attrib['href'] account._card_links = [] accounts.append(account) if len(accounts) == 0: # Sometimes, accounts are only in javascript... for script in self.document.xpath('//script'): text = script.text if text is None: continue if 'remotePerso' not in text: continue account = None attribs = {} account_type = Account.TYPE_UNKNOWN for line in text.split('\n'): line = line.strip() m = re.match("data.libelle = '(.*)';", line) if m: account_type = self.ACCOUNT_TYPES.get(m.group(1), Account.TYPE_UNKNOWN) elif line == 'var remotePerso = new Object;': account = Account() elif account is not None: m = re.match("remotePerso.(\w+) = '?(.*?)'?;", line) if m: attribs[m.group(1)] = m.group(2) elif line.startswith('listProduitsGroup'): account.id = attribs['refContrat'] account.label = attribs['libelle'] account.type = account_type account.balance = Decimal(FrenchTransaction.clean_amount(attribs['soldeDateOpeValeurFormatted'])) account.currency = account.get_currency(attribs['codeDevise']) account._link = 'tbord.do?id=%s' % attribs['id'] account._card_links = [] if account.id.endswith('CRT'): a = accounts[-1] a._card_links.append(account._link) if not a.coming: a.coming = Decimal('0.0') a.coming += account.balance else: accounts.append(account) account = None return accounts class Transaction(FrenchTransaction): PATTERNS = [(re.compile('^RET DAB (?P<text>.*?) RETRAIT DU (?P<dd>\d{2})(?P<mm>\d{2})(?P<yy>\d{2}).*'), FrenchTransaction.TYPE_WITHDRAWAL), (re.compile('^RET DAB (?P<text>.*?) CARTE ?:.*'), FrenchTransaction.TYPE_WITHDRAWAL), (re.compile('^RET DAB (?P<dd>\d{2})/(?P<mm>\d{2})/(?P<yy>\d{2}) (?P<text>.*?) CARTE .*'), FrenchTransaction.TYPE_WITHDRAWAL), (re.compile('^(?P<text>.*) RETRAIT DU (?P<dd>\d{2})(?P<mm>\d{2})(?P<yy>\d{2}) .*'), FrenchTransaction.TYPE_WITHDRAWAL), (re.compile('(\w+) (?P<dd>\d{2})(?P<mm>\d{2})(?P<yy>\d{2}) CB[:\*][^ ]+ (?P<text>.*)'), FrenchTransaction.TYPE_CARD), (re.compile('^(?P<category>VIR(EMEN)?T? (SEPA)?(RECU|FAVEUR)?)( /FRM)?(?P<text>.*)'), FrenchTransaction.TYPE_TRANSFER), (re.compile('^PRLV (?P<text>.*) (REF \w+)?$'),FrenchTransaction.TYPE_ORDER), (re.compile('^CHEQUE.*? (REF \w+)?$'), FrenchTransaction.TYPE_CHECK), (re.compile('^(AGIOS /|FRAIS) (?P<text>.*)'), FrenchTransaction.TYPE_BANK), (re.compile('^(CONVENTION \d+ )?COTIS(ATION)? (?P<text>.*)'), FrenchTransaction.TYPE_BANK), (re.compile('^REMISE (?P<text>.*)'), FrenchTransaction.TYPE_DEPOSIT), (re.compile('^(?P<text>.*)( \d+)? QUITTANCE .*'), FrenchTransaction.TYPE_ORDER), (re.compile('^.* LE (?P<dd>\d{2})/(?P<mm>\d{2})/(?P<yy>\d{2})$'), FrenchTransaction.TYPE_UNKNOWN), ] class HistoryBasePage(Page): def get_history(self): self.logger.warning('Do not support account of type %s' % type(self).__name__) return iter([]) class TransactionsPage(HistoryBasePage): def get_history(self): for tr in self.document.xpath('//table[@id="operation"]/tbody/tr'): tds = tr.findall('td') if len(tds) < 5: continue t = Transaction(tds[-1].findall('img')[-1].attrib.get('id', '')) date = u''.join([txt.strip() for txt in tds[0].itertext()]) raw = u' '.join([txt.strip() for txt in tds[1].itertext()]) debit = u''.join([txt.strip() for txt in tds[-3].itertext()]) credit = u''.join([txt.strip() for txt in tds[-2].itertext()]) t.parse(date, re.sub(r'[ ]+', ' ', raw)) t.set_amount(credit, debit) t._coming = False if t.raw.startswith('ACHAT CARTE -DEBIT DIFFERE'): continue yield t class CardPage(HistoryBasePage): def get_history(self): debit_date = None coming = True for tr in self.document.xpath('//table[@class="report"]/tbody/tr'): tds = tr.findall('td') if len(tds) == 2: # headers m = re.match('.* (\d+)/(\d+)/(\d+)', tds[0].text.strip()) debit_date = datetime.date(int(m.group(3)), int(m.group(2)), int(m.group(1))) if debit_date < datetime.date.today(): coming = False if len(tds) != 3: continue t = Transaction(0) date = u''.join([txt.strip() for txt in tds[0].itertext()]) raw = u' '.join([txt.strip() for txt in tds[1].itertext()]) amount = u''.join([txt.strip() for txt in tds[-1].itertext()]) t.parse(date, re.sub(r'[ ]+', ' ', raw)) if debit_date is not None: t.date = debit_date t.label = unicode(tds[1].find('span').text.strip()) t.type = t.TYPE_CARD t._coming = coming t.set_amount(amount) yield t class ValuationPage(HistoryBasePage): pass class LoanPage(HistoryBasePage): pass class MarketPage(HistoryBasePage): pass class AssurancePage(HistoryBasePage): pass
# Copyright 2011 VMware, Inc # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import inspect import logging as std_logging import os import random from oslo.config import cfg from oslo.messaging import server as rpc_server from neutron.common import config from neutron.common import rpc as n_rpc from neutron import context from neutron.db import api as session from neutron import manager from neutron.openstack.common import excutils from neutron.openstack.common import importutils from neutron.openstack.common import log as logging from neutron.openstack.common import loopingcall from neutron.openstack.common import service as common_service from neutron import wsgi service_opts = [ cfg.IntOpt('periodic_interval', default=40, help=_('Seconds between running periodic tasks')), cfg.IntOpt('api_workers', default=0, help=_('Number of separate API worker processes for service')), cfg.IntOpt('rpc_workers', default=0, help=_('Number of RPC worker processes for service')), cfg.IntOpt('periodic_fuzzy_delay', default=5, help=_('Range of seconds to randomly delay when starting the ' 'periodic task scheduler to reduce stampeding. ' '(Disable by setting to 0)')), ] CONF = cfg.CONF CONF.register_opts(service_opts) LOG = logging.getLogger(__name__) class WsgiService(object): """Base class for WSGI based services. For each api you define, you must also define these flags: :<api>_listen: The address on which to listen :<api>_listen_port: The port on which to listen """ def __init__(self, app_name): self.app_name = app_name self.wsgi_app = None def start(self): self.wsgi_app = _run_wsgi(self.app_name) def wait(self): self.wsgi_app.wait() class NeutronApiService(WsgiService): """Class for neutron-api service.""" @classmethod def create(cls, app_name='neutron'): # Setup logging early, supplying both the CLI options and the # configuration mapping from the config file # We only update the conf dict for the verbose and debug # flags. Everything else must be set up in the conf file... # Log the options used when starting if we're in debug mode... config.setup_logging() # Dump the initial option values cfg.CONF.log_opt_values(LOG, std_logging.DEBUG) service = cls(app_name) return service def serve_wsgi(cls): try: service = cls.create() service.start() except Exception: with excutils.save_and_reraise_exception(): LOG.exception(_('Unrecoverable error: please check log ' 'for details.')) return service class RpcWorker(object): """Wraps a worker to be handled by ProcessLauncher""" def __init__(self, plugin): self._plugin = plugin self._servers = [] def start(self): # We may have just forked from parent process. A quick disposal of the # existing sql connections avoids producing errors later when they are # discovered to be broken. session.get_engine().pool.dispose() self._servers = self._plugin.start_rpc_listeners() def wait(self): for server in self._servers: if isinstance(server, rpc_server.MessageHandlingServer): server.wait() def stop(self): for server in self._servers: if isinstance(server, rpc_server.MessageHandlingServer): server.kill() self._servers = [] def serve_rpc(): plugin = manager.NeutronManager.get_plugin() # If 0 < rpc_workers then start_rpc_listeners would be called in a # subprocess and we cannot simply catch the NotImplementedError. It is # simpler to check this up front by testing whether the plugin supports # multiple RPC workers. if not plugin.rpc_workers_supported(): LOG.debug(_("Active plugin doesn't implement start_rpc_listeners")) if 0 < cfg.CONF.rpc_workers: msg = _("'rpc_workers = %d' ignored because start_rpc_listeners " "is not implemented.") LOG.error(msg, cfg.CONF.rpc_workers) raise NotImplementedError() try: rpc = RpcWorker(plugin) if cfg.CONF.rpc_workers < 1: rpc.start() return rpc else: launcher = common_service.ProcessLauncher(wait_interval=1.0) launcher.launch_service(rpc, workers=cfg.CONF.rpc_workers) return launcher except Exception: with excutils.save_and_reraise_exception(): LOG.exception(_('Unrecoverable error: please check log ' 'for details.')) def _run_wsgi(app_name): app = config.load_paste_app(app_name) if not app: LOG.error(_('No known API applications configured.')) return server = wsgi.Server("Neutron") server.start(app, cfg.CONF.bind_port, cfg.CONF.bind_host, workers=cfg.CONF.api_workers) # Dump all option values here after all options are parsed cfg.CONF.log_opt_values(LOG, std_logging.DEBUG) LOG.info(_("Neutron service started, listening on %(host)s:%(port)s"), {'host': cfg.CONF.bind_host, 'port': cfg.CONF.bind_port}) return server class Service(n_rpc.Service): """Service object for binaries running on hosts. A service takes a manager and enables rpc by listening to queues based on topic. It also periodically runs tasks on the manager. """ def __init__(self, host, binary, topic, manager, report_interval=None, periodic_interval=None, periodic_fuzzy_delay=None, *args, **kwargs): self.binary = binary self.manager_class_name = manager manager_class = importutils.import_class(self.manager_class_name) self.manager = manager_class(host=host, *args, **kwargs) self.report_interval = report_interval self.periodic_interval = periodic_interval self.periodic_fuzzy_delay = periodic_fuzzy_delay self.saved_args, self.saved_kwargs = args, kwargs self.timers = [] super(Service, self).__init__(host, topic, manager=self.manager) def start(self): self.manager.init_host() super(Service, self).start() if self.report_interval: pulse = loopingcall.FixedIntervalLoopingCall(self.report_state) pulse.start(interval=self.report_interval, initial_delay=self.report_interval) self.timers.append(pulse) if self.periodic_interval: if self.periodic_fuzzy_delay: initial_delay = random.randint(0, self.periodic_fuzzy_delay) else: initial_delay = None periodic = loopingcall.FixedIntervalLoopingCall( self.periodic_tasks) periodic.start(interval=self.periodic_interval, initial_delay=initial_delay) self.timers.append(periodic) self.manager.after_start() def __getattr__(self, key): manager = self.__dict__.get('manager', None) return getattr(manager, key) @classmethod def create(cls, host=None, binary=None, topic=None, manager=None, report_interval=None, periodic_interval=None, periodic_fuzzy_delay=None): """Instantiates class and passes back application object. :param host: defaults to CONF.host :param binary: defaults to basename of executable :param topic: defaults to bin_name - 'nova-' part :param manager: defaults to CONF.<topic>_manager :param report_interval: defaults to CONF.report_interval :param periodic_interval: defaults to CONF.periodic_interval :param periodic_fuzzy_delay: defaults to CONF.periodic_fuzzy_delay """ if not host: host = CONF.host if not binary: binary = os.path.basename(inspect.stack()[-1][1]) if not topic: topic = binary.rpartition('neutron-')[2] topic = topic.replace("-", "_") if not manager: manager = CONF.get('%s_manager' % topic, None) if report_interval is None: report_interval = CONF.report_interval if periodic_interval is None: periodic_interval = CONF.periodic_interval if periodic_fuzzy_delay is None: periodic_fuzzy_delay = CONF.periodic_fuzzy_delay service_obj = cls(host, binary, topic, manager, report_interval=report_interval, periodic_interval=periodic_interval, periodic_fuzzy_delay=periodic_fuzzy_delay) return service_obj def kill(self): """Destroy the service object.""" self.stop() def stop(self): super(Service, self).stop() for x in self.timers: try: x.stop() except Exception: LOG.exception(_("Exception occurs when timer stops")) pass self.timers = [] def wait(self): super(Service, self).wait() for x in self.timers: try: x.wait() except Exception: LOG.exception(_("Exception occurs when waiting for timer")) pass def periodic_tasks(self, raise_on_error=False): """Tasks to be run at a periodic interval.""" ctxt = context.get_admin_context() self.manager.periodic_tasks(ctxt, raise_on_error=raise_on_error) def report_state(self): """Update the state of this service.""" # Todo(gongysh) report state to neutron server pass
''' This is a dummy file for me to get started making an Ising model. I'll get this 2-D Ising running, then generalize. ''' import argparse from itertools import izip import numpy as np from matplotlib import pyplot as plt import seaborn as sns sns.set() def run_ising(N, d, K, J,h, n_steps, plot = False): ''' :param N: :param d: :param K: :param J: :param h: :param n_steps: :param plot: :return: ''' if plot: try: assert d <= 2 except AssertionError: raise AssertionError("Can only plot in one or two dimensions.") #TODO wrap these better assert N >0 and N < 1000 assert d > 0 assert n_steps > 0 np.random.seed(0) size = tuple(N for i in xrange(d)) lattice = np.ones(size) #make a random initial state lattice-= np.random.randint(0,2, size =size)*2 # do different initialization E_0 = energy(lattice, potential, K, h) if plot: plt.ion() for step in xrange(n_steps): if step%1000 == 0: print step site = tuple(np.random.randint(0, N, size=d)) # consider flipping this site lattice[site] *= -1 E_f = energy(lattice, potential, K, h) # if E_F < E_0, keep # if E_F > E_0, keep randomly given change of energies if E_f >= E_0: keep = np.random.uniform() < np.exp(K / J * (E_0 - E_f)) else: keep = True if keep: E_0 = E_f else: lattice[site] *= -1 # fig = plt.figure() if plot and step % 100 == 0: if d == 1: plt.imshow(lattice.reshape((1, -1)),interpolation='none') else: plt.imshow(lattice, interpolation='none') plt.title(correlation(lattice, N/2)) plt.pause(0.01) plt.clf() return np.array([correlation(lattice, r) for r in xrange(1, N/2+1)]) def get_NN(site, N, d, r= 1): ''' The NN of the site. Will only return those UP in index (east, south, and down) to avoid double counting. Accounts for PBC :param site: (d,) array of coordinates in the lattice :param N: Size of one side of the lattice :param d: dimension of the lattice :return: dxd numpy array where each row corresponds to the nearest neighbors. ''' mult_sites = np.r_[ [site for i in xrange(d)]] adjustment = np.eye(d)*r return ((mult_sites+adjustment)%N).astype(int) def potential(s1, s2, K, h): ''' Basic Ising potential :param s1: First spin (-1 or 1) :param s2: Second spin :param K: Coupling constant :return: Energy of this particular bond ''' return -1*K*s1*s2 - h/2*(s1+s2)#should this be abstracted to call the NN function? def energy(lattice, potential, K, h = 0): ''' Calculate the energy of a lattice :param lattice: Lattice to calculate the energy on :param potential: Function defining the potential of a given site. :return: Energy of the lattice ''' N = lattice.shape[0] d = len(lattice.shape) dim_slices = np.meshgrid(*(xrange(N) for i in xrange(d)), indexing = 'ij') all_sites = izip(*[slice.flatten() for slice in dim_slices]) E = 0 for site in all_sites: nn = get_NN(site, N, d) for neighbor in nn: E+=potential(lattice[site], lattice[tuple(neighbor)],K = K, h = h) return E def magnetization(lattice): return lattice.mean() def correlation(lattice, r): ''' The average spin correlation at distance r. :param lattice: The lattice to calculate the statistic on. :param r: Distance to measure correlation :return: ''' N = lattice.shape[0] d = len(lattice.shape) dim_slices = np.meshgrid(*(xrange(N) for i in xrange(d)), indexing='ij') all_sites = izip(*[slice.flatten() for slice in dim_slices]) xi = 0 for site in all_sites: nn = get_NN(site, N, d, r) for neighbor in nn: xi += lattice[site]*lattice[tuple(neighbor)] return xi/((N**d)*d) if __name__ == '__main__': parser = argparse.ArgumentParser(description='Simulate an ising model') parser.add_argument('N', type = int, help = 'Length of one side of the cube.') parser.add_argument('d', type = int, help = 'Number of dimensions of the cube.') #parser.add_argument('K', type = float, help ='Bond coupling strength.') parser.add_argument('J', type = float, default = 1.0, nargs = '?',\ help = 'Energy of bond strength. Optional, default is 1.') parser.add_argument('h', type = float, default=0.0, nargs = '?',\ help = 'Magnetic field strength. Optional, default is 0.') parser.add_argument('n_steps', type = int, default = 1000, nargs = '?',\ help = 'Number of steps to simulate. Default is 1e5') parser.add_argument('--plot', action = 'store_true',\ help = 'Whether or not to plot results. Only allowed with d = 1 or 2.') args = parser.parse_args() spins = [] Ks = [ 0.5,0.6,0.65, 0.7,0.8, 0.9] for K in Ks: print K spins.append(run_ising(K = K, **vars(args))) for K, spin in izip(Ks, spins): plt.plot(spin, label = K ) plt.legend(loc = 'best') plt.ylim([-0.1, 1.1]) plt.show()
""" HTMLParser-based link extractor """ from HTMLParser import HTMLParser from urlparse import urljoin from w3lib.url import safe_url_string from scrapy.link import Link from scrapy.utils.python import unique as unique_list class HtmlParserLinkExtractor(HTMLParser): def __init__(self, tag="a", attr="href", process=None, unique=False): HTMLParser.__init__(self) self.scan_tag = tag if callable(tag) else lambda t: t == tag self.scan_attr = attr if callable(attr) else lambda a: a == attr self.process_attr = process if callable(process) else lambda v: v self.unique = unique def _extract_links(self, response_text, response_url, response_encoding): self.reset() self.feed(response_text) self.close() links = unique_list(self.links, key=lambda link: link.url) if self.unique else self.links ret = [] base_url = urljoin(response_url, self.base_url) if self.base_url else response_url for link in links: if isinstance(link.url, unicode): link.url = link.url.encode(response_encoding) link.url = urljoin(base_url, link.url) link.url = safe_url_string(link.url, response_encoding) link.text = link.text.decode(response_encoding) ret.append(link) return ret def extract_links(self, response): # wrapper needed to allow to work directly with text return self._extract_links(response.body, response.url, response.encoding) def reset(self): HTMLParser.reset(self) self.base_url = None self.current_link = None self.links = [] def handle_starttag(self, tag, attrs): if tag == 'base': self.base_url = dict(attrs).get('href') if self.scan_tag(tag): for attr, value in attrs: if self.scan_attr(attr): url = self.process_attr(value) link = Link(url=url) self.links.append(link) self.current_link = link def handle_endtag(self, tag): if self.scan_tag(tag): self.current_link = None def handle_data(self, data): if self.current_link: self.current_link.text = self.current_link.text + data def matches(self, url): """This extractor matches with any url, since it doesn't contain any patterns""" return True
# # soar # io.py - object-oriented interface to the robot # # This io file makes use of the "official" soar interface # (sonarDistances, etc), and it is still ugly, since it relies on having # a handle on the brain environment, but it is arguably neater than # the io.py file. However it seems to introduce some kind of lag that # makes the really complicated labs with localization stuff work poorly import soar.util from soar.util import * robotRadius = 0.2 def configure_io(namespace): # need to use global 'cause we don't want to accidentally overwrite # the brain environ by setting it to None when io.py is imported global io_environ io_environ = namespace class SensorInput(): global io_environ """ Represents one set of sensor readings from the robot, incluing sonars, odometry, and readings from the analogInputs """ def __init__(self, cheat=False): self.sonars = io_environ['sonarDistances']() if cheat: p = io_environ['cheatPose']() else: p = io_environ['pose']() self.odometry = valueListToPose(p) self.analogInputs = io_environ['analogInputs']() def __str__(self): return 'Sonar: ' + util.prettyString(self.sonars) + \ "; Odo: " + util.prettyString(self.odometry) +\ "; Analog: " + util.prettyString(self.analogInputs) referenceVoltage = 5.0 class Action: """ One set of commands to send to the robot """ def __init__(self, fvel = 0.0, rvel = 0.0, voltage = referenceVoltage, discreteStepLength = None): """ @param fvel: signed number indicating forward velocity in m/s @param rvel: signed number indicating rotational velocity in rad/sec (?) positive is left, negative is right @param voltage: voltage to send to analog input port of control board; should be between 0 and 10v ?? @param discreteStepLength: if C{None}, then the robot continues driving at the last commanded velocity until a new action command is received; if set to a positive value, the robot will drive at the last commanded velocity until C{discreteStepLength} seconds have passed, and then stop. Setting the step length to, e.g., 0.1, is useful when the brain is doing so much computation that the robot drives too far between steps. """ self.fvel = fvel self.rvel = rvel self.voltage = voltage self.discreteStepLength = discreteStepLength def execute(self): if self.discreteStepLength: io_environ['discreteMotorOutput'](self.fvel, self.rvel, self.discreteStepLength) else: io_environ['motorOutput'](self.fvel, self.rvel) io_environ['analogOutput'](self.voltage) def __str__(self): return 'Act: ' + \ util.prettyString([self.fvel, self.rvel, self.voltage]) def registerUserFunction(type, f): io_environ['registerUserFunction'](type, f)
#!/usr/bin/env python # Copyright 2015 The Kubernetes Authors. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. import os from charms import layer from charms.reactive import hook from charms.reactive import is_state from charms.reactive import remove_state from charms.reactive import set_state from charms.reactive import when from charms.reactive import when_not from charmhelpers.core import hookenv from shlex import split from subprocess import call from subprocess import check_call from subprocess import check_output @hook('upgrade-charm') def reset_delivery_states(): ''' Remove the state set when resources are unpacked. ''' remove_state('kubernetes-e2e.installed') @when('kubernetes-e2e.installed') def messaging(): ''' Probe our relations to determine the propper messaging to the end user ''' missing_services = [] if not is_state('kubernetes-master.available'): missing_services.append('kubernetes-master') if not is_state('certificates.available'): missing_services.append('certificates') if missing_services: if len(missing_services) > 1: subject = 'relations' else: subject = 'relation' services = ','.join(missing_services) message = 'Missing {0}: {1}'.format(subject, services) hookenv.status_set('blocked', message) return hookenv.status_set('active', 'Ready to test.') @when_not('kubernetes-e2e.installed') def install_kubernetes_e2e(): ''' Deliver the e2e and kubectl components from the binary resource stream packages declared in the charm ''' charm_dir = os.getenv('CHARM_DIR') arch = determine_arch() # Get the resource via resource_get resource = 'e2e_{}'.format(arch) try: archive = hookenv.resource_get(resource) except Exception: message = 'Error fetching the {} resource.'.format(resource) hookenv.log(message) hookenv.status_set('blocked', message) return if not archive: hookenv.log('Missing {} resource.'.format(resource)) hookenv.status_set('blocked', 'Missing {} resource.'.format(resource)) return # Handle null resource publication, we check if filesize < 1mb filesize = os.stat(archive).st_size if filesize < 1000000: hookenv.status_set('blocked', 'Incomplete {} resource.'.format(resource)) return hookenv.status_set('maintenance', 'Unpacking {} resource.'.format(resource)) unpack_path = '{}/files/kubernetes'.format(charm_dir) os.makedirs(unpack_path, exist_ok=True) cmd = ['tar', 'xfvz', archive, '-C', unpack_path] hookenv.log(cmd) check_call(cmd) services = ['e2e.test', 'ginkgo', 'kubectl'] for service in services: unpacked = '{}/{}'.format(unpack_path, service) app_path = '/usr/local/bin/{}'.format(service) install = ['install', '-v', unpacked, app_path] call(install) set_state('kubernetes-e2e.installed') @when('tls_client.ca.saved', 'tls_client.client.certificate.saved', 'tls_client.client.key.saved', 'kubernetes-master.available', 'kubernetes-e2e.installed') @when_not('kubeconfig.ready') def prepare_kubeconfig_certificates(master): ''' Prepare the data to feed to create the kubeconfig file. ''' layer_options = layer.options('tls-client') # Get all the paths to the tls information required for kubeconfig. ca = layer_options.get('ca_certificate_path') key = layer_options.get('client_key_path') cert = layer_options.get('client_certificate_path') servers = get_kube_api_servers(master) # pedantry kubeconfig_path = '/home/ubuntu/.kube/config' # Create kubernetes configuration in the default location for ubuntu. create_kubeconfig('/root/.kube/config', servers[0], ca, key, cert, user='root') create_kubeconfig(kubeconfig_path, servers[0], ca, key, cert, user='ubuntu') # Set permissions on the ubuntu users kubeconfig to ensure a consistent UX cmd = ['chown', 'ubuntu:ubuntu', kubeconfig_path] check_call(cmd) set_state('kubeconfig.ready') @when('kubernetes-e2e.installed', 'kubeconfig.ready') def set_app_version(): ''' Declare the application version to juju ''' cmd = ['kubectl', 'version', '--client'] from subprocess import CalledProcessError try: version = check_output(cmd).decode('utf-8') except CalledProcessError: message = "Missing kubeconfig causes errors. Skipping version set." hookenv.log(message) return git_version = version.split('GitVersion:"v')[-1] version_from = git_version.split('",')[0] hookenv.application_version_set(version_from.rstrip()) def create_kubeconfig(kubeconfig, server, ca, key, certificate, user='ubuntu', context='juju-context', cluster='juju-cluster'): '''Create a configuration for Kubernetes based on path using the supplied arguments for values of the Kubernetes server, CA, key, certificate, user context and cluster.''' # Create the config file with the address of the master server. cmd = 'kubectl config --kubeconfig={0} set-cluster {1} ' \ '--server={2} --certificate-authority={3} --embed-certs=true' check_call(split(cmd.format(kubeconfig, cluster, server, ca))) # Create the credentials using the client flags. cmd = 'kubectl config --kubeconfig={0} set-credentials {1} ' \ '--client-key={2} --client-certificate={3} --embed-certs=true' check_call(split(cmd.format(kubeconfig, user, key, certificate))) # Create a default context with the cluster. cmd = 'kubectl config --kubeconfig={0} set-context {1} ' \ '--cluster={2} --user={3}' check_call(split(cmd.format(kubeconfig, context, cluster, user))) # Make the config use this new context. cmd = 'kubectl config --kubeconfig={0} use-context {1}' check_call(split(cmd.format(kubeconfig, context))) def get_kube_api_servers(master): '''Return the kubernetes api server address and port for this relationship.''' hosts = [] # Iterate over every service from the relation object. for service in master.services(): for unit in service['hosts']: hosts.append('https://{0}:{1}'.format(unit['hostname'], unit['port'])) return hosts def determine_arch(): ''' dpkg wrapper to surface the architecture we are tied to''' cmd = ['dpkg', '--print-architecture'] output = check_output(cmd).decode('utf-8') return output.rstrip()
#!/usr/bin/env python2.7 from __future__ import absolute_import, unicode_literals, print_function, division from sys import argv from os import environ, stat, remove as _delete_file from os.path import isfile, dirname, basename, abspath from hashlib import sha256 from subprocess import check_call as run from boto.s3.connection import S3Connection from boto.s3.key import Key from boto.exception import S3ResponseError NEED_TO_UPLOAD_MARKER = '.need-to-upload' BYTES_PER_MB = 1024 * 1024 try: BUCKET_NAME = environ['TWBS_S3_BUCKET'] except KeyError: raise SystemExit("TWBS_S3_BUCKET environment variable not set!") def _sha256_of_file(filename): hasher = sha256() with open(filename, 'rb') as input_file: hasher.update(input_file.read()) file_hash = hasher.hexdigest() print('sha256({}) = {}'.format(filename, file_hash)) return file_hash def _delete_file_quietly(filename): try: _delete_file(filename) except (OSError, IOError): pass def _tarball_size(directory): kib = stat(_tarball_filename_for(directory)).st_size // BYTES_PER_MB return "{} MiB".format(kib) def _tarball_filename_for(directory): return abspath('./{}.tar.gz'.format(basename(directory))) def _create_tarball(directory): print("Creating tarball of {}...".format(directory)) run(['tar', '-czf', _tarball_filename_for(directory), '-C', dirname(directory), basename(directory)]) def _extract_tarball(directory): print("Extracting tarball of {}...".format(directory)) run(['tar', '-xzf', _tarball_filename_for(directory), '-C', dirname(directory)]) def download(directory): _delete_file_quietly(NEED_TO_UPLOAD_MARKER) try: print("Downloading {} tarball from S3...".format(friendly_name)) key.get_contents_to_filename(_tarball_filename_for(directory)) except S3ResponseError as err: open(NEED_TO_UPLOAD_MARKER, 'a').close() print(err) raise SystemExit("Cached {} download failed!".format(friendly_name)) print("Downloaded {}.".format(_tarball_size(directory))) _extract_tarball(directory) print("{} successfully installed from cache.".format(friendly_name)) def upload(directory): _create_tarball(directory) print("Uploading {} tarball to S3... ({})".format(friendly_name, _tarball_size(directory))) key.set_contents_from_filename(_tarball_filename_for(directory)) print("{} cache successfully updated.".format(friendly_name)) _delete_file_quietly(NEED_TO_UPLOAD_MARKER) if __name__ == '__main__': # Uses environment variables: # AWS_ACCESS_KEY_ID -- AWS Access Key ID # AWS_SECRET_ACCESS_KEY -- AWS Secret Access Key argv.pop(0) if len(argv) != 4: raise SystemExit("USAGE: s3_cache.py <download | upload> <friendly name> <dependencies file> <directory>") mode, friendly_name, dependencies_file, directory = argv conn = S3Connection() bucket = conn.lookup(BUCKET_NAME, validate=False) if bucket is None: raise SystemExit("Could not access bucket!") dependencies_file_hash = _sha256_of_file(dependencies_file) key = Key(bucket, dependencies_file_hash) key.storage_class = 'REDUCED_REDUNDANCY' if mode == 'download': download(directory) elif mode == 'upload': if isfile(NEED_TO_UPLOAD_MARKER): # FIXME upload(directory) else: print("No need to upload anything.") else: raise SystemExit("Unrecognized mode {!r}".format(mode))
#========================================================================== # imViewer-Simple.py # # An example program that opens uncompressed DICOM images and # converts them via numPy and PIL to be viewed in wxWidgets GUI # apps. The conversion is currently: # # pydicom->NumPy->PIL->wxPython.Image->wxPython.Bitmap # # Gruesome but it mostly works. Surely there is at least one # of these steps that could be eliminated (probably PIL) but # haven't tried that yet and I may want some of the PIL manipulation # functions. # # This won't handle RLE, embedded JPEG-Lossy, JPEG-lossless, # JPEG2000, old ACR/NEMA files, or anything wierd. Also doesn't # handle some RGB images that I tried. # # Have added Adit Panchal's LUT code. It helps a lot, but needs # to be further generalized. Added test for window and/or level # as 'list' type - crude, but it worked for a bunch of old MR and # CT slices I have. # # Testing: minimal # Tried only on WinXP sp2 using numpy 1.3.0 # and PIL 1.1.7b1, Python 2.6.4, and wxPython 2.8.10.1 # # Dave Witten: Nov. 11, 2009 #========================================================================== import os import os.path import sys import dicom import wx have_PIL = True try: import PIL.Image except: have_PIL = False have_numpy = True try: import numpy as np except: have_numpy = False #---------------------------------------------------------------- # Initialize image capabilities. #---------------------------------------------------------------- wx.InitAllImageHandlers() def MsgDlg(window, string, caption='OFAImage', style=wx.YES_NO | wx.CANCEL): """Common MessageDialog.""" dlg = wx.MessageDialog(window, string, caption, style) result = dlg.ShowModal() dlg.Destroy() return result class ImFrame(wx.Frame): """Class for main window.""" def __init__(self, parent, title): """Create the pydicom image example's main frame window.""" wx.Frame.__init__(self, parent, id=-1, title="", pos=wx.DefaultPosition, size=wx.Size(w=1024, h=768), style=wx.DEFAULT_FRAME_STYLE | wx.SUNKEN_BORDER | wx.CLIP_CHILDREN) #-------------------------------------------------------- # Set up the menubar. #-------------------------------------------------------- self.mainmenu = wx.MenuBar() # Make the 'File' menu. menu = wx.Menu() item = menu.Append(wx.ID_ANY, '&Open', 'Open file for editing') self.Bind(wx.EVT_MENU, self.OnFileOpen, item) item = menu.Append(wx.ID_ANY, 'E&xit', 'Exit Program') self.Bind(wx.EVT_MENU, self.OnFileExit, item) self.mainmenu.Append(menu, '&File') # Attach the menu bar to the window. self.SetMenuBar(self.mainmenu) #-------------------------------------------------------- # Set up the main splitter window. #-------------------------------------------------------- self.mainSplitter = wx.SplitterWindow(self, style=wx.NO_3D | wx.SP_3D) self.mainSplitter.SetMinimumPaneSize(1) #------------------------------------------------------------- # Create the folderTreeView on the left. #------------------------------------------------------------- self.dsTreeView = wx.TreeCtrl(self.mainSplitter, style=wx.TR_LINES_AT_ROOT | wx.TR_HAS_BUTTONS) #-------------------------------------------------------- # Create the ImageView on the right pane. #-------------------------------------------------------- self.imView = wx.Panel(self.mainSplitter, style=wx.VSCROLL | wx.HSCROLL | wx.CLIP_CHILDREN) self.imView.Bind(wx.EVT_PAINT, self.OnPaint) self.imView.Bind(wx.EVT_ERASE_BACKGROUND, self.OnEraseBackground) self.imView.Bind(wx.EVT_SIZE, self.OnSize) #-------------------------------------------------------- # Install the splitter panes. #-------------------------------------------------------- self.mainSplitter.SplitVertically(self.dsTreeView, self.imView) self.mainSplitter.SetSashPosition(300, True) #-------------------------------------------------------- # Initialize some values #-------------------------------------------------------- self.dcmdsRoot = False self.foldersRoot = False self.loadCentered = True self.bitmap = None self.Show(True) def OnFileExit(self, event): """Exits the program.""" self.Destroy() event.Skip() def OnSize(self, event): "Window 'size' event." self.Refresh() def OnEraseBackground(self, event): "Window 'erase background' event." pass def populateTree(self, ds): """ Populate the tree in the left window with the [desired] dataset values""" if not self.dcmdsRoot: self.dcmdsRoot = self.dsTreeView.AddRoot(text="DICOM Objects") else: self.dsTreeView.DeleteChildren(self.dcmdsRoot) self.recurse_tree(ds, self.dcmdsRoot) self.dsTreeView.ExpandAll() def recurse_tree(self, ds, parent, hide=False): """ order the dicom tags """ for data_element in ds: if isinstance(data_element.value, unicode): ip = self.dsTreeView.AppendItem(parent, text=unicode(data_element)) else: ip = self.dsTreeView.AppendItem(parent, text=str(data_element)) if data_element.VR == "SQ": for i, ds in enumerate(data_element.value): sq_item_description = data_element.name.replace(" Sequence", "") item_text = "%s %d" % (sq_item_description, i + 1) parentNodeID = self.dsTreeView.AppendItem(ip, text=item_text.rjust(128)) self.recurse_tree(ds, parentNodeID) ## --- Most of what is important happens below this line --------------------- def OnFileOpen(self, event): """Opens a selected file.""" dlg = wx.FileDialog(self, 'Choose a file to add.', '', '', '*.*', wx.OPEN) if dlg.ShowModal() == wx.ID_OK: fullPath = dlg.GetPath() imageFile = dlg.GetFilename() #checkDICMHeader() self.show_file(imageFile, fullPath) def OnPaint(self, event): "Window 'paint' event." dc = wx.PaintDC(self.imView) dc = wx.BufferedDC(dc) # paint a background just so it isn't *so* boring. dc.SetBackground(wx.Brush("WHITE")) dc.Clear() dc.SetBrush(wx.Brush("GREY", wx.CROSSDIAG_HATCH)) windowsize = self.imView.GetSizeTuple() dc.DrawRectangle(0, 0, windowsize[0], windowsize[1]) bmpX0 = 0 bmpY0 = 0 if self.bitmap is not None: if self.loadCentered: bmpX0 = (windowsize[0] - self.bitmap.Width) / 2 bmpY0 = (windowsize[1] - self.bitmap.Height) / 2 dc.DrawBitmap(self.bitmap, bmpX0, bmpY0, False) #------------------------------------------------------------ # ImFrame.ConvertWXToPIL() # Expropriated from Andrea Gavana's # ShapedButton.py in the wxPython dist #------------------------------------------------------------ def ConvertWXToPIL(self, bmp): """ Convert wx.Image Into PIL Image. """ width = bmp.GetWidth() height = bmp.GetHeight() im = wx.EmptyImage(width, height) im.fromarray("RGBA", (width, height), bmp.GetData()) return img #------------------------------------------------------------ # ImFrame.ConvertPILToWX() # Expropriated from Andrea Gavana's # ShapedButton.py in the wxPython dist #------------------------------------------------------------ def ConvertPILToWX(self, pil, alpha=True): """ Convert PIL Image Into wx.Image. """ if alpha: image = apply(wx.EmptyImage, pil.size) image.SetData(pil.convert("RGB").tostring()) image.SetAlphaData(pil.convert("RGBA").tostring()[3::4]) else: image = wx.EmptyImage(pil.size[0], pil.size[1]) new_image = pil.convert('RGB') data = new_image.tostring() image.SetData(data) return image def get_LUT_value(self, data, window, level): """Apply the RGB Look-Up Table for the given data and window/level value.""" if not have_numpy: raise ImportError("Numpy is not available. See http://numpy.scipy.org/ to download and install") if isinstance(window, list): window = window[0] if isinstance(level, list): level = level[0] return np.piecewise(data, [data <= (level - 0.5 - (window - 1) / 2), data > (level - 0.5 + (window - 1) / 2)], [0, 255, lambda data: ((data - (level - 0.5)) / (window - 1) + 0.5) * (255 - 0)] ) #----------------------------------------------------------- # ImFrame.loadPIL_LUT(dataset) # Display an image using the Python Imaging Library (PIL) #----------------------------------------------------------- def loadPIL_LUT(self, dataset): if not have_PIL: raise ImportError("Python Imaging Library is not available. See http://www.pythonware.com/products/pil/ to download and install") if('PixelData' not in dataset): raise TypeError("Cannot show image -- DICOM dataset does not have pixel data") if('WindowWidth' not in dataset) or ('WindowCenter' not in dataset): # can only apply LUT if these values exist bits = dataset.BitsAllocated samples = dataset.SamplesPerPixel if bits == 8 and samples == 1: mode = "L" elif bits == 8 and samples == 3: mode = "RGB" elif bits == 16: # not sure about this -- PIL source says is 'experimental' and no documentation. mode = "I;16" # Also, should bytes swap depending on endian of file and system?? else: raise TypeError("Don't know PIL mode for %d BitsAllocated and %d SamplesPerPixel" % (bits, samples)) size = (dataset.Columns, dataset.Rows) im = PIL.Image.frombuffer(mode, size, dataset.PixelData, "raw", mode, 0, 1) # Recommended to specify all details by http://www.pythonware.com/library/pil/handbook/image.htm else: image = self.get_LUT_value(dataset.pixel_array, dataset.WindowWidth, dataset.WindowCenter) im = PIL.Image.fromarray(image).convert('L') # Convert mode to L since LUT has only 256 values: http://www.pythonware.com/library/pil/handbook/image.htm return im def show_file(self, imageFile, fullPath): """ Load the DICOM file, make sure it contains at least one image, and set it up for display by OnPaint(). ** be careful not to pass a unicode string to read_file or it will give you 'fp object does not have a defer_size attribute, or some such.""" ds = dicom.read_file(str(fullPath)) ds.decode() # change strings to unicode self.populateTree(ds) if 'PixelData' in ds: self.dImage = self.loadPIL_LUT(ds) if self.dImage is not None: tmpImage = self.ConvertPILToWX(self.dImage, False) self.bitmap = wx.BitmapFromImage(tmpImage) self.Refresh() ##------ This is just the initialization of the App ------------------------- #======================================================= # The main App Class. #======================================================= class App(wx.App): """Image Application.""" def OnInit(self): """Create the Image Application.""" frame = ImFrame(None, 'wxImage Example') return True #--------------------------------------------------------------------- # If this file is running as main or a standalone test, begin execution here. #--------------------------------------------------------------------- if __name__ == '__main__': app = App(0) app.MainLoop()
from pyasn1.type import constraint, error from pyasn1.error import PyAsn1Error from sys import version_info if version_info[0:2] < (2, 7) or \ version_info[0:2] in ( (3, 0), (3, 1) ): try: import unittest2 as unittest except ImportError: import unittest else: import unittest class SingleValueConstraintTestCase(unittest.TestCase): def setUp(self): self.c1 = constraint.SingleValueConstraint(1,2) self.c2 = constraint.SingleValueConstraint(3,4) def testCmp(self): assert self.c1 == self.c1, 'comparation fails' def testHash(self): assert hash(self.c1) != hash(self.c2), 'hash() fails' def testGoodVal(self): try: self.c1(1) except error.ValueConstraintError: assert 0, 'constraint check fails' def testBadVal(self): try: self.c1(4) except error.ValueConstraintError: pass else: assert 0, 'constraint check fails' class ContainedSubtypeConstraintTestCase(unittest.TestCase): def setUp(self): self.c1 = constraint.ContainedSubtypeConstraint( constraint.SingleValueConstraint(12) ) def testGoodVal(self): try: self.c1(12) except error.ValueConstraintError: assert 0, 'constraint check fails' def testBadVal(self): try: self.c1(4) except error.ValueConstraintError: pass else: assert 0, 'constraint check fails' class ValueRangeConstraintTestCase(unittest.TestCase): def setUp(self): self.c1 = constraint.ValueRangeConstraint(1,4) def testGoodVal(self): try: self.c1(1) except error.ValueConstraintError: assert 0, 'constraint check fails' def testBadVal(self): try: self.c1(-5) except error.ValueConstraintError: pass else: assert 0, 'constraint check fails' class ValueSizeConstraintTestCase(unittest.TestCase): def setUp(self): self.c1 = constraint.ValueSizeConstraint(1,2) def testGoodVal(self): try: self.c1('a') except error.ValueConstraintError: assert 0, 'constraint check fails' def testBadVal(self): try: self.c1('abc') except error.ValueConstraintError: pass else: assert 0, 'constraint check fails' class PermittedAlphabetConstraintTestCase(SingleValueConstraintTestCase): def setUp(self): self.c1 = constraint.PermittedAlphabetConstraint('A', 'B', 'C') self.c2 = constraint.PermittedAlphabetConstraint('DEF') def testGoodVal(self): try: self.c1('A') except error.ValueConstraintError: assert 0, 'constraint check fails' def testBadVal(self): try: self.c1('E') except error.ValueConstraintError: pass else: assert 0, 'constraint check fails' class ConstraintsIntersectionTestCase(unittest.TestCase): def setUp(self): self.c1 = constraint.ConstraintsIntersection( constraint.SingleValueConstraint(4), constraint.ValueRangeConstraint(2, 4) ) def testCmp1(self): assert constraint.SingleValueConstraint(4) in self.c1, '__cmp__() fails' def testCmp2(self): assert constraint.SingleValueConstraint(5) not in self.c1, \ '__cmp__() fails' def testCmp3(self): c = constraint.ConstraintsUnion(constraint.ConstraintsIntersection( constraint.SingleValueConstraint(4), constraint.ValueRangeConstraint(2, 4) )) assert self.c1 in c, '__cmp__() fails' def testCmp4(self): c = constraint.ConstraintsUnion( constraint.ConstraintsIntersection(constraint.SingleValueConstraint(5)) ) assert self.c1 not in c, '__cmp__() fails' def testGoodVal(self): try: self.c1(4) except error.ValueConstraintError: assert 0, 'constraint check fails' def testBadVal(self): try: self.c1(-5) except error.ValueConstraintError: pass else: assert 0, 'constraint check fails' class InnerTypeConstraintTestCase(unittest.TestCase): def testConst1(self): c = constraint.InnerTypeConstraint( constraint.SingleValueConstraint(4) ) try: c(4, 32) except error.ValueConstraintError: assert 0, 'constraint check fails' try: c(5, 32) except error.ValueConstraintError: pass else: assert 0, 'constraint check fails' def testConst2(self): c = constraint.InnerTypeConstraint( (0, constraint.SingleValueConstraint(4), 'PRESENT'), (1, constraint.SingleValueConstraint(4), 'ABSENT') ) try: c(4, 0) except error.ValueConstraintError: raise assert 0, 'constraint check fails' try: c(4, 1) except error.ValueConstraintError: pass else: assert 0, 'constraint check fails' try: c(3, 0) except error.ValueConstraintError: pass else: assert 0, 'constraint check fails' # Constraints compositions class ConstraintsIntersectionTestCase(unittest.TestCase): def setUp(self): self.c1 = constraint.ConstraintsIntersection( constraint.ValueRangeConstraint(1, 9), constraint.ValueRangeConstraint(2, 5) ) def testGoodVal(self): try: self.c1(3) except error.ValueConstraintError: assert 0, 'constraint check fails' def testBadVal(self): try: self.c1(0) except error.ValueConstraintError: pass else: assert 0, 'constraint check fails' class ConstraintsUnionTestCase(unittest.TestCase): def setUp(self): self.c1 = constraint.ConstraintsUnion( constraint.SingleValueConstraint(5), constraint.ValueRangeConstraint(1, 3) ) def testGoodVal(self): try: self.c1(2) self.c1(5) except error.ValueConstraintError: assert 0, 'constraint check fails' def testBadVal(self): try: self.c1(-5) except error.ValueConstraintError: pass else: assert 0, 'constraint check fails' class ConstraintsExclusionTestCase(unittest.TestCase): def setUp(self): self.c1 = constraint.ConstraintsExclusion( constraint.ValueRangeConstraint(2, 4) ) def testGoodVal(self): try: self.c1(6) except error.ValueConstraintError: assert 0, 'constraint check fails' def testBadVal(self): try: self.c1(2) except error.ValueConstraintError: pass else: assert 0, 'constraint check fails' # Constraints derivations class DirectDerivationTestCase(unittest.TestCase): def setUp(self): self.c1 = constraint.SingleValueConstraint(5) self.c2 = constraint.ConstraintsUnion( self.c1, constraint.ValueRangeConstraint(1, 3) ) def testGoodVal(self): assert self.c1.isSuperTypeOf(self.c2), 'isSuperTypeOf failed' assert not self.c1.isSubTypeOf(self.c2) , 'isSubTypeOf failed' def testBadVal(self): assert not self.c2.isSuperTypeOf(self.c1) , 'isSuperTypeOf failed' assert self.c2.isSubTypeOf(self.c1) , 'isSubTypeOf failed' class IndirectDerivationTestCase(unittest.TestCase): def setUp(self): self.c1 = constraint.ConstraintsIntersection( constraint.ValueRangeConstraint(1, 30) ) self.c2 = constraint.ConstraintsIntersection( self.c1, constraint.ValueRangeConstraint(1, 20) ) self.c2 = constraint.ConstraintsIntersection( self.c2, constraint.ValueRangeConstraint(1, 10) ) def testGoodVal(self): assert self.c1.isSuperTypeOf(self.c2), 'isSuperTypeOf failed' assert not self.c1.isSubTypeOf(self.c2) , 'isSubTypeOf failed' def testBadVal(self): assert not self.c2.isSuperTypeOf(self.c1) , 'isSuperTypeOf failed' assert self.c2.isSubTypeOf(self.c1) , 'isSubTypeOf failed' if __name__ == '__main__': unittest.main() # how to apply size constriants to constructed types?
"""Platform for Time of Flight sensor VL53L1X from STMicroelectronics.""" import asyncio from functools import partial import logging from VL53L1X2 import VL53L1X # pylint: disable=import-error import voluptuous as vol from homeassistant.components import rpi_gpio from homeassistant.components.sensor import PLATFORM_SCHEMA from homeassistant.const import CONF_NAME import homeassistant.helpers.config_validation as cv from homeassistant.helpers.entity import Entity _LOGGER = logging.getLogger(__name__) LENGTH_MILLIMETERS = "mm" CONF_I2C_ADDRESS = "i2c_address" CONF_I2C_BUS = "i2c_bus" CONF_XSHUT = "xshut" DEFAULT_NAME = "VL53L1X" DEFAULT_I2C_ADDRESS = 0x29 DEFAULT_I2C_BUS = 1 DEFAULT_XSHUT = 16 DEFAULT_RANGE = 2 PLATFORM_SCHEMA = PLATFORM_SCHEMA.extend( { vol.Optional(CONF_NAME, default=DEFAULT_NAME): cv.string, vol.Optional(CONF_I2C_ADDRESS, default=DEFAULT_I2C_ADDRESS): vol.Coerce(int), vol.Optional(CONF_I2C_BUS, default=DEFAULT_I2C_BUS): vol.Coerce(int), vol.Optional(CONF_XSHUT, default=DEFAULT_XSHUT): cv.positive_int, } ) def init_tof_0(xshut, sensor): """XSHUT port LOW resets the device.""" sensor.open() rpi_gpio.setup_output(xshut) rpi_gpio.write_output(xshut, 0) def init_tof_1(xshut): """XSHUT port HIGH enables the device.""" rpi_gpio.setup_output(xshut) rpi_gpio.write_output(xshut, 1) async def async_setup_platform(hass, config, async_add_entities, discovery_info=None): """Reset and initialize the VL53L1X ToF Sensor from STMicroelectronics.""" name = config.get(CONF_NAME) bus_number = config.get(CONF_I2C_BUS) i2c_address = config.get(CONF_I2C_ADDRESS) unit = LENGTH_MILLIMETERS xshut = config.get(CONF_XSHUT) sensor = await hass.async_add_executor_job(partial(VL53L1X, bus_number)) await hass.async_add_executor_job(init_tof_0, xshut, sensor) await asyncio.sleep(0.01) await hass.async_add_executor_job(init_tof_1, xshut) await asyncio.sleep(0.01) dev = [VL53L1XSensor(sensor, name, unit, i2c_address)] async_add_entities(dev, True) class VL53L1XSensor(Entity): """Implementation of VL53L1X sensor.""" def __init__(self, vl53l1x_sensor, name, unit, i2c_address): """Initialize the sensor.""" self._name = name self._unit_of_measurement = unit self.vl53l1x_sensor = vl53l1x_sensor self.i2c_address = i2c_address self._state = None self.init = True @property def name(self) -> str: """Return the name of the sensor.""" return self._name @property def state(self) -> int: """Return the state of the sensor.""" return self._state @property def unit_of_measurement(self) -> str: """Return the unit of measurement.""" return self._unit_of_measurement def update(self): """Get the latest measurement and update state.""" if self.init: self.vl53l1x_sensor.add_sensor(self.i2c_address, self.i2c_address) self.init = False self.vl53l1x_sensor.start_ranging(self.i2c_address, DEFAULT_RANGE) self.vl53l1x_sensor.update(self.i2c_address) self.vl53l1x_sensor.stop_ranging(self.i2c_address) self._state = self.vl53l1x_sensor.distance
#!/usr/bin/env python # Copyright (C) 2009-2014: # Gabes Jean, naparuba@gmail.com # Gerhard Lausser, Gerhard.Lausser@consol.de # # This file is part of Shinken. # # Shinken is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Shinken is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with Shinken. If not, see <http://www.gnu.org/licenses/>. # # This file is used to test reading and processing of config files # from shinken_test import * class TestsericeTplNoHostname(ShinkenTest): def setUp(self): self.setup_with_file('etc/shinken_servicetpl_no_hostname.cfg') def test_dummy(self): # # Config is not correct because of a wrong relative path # in the main config file # print "Get the hosts and services" now = time.time() host = self.sched.hosts.find_by_name("test_host_0") host.checks_in_progress = [] host.act_depend_of = [] # ignore the router router = self.sched.hosts.find_by_name("test_router_0") router.checks_in_progress = [] router.act_depend_of = [] # ignore the router svc = self.sched.services.find_srv_by_name_and_hostname("test_host_0", "test_ok_0") svc.checks_in_progress = [] svc.act_depend_of = [] # no hostchecks on critical checkresults self.scheduler_loop(2, [[host, 0, 'UP | value1=1 value2=2'], [router, 0, 'UP | rtt=10'], [svc, 2, 'BAD | value1=0 value2=0']]) self.assertEqual('UP', host.state) self.assertEqual('HARD', host.state_type) if __name__ == '__main__': unittest.main()
# coding=utf-8 # -------------------------------------------------------------------------- # Copyright (c) Microsoft Corporation. All rights reserved. # Licensed under the MIT License. See License.txt in the project root for # license information. # # Code generated by Microsoft (R) AutoRest Code Generator. # Changes may cause incorrect behavior and will be lost if the code is # regenerated. # -------------------------------------------------------------------------- from msrest.service_client import ServiceClient from msrest import Configuration, Serializer, Deserializer from .version import VERSION from .operations.datetime_model_operations import DatetimeModelOperations from . import models class AutoRestDateTimeTestServiceConfiguration(Configuration): """Configuration for AutoRestDateTimeTestService Note that all parameters used to create this instance are saved as instance attributes. :param str base_url: Service URL """ def __init__( self, base_url=None): if not base_url: base_url = 'https://localhost' super(AutoRestDateTimeTestServiceConfiguration, self).__init__(base_url) self.add_user_agent('autorestdatetimetestservice/{}'.format(VERSION)) class AutoRestDateTimeTestService(object): """Test Infrastructure for AutoRest :ivar config: Configuration for client. :vartype config: AutoRestDateTimeTestServiceConfiguration :ivar datetime_model: DatetimeModel operations :vartype datetime_model: .operations.DatetimeModelOperations :param str base_url: Service URL """ def __init__( self, base_url=None): self.config = AutoRestDateTimeTestServiceConfiguration(base_url) self._client = ServiceClient(None, self.config) client_models = {k: v for k, v in models.__dict__.items() if isinstance(v, type)} self._serialize = Serializer(client_models) self._deserialize = Deserializer(client_models) self.datetime_model = DatetimeModelOperations( self._client, self.config, self._serialize, self._deserialize)
#!/usr/bin/python3 import csv import pymysql singleList = [] multipleList = [] # Connect to the database connection = pymysql.connect( host='localhost', user='operator', passwd='operator', db='chunk4_images', charset='utf8mb4', cursorclass=pymysql.cursors.DictCursor) with open('updated4_images.csv', 'r') as csvfile: for myupdatedCSV in csv.reader(csvfile): if myupdatedCSV[3] == "UML": updatedCSV = myupdatedCSV[2].split('/') repo = updatedCSV[3] + "/" + updatedCSV[4] fileurl = myupdatedCSV[2] filename = fileurl.split('/')[-1] filepath = '/'.join(fileurl.split('/')[6:]) if 'https://raw.githubusercontent.com/' not in fileurl: continue # Get repo id from database cursor = connection.cursor() sql = 'SELECT id FROM repositories WHERE uri="' sql += 'https://github.com/{0}"'.format(repo) # print(sql) cursor.execute(sql) result = cursor.fetchone() try: repo_id = result['id'] # print(repo_id) except: #print("# Error", result, repo) continue # Get file id from database sql = 'SELECT id FROM files WHERE repository_id={0} and file_name="{1}"'.format(repo_id, filename) # print(sql) cursor.execute(sql) if cursor.rowcount == 1: result = cursor.fetchone() file_id = result['id'] singleList.append((file_id, repo_id, fileurl.replace("'", "\\'"), filepath.replace("'", "\\'"))) else: result = cursor.fetchall() # print("Warning:", result, filepath) found = 0 for file in result: sql = 'SELECT file_path from file_links WHERE file_id={0}'.format(file['id']) # print(sql) cursor.execute(sql) result = cursor.fetchone() db_path = result['file_path'] if db_path == filepath: singleList.append((file['id'], repo_id, fileurl.replace("'", "\\'"), filepath.replace("'", "\\'"))) found = 1 break #if not found: #print("# ERROR:", filepath, "not found") connection.close() # Write data into database create = """ USE chunk4_images; CREATE TABLE uml_files ( id int, repository_id int, file_url VARCHAR(255), file_path VARCHAR(255) ); """ print(create) for entry in singleList: print("INSERT INTO uml_files (id, repository_id, file_url, file_path) VALUES ({0}, {1}, '{2}', '{3}');".format(*entry))
import numpy as np from unittest import TestCase from diffprivlib.mechanisms import ExponentialCategorical from diffprivlib.utils import global_seed class TestExponential(TestCase): def setup_method(self, method): if method.__name__ .endswith("prob"): global_seed(314159) self.mech = ExponentialCategorical def teardown_method(self, method): del self.mech def test_class(self): from diffprivlib.mechanisms import DPMechanism self.assertTrue(issubclass(ExponentialCategorical, DPMechanism)) def test_inf_epsilon(self): utility_list = [ ["A", "B", 1], ["A", "C", 2], ["B", "C", 2] ] mech = self.mech(epsilon=float("inf"), utility_list=utility_list) # print(_mech.randomise("A")) for i in range(1000): self.assertEqual(mech.randomise("A"), "A") def test_nonzero_delta(self): utility_list = [ ["A", "B", 1], ["A", "C", 2], ["B", "C", 2] ] mech = self.mech(epsilon=1, utility_list=utility_list) mech.delta = 0.1 with self.assertRaises(ValueError): mech.randomise("A") def test_non_string_hierarchy(self): utility_list = [ ["A", "B", 1], ["A", 2, 2], ["B", 2, 2] ] with self.assertRaises(TypeError): self.mech(epsilon=1, utility_list=utility_list) def test_missing_utilities(self): utility_list = [ ["A", "B", 1], ["A", "C", 2] ] with self.assertRaises(ValueError): self.mech(epsilon=1, utility_list=utility_list) def test_wrong_utilities(self): utility_list = ( ["A", "B", 1], ["A", "C", 2], ["B", "C", 2] ) with self.assertRaises(TypeError): self.mech(epsilon=1, utility_list=utility_list) utility_list = [ ["A", "B", 1], ["A", "C", 2], ["B", "C", "2"] ] with self.assertRaises(TypeError): self.mech(epsilon=1, utility_list=utility_list) utility_list = [ ["A", "B", 1], ["A", "C", 2], ["B", "C", -2] ] with self.assertRaises(ValueError): self.mech(epsilon=1, utility_list=utility_list) def test_non_string_input(self): utility_list = [ ["A", "B", 1], ["A", "C", 2], ["B", "C", 2] ] mech = self.mech(epsilon=1, utility_list=utility_list) with self.assertRaises(TypeError): mech.randomise(2) def test_outside_domain(self): utility_list = [ ["A", "B", 1], ["A", "C", 2], ["B", "C", 2] ] mech = self.mech(epsilon=1, utility_list=utility_list) with self.assertRaises(ValueError): mech.randomise("D") def test_get_utility_list(self): utility_list = [ ["A", "B", 1], ["A", "C", 2], ["C", "B", 2] ] mech = self.mech(epsilon=1, utility_list=utility_list) _utility_list = mech.utility_list self.assertEqual(len(_utility_list), len(utility_list)) def test_self_in_utility(self): utility_list = [ ["A", "B", 1], ["A", "C", 2], ["B", "C", 2], ["A", "A", 5] ] mech = self.mech(epsilon=1, utility_list=utility_list) _utility_list = mech.utility_list self.assertEqual(len(_utility_list) + 1, len(utility_list)) self.assertEqual(mech._get_utility("A", "A"), 0) def test_distrib_prob(self): epsilon = np.log(2) runs = 20000 utility_list = [ ["A", "B", 1], ["A", "C", 2], ["B", "C", 2] ] mech = self.mech(epsilon=epsilon, utility_list=utility_list) count = [0, 0, 0] for i in range(runs): val = mech.randomise("A") if val == "A": count[0] += 1 elif val == "B": count[1] += 1 elif val == "C": count[2] += 1 # print("A: %d, B: %d, C: %d" % (count[0], count[1], count[2])) self.assertLessEqual(count[0] / runs, np.exp(epsilon) * count[2] / runs + 0.05) self.assertAlmostEqual(count[0] / count[1], count[1] / count[2], delta=0.15) def test_repr(self): repr_ = repr(self.mech(epsilon=1, utility_list=[])) self.assertIn(".ExponentialCategorical(", repr_) def test_bias(self): self.assertRaises(NotImplementedError, self.mech(epsilon=1, utility_list=[]).bias, 0) def test_variance(self): self.assertRaises(NotImplementedError, self.mech(epsilon=1, utility_list=[]).variance, 0)
#!/usr/bin/env python2 # Copyright (c) 2014 The Bitcoin Core developers # Distributed under the MIT software license, see the accompanying # file COPYING or http://www.opensource.org/licenses/mit-license.php. # # Test spending coinbase transactions. # The coinbase transaction in block N can appear in block # N+100... so is valid in the mempool when the best block # height is N+99. # This test makes sure coinbase spends that will be mature # in the next block are accepted into the memory pool, # but less mature coinbase spends are NOT. # from test_framework import BitcoinTestFramework from bitcoinrpc.authproxy import AuthServiceProxy, JSONRPCException from util import * import os import shutil # Create one-input, one-output, no-fee transaction: class MempoolSpendCoinbaseTest(BitcoinTestFramework): def setup_network(self): # Just need one node for this test args = ["-checkmempool", "-debug=mempool"] self.nodes = [] self.nodes.append(start_node(0, self.options.tmpdir, args)) self.is_network_split = False def create_tx(self, from_txid, to_address, amount): inputs = [{ "txid" : from_txid, "vout" : 0}] outputs = { to_address : amount } rawtx = self.nodes[0].createrawtransaction(inputs, outputs) signresult = self.nodes[0].signrawtransaction(rawtx) assert_equal(signresult["complete"], True) return signresult["hex"] def run_test(self): chain_height = self.nodes[0].getblockcount() assert_equal(chain_height, 200) node0_address = self.nodes[0].getnewaddress() # Coinbase at height chain_height-100+1 ok in mempool, should # get mined. Coinbase at height chain_height-100+2 is # is too immature to spend. b = [ self.nodes[0].getblockhash(n) for n in range(101, 103) ] coinbase_txids = [ self.nodes[0].getblock(h)['tx'][0] for h in b ] spends_raw = [ self.create_tx(txid, node0_address, 50) for txid in coinbase_txids ] spend_101_id = self.nodes[0].sendrawtransaction(spends_raw[0]) # coinbase at height 102 should be too immature to spend assert_raises(JSONRPCException, self.nodes[0].sendrawtransaction, spends_raw[1]) # mempool should have just spend_101: assert_equal(self.nodes[0].getrawmempool(), [ spend_101_id ]) # mine a block, spend_101 should get confirmed self.nodes[0].generate(1) assert_equal(set(self.nodes[0].getrawmempool()), set()) # ... and now height 102 can be spent: spend_102_id = self.nodes[0].sendrawtransaction(spends_raw[1]) assert_equal(self.nodes[0].getrawmempool(), [ spend_102_id ]) if __name__ == '__main__': MempoolSpendCoinbaseTest().main()
import honeyd import time import support from htmltmpl import TemplateManager, TemplateProcessor global counter self.send_response(200) self.send_header("Content-Type", "text/html") self.send_nocache() self.end_headers() # Compile or load already precompiled template. template = TemplateManager().prepare(self.root+"/templates/index.tmpl") tproc = TemplateProcessor(0) # Process commands given to us message = support.parse_query(self.query) # Set the title. tproc.set("title", "Honeyd Administration Interface") # Test try: counter += 1 except: counter = 1 greeting = ("Welcome to the Honeyd Administration Interface." "You are visitor %d.<p>") % counter content = support.interface_table() content += "<p>" + support.stats_table(self.root) + "</p>\n" content += "<p>" + support.status_connections(self.root, "tcp") + "</p>\n" content += "<p>" + support.status_connections(self.root, "udp") + "</p>\n" side_content = ("<div class=graphs>" "<img height=155 width=484 src=/graphs/traffic_hourly.gif><br>" "<img height=155 width=484 src=/graphs/traffic_daily.gif>" "</div>") support.security_check(tproc) if message: tproc.set("message", message) tproc.set("greeting", greeting) tproc.set("content", content) tproc.set("side_content", side_content) tproc.set("uptime", support.uptime()) # Print the processed template. self.wfile.write(tproc.process(template))
######################## BEGIN LICENSE BLOCK ######################## # The Original Code is mozilla.org code. # # The Initial Developer of the Original Code is # Netscape Communications Corporation. # Portions created by the Initial Developer are Copyright (C) 1998 # the Initial Developer. All Rights Reserved. # # Contributor(s): # Mark Pilgrim - port to Python # # This library is free software; you can redistribute it and/or # modify it under the terms of the GNU Lesser General Public # License as published by the Free Software Foundation; either # version 2.1 of the License, or (at your option) any later version. # # This library is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU # Lesser General Public License for more details. # # You should have received a copy of the GNU Lesser General Public # License along with this library; if not, write to the Free Software # Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA # 02110-1301 USA ######################### END LICENSE BLOCK ######################### from .enums import MachineState HZ_CLS = ( 1,0,0,0,0,0,0,0, # 00 - 07 0,0,0,0,0,0,0,0, # 08 - 0f 0,0,0,0,0,0,0,0, # 10 - 17 0,0,0,1,0,0,0,0, # 18 - 1f 0,0,0,0,0,0,0,0, # 20 - 27 0,0,0,0,0,0,0,0, # 28 - 2f 0,0,0,0,0,0,0,0, # 30 - 37 0,0,0,0,0,0,0,0, # 38 - 3f 0,0,0,0,0,0,0,0, # 40 - 47 0,0,0,0,0,0,0,0, # 48 - 4f 0,0,0,0,0,0,0,0, # 50 - 57 0,0,0,0,0,0,0,0, # 58 - 5f 0,0,0,0,0,0,0,0, # 60 - 67 0,0,0,0,0,0,0,0, # 68 - 6f 0,0,0,0,0,0,0,0, # 70 - 77 0,0,0,4,0,5,2,0, # 78 - 7f 1,1,1,1,1,1,1,1, # 80 - 87 1,1,1,1,1,1,1,1, # 88 - 8f 1,1,1,1,1,1,1,1, # 90 - 97 1,1,1,1,1,1,1,1, # 98 - 9f 1,1,1,1,1,1,1,1, # a0 - a7 1,1,1,1,1,1,1,1, # a8 - af 1,1,1,1,1,1,1,1, # b0 - b7 1,1,1,1,1,1,1,1, # b8 - bf 1,1,1,1,1,1,1,1, # c0 - c7 1,1,1,1,1,1,1,1, # c8 - cf 1,1,1,1,1,1,1,1, # d0 - d7 1,1,1,1,1,1,1,1, # d8 - df 1,1,1,1,1,1,1,1, # e0 - e7 1,1,1,1,1,1,1,1, # e8 - ef 1,1,1,1,1,1,1,1, # f0 - f7 1,1,1,1,1,1,1,1, # f8 - ff ) HZ_ST = ( MachineState.START,MachineState.ERROR, 3,MachineState.START,MachineState.START,MachineState.START,MachineState.ERROR,MachineState.ERROR,# 00-07 MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,# 08-0f MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ERROR,MachineState.ERROR,MachineState.START,MachineState.START, 4,MachineState.ERROR,# 10-17 5,MachineState.ERROR, 6,MachineState.ERROR, 5, 5, 4,MachineState.ERROR,# 18-1f 4,MachineState.ERROR, 4, 4, 4,MachineState.ERROR, 4,MachineState.ERROR,# 20-27 4,MachineState.ITS_ME,MachineState.START,MachineState.START,MachineState.START,MachineState.START,MachineState.START,MachineState.START,# 28-2f ) HZ_CHAR_LEN_TABLE = (0, 0, 0, 0, 0, 0) HZ_SM_MODEL = {'class_table': HZ_CLS, 'class_factor': 6, 'state_table': HZ_ST, 'char_len_table': HZ_CHAR_LEN_TABLE, 'name': "HZ-GB-2312", 'language': 'Chinese'} ISO2022CN_CLS = ( 2,0,0,0,0,0,0,0, # 00 - 07 0,0,0,0,0,0,0,0, # 08 - 0f 0,0,0,0,0,0,0,0, # 10 - 17 0,0,0,1,0,0,0,0, # 18 - 1f 0,0,0,0,0,0,0,0, # 20 - 27 0,3,0,0,0,0,0,0, # 28 - 2f 0,0,0,0,0,0,0,0, # 30 - 37 0,0,0,0,0,0,0,0, # 38 - 3f 0,0,0,4,0,0,0,0, # 40 - 47 0,0,0,0,0,0,0,0, # 48 - 4f 0,0,0,0,0,0,0,0, # 50 - 57 0,0,0,0,0,0,0,0, # 58 - 5f 0,0,0,0,0,0,0,0, # 60 - 67 0,0,0,0,0,0,0,0, # 68 - 6f 0,0,0,0,0,0,0,0, # 70 - 77 0,0,0,0,0,0,0,0, # 78 - 7f 2,2,2,2,2,2,2,2, # 80 - 87 2,2,2,2,2,2,2,2, # 88 - 8f 2,2,2,2,2,2,2,2, # 90 - 97 2,2,2,2,2,2,2,2, # 98 - 9f 2,2,2,2,2,2,2,2, # a0 - a7 2,2,2,2,2,2,2,2, # a8 - af 2,2,2,2,2,2,2,2, # b0 - b7 2,2,2,2,2,2,2,2, # b8 - bf 2,2,2,2,2,2,2,2, # c0 - c7 2,2,2,2,2,2,2,2, # c8 - cf 2,2,2,2,2,2,2,2, # d0 - d7 2,2,2,2,2,2,2,2, # d8 - df 2,2,2,2,2,2,2,2, # e0 - e7 2,2,2,2,2,2,2,2, # e8 - ef 2,2,2,2,2,2,2,2, # f0 - f7 2,2,2,2,2,2,2,2, # f8 - ff ) ISO2022CN_ST = ( MachineState.START, 3,MachineState.ERROR,MachineState.START,MachineState.START,MachineState.START,MachineState.START,MachineState.START,# 00-07 MachineState.START,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,# 08-0f MachineState.ERROR,MachineState.ERROR,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,# 10-17 MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR, 4,MachineState.ERROR,# 18-1f MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ITS_ME,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,# 20-27 5, 6,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,# 28-2f MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ITS_ME,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,# 30-37 MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ITS_ME,MachineState.ERROR,MachineState.START,# 38-3f ) ISO2022CN_CHAR_LEN_TABLE = (0, 0, 0, 0, 0, 0, 0, 0, 0) ISO2022CN_SM_MODEL = {'class_table': ISO2022CN_CLS, 'class_factor': 9, 'state_table': ISO2022CN_ST, 'char_len_table': ISO2022CN_CHAR_LEN_TABLE, 'name': "ISO-2022-CN", 'language': 'Chinese'} ISO2022JP_CLS = ( 2,0,0,0,0,0,0,0, # 00 - 07 0,0,0,0,0,0,2,2, # 08 - 0f 0,0,0,0,0,0,0,0, # 10 - 17 0,0,0,1,0,0,0,0, # 18 - 1f 0,0,0,0,7,0,0,0, # 20 - 27 3,0,0,0,0,0,0,0, # 28 - 2f 0,0,0,0,0,0,0,0, # 30 - 37 0,0,0,0,0,0,0,0, # 38 - 3f 6,0,4,0,8,0,0,0, # 40 - 47 0,9,5,0,0,0,0,0, # 48 - 4f 0,0,0,0,0,0,0,0, # 50 - 57 0,0,0,0,0,0,0,0, # 58 - 5f 0,0,0,0,0,0,0,0, # 60 - 67 0,0,0,0,0,0,0,0, # 68 - 6f 0,0,0,0,0,0,0,0, # 70 - 77 0,0,0,0,0,0,0,0, # 78 - 7f 2,2,2,2,2,2,2,2, # 80 - 87 2,2,2,2,2,2,2,2, # 88 - 8f 2,2,2,2,2,2,2,2, # 90 - 97 2,2,2,2,2,2,2,2, # 98 - 9f 2,2,2,2,2,2,2,2, # a0 - a7 2,2,2,2,2,2,2,2, # a8 - af 2,2,2,2,2,2,2,2, # b0 - b7 2,2,2,2,2,2,2,2, # b8 - bf 2,2,2,2,2,2,2,2, # c0 - c7 2,2,2,2,2,2,2,2, # c8 - cf 2,2,2,2,2,2,2,2, # d0 - d7 2,2,2,2,2,2,2,2, # d8 - df 2,2,2,2,2,2,2,2, # e0 - e7 2,2,2,2,2,2,2,2, # e8 - ef 2,2,2,2,2,2,2,2, # f0 - f7 2,2,2,2,2,2,2,2, # f8 - ff ) ISO2022JP_ST = ( MachineState.START, 3,MachineState.ERROR,MachineState.START,MachineState.START,MachineState.START,MachineState.START,MachineState.START,# 00-07 MachineState.START,MachineState.START,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,# 08-0f MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,# 10-17 MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ERROR,MachineState.ERROR,# 18-1f MachineState.ERROR, 5,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR, 4,MachineState.ERROR,MachineState.ERROR,# 20-27 MachineState.ERROR,MachineState.ERROR,MachineState.ERROR, 6,MachineState.ITS_ME,MachineState.ERROR,MachineState.ITS_ME,MachineState.ERROR,# 28-2f MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ITS_ME,MachineState.ITS_ME,# 30-37 MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ITS_ME,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,# 38-3f MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ITS_ME,MachineState.ERROR,MachineState.START,MachineState.START,# 40-47 ) ISO2022JP_CHAR_LEN_TABLE = (0, 0, 0, 0, 0, 0, 0, 0, 0, 0) ISO2022JP_SM_MODEL = {'class_table': ISO2022JP_CLS, 'class_factor': 10, 'state_table': ISO2022JP_ST, 'char_len_table': ISO2022JP_CHAR_LEN_TABLE, 'name': "ISO-2022-JP", 'language': 'Japanese'} ISO2022KR_CLS = ( 2,0,0,0,0,0,0,0, # 00 - 07 0,0,0,0,0,0,0,0, # 08 - 0f 0,0,0,0,0,0,0,0, # 10 - 17 0,0,0,1,0,0,0,0, # 18 - 1f 0,0,0,0,3,0,0,0, # 20 - 27 0,4,0,0,0,0,0,0, # 28 - 2f 0,0,0,0,0,0,0,0, # 30 - 37 0,0,0,0,0,0,0,0, # 38 - 3f 0,0,0,5,0,0,0,0, # 40 - 47 0,0,0,0,0,0,0,0, # 48 - 4f 0,0,0,0,0,0,0,0, # 50 - 57 0,0,0,0,0,0,0,0, # 58 - 5f 0,0,0,0,0,0,0,0, # 60 - 67 0,0,0,0,0,0,0,0, # 68 - 6f 0,0,0,0,0,0,0,0, # 70 - 77 0,0,0,0,0,0,0,0, # 78 - 7f 2,2,2,2,2,2,2,2, # 80 - 87 2,2,2,2,2,2,2,2, # 88 - 8f 2,2,2,2,2,2,2,2, # 90 - 97 2,2,2,2,2,2,2,2, # 98 - 9f 2,2,2,2,2,2,2,2, # a0 - a7 2,2,2,2,2,2,2,2, # a8 - af 2,2,2,2,2,2,2,2, # b0 - b7 2,2,2,2,2,2,2,2, # b8 - bf 2,2,2,2,2,2,2,2, # c0 - c7 2,2,2,2,2,2,2,2, # c8 - cf 2,2,2,2,2,2,2,2, # d0 - d7 2,2,2,2,2,2,2,2, # d8 - df 2,2,2,2,2,2,2,2, # e0 - e7 2,2,2,2,2,2,2,2, # e8 - ef 2,2,2,2,2,2,2,2, # f0 - f7 2,2,2,2,2,2,2,2, # f8 - ff ) ISO2022KR_ST = ( MachineState.START, 3,MachineState.ERROR,MachineState.START,MachineState.START,MachineState.START,MachineState.ERROR,MachineState.ERROR,# 00-07 MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,# 08-0f MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR, 4,MachineState.ERROR,MachineState.ERROR,# 10-17 MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR, 5,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,# 18-1f MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ITS_ME,MachineState.START,MachineState.START,MachineState.START,MachineState.START,# 20-27 ) ISO2022KR_CHAR_LEN_TABLE = (0, 0, 0, 0, 0, 0) ISO2022KR_SM_MODEL = {'class_table': ISO2022KR_CLS, 'class_factor': 6, 'state_table': ISO2022KR_ST, 'char_len_table': ISO2022KR_CHAR_LEN_TABLE, 'name': "ISO-2022-KR", 'language': 'Korean'}
# Copyright (C) 2014 Nippon Telegraph and Telephone Corporation. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or # implied. # See the License for the specific language governing permissions and # limitations under the License. """ Defines data types and models required specifically for RTC support. """ import logging from ryu.lib.packet.bgp import RF_RTC_UC from ryu.services.protocols.bgp.info_base.base import Destination from ryu.services.protocols.bgp.info_base.base import NonVrfPathProcessingMixin from ryu.services.protocols.bgp.info_base.base import Path from ryu.services.protocols.bgp.info_base.base import Table LOG = logging.getLogger('bgpspeaker.info_base.rtc') class RtcTable(Table): """Global table to store RT membership information. Uses `RtDest` to store destination information for each known RT NLRI path. """ ROUTE_FAMILY = RF_RTC_UC def __init__(self, core_service, signal_bus): Table.__init__(self, None, core_service, signal_bus) def _table_key(self, rtc_nlri): """Return a key that will uniquely identify this RT NLRI inside this table. """ return str(rtc_nlri.origin_as) + ':' + rtc_nlri.route_target def _create_dest(self, nlri): return RtcDest(self, nlri) def __str__(self): return 'RtcTable(scope_id: %s, rf: %s)' % (self.scope_id, self.route_family) class RtcDest(Destination, NonVrfPathProcessingMixin): ROUTE_FAMILY = RF_RTC_UC def _new_best_path(self, new_best_path): NonVrfPathProcessingMixin._new_best_path(self, new_best_path) def _best_path_lost(self): NonVrfPathProcessingMixin._best_path_lost(self) class RtcPath(Path): ROUTE_FAMILY = RF_RTC_UC def __init__(self, source, nlri, src_ver_num, pattrs=None, nexthop='0.0.0.0', is_withdraw=False, med_set_by_target_neighbor=False): Path.__init__(self, source, nlri, src_ver_num, pattrs, nexthop, is_withdraw, med_set_by_target_neighbor)
# Copyright 2016 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== """TensorFlow Ops for Sequence to Sequence models.""" from __future__ import absolute_import from __future__ import division from __future__ import print_function from tensorflow.contrib import rnn from tensorflow.python.framework import dtypes from tensorflow.python.framework import ops from tensorflow.python.ops import array_ops from tensorflow.python.ops import math_ops from tensorflow.python.ops import nn from tensorflow.python.ops import variable_scope as vs def sequence_classifier(decoding, labels, sampling_decoding=None, name=None): """Returns predictions and loss for sequence of predictions. Args: decoding: List of Tensors with predictions. labels: List of Tensors with labels. sampling_decoding: Optional, List of Tensor with predictions to be used in sampling. E.g. they shouldn't have dependncy on outputs. If not provided, decoding is used. name: Operation name. Returns: Predictions and losses tensors. """ with ops.name_scope(name, "sequence_classifier", [decoding, labels]): predictions, xent_list = [], [] for i, pred in enumerate(decoding): xent_list.append(nn.softmax_cross_entropy_with_logits( labels=labels[i], logits=pred, name="sequence_loss/xent_raw{0}".format(i))) if sampling_decoding: predictions.append(nn.softmax(sampling_decoding[i])) else: predictions.append(nn.softmax(pred)) xent = math_ops.add_n(xent_list, name="sequence_loss/xent") loss = math_ops.reduce_sum(xent, name="sequence_loss") return array_ops.stack(predictions, axis=1), loss def seq2seq_inputs(x, y, input_length, output_length, sentinel=None, name=None): """Processes inputs for Sequence to Sequence models. Args: x: Input Tensor [batch_size, input_length, embed_dim]. y: Output Tensor [batch_size, output_length, embed_dim]. input_length: length of input x. output_length: length of output y. sentinel: optional first input to decoder and final output expected. If sentinel is not provided, zeros are used. Due to fact that y is not available in sampling time, shape of sentinel will be inferred from x. name: Operation name. Returns: Encoder input from x, and decoder inputs and outputs from y. """ with ops.name_scope(name, "seq2seq_inputs", [x, y]): in_x = array_ops.unstack(x, axis=1) y = array_ops.unstack(y, axis=1) if not sentinel: # Set to zeros of shape of y[0], using x for batch size. sentinel_shape = array_ops.stack( [array_ops.shape(x)[0], y[0].get_shape()[1]]) sentinel = array_ops.zeros(sentinel_shape) sentinel.set_shape(y[0].get_shape()) in_y = [sentinel] + y out_y = y + [sentinel] return in_x, in_y, out_y def rnn_decoder(decoder_inputs, initial_state, cell, scope=None): """RNN Decoder that creates training and sampling sub-graphs. Args: decoder_inputs: Inputs for decoder, list of tensors. This is used only in training sub-graph. initial_state: Initial state for the decoder. cell: RNN cell to use for decoder. scope: Scope to use, if None new will be produced. Returns: List of tensors for outputs and states for training and sampling sub-graphs. """ with vs.variable_scope(scope or "dnn_decoder"): states, sampling_states = [initial_state], [initial_state] outputs, sampling_outputs = [], [] with ops.name_scope("training", values=[decoder_inputs, initial_state]): for i, inp in enumerate(decoder_inputs): if i > 0: vs.get_variable_scope().reuse_variables() output, new_state = cell(inp, states[-1]) outputs.append(output) states.append(new_state) with ops.name_scope("sampling", values=[initial_state]): for i, _ in enumerate(decoder_inputs): if i == 0: sampling_outputs.append(outputs[i]) sampling_states.append(states[i]) else: sampling_output, sampling_state = cell(sampling_outputs[-1], sampling_states[-1]) sampling_outputs.append(sampling_output) sampling_states.append(sampling_state) return outputs, states, sampling_outputs, sampling_states def rnn_seq2seq(encoder_inputs, decoder_inputs, encoder_cell, decoder_cell=None, dtype=dtypes.float32, scope=None): """RNN Sequence to Sequence model. Args: encoder_inputs: List of tensors, inputs for encoder. decoder_inputs: List of tensors, inputs for decoder. encoder_cell: RNN cell to use for encoder. decoder_cell: RNN cell to use for decoder, if None encoder_cell is used. dtype: Type to initialize encoder state with. scope: Scope to use, if None new will be produced. Returns: List of tensors for outputs and states for trianing and sampling sub-graphs. """ with vs.variable_scope(scope or "rnn_seq2seq"): _, last_enc_state = rnn.static_rnn( encoder_cell, encoder_inputs, dtype=dtype) return rnn_decoder(decoder_inputs, last_enc_state, decoder_cell or encoder_cell)
# -*- test-case-name: twisted.test.test_reflect -*- # Copyright (c) Twisted Matrix Laboratories. # See LICENSE for details. """ Standardized versions of various cool and/or strange things that you can do with Python's reflection capabilities. """ import sys from .compat import PY3 class _NoModuleFound(Exception): """ No module was found because none exists. """ class InvalidName(ValueError): """ The given name is not a dot-separated list of Python objects. """ class ModuleNotFound(InvalidName): """ The module associated with the given name doesn't exist and it can't be imported. """ class ObjectNotFound(InvalidName): """ The object associated with the given name doesn't exist and it can't be imported. """ if PY3: def reraise(exception, traceback): raise exception.with_traceback(traceback) else: exec("""def reraise(exception, traceback): raise exception.__class__, exception, traceback""") reraise.__doc__ = """ Re-raise an exception, with an optional traceback, in a way that is compatible with both Python 2 and Python 3. Note that on Python 3, re-raised exceptions will be mutated, with their C{__traceback__} attribute being set. @param exception: The exception instance. @param traceback: The traceback to use, or C{None} indicating a new traceback. """ def _importAndCheckStack(importName): """ Import the given name as a module, then walk the stack to determine whether the failure was the module not existing, or some code in the module (for example a dependent import) failing. This can be helpful to determine whether any actual application code was run. For example, to distiguish administrative error (entering the wrong module name), from programmer error (writing buggy code in a module that fails to import). @param importName: The name of the module to import. @type importName: C{str} @raise Exception: if something bad happens. This can be any type of exception, since nobody knows what loading some arbitrary code might do. @raise _NoModuleFound: if no module was found. """ try: return __import__(importName) except ImportError: excType, excValue, excTraceback = sys.exc_info() while excTraceback: execName = excTraceback.tb_frame.f_globals["__name__"] # in Python 2 execName is None when an ImportError is encountered, # where in Python 3 execName is equal to the importName. if execName is None or execName == importName: reraise(excValue, excTraceback) excTraceback = excTraceback.tb_next raise _NoModuleFound() def namedAny(name): """ Retrieve a Python object by its fully qualified name from the global Python module namespace. The first part of the name, that describes a module, will be discovered and imported. Each subsequent part of the name is treated as the name of an attribute of the object specified by all of the name which came before it. For example, the fully-qualified name of this object is 'twisted.python.reflect.namedAny'. @type name: L{str} @param name: The name of the object to return. @raise InvalidName: If the name is an empty string, starts or ends with a '.', or is otherwise syntactically incorrect. @raise ModuleNotFound: If the name is syntactically correct but the module it specifies cannot be imported because it does not appear to exist. @raise ObjectNotFound: If the name is syntactically correct, includes at least one '.', but the module it specifies cannot be imported because it does not appear to exist. @raise AttributeError: If an attribute of an object along the way cannot be accessed, or a module along the way is not found. @return: the Python object identified by 'name'. """ if not name: raise InvalidName('Empty module name') names = name.split('.') # if the name starts or ends with a '.' or contains '..', the __import__ # will raise an 'Empty module name' error. This will provide a better error # message. if '' in names: raise InvalidName( "name must be a string giving a '.'-separated list of Python " "identifiers, not %r" % (name,)) topLevelPackage = None moduleNames = names[:] while not topLevelPackage: if moduleNames: trialname = '.'.join(moduleNames) try: topLevelPackage = _importAndCheckStack(trialname) except _NoModuleFound: moduleNames.pop() else: if len(names) == 1: raise ModuleNotFound("No module named %r" % (name,)) else: raise ObjectNotFound('%r does not name an object' % (name,)) obj = topLevelPackage for n in names[1:]: obj = getattr(obj, n) return obj
######################## BEGIN LICENSE BLOCK ######################## # The Original Code is Mozilla Communicator client code. # # The Initial Developer of the Original Code is # Netscape Communications Corporation. # Portions created by the Initial Developer are Copyright (C) 1998 # the Initial Developer. All Rights Reserved. # # Contributor(s): # Mark Pilgrim - port to Python # # This library is free software; you can redistribute it and/or # modify it under the terms of the GNU Lesser General Public # License as published by the Free Software Foundation; either # version 2.1 of the License, or (at your option) any later version. # # This library is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU # Lesser General Public License for more details. # # You should have received a copy of the GNU Lesser General Public # License along with this library; if not, write to the Free Software # Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA # 02110-1301 USA ######################### END LICENSE BLOCK ######################### from .mbcharsetprober import MultiByteCharSetProber from .codingstatemachine import CodingStateMachine from .chardistribution import Big5DistributionAnalysis from .mbcssm import Big5SMModel class Big5Prober(MultiByteCharSetProber): def __init__(self): MultiByteCharSetProber.__init__(self) self._mCodingSM = CodingStateMachine(Big5SMModel) self._mDistributionAnalyzer = Big5DistributionAnalysis() self.reset() def get_charset_name(self): return "Big5"
# -*- coding: utf-8 -*- import time import unittest from nive.security import AdminUser, UserFound from db_app import * class ObjectTest(unittest.TestCase): def setUp(self): self.app = app() def tearDown(self): self.app.Close() pass def test_add(self): a=self.app root=a.root() user = User("test") # root root.DeleteUser("user1") root.DeleteUser("user2") root.DeleteUser("user3") data = {"password": "11111", "surname": "surname", "lastname": "lastname", "organistion": "organisation"} data["name"] = "user1" data["email"] = "user1@aaa.ccc" o,r = root.AddUser(data, activate=1, generatePW=0, mail=None, notify=False, groups="", currentUser=user) self.assert_(o,r) o,r = root.AddUser(data, activate=1, generatePW=0, mail=None, notify=False, groups="", currentUser=user) self.assertFalse(o,r) data["name"] = "user2" data["email"] = "user2@aaa.ccc" o,r = root.AddUser(data, activate=1, generatePW=1, mail=None, notify=False, groups="group:author", currentUser=user) self.assert_(o,r) data["name"] = "user3" data["email"] = "user3@aaa.ccc" o,r = root.AddUser(data, activate=0, generatePW=1, mail=None, notify=False, groups="group:editor", currentUser=user) self.assert_(o,r) self.assert_("group:editor" in o.data.groups, o.data.groups) self.assert_(o.data.password != "11111") self.assertFalse(o.meta.pool_state) root.MailUserPass(email = "user1", mailtmpl = None) root.MailUserPass(email = "user2@aaa.ccc", mailtmpl = None, createNewPasswd=False) root.MailUserPass(email = "user3@aaa.ccc", mailtmpl = None) self.assert_(root.GetUserByName("user2", activeOnly=1)) self.assert_(root.GetUserByID(o.id, activeOnly=0)) self.assert_(root.GetUserByMail("user2@aaa.ccc", activeOnly=1)) self.assert_(root.LookupUser(name="user1", id=None, activeOnly=1)) self.assertFalse(root.LookupUser(name="user3", id=None, activeOnly=1)) self.assert_(root.LookupUser(name="user3", id=None, activeOnly=0)) self.assert_(len(root.GetUserInfos(["user1", "user2"], fields=["name", "email", "title"], activeOnly=True))) self.assert_(len(root.GetUsersWithGroup("group:author", fields=["name"], activeOnly=True))) self.assert_(len(root.GetUsersWithGroup("group:editor", fields=["name"], activeOnly=False))) self.assertFalse(len(root.GetUsersWithGroup("group:editor", fields=["name"], activeOnly=True))) self.assert_(len(root.GetUsers())) root.DeleteUser("user1") root.DeleteUser("user2") root.DeleteUser("user3") def test_login(self): a=self.app root=a.root() root.identityField=u"name" user = User("test") # root root.DeleteUser("user1") root.DeleteUser("user2") root.DeleteUser("user3") data = {"password": "11111", "surname": "surname", "lastname": "lastname", "organistion": "organisation"} data["name"] = "user1" data["email"] = "user1@aaa.ccc" o,r = root.AddUser(data, activate=1, generatePW=0, mail=None, notify=False, groups="", currentUser=user) self.assert_(o,r) l,r = root.Login("user1", "11111", raiseUnauthorized = 0) self.assert_(l,r) self.assert_(root.Logout("user1")) l,r = root.Login("user1", "aaaaa", raiseUnauthorized = 0) self.assertFalse(l,r) l,r = root.Login("user1", "", raiseUnauthorized = 0) self.assertFalse(l,r) data["name"] = "user2" data["email"] = "user2@aaa.ccc" o,r = root.AddUser(data, activate=1, generatePW=1, mail=None, notify=False, groups="", currentUser=user) self.assert_(o,r) l,r = root.Login("user2", o.data.password, raiseUnauthorized = 0) self.assertFalse(l,r) self.assert_(root.Logout("user1")) l,r = root.Login("user2", "11111", raiseUnauthorized = 0) self.assertFalse(l,r) data["name"] = "user3" data["email"] = "user3@aaa.ccc" o,r = root.AddUser(data, activate=0, generatePW=1, mail=None, notify=False, groups="group:author", currentUser=user) self.assert_(o,r) l,r = root.Login("user3", o.data.password, raiseUnauthorized = 0) self.assertFalse(l,r) self.assertFalse(root.Logout("user3")) root.DeleteUser("user1") root.DeleteUser("user2") root.DeleteUser("user3") def test_user(self): a=self.app root=a.root() user = User("test") # root root.DeleteUser("user1") data = {"password": "11111", "surname": "surname", "lastname": "lastname", "organistion": "organisation"} data["name"] = "user1" data["email"] = "user1@aaa.ccc" o,r = root.AddUser(data, activate=1, generatePW=0, mail=None, notify=False, groups="", currentUser=user) self.assert_(o.SecureUpdate(data, user)) self.assert_(o.UpdateGroups(["group:author"])) self.assert_(o.GetGroups()==("group:author",), o.GetGroups()) self.assert_(o.AddGroup("group:editor", user)) self.assert_(o.GetGroups()==("group:author","group:editor"), o.GetGroups()) self.assert_(o.InGroups("group:editor")) self.assert_(o.InGroups("group:author")) self.assert_(o.ReadableName()=="surname lastname") root.DeleteUser("user1") class AdminuserTest(unittest.TestCase): def setUp(self): self.app = app() self.app.configuration.unlock() self.app.configuration.admin = {"name":"admin", "password":"11111", "email":"admin@aaa.ccc", "groups":("group:admin",)} self.app.configuration.loginByEmail = True self.app.configuration.lock() def tearDown(self): self.app.Close() pass def test_login(self): user = User("test") a=self.app root=a.root() root.identityField=u"name" root.DeleteUser("adminXXXXX") root.DeleteUser("admin") data = {"password": "11111", "surname": "surname", "lastname": "lastname", "organistion": "organisation"} data["name"] = "admin" data["email"] = "admin@aaa.cccXXXXX" o,r = root.AddUser(data, activate=1, generatePW=0, mail=None, notify=False, groups="", currentUser=user) self.assertFalse(o,r) data["name"] = "adminXXXXX" data["email"] = "admin@aaa.ccc" o,r = root.AddUser(data, activate=1, generatePW=0, mail=None, notify=False, groups="", currentUser=user) self.assertFalse(o,r) l,r = root.Login("admin", "11111", raiseUnauthorized = 0) self.assert_(l,r) self.assert_(root.Logout("admin")) l,r = root.Login("admin", "aaaaa", raiseUnauthorized = 0) self.assertFalse(l,r) l,r = root.Login("admin", "", raiseUnauthorized = 0) self.assertFalse(l,r)
import py import types import sys def checksubpackage(name): obj = getattr(py, name) if hasattr(obj, '__map__'): # isinstance(obj, Module): keys = dir(obj) assert len(keys) > 0 print (obj.__map__) for name in list(obj.__map__): assert hasattr(obj, name), (obj, name) def test_dir(): for name in dir(py): if not name.startswith('_'): yield checksubpackage, name def test_virtual_module_identity(): from py import path as path1 from py import path as path2 assert path1 is path2 from py.path import local as local1 from py.path import local as local2 assert local1 is local2 def test_importall(): base = py._pydir nodirs = [ ] if sys.version_info >= (3,0): nodirs.append(base.join('_code', '_assertionold.py')) else: nodirs.append(base.join('_code', '_assertionnew.py')) def recurse(p): return p.check(dotfile=0) and p.basename != "attic" for p in base.visit('*.py', recurse): if p.basename == '__init__.py': continue relpath = p.new(ext='').relto(base) if base.sep in relpath: # not py/*.py itself for x in nodirs: if p == x or p.relto(x): break else: relpath = relpath.replace(base.sep, '.') modpath = 'py.%s' % relpath try: check_import(modpath) except py.test.skip.Exception: pass def check_import(modpath): py.builtin.print_("checking import", modpath) assert __import__(modpath) def test_all_resolves(): seen = py.builtin.set([py]) lastlength = None while len(seen) != lastlength: lastlength = len(seen) for item in py.builtin.frozenset(seen): for value in item.__dict__.values(): if isinstance(value, type(py.test)): seen.add(value)
import re import sys import subprocess import sublime from .abstract import AbstractRegexLinkResolver try: import urllib.request, urllib.parse, urllib.error except ImportError: import urllib PATTERN_SETTING = 'orgmode.open_link.resolver.https.pattern' PATTERN_DEFAULT = r'^(https):(?P<url>.+)$' URL_SETTING = 'orgmode.open_link.resolver.https.url' URL_DEFAULT = 'https:%s' DEFAULT_OPEN_HTTP_LINK_COMMANDS = dict( darwin=['open'], win32=['cmd', '/C'], linux=['xdg-open'], ) class Resolver(AbstractRegexLinkResolver): def __init__(self, view): super(Resolver, self).__init__(view) get = self.settings.get pattern = get(PATTERN_SETTING, PATTERN_DEFAULT) self.regex = re.compile(pattern) self.url = get(URL_SETTING, URL_DEFAULT) self.link_commands = self.settings.get( 'orgmode.open_link.resolver.abstract.commands', DEFAULT_OPEN_HTTP_LINK_COMMANDS) def replace(self, match): return self.url % match.group('url') def execute(self, content): command = self.get_link_command() if not command: sublime.error_message( 'Could not get link opener command.\nNot yet supported.') return None # cmd.exe quote is needed, http://ss64.com/nt/syntax-esc.html # escape these: ^\ ^& ^| ^> ^< ^^ if sys.platform == 'win32': content = content.replace("^", "^^") content = content.replace("&", "^&") content = content.replace("\\", "^\\") content = content.replace("|", "^|") content = content.replace("<", "^<") content = content.replace(">", "^>") if sys.version_info[0] < 3: content = content.encode(sys.getfilesystemencoding()) if sys.platform != 'win32': cmd = command + [content] else: cmd = command + ['start ' + content] print('HTTP*****') print(repr(content), content) print(repr(cmd)) print(cmd) sublime.status_message('Executing: %s' % cmd) if sys.platform != 'win32': process = subprocess.Popen( cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE) else: process = subprocess.Popen( cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True) stdout, stderr = process.communicate() if stdout: stdout = str(stdout, sys.getfilesystemencoding()) sublime.status_message(stdout) if stderr: stderr = str(stderr, sys.getfilesystemencoding()) sublime.error_message(stderr)
# -*- coding: utf-8 -*- ######################################################################### # # Copyright (C) 2018 OSGeo # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ######################################################################### """unit tests for geonode.upload.files module""" from geonode.tests.base import GeoNodeBaseTestSupport from geonode.upload import files class FilesTestCase(GeoNodeBaseTestSupport): def test_scan_hint_kml_ground_overlay(self): result = files.get_scan_hint(["kml", "other"]) kml_file_type = files.get_type("KML Ground Overlay") self.assertEqual(result, kml_file_type.code) def test_scan_hint_kmz_ground_overlay(self): result = files.get_scan_hint(["kmz", "other"]) self.assertEqual(result, "kmz") def test_get_type_non_existing_type(self): self.assertIsNone(files.get_type("fake")) def test_get_type_kml_ground_overlay(self): file_type = files.get_type("KML Ground Overlay") self.assertEqual(file_type.code, "kml-overlay") self.assertIn("kmz", file_type.aliases)
# Copyright 2014 "cheebee7i". # Copyright 2014 "alexbrc". # Copyright 2014 Jeffrey Finkelstein <jeffrey.finkelstein@gmail.com>. """Unit tests for the :mod:`networkx.generators.expanders` module. """ try: import scipy is_scipy_available = True except: is_scipy_available = False import networkx as nx from networkx import adjacency_matrix from networkx import number_of_nodes from networkx.generators.expanders import chordal_cycle_graph from networkx.generators.expanders import margulis_gabber_galil_graph from nose import SkipTest from nose.tools import assert_equal from nose.tools import assert_less from nose.tools import assert_raises from nose.tools import assert_true def test_margulis_gabber_galil_graph(): try: # Scipy is required for conversion to an adjacency matrix. # We also use scipy for computing the eigenvalues, # but this second use could be done using only numpy. import numpy as np import scipy.linalg has_scipy = True except ImportError as e: has_scipy = False for n in 2, 3, 5, 6, 10: g = margulis_gabber_galil_graph(n) assert_equal(number_of_nodes(g), n*n) for node in g: assert_equal(g.degree(node), 8) assert_equal(len(node), 2) for i in node: assert_equal(int(i), i) assert_true(0 <= i < n) if has_scipy: # Eigenvalues are already sorted using the scipy eigvalsh, # but the implementation in numpy does not guarantee order. w = sorted(scipy.linalg.eigvalsh(adjacency_matrix(g).A)) assert_less(w[-2], 5*np.sqrt(2)) def test_chordal_cycle_graph(): """Test for the :func:`networkx.chordal_cycle_graph` function.""" if not is_scipy_available: raise SkipTest('SciPy is not available') primes = [3, 5, 7, 11] for p in primes: G = chordal_cycle_graph(p) assert_equal(len(G), p) # TODO The second largest eigenvalue should be smaller than a constant, # independent of the number of nodes in the graph: # # eigs = sorted(scipy.linalg.eigvalsh(adjacency_matrix(G).A)) # assert_less(eigs[-2], ...) # def test_margulis_gabber_galil_graph_badinput(): assert_raises(nx.NetworkXError, margulis_gabber_galil_graph, 3, nx.DiGraph()) assert_raises(nx.NetworkXError, margulis_gabber_galil_graph, 3, nx.Graph())
############################################################################### ## ## Copyright (C) 2014-2015, New York University. ## Copyright (C) 2011-2014, NYU-Poly. ## Copyright (C) 2006-2011, University of Utah. ## All rights reserved. ## Contact: contact@vistrails.org ## ## This file is part of VisTrails. ## ## "Redistribution and use in source and binary forms, with or without ## modification, are permitted provided that the following conditions are met: ## ## - Redistributions of source code must retain the above copyright notice, ## this list of conditions and the following disclaimer. ## - Redistributions in binary form must reproduce the above copyright ## notice, this list of conditions and the following disclaimer in the ## documentation and/or other materials provided with the distribution. ## - Neither the name of the New York University nor the names of its ## contributors may be used to endorse or promote products derived from ## this software without specific prior written permission. ## ## THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" ## AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, ## THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR ## PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR ## CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, ## EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, ## PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; ## OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, ## WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR ## OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ## ADVISED OF THE POSSIBILITY OF SUCH DAMAGE." ## ############################################################################### from __future__ import division from PyQt4 import QtCore, QtGui from vistrails.core.configuration import get_vistrails_configuration, \ get_vistrails_persistent_configuration from vistrails.core.system import systemType, vistrails_root_directory from vistrails.core.utils import versions_increasing from vistrails.gui.common_widgets import QDockPushButton from vistrails.gui.module_annotation import QModuleAnnotationTable from vistrails.gui.ports_pane import PortsList, letterIcon from vistrails.gui.version_prop import QVersionProp from vistrails.gui.vistrails_palette import QVistrailsPaletteInterface import os class QModuleInfo(QtGui.QWidget, QVistrailsPaletteInterface): def __init__(self, parent=None, flags=QtCore.Qt.Widget): QtGui.QWidget.__init__(self, parent, flags) self.ports_visible = True self.types_visible = True self.build_widget() self.controller = None self.module = None self.pipeline_view = None # pipeline_view self.read_only = False self.is_updating = False self.addButtonsToToolbar() def addButtonsToToolbar(self): # button for toggling executions eye_open_icon = \ QtGui.QIcon(os.path.join(vistrails_root_directory(), 'gui/resources/images/eye.png')) self.portVisibilityAction = QtGui.QAction(eye_open_icon, "Show/hide port visibility toggle buttons", None, triggered=self.showPortVisibility) self.portVisibilityAction.setCheckable(True) self.portVisibilityAction.setChecked(True) self.toolWindow().toolbar.insertAction(self.toolWindow().pinAction, self.portVisibilityAction) self.showTypesAction = QtGui.QAction(letterIcon('T'), "Show/hide type information", None, triggered=self.showTypes) self.showTypesAction.setCheckable(True) self.showTypesAction.setChecked(True) self.toolWindow().toolbar.insertAction(self.toolWindow().pinAction, self.showTypesAction) self.showEditsAction = QtGui.QAction( QtGui.QIcon(os.path.join(vistrails_root_directory(), 'gui/resources/images/pencil.png')), "Show/hide parameter widgets", None, triggered=self.showEdits) self.showEditsAction.setCheckable(True) self.showEditsAction.setChecked( get_vistrails_configuration().check('showInlineParameterWidgets')) self.toolWindow().toolbar.insertAction(self.toolWindow().pinAction, self.showEditsAction) def showPortVisibility(self, checked): self.ports_visible = checked self.update_module(self.module) def showTypes(self, checked): self.types_visible = checked self.update_module(self.module) def showEdits(self, checked): get_vistrails_configuration().showInlineParameterWidgets = checked get_vistrails_persistent_configuration().showInlineParameterWidgets = checked scene = self.controller.current_pipeline_scene scene.setupScene(self.controller.current_pipeline) def build_widget(self): name_label = QtGui.QLabel("Name:") self.name_edit = QtGui.QLineEdit() self.connect(self.name_edit, QtCore.SIGNAL('editingFinished()'), self.name_editing_finished) self.name_edit.setMinimumSize(50, 22) type_label = QtGui.QLabel("Type:") self.type_edit = QtGui.QLabel("") package_label = QtGui.QLabel("Package:") self.package_edit = QtGui.QLabel("") namespace_label = QtGui.QLabel("Namespace:") self.namespace_edit = QtGui.QLabel("") id = QtGui.QLabel("Id:") self.module_id = QtGui.QLabel("") self.configure_button = QDockPushButton("Configure") self.connect(self.configure_button, QtCore.SIGNAL('clicked()'), self.configure) self.doc_button = QDockPushButton("Documentation") self.connect(self.doc_button, QtCore.SIGNAL('clicked()'), self.documentation) layout = QtGui.QVBoxLayout() layout.setMargin(2) layout.setSpacing(4) def add_line(left, right): h_layout = QtGui.QHBoxLayout() h_layout.setMargin(2) h_layout.setSpacing(2) h_layout.setAlignment(QtCore.Qt.AlignLeft) h_layout.addWidget(left) h_layout.addWidget(right) h_widget = QtGui.QWidget() h_widget.setLayout(h_layout) h_widget.setSizePolicy(QtGui.QSizePolicy.Ignored, QtGui.QSizePolicy.Preferred) layout.addWidget(h_widget) add_line(name_label, self.name_edit) add_line(type_label, self.type_edit) add_line(package_label, self.package_edit) add_line(namespace_label, self.namespace_edit) add_line(id, self.module_id) h_layout = QtGui.QHBoxLayout() h_layout.setMargin(2) h_layout.setSpacing(5) h_layout.setAlignment(QtCore.Qt.AlignCenter) h_layout.addWidget(self.configure_button) h_layout.addWidget(self.doc_button) layout.addLayout(h_layout) self.tab_widget = QtGui.QTabWidget() # keep from overflowing on mac if systemType in ['Darwin']: self.tab_widget.tabBar().setStyleSheet('font-size: 12pt') # this causes a crash when undocking the palette in Mac OS X # see https://bugreports.qt-project.org/browse/QTBUG-16851 # self.tab_widget.setDocumentMode(True) self.input_ports_list = PortsList('input') self.tab_widget.addTab(self.input_ports_list, 'Inputs') self.output_ports_list = PortsList('output') self.tab_widget.addTab(self.output_ports_list, 'Outputs') self.ports_lists = [self.input_ports_list, self.output_ports_list] self.annotations = QModuleAnnotationTable() self.tab_widget.addTab(self.annotations, 'Annotations') layout.addWidget(self.tab_widget, 1) layout.setAlignment(QtCore.Qt.AlignTop) self.setLayout(layout) self.setWindowTitle('Module Info') def setReadOnly(self, read_only): if read_only != self.read_only: self.read_only = read_only for widget in self.ports_lists + [self.annotations]: widget.setReadOnly(read_only) def set_controller(self, controller): if self.controller == controller: return self.controller = controller for ports_list in self.ports_lists: ports_list.set_controller(controller) self.annotations.set_controller(controller) if self.controller is not None: scene = self.controller.current_pipeline_scene selected_ids = scene.get_selected_module_ids() modules = [self.controller.current_pipeline.modules[i] for i in selected_ids] if len(modules) == 1: self.update_module(modules[0]) else: self.update_module(None) else: self.update_module() def set_visible(self, enabled): if enabled and \ self.module is None and \ not self.toolWindow().isFloating() and \ not QVersionProp.instance().toolWindow().isFloating() and \ not self.toolWindow().visibleRegion().isEmpty(): QVersionProp.instance().set_visible(True) else: super(QModuleInfo, self).set_visible(enabled) def update_module(self, module=None): for plist in self.ports_lists: plist.types_visible = self.types_visible plist.ports_visible = self.ports_visible self.module = module for ports_list in self.ports_lists: ports_list.update_module(module) self.annotations.updateModule(module) if module is None: # We show the version properties tab if both are tabified and # self is visible if not self.toolWindow().isFloating() and \ not QVersionProp.instance().toolWindow().isFloating() and \ not self.toolWindow().visibleRegion().isEmpty(): QVersionProp.instance().set_visible(True) self.name_edit.setText("") if not versions_increasing(QtCore.QT_VERSION_STR, '4.7.0'): self.name_edit.setPlaceholderText("") # self.name_edit.setEnabled(False) self.type_edit.setText("") # self.type_edit.setEnabled(False) self.package_edit.setText("") self.namespace_edit.setText("") self.module_id.setText("") else: # We show self if both are tabified and # the version properties tab is visible if not self.toolWindow().isFloating() and \ not QVersionProp.instance().toolWindow().isFloating() and \ not QVersionProp.instance().toolWindow().visibleRegion().isEmpty(): self.set_visible(True) if module.has_annotation_with_key('__desc__'): label = module.get_annotation_by_key('__desc__').value.strip() else: label = '' self.name_edit.setText(label) if not label and not versions_increasing(QtCore.QT_VERSION_STR, '4.7.0'): self.name_edit.setPlaceholderText(self.module.name) self.type_edit.setText(self.module.name) self.package_edit.setText(self.module.package) if self.module.namespace is not None: self.namespace_edit.setText(self.module.namespace.replace('|', '/')) else: self.namespace_edit.setText('') self.module_id.setText('%d' % self.module.id) def name_editing_finished(self): # updating module may trigger a second call so we check for that if self.is_updating or self.module is None: return try: self.is_updating = True old_text = '' if self.module.has_annotation_with_key('__desc__'): old_text = self.module.get_annotation_by_key('__desc__').value new_text = str(self.name_edit.text()).strip() if not new_text: if old_text: self.controller.delete_annotation('__desc__', self.module.id) elif old_text != new_text: self.controller.add_annotation(('__desc__', new_text), self.module.id) scene = self.controller.current_pipeline_scene scene.recreate_module(self.controller.current_pipeline, self.module.id) finally: self.is_updating = False def configure(self): from vistrails.gui.vistrails_window import _app _app.configure_module() def documentation(self): from vistrails.gui.vistrails_window import _app _app.show_documentation() def update_entry_klass(self, entry_klass): self.input_ports_list.set_entry_klass(entry_klass) def show_annotations(self): if self.module is not None: self.tab_widget.setCurrentWidget(self.annotations) self.annotations.editNextAvailableCell()
# Copyright 2013 X35 # # This file is part of gamemod. # # gamemod is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # gamemod is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with gamemod. If not, see <http:#www.gnu.org/licenses/>. from requestcounter import requestcounter from debug import debug # provide the gamemod gui & server list class guiprovider: FILECHECK_INTERVAL = 60*60 # 1h DBGTAG = "guiprovider" DBGTAG_REQUEST = DBGTAG+"/request" DBGTAG_REPLY = DBGTAG+"/reply" LIST_REQUEST = "list" READABLELIST_REQUEST = "readablelist" def __init__(self, reqfunc): self.reqfunc = reqfunc self.counter = requestcounter() def request(self, readable=False): return self.reqfunc(readable) def onrequest(self, line, addr, build): # return (reply, close) if line == guiprovider.LIST_REQUEST: debug.msg(guiprovider.DBGTAG_REQUEST, "%s request from %s:%d (%sbuild)" % ((line,)+addr+("" if build else "don't ",))) self.counter.add(addr[0]) s = (self.request() if build else True) debug.msg(guiprovider.DBGTAG_REQUEST, "sending reply to %s request to %s:%d" % ((line,)+addr)) return s, True elif line == guiprovider.READABLELIST_REQUEST: debug.msg(guiprovider.DBGTAG_REQUEST, "%s request from %s:%d (%sbuild)" % ((line,)+addr+("" if build else "don't ",))) s = (self.request(True) if build else True) debug.msg(guiprovider.DBGTAG_REQUEST, "sending reply to %s request to %s:%d" % ((line,)+addr)) return s, True return None, False def differentips(self): return self.counter.differentips() def requests(self): return self.counter.requests()
# acl.py - changeset access control for mercurial # # Copyright 2006 Vadim Gelfer <vadim.gelfer@gmail.com> # # This software may be used and distributed according to the terms of the # GNU General Public License version 2 or any later version. '''hooks for controlling repository access This hook makes it possible to allow or deny write access to given branches and paths of a repository when receiving incoming changesets via pretxnchangegroup and pretxncommit. The authorization is matched based on the local user name on the system where the hook runs, and not the committer of the original changeset (since the latter is merely informative). The acl hook is best used along with a restricted shell like hgsh, preventing authenticating users from doing anything other than pushing or pulling. The hook is not safe to use if users have interactive shell access, as they can then disable the hook. Nor is it safe if remote users share an account, because then there is no way to distinguish them. The order in which access checks are performed is: 1) Deny list for branches (section ``acl.deny.branches``) 2) Allow list for branches (section ``acl.allow.branches``) 3) Deny list for paths (section ``acl.deny``) 4) Allow list for paths (section ``acl.allow``) The allow and deny sections take key-value pairs. Branch-based Access Control --------------------------- Use the ``acl.deny.branches`` and ``acl.allow.branches`` sections to have branch-based access control. Keys in these sections can be either: - a branch name, or - an asterisk, to match any branch; The corresponding values can be either: - a comma-separated list containing users and groups, or - an asterisk, to match anyone; You can add the "!" prefix to a user or group name to invert the sense of the match. Path-based Access Control ------------------------- Use the ``acl.deny`` and ``acl.allow`` sections to have path-based access control. Keys in these sections accept a subtree pattern (with a glob syntax by default). The corresponding values follow the same syntax as the other sections above. Groups ------ Group names must be prefixed with an ``@`` symbol. Specifying a group name has the same effect as specifying all the users in that group. You can define group members in the ``acl.groups`` section. If a group name is not defined there, and Mercurial is running under a Unix-like system, the list of users will be taken from the OS. Otherwise, an exception will be raised. Example Configuration --------------------- :: [hooks] # Use this if you want to check access restrictions at commit time pretxncommit.acl = python:hgext.acl.hook # Use this if you want to check access restrictions for pull, push, # bundle and serve. pretxnchangegroup.acl = python:hgext.acl.hook [acl] # Allow or deny access for incoming changes only if their source is # listed here, let them pass otherwise. Source is "serve" for all # remote access (http or ssh), "push", "pull" or "bundle" when the # related commands are run locally. # Default: serve sources = serve [acl.deny.branches] # Everyone is denied to the frozen branch: frozen-branch = * # A bad user is denied on all branches: * = bad-user [acl.allow.branches] # A few users are allowed on branch-a: branch-a = user-1, user-2, user-3 # Only one user is allowed on branch-b: branch-b = user-1 # The super user is allowed on any branch: * = super-user # Everyone is allowed on branch-for-tests: branch-for-tests = * [acl.deny] # This list is checked first. If a match is found, acl.allow is not # checked. All users are granted access if acl.deny is not present. # Format for both lists: glob pattern = user, ..., @group, ... # To match everyone, use an asterisk for the user: # my/glob/pattern = * # user6 will not have write access to any file: ** = user6 # Group "hg-denied" will not have write access to any file: ** = @hg-denied # Nobody will be able to change "DONT-TOUCH-THIS.txt", despite # everyone being able to change all other files. See below. src/main/resources/DONT-TOUCH-THIS.txt = * [acl.allow] # if acl.allow is not present, all users are allowed by default # empty acl.allow = no users allowed # User "doc_writer" has write access to any file under the "docs" # folder: docs/** = doc_writer # User "jack" and group "designers" have write access to any file # under the "images" folder: images/** = jack, @designers # Everyone (except for "user6" and "@hg-denied" - see acl.deny above) # will have write access to any file under the "resources" folder # (except for 1 file. See acl.deny): src/main/resources/** = * .hgtags = release_engineer Examples using the "!" prefix ............................. Suppose there's a branch that only a given user (or group) should be able to push to, and you don't want to restrict access to any other branch that may be created. The "!" prefix allows you to prevent anyone except a given user or group to push changesets in a given branch or path. In the examples below, we will: 1) Deny access to branch "ring" to anyone but user "gollum" 2) Deny access to branch "lake" to anyone but members of the group "hobbit" 3) Deny access to a file to anyone but user "gollum" :: [acl.allow.branches] # Empty [acl.deny.branches] # 1) only 'gollum' can commit to branch 'ring'; # 'gollum' and anyone else can still commit to any other branch. ring = !gollum # 2) only members of the group 'hobbit' can commit to branch 'lake'; # 'hobbit' members and anyone else can still commit to any other branch. lake = !@hobbit # You can also deny access based on file paths: [acl.allow] # Empty [acl.deny] # 3) only 'gollum' can change the file below; # 'gollum' and anyone else can still change any other file. /misty/mountains/cave/ring = !gollum ''' from mercurial.i18n import _ from mercurial import util, match import getpass, urllib testedwith = 'internal' def _getusers(ui, group): # First, try to use group definition from section [acl.groups] hgrcusers = ui.configlist('acl.groups', group) if hgrcusers: return hgrcusers ui.debug('acl: "%s" not defined in [acl.groups]\n' % group) # If no users found in group definition, get users from OS-level group try: return util.groupmembers(group) except KeyError: raise util.Abort(_("group '%s' is undefined") % group) def _usermatch(ui, user, usersorgroups): if usersorgroups == '*': return True for ug in usersorgroups.replace(',', ' ').split(): if ug.startswith('!'): # Test for excluded user or group. Format: # if ug is a user name: !username # if ug is a group name: !@groupname ug = ug[1:] if not ug.startswith('@') and user != ug \ or ug.startswith('@') and user not in _getusers(ui, ug[1:]): return True # Test for user or group. Format: # if ug is a user name: username # if ug is a group name: @groupname elif user == ug \ or ug.startswith('@') and user in _getusers(ui, ug[1:]): return True return False def buildmatch(ui, repo, user, key): '''return tuple of (match function, list enabled).''' if not ui.has_section(key): ui.debug('acl: %s not enabled\n' % key) return None pats = [pat for pat, users in ui.configitems(key) if _usermatch(ui, user, users)] ui.debug('acl: %s enabled, %d entries for user %s\n' % (key, len(pats), user)) # Branch-based ACL if not repo: if pats: # If there's an asterisk (meaning "any branch"), always return True; # Otherwise, test if b is in pats if '*' in pats: return util.always return lambda b: b in pats return util.never # Path-based ACL if pats: return match.match(repo.root, '', pats) return util.never def hook(ui, repo, hooktype, node=None, source=None, **kwargs): if hooktype not in ['pretxnchangegroup', 'pretxncommit']: raise util.Abort(_('config error - hook type "%s" cannot stop ' 'incoming changesets nor commits') % hooktype) if (hooktype == 'pretxnchangegroup' and source not in ui.config('acl', 'sources', 'serve').split()): ui.debug('acl: changes have source "%s" - skipping\n' % source) return user = None if source == 'serve' and 'url' in kwargs: url = kwargs['url'].split(':') if url[0] == 'remote' and url[1].startswith('http'): user = urllib.unquote(url[3]) if user is None: user = getpass.getuser() ui.debug('acl: checking access for user "%s"\n' % user) cfg = ui.config('acl', 'config') if cfg: ui.readconfig(cfg, sections = ['acl.groups', 'acl.allow.branches', 'acl.deny.branches', 'acl.allow', 'acl.deny']) allowbranches = buildmatch(ui, None, user, 'acl.allow.branches') denybranches = buildmatch(ui, None, user, 'acl.deny.branches') allow = buildmatch(ui, repo, user, 'acl.allow') deny = buildmatch(ui, repo, user, 'acl.deny') for rev in xrange(repo[node], len(repo)): ctx = repo[rev] branch = ctx.branch() if denybranches and denybranches(branch): raise util.Abort(_('acl: user "%s" denied on branch "%s"' ' (changeset "%s")') % (user, branch, ctx)) if allowbranches and not allowbranches(branch): raise util.Abort(_('acl: user "%s" not allowed on branch "%s"' ' (changeset "%s")') % (user, branch, ctx)) ui.debug('acl: branch access granted: "%s" on branch "%s"\n' % (ctx, branch)) for f in ctx.files(): if deny and deny(f): raise util.Abort(_('acl: user "%s" denied on "%s"' ' (changeset "%s")') % (user, f, ctx)) if allow and not allow(f): raise util.Abort(_('acl: user "%s" not allowed on "%s"' ' (changeset "%s")') % (user, f, ctx)) ui.debug('acl: path access granted: "%s"\n' % ctx)
#!/usr/bin/env python # # Copyright 2007 Google Inc. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # """Search API module.""" from search import AtomField from search import Cursor from search import DateField from search import DeleteError from search import DeleteResult from search import Document from search import DOCUMENT_ID_FIELD_NAME from search import Error from search import ExpressionError from search import Field from search import FieldExpression from search import GeoField from search import GeoPoint from search import get_indexes from search import GetResponse from search import HtmlField from search import Index from search import InternalError from search import InvalidRequest from search import LANGUAGE_FIELD_NAME from search import MatchScorer from search import MAXIMUM_DOCUMENT_ID_LENGTH from search import MAXIMUM_DOCUMENTS_PER_PUT_REQUEST from search import MAXIMUM_DOCUMENTS_RETURNED_PER_SEARCH from search import MAXIMUM_EXPRESSION_LENGTH from search import MAXIMUM_FIELD_ATOM_LENGTH from search import MAXIMUM_FIELD_NAME_LENGTH from search import MAXIMUM_FIELD_VALUE_LENGTH from search import MAXIMUM_FIELDS_RETURNED_PER_SEARCH from search import MAXIMUM_GET_INDEXES_OFFSET from search import MAXIMUM_INDEX_NAME_LENGTH from search import MAXIMUM_INDEXES_RETURNED_PER_GET_REQUEST from search import MAXIMUM_NUMBER_FOUND_ACCURACY from search import MAXIMUM_QUERY_LENGTH from search import MAXIMUM_SEARCH_OFFSET from search import MAXIMUM_SORTED_DOCUMENTS from search import NumberField from search import OperationResult from search import PutError from search import PutResult from search import Query from search import QueryError from search import QueryOptions from search import RANK_FIELD_NAME from search import RescoringMatchScorer from search import SCORE_FIELD_NAME from search import ScoredDocument from search import SearchResults from search import SortExpression from search import SortOptions from search import TextField from search import TIMESTAMP_FIELD_NAME from search import TransientError
# Copyright (C) 2014 eNovance SAS <licensing@enovance.com> # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from oslo_log import log as logging import oslo_messaging from neutron.common import constants as consts from neutron.common import utils from neutron.i18n import _LE from neutron import manager from neutron.plugins.common import constants as service_constants LOG = logging.getLogger(__name__) class MeteringRpcCallbacks(object): target = oslo_messaging.Target(version='1.0') def __init__(self, meter_plugin): self.meter_plugin = meter_plugin def get_sync_data_metering(self, context, **kwargs): l3_plugin = manager.NeutronManager.get_service_plugins().get( service_constants.L3_ROUTER_NAT) if not l3_plugin: return host = kwargs.get('host') if not utils.is_extension_supported( l3_plugin, consts.L3_AGENT_SCHEDULER_EXT_ALIAS) or not host: return self.meter_plugin.get_sync_data_metering(context) else: agents = l3_plugin.get_l3_agents(context, filters={'host': [host]}) if not agents: LOG.error(_LE('Unable to find agent %s.'), host) return routers = l3_plugin.list_routers_on_l3_agent(context, agents[0].id) router_ids = [router['id'] for router in routers['routers']] if not router_ids: return return self.meter_plugin.get_sync_data_metering(context, router_ids=router_ids)
#!/usr/bin/env python # Copyright 2015 The Kubernetes Authors All rights reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. from mock import patch, Mock, MagicMock from path import Path import pytest import sys # Munge the python path so we can find our hook code d = Path('__file__').parent.abspath() / 'hooks' sys.path.insert(0, d.abspath()) # Import the modules from the hook import install class TestInstallHook(): @patch('install.path') def test_update_rc_files(self, pmock): """ Test happy path on updating env files. Assuming everything exists and is in place. """ pmock.return_value.lines.return_value = ['line1', 'line2'] install.update_rc_files(['test1', 'test2']) pmock.return_value.write_lines.assert_called_with(['line1', 'line2', 'test1', 'test2']) def test_update_rc_files_with_nonexistant_path(self): """ Test an unhappy path if the bashrc/users do not exist. """ with pytest.raises(OSError) as exinfo: install.update_rc_files(['test1','test2']) @patch('install.fetch') @patch('install.hookenv') def test_package_installation(self, hemock, ftmock): """ Verify we are calling the known essentials to build and syndicate kubes. """ pkgs = ['build-essential', 'git', 'make', 'nginx', 'python-pip'] install.install_packages() hemock.log.assert_called_with('Installing Debian packages') ftmock.filter_installed_packages.assert_called_with(pkgs) @patch('install.archiveurl.ArchiveUrlFetchHandler') def test_go_download(self, aumock): """ Test that we are actually handing off to charm-helpers to download a specific archive of Go. This is non-configurable so its reasonably safe to assume we're going to always do this, and when it changes we shall curse the brittleness of this test. """ ins_mock = aumock.return_value.install install.download_go() url = 'https://storage.googleapis.com/golang/go1.4.2.linux-amd64.tar.gz' sha1='5020af94b52b65cc9b6f11d50a67e4bae07b0aff' ins_mock.assert_called_with(url, '/usr/local', sha1, 'sha1') @patch('install.subprocess') def test_clone_repository(self, spmock): """ We're not using a unit-tested git library - so ensure our subprocess call is consistent. If we change this, we want to know we've broken it. """ install.clone_repository() repo = 'https://github.com/GoogleCloudPlatform/kubernetes.git' direct = '/opt/kubernetes' spmock.check_output.assert_called_with(['git', 'clone', repo, direct]) @patch('install.install_packages') @patch('install.download_go') @patch('install.clone_repository') @patch('install.update_rc_files') @patch('install.hookenv') def test_install_main(self, hemock, urmock, crmock, dgmock, ipmock): """ Ensure the driver/main method is calling all the supporting methods. """ strings = [ 'export GOROOT=/usr/local/go\n', 'export PATH=$PATH:$GOROOT/bin\n', 'export KUBE_MASTER_IP=0.0.0.0\n', 'export KUBERNETES_MASTER=http://$KUBE_MASTER_IP\n', ] install.install() crmock.assert_called_once() dgmock.assert_called_once() crmock.assert_called_once() urmock.assert_called_with(strings) hemock.open_port.assert_called_with(8080)
import netCDF4 import numpy from hamcrest import assert_that, is_ import unittest from cis.cis_main import evaluate_cmd, col_cmd from cis.test.integration.base_integration_test import BaseIntegrationTest from cis.test.integration_test_data import * from cis.parse import parse_args from cis.test.unit.eval.test_calc import compare_masked_arrays class TestEval(BaseIntegrationTest): def test_Aeronet_wavelength_calculation(self): # Example from the CIS Phase 3 Software spec: # ... a user should be able to write a plugin to calculate the Aeronet AOD at 550nm from the AOD at 500 nm as # AOD550 = AOD500 * (550/500)^(-1*Angstrom500-870)" # Takes 3s args = ['eval', 'AOT_500,500-870Angstrom=a550to870:' + escape_colons(another_valid_aeronet_filename), 'AOT_500 * (550.0/500)**(-1*a550to870)', '1', '-o', self.OUTPUT_FILENAME] arguments = parse_args(args) evaluate_cmd(arguments) # Check correct: self.ds = netCDF4.Dataset(self.OUTPUT_FILENAME) calculated_result = self.ds.variables['calculated_variable'][:] expected_result = [0.2341039087, 0.2285401152, 0.2228799533, 0.1953746746, 0.2094051561, 0.1696889668, 0.3137791803, 0.2798929273, 0.1664194279, 0.1254619092, 0.1258309124, 0.1496960031, 0.0768447737, 0.0550896430, 0.0534543107, 0.0538315909, 0.0666742975, 0.0512935449, 0.0699585189, 0.0645033944] assert_that(calculated_result.shape, is_((3140,))) assert numpy.allclose(expected_result, calculated_result[0:20]) def test_ECHAMHAM_wavelength_sum(self): args = ['eval', "%s,%s:%s" % (valid_echamham_variable_1, valid_echamham_variable_2, escape_colons(valid_echamham_filename)), '%s+%s' % (valid_echamham_variable_1, valid_echamham_variable_2), '1', '-o', self.OUTPUT_FILENAME] arguments = parse_args(args) evaluate_cmd(arguments) # Check correct: self.ds = netCDF4.Dataset(self.OUTPUT_FILENAME) calculated_result = self.ds.variables['calculated_variable'][:] # A hand calculated selection of values expected_result = [0.007633533, 0.007646653, 0.007749859, 0.007744226, 0.007761176] assert_that(calculated_result.shape, is_((96, 192))) assert numpy.allclose(expected_result, calculated_result[:][0][0:5]) def test_collocated_NetCDF_Gridded_onto_GASSP(self): # First do a collocation of ECHAMHAM onto GASSP vars = valid_echamham_variable_1, valid_echamham_variable_2 filename = escape_colons(valid_echamham_filename) sample_file = escape_colons(valid_GASSP_aeroplane_filename) sample_var = valid_GASSP_aeroplane_variable collocator_and_opts = 'nn[missing_data_for_missing_sample=True],variable=%s' % sample_var arguments = ['col', ",".join(vars) + ':' + filename, sample_file + ':collocator=' + collocator_and_opts, '-o', 'collocated_gassp'] main_arguments = parse_args(arguments) col_cmd(main_arguments) # Check collocation is the same self.ds = netCDF4.Dataset('collocated_gassp.nc') col_var1 = self.ds.variables[valid_echamham_variable_1][:] col_var2 = self.ds.variables[valid_echamham_variable_2][:] # A hand calculated selection of values expected_col1 = numpy.ma.masked_invalid( [float('Nan'), float('Nan'), float('Nan'), 0.0814601778984, 0.0814601778984]) compare_masked_arrays(expected_col1, col_var1[:][0:5]) expected_col2 = numpy.ma.masked_invalid( [float('Nan'), float('Nan'), float('Nan'), 0.0741240680218, 0.0741240680218]) compare_masked_arrays(expected_col2, col_var2[:][0:5]) # Then do an evaluation using the collocated data: args = ['eval', "%s,%s:%s" % (valid_echamham_variable_1, valid_echamham_variable_2, 'collocated_gassp.nc'), "%s=gassp_alias:%s" % (valid_GASSP_aeroplane_variable, escape_colons(valid_GASSP_aeroplane_filename)), "(%s + %s) / gassp_alias " % (valid_echamham_variable_1, valid_echamham_variable_2), '1', '-o', self.OUTPUT_FILENAME] arguments = parse_args(args) evaluate_cmd(arguments) self.ds.close() # Check correct self.ds = netCDF4.Dataset(self.OUTPUT_FILENAME) calculated_result = self.ds.variables['calculated_variable'][:] # A hand calculated selection of values expected_result = numpy.ma.masked_invalid([0.00196121983491, 0.00197255626472, 0.00120850731992]) assert_that(calculated_result.shape, is_((311,))) # Check the first 3 vald values compare_masked_arrays(expected_result, calculated_result[:][10:13]) os.remove('collocated_gassp.nc') @skip_pyhdf def test_CloudSat(self): args = ['eval', "%s,%s:%s" % (valid_cloudsat_RVOD_sdata_variable, valid_cloudsat_RVOD_vdata_variable, escape_colons(valid_cloudsat_RVOD_file)), '%s/%s' % (valid_cloudsat_RVOD_sdata_variable, valid_cloudsat_RVOD_vdata_variable), 'ppm', '-o', 'cloudsat_var:' + self.OUTPUT_FILENAME] arguments = parse_args(args) evaluate_cmd(arguments) self.ds = netCDF4.Dataset(self.OUTPUT_FILENAME) assert_that(self.ds.variables['cloudsat_var'].units, is_('ppm')) def test_can_specify_output_variable(self): args = ['eval', "%s,%s:%s" % (valid_echamham_variable_1, valid_echamham_variable_2, escape_colons(valid_echamham_filename)), '%s+%s' % (valid_echamham_variable_1, valid_echamham_variable_2), 'kg m^-3', '-o', 'var_out:' + self.OUTPUT_FILENAME] arguments = parse_args(args) evaluate_cmd(arguments) self.ds = netCDF4.Dataset(self.OUTPUT_FILENAME) assert 'var_out' in self.ds.variables def test_can_specify_attributes_gridded(self): args = ['eval', "%s,%s:%s" % (valid_echamham_variable_1, valid_echamham_variable_2, escape_colons(valid_echamham_filename)), '%s+%s' % (valid_echamham_variable_1, valid_echamham_variable_2), 'kg m^-3', '-o', 'var_out:' + self.OUTPUT_FILENAME, '-a', 'att1=val1,att2=val2'] arguments = parse_args(args) evaluate_cmd(arguments) self.ds = netCDF4.Dataset(self.OUTPUT_FILENAME) assert_that(self.ds.variables['var_out'].att1, is_('val1')) assert_that(self.ds.variables['var_out'].att2, is_('val2')) def test_can_specify_units_gridded(self): args = ['eval', "%s,%s:%s" % (valid_echamham_variable_1, valid_echamham_variable_2, escape_colons(valid_echamham_filename)), '%s+%s' % (valid_echamham_variable_1, valid_echamham_variable_2), 'kg m^-3', '-o', 'var_out:' + self.OUTPUT_FILENAME, '-a', 'att1=val1,att2=val2'] arguments = parse_args(args) evaluate_cmd(arguments) self.ds = netCDF4.Dataset(self.OUTPUT_FILENAME) assert_that(self.ds.variables['var_out'].units, is_('kg m^-3')) def test_can_specify_units_gridded_no_output_var(self): args = ['eval', "%s:%s" % (valid_hadgem_variable, escape_colons(valid_hadgem_filename)), "od550aer", "ppm", "-o", self.OUTPUT_FILENAME, "-a", "att1=val1"] arguments = parse_args(args) evaluate_cmd(arguments) self.ds = netCDF4.Dataset(self.OUTPUT_FILENAME) assert_that(self.ds.variables['calculated_variable'].units, is_('ppm')) assert_that(self.ds.variables['calculated_variable'].att1, is_('val1'))
""" Wrapper for loading templates from "templates" directories in INSTALLED_APPS packages. """ import io from django.core.exceptions import SuspiciousFileOperation from django.template.base import TemplateDoesNotExist from django.template.utils import get_app_template_dirs from django.utils._os import safe_join from .base import Loader as BaseLoader class Loader(BaseLoader): is_usable = True def get_template_sources(self, template_name, template_dirs=None): """ Returns the absolute paths to "template_name", when appended to each directory in "template_dirs". Any paths that don't lie inside one of the template dirs are excluded from the result set, for security reasons. """ if not template_dirs: template_dirs = get_app_template_dirs('templates') for template_dir in template_dirs: try: yield safe_join(template_dir, template_name) except SuspiciousFileOperation: # The joined path was located outside of this template_dir # (it might be inside another one, so this isn't fatal). pass def load_template_source(self, template_name, template_dirs=None): for filepath in self.get_template_sources(template_name, template_dirs): try: with io.open(filepath, encoding=self.engine.file_charset) as fp: return fp.read(), filepath except IOError: pass raise TemplateDoesNotExist(template_name)
# -*- coding: utf-8 -*- from __future__ import unicode_literals from django.db import migrations, models class Migration(migrations.Migration): dependencies = [ ('sites', '0001_initial'), ] operations = [ migrations.CreateModel( name='Redirect', fields=[ ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)), ('site', models.ForeignKey(to='sites.Site', to_field='id')), ('old_path', models.CharField(help_text="This should be an absolute path, excluding the domain name. Example: '/events/search/'.", max_length=200, verbose_name='redirect from', db_index=True)), ('new_path', models.CharField(help_text="This can be either an absolute path (as above) or a full URL starting with 'http://'.", max_length=200, verbose_name='redirect to', blank=True)), ], options={ 'ordering': ('old_path',), 'unique_together': set([('site', 'old_path')]), 'db_table': 'django_redirect', 'verbose_name': 'redirect', 'verbose_name_plural': 'redirects', }, bases=(models.Model,), ), ]
from __future__ import unicode_literals from operator import attrgetter from django.db import connection from django.test import TestCase, skipIfDBFeature, skipUnlessDBFeature from django.test.utils import override_settings from .models import Country, Restaurant, Pizzeria, State, TwoFields class BulkCreateTests(TestCase): def setUp(self): self.data = [ Country(name="United States of America", iso_two_letter="US"), Country(name="The Netherlands", iso_two_letter="NL"), Country(name="Germany", iso_two_letter="DE"), Country(name="Czech Republic", iso_two_letter="CZ") ] def test_simple(self): created = Country.objects.bulk_create(self.data) self.assertEqual(len(created), 4) self.assertQuerysetEqual(Country.objects.order_by("-name"), [ "United States of America", "The Netherlands", "Germany", "Czech Republic" ], attrgetter("name")) created = Country.objects.bulk_create([]) self.assertEqual(created, []) self.assertEqual(Country.objects.count(), 4) @skipUnlessDBFeature('has_bulk_insert') def test_efficiency(self): with self.assertNumQueries(1): Country.objects.bulk_create(self.data) def test_inheritance(self): Restaurant.objects.bulk_create([ Restaurant(name="Nicholas's") ]) self.assertQuerysetEqual(Restaurant.objects.all(), [ "Nicholas's", ], attrgetter("name")) with self.assertRaises(ValueError): Pizzeria.objects.bulk_create([ Pizzeria(name="The Art of Pizza") ]) self.assertQuerysetEqual(Pizzeria.objects.all(), []) self.assertQuerysetEqual(Restaurant.objects.all(), [ "Nicholas's", ], attrgetter("name")) def test_non_auto_increment_pk(self): State.objects.bulk_create([ State(two_letter_code=s) for s in ["IL", "NY", "CA", "ME"] ]) self.assertQuerysetEqual(State.objects.order_by("two_letter_code"), [ "CA", "IL", "ME", "NY", ], attrgetter("two_letter_code")) @skipUnlessDBFeature('has_bulk_insert') def test_non_auto_increment_pk_efficiency(self): with self.assertNumQueries(1): State.objects.bulk_create([ State(two_letter_code=s) for s in ["IL", "NY", "CA", "ME"] ]) self.assertQuerysetEqual(State.objects.order_by("two_letter_code"), [ "CA", "IL", "ME", "NY", ], attrgetter("two_letter_code")) @skipIfDBFeature('allows_primary_key_0') def test_zero_as_autoval(self): """ Zero as id for AutoField should raise exception in MySQL, because MySQL does not allow zero for automatic primary key. """ valid_country = Country(name='Germany', iso_two_letter='DE') invalid_country = Country(id=0, name='Poland', iso_two_letter='PL') with self.assertRaises(ValueError): Country.objects.bulk_create([valid_country, invalid_country]) def test_batch_same_vals(self): # Sqlite had a problem where all the same-valued models were # collapsed to one insert. Restaurant.objects.bulk_create([ Restaurant(name='foo') for i in range(0, 2) ]) self.assertEqual(Restaurant.objects.count(), 2) def test_large_batch(self): with override_settings(DEBUG=True): connection.queries = [] TwoFields.objects.bulk_create([ TwoFields(f1=i, f2=i+1) for i in range(0, 1001) ]) self.assertEqual(TwoFields.objects.count(), 1001) self.assertEqual( TwoFields.objects.filter(f1__gte=450, f1__lte=550).count(), 101) self.assertEqual(TwoFields.objects.filter(f2__gte=901).count(), 101) @skipUnlessDBFeature('has_bulk_insert') def test_large_single_field_batch(self): # SQLite had a problem with more than 500 UNIONed selects in single # query. Restaurant.objects.bulk_create([ Restaurant() for i in range(0, 501) ]) @skipUnlessDBFeature('has_bulk_insert') def test_large_batch_efficiency(self): with override_settings(DEBUG=True): connection.queries = [] TwoFields.objects.bulk_create([ TwoFields(f1=i, f2=i+1) for i in range(0, 1001) ]) self.assertTrue(len(connection.queries) < 10) def test_large_batch_mixed(self): """ Test inserting a large batch with objects having primary key set mixed together with objects without PK set. """ with override_settings(DEBUG=True): connection.queries = [] TwoFields.objects.bulk_create([ TwoFields(id=i if i % 2 == 0 else None, f1=i, f2=i+1) for i in range(100000, 101000)]) self.assertEqual(TwoFields.objects.count(), 1000) # We can't assume much about the ID's created, except that the above # created IDs must exist. id_range = range(100000, 101000, 2) self.assertEqual(TwoFields.objects.filter(id__in=id_range).count(), 500) self.assertEqual(TwoFields.objects.exclude(id__in=id_range).count(), 500) @skipUnlessDBFeature('has_bulk_insert') def test_large_batch_mixed_efficiency(self): """ Test inserting a large batch with objects having primary key set mixed together with objects without PK set. """ with override_settings(DEBUG=True): connection.queries = [] TwoFields.objects.bulk_create([ TwoFields(id=i if i % 2 == 0 else None, f1=i, f2=i+1) for i in range(100000, 101000)]) self.assertTrue(len(connection.queries) < 10) def test_explicit_batch_size(self): objs = [TwoFields(f1=i, f2=i) for i in range(0, 4)] TwoFields.objects.bulk_create(objs, 2) self.assertEqual(TwoFields.objects.count(), len(objs)) TwoFields.objects.all().delete() TwoFields.objects.bulk_create(objs, len(objs)) self.assertEqual(TwoFields.objects.count(), len(objs)) @skipUnlessDBFeature('has_bulk_insert') def test_explicit_batch_size_efficiency(self): objs = [TwoFields(f1=i, f2=i) for i in range(0, 100)] with self.assertNumQueries(2): TwoFields.objects.bulk_create(objs, 50) TwoFields.objects.all().delete() with self.assertNumQueries(1): TwoFields.objects.bulk_create(objs, len(objs))
# mysql/gaerdbms.py # Copyright (C) 2005-2015 the SQLAlchemy authors and contributors # <see AUTHORS file> # # This module is part of SQLAlchemy and is released under # the MIT License: http://www.opensource.org/licenses/mit-license.php """ .. dialect:: mysql+gaerdbms :name: Google Cloud SQL :dbapi: rdbms :connectstring: mysql+gaerdbms:///<dbname>?instance=<instancename> :url: https://developers.google.com/appengine/docs/python/cloud-sql/\ developers-guide This dialect is based primarily on the :mod:`.mysql.mysqldb` dialect with minimal changes. .. versionadded:: 0.7.8 .. deprecated:: 1.0 This dialect is **no longer necessary** for Google Cloud SQL; the MySQLdb dialect can be used directly. Cloud SQL now recommends creating connections via the mysql dialect using the URL format ``mysql+mysqldb://root@/<dbname>?unix_socket=/cloudsql/<projectid>:<instancename>`` Pooling ------- Google App Engine connections appear to be randomly recycled, so the dialect does not pool connections. The :class:`.NullPool` implementation is installed within the :class:`.Engine` by default. """ import os from .mysqldb import MySQLDialect_mysqldb from ...pool import NullPool import re from sqlalchemy.util import warn_deprecated def _is_dev_environment(): return os.environ.get('SERVER_SOFTWARE', '').startswith('Development/') class MySQLDialect_gaerdbms(MySQLDialect_mysqldb): @classmethod def dbapi(cls): warn_deprecated( "Google Cloud SQL now recommends creating connections via the " "MySQLdb dialect directly, using the URL format " "mysql+mysqldb://root@/<dbname>?unix_socket=/cloudsql/" "<projectid>:<instancename>" ) # from django: # http://code.google.com/p/googleappengine/source/ # browse/trunk/python/google/storage/speckle/ # python/django/backend/base.py#118 # see also [ticket:2649] # see also http://stackoverflow.com/q/14224679/34549 from google.appengine.api import apiproxy_stub_map if _is_dev_environment(): from google.appengine.api import rdbms_mysqldb return rdbms_mysqldb elif apiproxy_stub_map.apiproxy.GetStub('rdbms'): from google.storage.speckle.python.api import rdbms_apiproxy return rdbms_apiproxy else: from google.storage.speckle.python.api import rdbms_googleapi return rdbms_googleapi @classmethod def get_pool_class(cls, url): # Cloud SQL connections die at any moment return NullPool def create_connect_args(self, url): opts = url.translate_connect_args() if not _is_dev_environment(): # 'dsn' and 'instance' are because we are skipping # the traditional google.api.rdbms wrapper opts['dsn'] = '' opts['instance'] = url.query['instance'] return [], opts def _extract_error_code(self, exception): match = re.compile(r"^(\d+)L?:|^\((\d+)L?,").match(str(exception)) # The rdbms api will wrap then re-raise some types of errors # making this regex return no matches. code = match.group(1) or match.group(2) if match else None if code: return int(code) dialect = MySQLDialect_gaerdbms
#!/usr/bin/env python """ Take a imgapi_images-*.gz manatee table dump and emit a JSON array of images. Usage: gzcat imgapi_images-2014-11-15-00-01-56.gz | ./manatee2images.py > images.json """ import json import sys import operator from pprint import pprint import codecs # TODO: ideally we wouldn't hardcode types here. This should come from # the imgapi_images bucket definition. type_from_key = { 'billing_tags': 'array', 'published_at': 'string', 'acl': 'array', 'public': 'bool', } def update_img_from_index(img, entry, header, key): try: type = type_from_key[key] idx = header.index(key) # cache this? val = entry[idx] # Postgres NULL if val == '\\N': if key in img: del img[key] return if type == 'array' and val.startswith('{') and val.endswith('}'): # Hack parsing of postgres arrays. val = [tag for tag in val[1:-1].split(',') if tag] elif type == 'bool': if val == 't': val = True elif val == 'f': val = False else: raise RuntimeError( 'unexpected index value for "%s" bool field: %r' % (key, val)) img[key] = val except ValueError: pass header = None published_at_idx = None acl_idx = None imgs = [] for line in sys.stdin: if header is None: header = json.loads(line)['keys'] assert header[3] == '_value' continue entry = json.loads(line)['entry'] img = json.loads(entry[3]) # Apply some of the index values. # TODO: eventually should do all of these for key in ['billing_tags', 'published_at', 'acl', 'public']: update_img_from_index(img, entry, header, key) imgs.append(img) imgs.sort(key=operator.itemgetter('uuid')) print json.dumps(imgs, sort_keys=True, indent=4)
""" Tools for converting old- to new-style metadata. """ from collections import namedtuple from .pkginfo import read_pkg_info from .util import OrderedDefaultDict try: from collections import OrderedDict except ImportError: OrderedDict = dict import re import os.path import textwrap import pkg_resources import email.parser from . import __version__ as wheel_version METADATA_VERSION = "2.0" PLURAL_FIELDS = { "classifier" : "classifiers", "provides_dist" : "provides", "provides_extra" : "extras" } SKIP_FIELDS = set() CONTACT_FIELDS = (({"email":"author_email", "name": "author"}, "author"), ({"email":"maintainer_email", "name": "maintainer"}, "maintainer")) # commonly filled out as "UNKNOWN" by distutils: UNKNOWN_FIELDS = set(("author", "author_email", "platform", "home_page", "license")) # Wheel itself is probably the only program that uses non-extras markers # in METADATA/PKG-INFO. Support its syntax with the extra at the end only. EXTRA_RE = re.compile("""^(?P<package>.*?)(;\s*(?P<condition>.*?)(extra == '(?P<extra>.*?)')?)$""") KEYWORDS_RE = re.compile("[\0-,]+") MayRequiresKey = namedtuple('MayRequiresKey', ('condition', 'extra')) def unique(iterable): """ Yield unique values in iterable, preserving order. """ seen = set() for value in iterable: if not value in seen: seen.add(value) yield value def handle_requires(metadata, pkg_info, key): """ Place the runtime requirements from pkg_info into metadata. """ may_requires = OrderedDefaultDict(list) for value in sorted(pkg_info.get_all(key)): extra_match = EXTRA_RE.search(value) if extra_match: groupdict = extra_match.groupdict() condition = groupdict['condition'] extra = groupdict['extra'] package = groupdict['package'] if condition.endswith(' and '): condition = condition[:-5] else: condition, extra = None, None package = value key = MayRequiresKey(condition, extra) may_requires[key].append(package) if may_requires: metadata['run_requires'] = [] def sort_key(item): # Both condition and extra could be None, which can't be compared # against strings in Python 3. key, value = item if key.condition is None: return '' return key.condition for key, value in sorted(may_requires.items(), key=sort_key): may_requirement = OrderedDict((('requires', value),)) if key.extra: may_requirement['extra'] = key.extra if key.condition: may_requirement['environment'] = key.condition metadata['run_requires'].append(may_requirement) if not 'extras' in metadata: metadata['extras'] = [] metadata['extras'].extend([key.extra for key in may_requires.keys() if key.extra]) def pkginfo_to_dict(path, distribution=None): """ Convert PKG-INFO to a prototype Metadata 2.0 (PEP 426) dict. The description is included under the key ['description'] rather than being written to a separate file. path: path to PKG-INFO file distribution: optional distutils Distribution() """ metadata = OrderedDefaultDict(lambda: OrderedDefaultDict(lambda: OrderedDefaultDict(OrderedDict))) metadata["generator"] = "bdist_wheel (" + wheel_version + ")" try: unicode pkg_info = read_pkg_info(path) except NameError: with open(path, 'rb') as pkg_info_file: pkg_info = email.parser.Parser().parsestr(pkg_info_file.read().decode('utf-8')) description = None if pkg_info['Summary']: metadata['summary'] = pkginfo_unicode(pkg_info, 'Summary') del pkg_info['Summary'] if pkg_info['Description']: description = dedent_description(pkg_info) del pkg_info['Description'] else: payload = pkg_info.get_payload() if isinstance(payload, bytes): # Avoid a Python 2 Unicode error. # We still suffer ? glyphs on Python 3. payload = payload.decode('utf-8') if payload: description = payload if description: pkg_info['description'] = description for key in sorted(unique(k.lower() for k in pkg_info.keys())): low_key = key.replace('-', '_') if low_key in SKIP_FIELDS: continue if low_key in UNKNOWN_FIELDS and pkg_info.get(key) == 'UNKNOWN': continue if low_key in sorted(PLURAL_FIELDS): metadata[PLURAL_FIELDS[low_key]] = pkg_info.get_all(key) elif low_key == "requires_dist": handle_requires(metadata, pkg_info, key) elif low_key == 'provides_extra': if not 'extras' in metadata: metadata['extras'] = [] metadata['extras'].extend(pkg_info.get_all(key)) elif low_key == 'home_page': metadata['extensions']['python.details']['project_urls'] = {'Home':pkg_info[key]} elif low_key == 'keywords': metadata['keywords'] = KEYWORDS_RE.split(pkg_info[key]) else: metadata[low_key] = pkg_info[key] metadata['metadata_version'] = METADATA_VERSION if 'extras' in metadata: metadata['extras'] = sorted(set(metadata['extras'])) # include more information if distribution is available if distribution: for requires, attr in (('test_requires', 'tests_require'),): try: requirements = getattr(distribution, attr) if isinstance(requirements, list): new_requirements = sorted(convert_requirements(requirements)) metadata[requires] = [{'requires':new_requirements}] except AttributeError: pass # handle contacts contacts = [] for contact_type, role in CONTACT_FIELDS: contact = OrderedDict() for key in sorted(contact_type): if contact_type[key] in metadata: contact[key] = metadata.pop(contact_type[key]) if contact: contact['role'] = role contacts.append(contact) if contacts: metadata['extensions']['python.details']['contacts'] = contacts # convert entry points to exports try: with open(os.path.join(os.path.dirname(path), "entry_points.txt"), "r") as ep_file: ep_map = pkg_resources.EntryPoint.parse_map(ep_file.read()) exports = OrderedDict() for group, items in sorted(ep_map.items()): exports[group] = OrderedDict() for item in sorted(map(str, items.values())): name, export = item.split(' = ', 1) exports[group][name] = export if exports: metadata['extensions']['python.exports'] = exports except IOError: pass # copy console_scripts entry points to commands if 'python.exports' in metadata['extensions']: for (ep_script, wrap_script) in (('console_scripts', 'wrap_console'), ('gui_scripts', 'wrap_gui')): if ep_script in metadata['extensions']['python.exports']: metadata['extensions']['python.commands'][wrap_script] = \ metadata['extensions']['python.exports'][ep_script] return metadata def requires_to_requires_dist(requirement): """Compose the version predicates for requirement in PEP 345 fashion.""" requires_dist = [] for op, ver in requirement.specs: requires_dist.append(op + ver) if not requires_dist: return '' return " (%s)" % ','.join(requires_dist) def convert_requirements(requirements): """Yield Requires-Dist: strings for parsed requirements strings.""" for req in requirements: parsed_requirement = pkg_resources.Requirement.parse(req) spec = requires_to_requires_dist(parsed_requirement) extras = ",".join(parsed_requirement.extras) if extras: extras = "[%s]" % extras yield (parsed_requirement.project_name + extras + spec) def generate_requirements(extras_require): """ Convert requirements from a setup()-style dictionary to ('Requires-Dist', 'requirement') and ('Provides-Extra', 'extra') tuples. extras_require is a dictionary of {extra: [requirements]} as passed to setup(), using the empty extra {'': [requirements]} to hold install_requires. """ for extra, depends in extras_require.items(): condition = '' if extra and ':' in extra: # setuptools extra:condition syntax extra, condition = extra.split(':', 1) extra = pkg_resources.safe_extra(extra) if extra: yield ('Provides-Extra', extra) if condition: condition += " and " condition += "extra == '%s'" % extra if condition: condition = '; ' + condition for new_req in convert_requirements(depends): yield ('Requires-Dist', new_req + condition) def pkginfo_to_metadata(egg_info_path, pkginfo_path): """ Convert .egg-info directory with PKG-INFO to the Metadata 1.3 aka old-draft Metadata 2.0 format. """ pkg_info = read_pkg_info(pkginfo_path) pkg_info.replace_header('Metadata-Version', '2.0') requires_path = os.path.join(egg_info_path, 'requires.txt') if os.path.exists(requires_path): with open(requires_path) as requires_file: requires = requires_file.read() for extra, reqs in sorted(pkg_resources.split_sections(requires), key=lambda x: x[0] or ''): for item in generate_requirements({extra: reqs}): pkg_info[item[0]] = item[1] description = pkg_info['Description'] if description: pkg_info.set_payload(dedent_description(pkg_info)) del pkg_info['Description'] return pkg_info def pkginfo_unicode(pkg_info, field): """Hack to coax Unicode out of an email Message() - Python 3.3+""" text = pkg_info[field] field = field.lower() if not isinstance(text, str): if not hasattr(pkg_info, 'raw_items'): # Python 3.2 return str(text) for item in pkg_info.raw_items(): if item[0].lower() == field: text = item[1].encode('ascii', 'surrogateescape')\ .decode('utf-8') break return text def dedent_description(pkg_info): """ Dedent and convert pkg_info['Description'] to Unicode. """ description = pkg_info['Description'] # Python 3 Unicode handling, sorta. surrogates = False if not isinstance(description, str): surrogates = True description = pkginfo_unicode(pkg_info, 'Description') description_lines = description.splitlines() description_dedent = '\n'.join( # if the first line of long_description is blank, # the first line here will be indented. (description_lines[0].lstrip(), textwrap.dedent('\n'.join(description_lines[1:])), '\n')) if surrogates: description_dedent = description_dedent\ .encode("utf8")\ .decode("ascii", "surrogateescape") return description_dedent if __name__ == "__main__": import sys, pprint pprint.pprint(pkginfo_to_dict(sys.argv[1]))
import cloud_detection_new as cloud_detection from matplotlib import pyplot as plt import views from skimage import exposure nir = cloud_detection.get_nir()[0:600,2000:2600] red = cloud_detection.get_red()[0:600,2000:2600] green = cloud_detection.get_green()[0:600,2000:2600] blue = cloud_detection.get_blue()[0:600,2000:2600] # or use coastal coastal = cloud_detection.get_coastal()[0:600,2000:2600] marine_shadow_index = (green-blue)/(green+blue) img = views.create_composite(red, green, blue) img_rescale = exposure.rescale_intensity(img, in_range=(0, 90)) plt.rcParams['savefig.facecolor'] = "0.8" vmin, vmax=0.0,0.1 def example_plot(ax, data, fontsize=12): ax.imshow(data, vmin=vmin, vmax=vmax) ax.locator_params(nbins=3) ax.set_xlabel('x-label', fontsize=fontsize) ax.set_ylabel('y-label', fontsize=fontsize) ax.set_title('Title', fontsize=fontsize) plt.close('all') fig = plt.figure ax1=plt.subplot(243) ax2=plt.subplot(244) ax3=plt.subplot(247) ax4=plt.subplot(248) ax5=plt.subplot(121) a_coastal = coastal[500:600, 500:600] a_blue = blue[500:600, 500:600] a_green = green[500:600, 500:600] a_red = red[500:600, 500:600] a_nir = nir[500:600, 500:600] a_img = img[500:600, 500:600] spec1 = [a_coastal[60, 60], a_blue[60, 60], a_green[60, 60], a_red[60, 60], a_nir[60, 60]] b_coastal = coastal[200:300, 100:200] b_blue = blue[200:300, 100:200] b_green = green[200:300, 100:200] b_red = red[200:300, 100:200] b_nir = nir[200:300, 100:200] b_img = img[200:300, 100:200] example_plot(ax1, coastal) example_plot(ax2, blue) example_plot(ax3, green) example_plot(ax4, red) ax5.imshow(img) # plt.tight_layout() plt.close('all') spec = [b_coastal[60, 60], b_blue[60, 60], b_green[60, 60], b_red[60, 60], b_nir[60, 60]] plt.plot(spec, 'k*-') plt.plot(spec1, 'k.-') plt.close('all') cbg = (coastal+blue+green)/3 plt.imshow(cbg/red)
''' DDS: DDS image loader ''' __all__ = ('ImageLoaderDDS', ) from kivy.lib.ddsfile import DDSFile from kivy.logger import Logger from kivy.core.image import ImageLoaderBase, ImageData, ImageLoader class ImageLoaderDDS(ImageLoaderBase): @staticmethod def extensions(): return ('dds', ) def load(self, filename): try: dds = DDSFile(filename=filename) except: Logger.warning('Image: Unable to load image <%s>' % filename) raise self.filename = filename width, height = dds.size im = ImageData(width, height, dds.dxt, dds.images[0], source=filename, flip_vertical=False) if len(dds.images) > 1: images = dds.images images_size = dds.images_size for index in range(1, len(dds.images)): w, h = images_size[index] data = images[index] im.add_mipmap(index, w, h, data) return [im] # register ImageLoader.register(ImageLoaderDDS)
""" This module holds simple classes to convert geospatial values from the database. """ from django.contrib.gis.db.models.fields import GeoSelectFormatMixin from django.contrib.gis.geometry.backend import Geometry from django.contrib.gis.measure import Area, Distance class BaseField(object): empty_strings_allowed = True def get_db_converters(self, connection): return [self.from_db_value] def select_format(self, compiler, sql, params): return sql, params class AreaField(BaseField): "Wrapper for Area values." def __init__(self, area_att): self.area_att = area_att def from_db_value(self, value, expression, connection, context): if value is not None: value = Area(**{self.area_att: value}) return value def get_internal_type(self): return 'AreaField' class DistanceField(BaseField): "Wrapper for Distance values." def __init__(self, distance_att): self.distance_att = distance_att def from_db_value(self, value, expression, connection, context): if value is not None: value = Distance(**{self.distance_att: value}) return value def get_internal_type(self): return 'DistanceField' class GeomField(GeoSelectFormatMixin, BaseField): """ Wrapper for Geometry values. It is a lightweight alternative to using GeometryField (which requires an SQL query upon instantiation). """ # Hacky marker for get_db_converters() geom_type = None def from_db_value(self, value, expression, connection, context): if value is not None: value = Geometry(value) return value def get_internal_type(self): return 'GeometryField' class GMLField(BaseField): """ Wrapper for GML to be used by Oracle to ensure Database.LOB conversion. """ def get_internal_type(self): return 'GMLField' def from_db_value(self, value, expression, connection, context): return value
# Copyright 2012 OpenStack Foundation # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from tempest import config from tempest.lib.common.utils import data_utils from tempest.lib.common.utils import test_utils import tempest.test CONF = config.CONF class BaseIdentityTest(tempest.test.BaseTestCase): @classmethod def setup_credentials(cls): # Create no network resources for these test. cls.set_network_resources() super(BaseIdentityTest, cls).setup_credentials() @classmethod def disable_user(cls, user_name): user = cls.get_user_by_name(user_name) cls.users_client.update_user_enabled(user['id'], enabled=False) @classmethod def disable_tenant(cls, tenant_name): tenant = cls.get_tenant_by_name(tenant_name) cls.tenants_client.update_tenant(tenant['id'], enabled=False) @classmethod def get_user_by_name(cls, name, domain_id=None): if domain_id: params = {'domain_id': domain_id} users = cls.users_client.list_users(**params)['users'] else: users = cls.users_client.list_users()['users'] user = [u for u in users if u['name'] == name] if user: return user[0] @classmethod def get_tenant_by_name(cls, name): try: tenants = cls.tenants_client.list_tenants()['tenants'] except AttributeError: tenants = cls.projects_client.list_projects()['projects'] tenant = [t for t in tenants if t['name'] == name] if tenant: return tenant[0] @classmethod def get_role_by_name(cls, name): roles = cls.roles_client.list_roles()['roles'] role = [r for r in roles if r['name'] == name] if role: return role[0] def create_test_user(self, **kwargs): if kwargs.get('password', None) is None: kwargs['password'] = data_utils.rand_password() if 'name' not in kwargs: kwargs['name'] = data_utils.rand_name('test_user') if 'email' not in kwargs: kwargs['email'] = kwargs['name'] + '@testmail.tm' user = self.users_client.create_user(**kwargs)['user'] # Delete the user at the end of the test self.addCleanup( test_utils.call_and_ignore_notfound_exc, self.users_client.delete_user, user['id']) return user def setup_test_role(self, name=None, domain_id=None): """Set up a test role.""" params = {'name': name or data_utils.rand_name('test_role')} if domain_id: params['domain_id'] = domain_id role = self.roles_client.create_role(**params)['role'] # Delete the role at the end of the test self.addCleanup( test_utils.call_and_ignore_notfound_exc, self.roles_client.delete_role, role['id']) return role class BaseIdentityV2Test(BaseIdentityTest): credentials = ['primary'] # identity v2 tests should obtain tokens and create accounts via v2 # regardless of the configured CONF.identity.auth_version identity_version = 'v2' @classmethod def setup_clients(cls): super(BaseIdentityV2Test, cls).setup_clients() cls.non_admin_client = cls.os_primary.identity_public_client cls.non_admin_token_client = cls.os_primary.token_client cls.non_admin_tenants_client = cls.os_primary.tenants_public_client cls.non_admin_users_client = cls.os_primary.users_public_client class BaseIdentityV2AdminTest(BaseIdentityV2Test): credentials = ['primary', 'admin'] # NOTE(andreaf) Identity tests work with credentials, so it is safer # for them to always use disposable credentials. Forcing dynamic creds # on regular identity tests would be however to restrictive, since it # would prevent any identity test from being executed against clouds where # admin credentials are not available. # Since All admin tests require admin credentials to be # executed, so this will not impact the ability to execute tests. force_tenant_isolation = True @classmethod def skip_checks(cls): super(BaseIdentityV2AdminTest, cls).skip_checks() if not CONF.identity_feature_enabled.api_v2_admin: raise cls.skipException('Identity v2 admin not available') @classmethod def setup_clients(cls): super(BaseIdentityV2AdminTest, cls).setup_clients() cls.client = cls.os_admin.identity_client cls.non_admin_client = cls.os_primary.identity_client cls.token_client = cls.os_admin.token_client cls.tenants_client = cls.os_admin.tenants_client cls.non_admin_tenants_client = cls.os_primary.tenants_client cls.roles_client = cls.os_admin.roles_client cls.non_admin_roles_client = cls.os_primary.roles_client cls.users_client = cls.os_admin.users_client cls.non_admin_users_client = cls.os_primary.users_client cls.services_client = cls.os_admin.identity_services_client cls.endpoints_client = cls.os_admin.endpoints_client @classmethod def resource_setup(cls): super(BaseIdentityV2AdminTest, cls).resource_setup() cls.projects_client = cls.tenants_client def setup_test_user(self, password=None): """Set up a test user.""" tenant = self.setup_test_tenant() user = self.create_test_user(tenantId=tenant['id'], password=password) return user def setup_test_tenant(self, **kwargs): """Set up a test tenant.""" if 'name' not in kwargs: kwargs['name'] = data_utils.rand_name('test_tenant') if 'description' not in kwargs: kwargs['description'] = data_utils.rand_name('desc') tenant = self.projects_client.create_tenant(**kwargs)['tenant'] # Delete the tenant at the end of the test self.addCleanup( test_utils.call_and_ignore_notfound_exc, self.tenants_client.delete_tenant, tenant['id']) return tenant class BaseIdentityV3Test(BaseIdentityTest): credentials = ['primary'] # identity v3 tests should obtain tokens and create accounts via v3 # regardless of the configured CONF.identity.auth_version identity_version = 'v3' @classmethod def setup_clients(cls): super(BaseIdentityV3Test, cls).setup_clients() cls.non_admin_client = cls.os_primary.identity_v3_client cls.non_admin_users_client = cls.os_primary.users_v3_client cls.non_admin_token = cls.os_primary.token_v3_client cls.non_admin_projects_client = cls.os_primary.projects_client cls.non_admin_catalog_client = cls.os_primary.catalog_client cls.non_admin_versions_client =\ cls.os_primary.identity_versions_v3_client class BaseIdentityV3AdminTest(BaseIdentityV3Test): credentials = ['primary', 'admin'] # NOTE(andreaf) Identity tests work with credentials, so it is safer # for them to always use disposable credentials. Forcing dynamic creds # on regular identity tests would be however to restrictive, since it # would prevent any identity test from being executed against clouds where # admin credentials are not available. # Since All admin tests require admin credentials to be # executed, so this will not impact the ability to execute tests. force_tenant_isolation = True @classmethod def setup_clients(cls): super(BaseIdentityV3AdminTest, cls).setup_clients() cls.client = cls.os_admin.identity_v3_client cls.domains_client = cls.os_admin.domains_client cls.users_client = cls.os_admin.users_v3_client cls.trusts_client = cls.os_admin.trusts_client cls.roles_client = cls.os_admin.roles_v3_client cls.inherited_roles_client = cls.os_admin.inherited_roles_client cls.token = cls.os_admin.token_v3_client cls.endpoints_client = cls.os_admin.endpoints_v3_client cls.regions_client = cls.os_admin.regions_client cls.services_client = cls.os_admin.identity_services_v3_client cls.policies_client = cls.os_admin.policies_client cls.creds_client = cls.os_admin.credentials_client cls.groups_client = cls.os_admin.groups_client cls.projects_client = cls.os_admin.projects_client cls.role_assignments = cls.os_admin.role_assignments_client cls.oauth_consumers_client = cls.os_admin.oauth_consumers_client cls.oauth_token_client = cls.os_admin.oauth_token_client cls.domain_config_client = cls.os_admin.domain_config_client cls.endpoint_filter_client = cls.os_admin.endpoint_filter_client cls.endpoint_groups_client = cls.os_admin.endpoint_groups_client if CONF.identity.admin_domain_scope: # NOTE(andreaf) When keystone policy requires it, the identity # admin clients for these tests shall use 'domain' scoped tokens. # As the client manager is already created by the base class, # we set the scope for the inner auth provider. cls.os_admin.auth_provider.scope = 'domain' @classmethod def disable_user(cls, user_name, domain_id=None): user = cls.get_user_by_name(user_name, domain_id) cls.users_client.update_user(user['id'], name=user_name, enabled=False) @classmethod def create_domain(cls, **kwargs): """Create a domain.""" if 'name' not in kwargs: kwargs['name'] = data_utils.rand_name('test_domain') if 'description' not in kwargs: kwargs['description'] = data_utils.rand_name('desc') domain = cls.domains_client.create_domain(**kwargs)['domain'] return domain def delete_domain(self, domain_id): # NOTE(mpavlase) It is necessary to disable the domain before deleting # otherwise it raises Forbidden exception self.domains_client.update_domain(domain_id, enabled=False) self.domains_client.delete_domain(domain_id) def setup_test_user(self, password=None): """Set up a test user.""" project = self.setup_test_project() user = self.create_test_user(project_id=project['id'], password=password) return user def setup_test_project(self, **kwargs): """Set up a test project.""" if 'name' not in kwargs: kwargs['name'] = data_utils.rand_name('test_project') if 'description' not in kwargs: kwargs['description'] = data_utils.rand_name('test_description') project = self.projects_client.create_project(**kwargs)['project'] # Delete the project at the end of the test self.addCleanup( test_utils.call_and_ignore_notfound_exc, self.projects_client.delete_project, project['id']) return project def setup_test_domain(self): """Set up a test domain.""" domain = self.create_domain() # Delete the domain at the end of the test self.addCleanup( test_utils.call_and_ignore_notfound_exc, self.delete_domain, domain['id']) return domain
# -*- coding: utf-8 -*- """QGIS Unit tests for QgsMessageLog. .. note:: This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version. """ __author__ = 'Nyall Dawson' __date__ = '18/06/2018' __copyright__ = 'Copyright 2018, The QGIS Project' import qgis # NOQA from qgis.core import (Qgis, QgsApplication, QgsMessageLog, QgsMessageLogNotifyBlocker) from qgis.PyQt.QtTest import QSignalSpy from qgis.testing import start_app, unittest from utilities import (unitTestDataPath) app = start_app() TEST_DATA_DIR = unitTestDataPath() class TestQgsMessageLog(unittest.TestCase): def testSignals(self): app_log = QgsApplication.messageLog() # signals should be emitted by application log app_spy = QSignalSpy(app_log.messageReceived) app_spy_received = QSignalSpy(app_log.messageReceived[bool]) QgsMessageLog.logMessage('test', 'tag', Qgis.Info, notifyUser=True) self.assertEqual(len(app_spy), 1) self.assertEqual(app_spy[-1], ['test', 'tag', Qgis.Info]) # info message, so messageReceived(bool) should not be emitted self.assertEqual(len(app_spy_received), 0) QgsMessageLog.logMessage('test', 'tag', Qgis.Warning, notifyUser=True) self.assertEqual(len(app_spy), 2) self.assertEqual(app_spy[-1], ['test', 'tag', Qgis.Warning]) # warning message, so messageReceived(bool) should be emitted self.assertEqual(len(app_spy_received), 1) QgsMessageLog.logMessage('test', 'tag', Qgis.Warning, notifyUser=False) self.assertEqual(len(app_spy), 3) # notifyUser was False self.assertEqual(len(app_spy_received), 1) def testBlocker(self): app_log = QgsApplication.messageLog() spy = QSignalSpy(app_log.messageReceived) spy_received = QSignalSpy(app_log.messageReceived[bool]) QgsMessageLog.logMessage('test', 'tag', Qgis.Warning, notifyUser=True) self.assertEqual(len(spy), 1) self.assertEqual(spy[-1], ['test', 'tag', Qgis.Warning]) self.assertEqual(len(spy_received), 1) # block notifications b = QgsMessageLogNotifyBlocker() QgsMessageLog.logMessage('test', 'tag', Qgis.Warning, notifyUser=True) self.assertEqual(len(spy), 2) # should not be blocked self.assertEqual(len(spy_received), 1) # should be blocked # another blocker b2 = QgsMessageLogNotifyBlocker() QgsMessageLog.logMessage('test', 'tag', Qgis.Warning, notifyUser=True) self.assertEqual(len(spy), 3) # should not be blocked self.assertEqual(len(spy_received), 1) # should be blocked del b # still blocked because of b2 QgsMessageLog.logMessage('test', 'tag', Qgis.Warning, notifyUser=True) self.assertEqual(len(spy), 4) # should not be blocked self.assertEqual(len(spy_received), 1) # should be blocked del b2 # not blocked QgsMessageLog.logMessage('test', 'tag', Qgis.Warning, notifyUser=True) self.assertEqual(len(spy), 5) # should not be blocked self.assertEqual(len(spy_received), 2) # should not be blocked if __name__ == '__main__': unittest.main()
from jsonrpc import ServiceProxy import sys import string import getpass # ===== BEGIN USER SETTINGS ===== # if you do not set these you will be prompted for a password for every command rpcuser = "" rpcpass = "" # ====== END USER SETTINGS ====== if rpcpass == "": access = ServiceProxy("http://127.0.0.1:8332") else: access = ServiceProxy("http://"+rpcuser+":"+rpcpass+"@127.0.0.1:8332") cmd = sys.argv[1].lower() if cmd == "backupwallet": try: path = raw_input("Enter destination path/filename: ") print access.backupwallet(path) except: print "\n---An error occurred---\n" elif cmd == "getaccount": try: addr = raw_input("Enter a Bitcoin address: ") print access.getaccount(addr) except: print "\n---An error occurred---\n" elif cmd == "getaccountaddress": try: acct = raw_input("Enter an account name: ") print access.getaccountaddress(acct) except: print "\n---An error occurred---\n" elif cmd == "getaddressesbyaccount": try: acct = raw_input("Enter an account name: ") print access.getaddressesbyaccount(acct) except: print "\n---An error occurred---\n" elif cmd == "getbalance": try: acct = raw_input("Enter an account (optional): ") mc = raw_input("Minimum confirmations (optional): ") try: print access.getbalance(acct, mc) except: print access.getbalance() except: print "\n---An error occurred---\n" elif cmd == "getblockbycount": try: height = raw_input("Height: ") print access.getblockbycount(height) except: print "\n---An error occurred---\n" elif cmd == "getblockcount": try: print access.getblockcount() except: print "\n---An error occurred---\n" elif cmd == "getblocknumber": try: print access.getblocknumber() except: print "\n---An error occurred---\n" elif cmd == "getconnectioncount": try: print access.getconnectioncount() except: print "\n---An error occurred---\n" elif cmd == "getdifficulty": try: print access.getdifficulty() except: print "\n---An error occurred---\n" elif cmd == "getgenerate": try: print access.getgenerate() except: print "\n---An error occurred---\n" elif cmd == "gethashespersec": try: print access.gethashespersec() except: print "\n---An error occurred---\n" elif cmd == "getinfo": try: print access.getinfo() except: print "\n---An error occurred---\n" elif cmd == "getnewaddress": try: acct = raw_input("Enter an account name: ") try: print access.getnewaddress(acct) except: print access.getnewaddress() except: print "\n---An error occurred---\n" elif cmd == "getreceivedbyaccount": try: acct = raw_input("Enter an account (optional): ") mc = raw_input("Minimum confirmations (optional): ") try: print access.getreceivedbyaccount(acct, mc) except: print access.getreceivedbyaccount() except: print "\n---An error occurred---\n" elif cmd == "getreceivedbyaddress": try: addr = raw_input("Enter a Bitcoin address (optional): ") mc = raw_input("Minimum confirmations (optional): ") try: print access.getreceivedbyaddress(addr, mc) except: print access.getreceivedbyaddress() except: print "\n---An error occurred---\n" elif cmd == "gettransaction": try: txid = raw_input("Enter a transaction ID: ") print access.gettransaction(txid) except: print "\n---An error occurred---\n" elif cmd == "getwork": try: data = raw_input("Data (optional): ") try: print access.gettransaction(data) except: print access.gettransaction() except: print "\n---An error occurred---\n" elif cmd == "help": try: cmd = raw_input("Command (optional): ") try: print access.help(cmd) except: print access.help() except: print "\n---An error occurred---\n" elif cmd == "listaccounts": try: mc = raw_input("Minimum confirmations (optional): ") try: print access.listaccounts(mc) except: print access.listaccounts() except: print "\n---An error occurred---\n" elif cmd == "listreceivedbyaccount": try: mc = raw_input("Minimum confirmations (optional): ") incemp = raw_input("Include empty? (true/false, optional): ") try: print access.listreceivedbyaccount(mc, incemp) except: print access.listreceivedbyaccount() except: print "\n---An error occurred---\n" elif cmd == "listreceivedbyaddress": try: mc = raw_input("Minimum confirmations (optional): ") incemp = raw_input("Include empty? (true/false, optional): ") try: print access.listreceivedbyaddress(mc, incemp) except: print access.listreceivedbyaddress() except: print "\n---An error occurred---\n" elif cmd == "listtransactions": try: acct = raw_input("Account (optional): ") count = raw_input("Number of transactions (optional): ") frm = raw_input("Skip (optional):") try: print access.listtransactions(acct, count, frm) except: print access.listtransactions() except: print "\n---An error occurred---\n" elif cmd == "move": try: frm = raw_input("From: ") to = raw_input("To: ") amt = raw_input("Amount:") mc = raw_input("Minimum confirmations (optional): ") comment = raw_input("Comment (optional): ") try: print access.move(frm, to, amt, mc, comment) except: print access.move(frm, to, amt) except: print "\n---An error occurred---\n" elif cmd == "sendfrom": try: frm = raw_input("From: ") to = raw_input("To: ") amt = raw_input("Amount:") mc = raw_input("Minimum confirmations (optional): ") comment = raw_input("Comment (optional): ") commentto = raw_input("Comment-to (optional): ") try: print access.sendfrom(frm, to, amt, mc, comment, commentto) except: print access.sendfrom(frm, to, amt) except: print "\n---An error occurred---\n" elif cmd == "sendmany": try: frm = raw_input("From: ") to = raw_input("To (in format address1:amount1,address2:amount2,...): ") mc = raw_input("Minimum confirmations (optional): ") comment = raw_input("Comment (optional): ") try: print access.sendmany(frm,to,mc,comment) except: print access.sendmany(frm,to) except: print "\n---An error occurred---\n" elif cmd == "sendtoaddress": try: to = raw_input("To (in format address1:amount1,address2:amount2,...): ") amt = raw_input("Amount:") comment = raw_input("Comment (optional): ") commentto = raw_input("Comment-to (optional): ") try: print access.sendtoaddress(to,amt,comment,commentto) except: print access.sendtoaddress(to,amt) except: print "\n---An error occurred---\n" elif cmd == "setaccount": try: addr = raw_input("Address: ") acct = raw_input("Account:") print access.setaccount(addr,acct) except: print "\n---An error occurred---\n" elif cmd == "setgenerate": try: gen= raw_input("Generate? (true/false): ") cpus = raw_input("Max processors/cores (-1 for unlimited, optional):") try: print access.setgenerate(gen, cpus) except: print access.setgenerate(gen) except: print "\n---An error occurred---\n" elif cmd == "settxfee": try: amt = raw_input("Amount:") print access.settxfee(amt) except: print "\n---An error occurred---\n" elif cmd == "stop": try: print access.stop() except: print "\n---An error occurred---\n" elif cmd == "validateaddress": try: addr = raw_input("Address: ") print access.validateaddress(addr) except: print "\n---An error occurred---\n" elif cmd == "walletpassphrase": try: pwd = getpass.getpass(prompt="Enter wallet passphrase: ") access.walletpassphrase(pwd, 60) print "\n---Wallet unlocked---\n" except: print "\n---An error occurred---\n" elif cmd == "walletpassphrasechange": try: pwd = getpass.getpass(prompt="Enter old wallet passphrase: ") pwd2 = getpass.getpass(prompt="Enter new wallet passphrase: ") access.walletpassphrasechange(pwd, pwd2) print print "\n---Passphrase changed---\n" except: print print "\n---An error occurred---\n" print else: print "Command not found or not supported"
""" Support for Microsoft face recognition. For more details about this component, please refer to the documentation at https://home-assistant.io/components/microsoft_face/ """ import asyncio import json import logging import aiohttp from aiohttp.hdrs import CONTENT_TYPE import async_timeout import voluptuous as vol from homeassistant.const import CONF_API_KEY, CONF_TIMEOUT, ATTR_NAME from homeassistant.exceptions import HomeAssistantError from homeassistant.helpers.aiohttp_client import async_get_clientsession import homeassistant.helpers.config_validation as cv from homeassistant.helpers.entity import Entity from homeassistant.util import slugify _LOGGER = logging.getLogger(__name__) ATTR_CAMERA_ENTITY = 'camera_entity' ATTR_GROUP = 'group' ATTR_PERSON = 'person' CONF_AZURE_REGION = 'azure_region' DATA_MICROSOFT_FACE = 'microsoft_face' DEFAULT_TIMEOUT = 10 DEPENDENCIES = ['camera'] DOMAIN = 'microsoft_face' FACE_API_URL = "api.cognitive.microsoft.com/face/v1.0/{0}" SERVICE_CREATE_GROUP = 'create_group' SERVICE_CREATE_PERSON = 'create_person' SERVICE_DELETE_GROUP = 'delete_group' SERVICE_DELETE_PERSON = 'delete_person' SERVICE_FACE_PERSON = 'face_person' SERVICE_TRAIN_GROUP = 'train_group' CONFIG_SCHEMA = vol.Schema({ DOMAIN: vol.Schema({ vol.Required(CONF_API_KEY): cv.string, vol.Optional(CONF_AZURE_REGION, default="westus"): cv.string, vol.Optional(CONF_TIMEOUT, default=DEFAULT_TIMEOUT): cv.positive_int, }), }, extra=vol.ALLOW_EXTRA) SCHEMA_GROUP_SERVICE = vol.Schema({ vol.Required(ATTR_NAME): cv.string, }) SCHEMA_PERSON_SERVICE = SCHEMA_GROUP_SERVICE.extend({ vol.Required(ATTR_GROUP): cv.slugify, }) SCHEMA_FACE_SERVICE = vol.Schema({ vol.Required(ATTR_PERSON): cv.string, vol.Required(ATTR_GROUP): cv.slugify, vol.Required(ATTR_CAMERA_ENTITY): cv.entity_id, }) SCHEMA_TRAIN_SERVICE = vol.Schema({ vol.Required(ATTR_GROUP): cv.slugify, }) async def async_setup(hass, config): """Set up Microsoft Face.""" entities = {} face = MicrosoftFace( hass, config[DOMAIN].get(CONF_AZURE_REGION), config[DOMAIN].get(CONF_API_KEY), config[DOMAIN].get(CONF_TIMEOUT), entities ) try: # read exists group/person from cloud and create entities await face.update_store() except HomeAssistantError as err: _LOGGER.error("Can't load data from face api: %s", err) return False hass.data[DATA_MICROSOFT_FACE] = face async def async_create_group(service): """Create a new person group.""" name = service.data[ATTR_NAME] g_id = slugify(name) try: await face.call_api( 'put', "persongroups/{0}".format(g_id), {'name': name}) face.store[g_id] = {} entities[g_id] = MicrosoftFaceGroupEntity(hass, face, g_id, name) await entities[g_id].async_update_ha_state() except HomeAssistantError as err: _LOGGER.error("Can't create group '%s' with error: %s", g_id, err) hass.services.async_register( DOMAIN, SERVICE_CREATE_GROUP, async_create_group, schema=SCHEMA_GROUP_SERVICE) async def async_delete_group(service): """Delete a person group.""" g_id = slugify(service.data[ATTR_NAME]) try: await face.call_api('delete', "persongroups/{0}".format(g_id)) face.store.pop(g_id) entity = entities.pop(g_id) hass.states.async_remove(entity.entity_id) except HomeAssistantError as err: _LOGGER.error("Can't delete group '%s' with error: %s", g_id, err) hass.services.async_register( DOMAIN, SERVICE_DELETE_GROUP, async_delete_group, schema=SCHEMA_GROUP_SERVICE) async def async_train_group(service): """Train a person group.""" g_id = service.data[ATTR_GROUP] try: await face.call_api( 'post', "persongroups/{0}/train".format(g_id)) except HomeAssistantError as err: _LOGGER.error("Can't train group '%s' with error: %s", g_id, err) hass.services.async_register( DOMAIN, SERVICE_TRAIN_GROUP, async_train_group, schema=SCHEMA_TRAIN_SERVICE) async def async_create_person(service): """Create a person in a group.""" name = service.data[ATTR_NAME] g_id = service.data[ATTR_GROUP] try: user_data = await face.call_api( 'post', "persongroups/{0}/persons".format(g_id), {'name': name} ) face.store[g_id][name] = user_data['personId'] await entities[g_id].async_update_ha_state() except HomeAssistantError as err: _LOGGER.error("Can't create person '%s' with error: %s", name, err) hass.services.async_register( DOMAIN, SERVICE_CREATE_PERSON, async_create_person, schema=SCHEMA_PERSON_SERVICE) async def async_delete_person(service): """Delete a person in a group.""" name = service.data[ATTR_NAME] g_id = service.data[ATTR_GROUP] p_id = face.store[g_id].get(name) try: await face.call_api( 'delete', "persongroups/{0}/persons/{1}".format(g_id, p_id)) face.store[g_id].pop(name) await entities[g_id].async_update_ha_state() except HomeAssistantError as err: _LOGGER.error("Can't delete person '%s' with error: %s", p_id, err) hass.services.async_register( DOMAIN, SERVICE_DELETE_PERSON, async_delete_person, schema=SCHEMA_PERSON_SERVICE) async def async_face_person(service): """Add a new face picture to a person.""" g_id = service.data[ATTR_GROUP] p_id = face.store[g_id].get(service.data[ATTR_PERSON]) camera_entity = service.data[ATTR_CAMERA_ENTITY] camera = hass.components.camera try: image = await camera.async_get_image(hass, camera_entity) await face.call_api( 'post', "persongroups/{0}/persons/{1}/persistedFaces".format( g_id, p_id), image.content, binary=True ) except HomeAssistantError as err: _LOGGER.error("Can't delete person '%s' with error: %s", p_id, err) hass.services.async_register( DOMAIN, SERVICE_FACE_PERSON, async_face_person, schema=SCHEMA_FACE_SERVICE) return True class MicrosoftFaceGroupEntity(Entity): """Person-Group state/data Entity.""" def __init__(self, hass, api, g_id, name): """Initialize person/group entity.""" self.hass = hass self._api = api self._id = g_id self._name = name @property def name(self): """Return the name of the entity.""" return self._name @property def entity_id(self): """Return entity id.""" return "{0}.{1}".format(DOMAIN, self._id) @property def state(self): """Return the state of the entity.""" return len(self._api.store[self._id]) @property def should_poll(self): """Return True if entity has to be polled for state.""" return False @property def device_state_attributes(self): """Return device specific state attributes.""" attr = {} for name, p_id in self._api.store[self._id].items(): attr[name] = p_id return attr class MicrosoftFace: """Microsoft Face api for HomeAssistant.""" def __init__(self, hass, server_loc, api_key, timeout, entities): """Initialize Microsoft Face api.""" self.hass = hass self.websession = async_get_clientsession(hass) self.timeout = timeout self._api_key = api_key self._server_url = "https://{0}.{1}".format(server_loc, FACE_API_URL) self._store = {} self._entities = entities @property def store(self): """Store group/person data and IDs.""" return self._store async def update_store(self): """Load all group/person data into local store.""" groups = await self.call_api('get', 'persongroups') tasks = [] for group in groups: g_id = group['personGroupId'] self._store[g_id] = {} self._entities[g_id] = MicrosoftFaceGroupEntity( self.hass, self, g_id, group['name']) persons = await self.call_api( 'get', "persongroups/{0}/persons".format(g_id)) for person in persons: self._store[g_id][person['name']] = person['personId'] tasks.append(self._entities[g_id].async_update_ha_state()) if tasks: await asyncio.wait(tasks, loop=self.hass.loop) async def call_api(self, method, function, data=None, binary=False, params=None): """Make an api call.""" headers = {"Ocp-Apim-Subscription-Key": self._api_key} url = self._server_url.format(function) payload = None if binary: headers[CONTENT_TYPE] = "application/octet-stream" payload = data else: headers[CONTENT_TYPE] = "application/json" if data is not None: payload = json.dumps(data).encode() else: payload = None try: with async_timeout.timeout(self.timeout, loop=self.hass.loop): response = await getattr(self.websession, method)( url, data=payload, headers=headers, params=params) answer = await response.json() _LOGGER.debug("Read from microsoft face api: %s", answer) if response.status < 300: return answer _LOGGER.warning("Error %d microsoft face api %s", response.status, response.url) raise HomeAssistantError(answer['error']['message']) except aiohttp.ClientError: _LOGGER.warning("Can't connect to microsoft face api") except asyncio.TimeoutError: _LOGGER.warning("Timeout from microsoft face api %s", response.url) raise HomeAssistantError("Network error on microsoft face api.")
# -*- coding: utf-8 -*- """ Plot oscilloscope files from MultiSim """ import numpy as np import matplotlib.pyplot as plt import sys import os from matplotlib import rc rc('font',family="Consolas") files=["real_zad5_05f_p2.txt"] for NazwaPliku in files: print NazwaPliku Plik=open(NazwaPliku) #print DeltaT Dane=Plik.readlines()#[4:] DeltaT=float(Dane[2].split()[3].replace(",",".")) #M=len(Dane[4].split())/2 M=2 Dane=Dane[5:] Plik.close() print M Ys=[np.zeros(len(Dane)) for i in range(M)] for m in range(M): for i in range(len(Dane)): try: Ys[m][i]=float(Dane[i].split()[2+3*m].replace(",",".")) except: print m, i, 2+3*m, len(Dane[i].split()), Dane[i].split() #print i, Y[i] X=np.zeros_like(Ys[0]) for i in range(len(X)): X[i]=i*DeltaT for y in Ys: print max(y)-min(y) Opis=u"Ukad szeregowy\nPoowa czstotliwoci rezonansowej" Nazwa=u"Z5W2" plt.title(u"Przebieg napiciowy\n"+Opis) plt.xlabel(u"Czas t [s]") plt.ylabel(u"Napicie [V]") plt.plot(X,Ys[0],label=u"Wejcie") plt.plot(X,Ys[1],label=u"Wyjcie") plt.grid() plt.legend(loc="best") plt.savefig(Nazwa + ".png", bbox_inches='tight') plt.show()
# Wrapper module for waagent # # waagent is not written as a module. This wrapper module is created # to use the waagent code as a module. # # Copyright 2014 Microsoft Corporation # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # # Requires Python 2.7+ # import imp import os import os.path # # The following code will search and load waagent code and expose # it as a submodule of current module # def searchWAAgent(): agentPath = '/usr/sbin/waagent' if(os.path.isfile(agentPath)): return agentPath user_paths = os.environ['PYTHONPATH'].split(os.pathsep) for user_path in user_paths: agentPath = os.path.join(user_path, 'waagent') if(os.path.isfile(agentPath)): return agentPath return None agentPath = searchWAAgent() if(agentPath): waagent = imp.load_source('waagent', agentPath) else: raise Exception("Can't load waagent.") if not hasattr(waagent, "AddExtensionEvent"): """ If AddExtensionEvent is not defined, provide a dummy impl. """ def _AddExtensionEvent(*args, **kwargs): pass waagent.AddExtensionEvent = _AddExtensionEvent if not hasattr(waagent, "WALAEventOperation"): class _WALAEventOperation: HeartBeat = "HeartBeat" Provision = "Provision" Install = "Install" UnIsntall = "UnInstall" Disable = "Disable" Enable = "Enable" Download = "Download" Upgrade = "Upgrade" Update = "Update" waagent.WALAEventOperation = _WALAEventOperation __ExtensionName__ = None def InitExtensionEventLog(name): __ExtensionName__ = name def AddExtensionEvent(name=__ExtensionName__, op=waagent.WALAEventOperation.Enable, isSuccess=False, message=None): if name is not None: waagent.AddExtensionEvent(name=name, op=op, isSuccess=isSuccess, message=message)
""" This module houses the GEOS ctypes prototype functions for the topological operations on geometries. """ __all__ = ['geos_boundary', 'geos_buffer', 'geos_centroid', 'geos_convexhull', 'geos_difference', 'geos_envelope', 'geos_intersection', 'geos_linemerge', 'geos_pointonsurface', 'geos_preservesimplify', 'geos_simplify', 'geos_symdifference', 'geos_union', 'geos_relate'] from ctypes import c_char_p, c_double, c_int from django.contrib.gis.geos.libgeos import GEOM_PTR, GEOS_PREPARE from django.contrib.gis.geos.prototypes.errcheck import check_geom, check_string from django.contrib.gis.geos.prototypes.geom import geos_char_p from django.contrib.gis.geos.prototypes.threadsafe import GEOSFunc def topology(func, *args): "For GEOS unary topology functions." argtypes = [GEOM_PTR] if args: argtypes += args func.argtypes = argtypes func.restype = GEOM_PTR func.errcheck = check_geom return func ### Topology Routines ### geos_boundary = topology(GEOSFunc('GEOSBoundary')) geos_buffer = topology(GEOSFunc('GEOSBuffer'), c_double, c_int) geos_centroid = topology(GEOSFunc('GEOSGetCentroid')) geos_convexhull = topology(GEOSFunc('GEOSConvexHull')) geos_difference = topology(GEOSFunc('GEOSDifference'), GEOM_PTR) geos_envelope = topology(GEOSFunc('GEOSEnvelope')) geos_intersection = topology(GEOSFunc('GEOSIntersection'), GEOM_PTR) geos_linemerge = topology(GEOSFunc('GEOSLineMerge')) geos_pointonsurface = topology(GEOSFunc('GEOSPointOnSurface')) geos_preservesimplify = topology(GEOSFunc('GEOSTopologyPreserveSimplify'), c_double) geos_simplify = topology(GEOSFunc('GEOSSimplify'), c_double) geos_symdifference = topology(GEOSFunc('GEOSSymDifference'), GEOM_PTR) geos_union = topology(GEOSFunc('GEOSUnion'), GEOM_PTR) # GEOSRelate returns a string, not a geometry. geos_relate = GEOSFunc('GEOSRelate') geos_relate.argtypes = [GEOM_PTR, GEOM_PTR] geos_relate.restype = geos_char_p geos_relate.errcheck = check_string # Routines only in GEOS 3.1+ if GEOS_PREPARE: geos_cascaded_union = GEOSFunc('GEOSUnionCascaded') geos_cascaded_union.argtypes = [GEOM_PTR] geos_cascaded_union.restype = GEOM_PTR __all__.append('geos_cascaded_union')
# -*- coding: utf-8 -*- # Copyright 2016 The Cartographer Authors # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # Cartographer documentation build configuration file, created by # sphinx-quickstart on Fri Jul 8 10:41:33 2016. # # This file is execfile()d with the current directory set to its # containing dir. # # Note that not all possible configuration values are present in this # autogenerated file. # # All configuration values have a default; values that are commented out # serve to show the default. import sys import os from datetime import datetime # If extensions (or modules to document with autodoc) are in another directory, # add these directories to sys.path here. If the directory is relative to the # documentation root, use os.path.abspath to make it absolute, like shown here. #sys.path.insert(0, os.path.abspath('.')) # -- General configuration ------------------------------------------------ # If your documentation needs a minimal Sphinx version, state it here. #needs_sphinx = '1.0' # Add any Sphinx extension module names here, as strings. They can be # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom # ones. extensions = [ 'sphinx.ext.todo', 'sphinx.ext.mathjax', ] # Add any paths that contain templates here, relative to this directory. templates_path = ['_templates'] # The suffix of source filenames. source_suffix = '.rst' # The encoding of source files. #source_encoding = 'utf-8-sig' # The master toctree document. master_doc = 'index' # General information about the project. project = u'Cartographer' copyright = u'{year} The Cartographer Authors'.format(year=datetime.now().year) # The version info for the project you're documenting, acts as replacement for # |version| and |release|, also used in various other places throughout the # built documents. # # The short X.Y version. #version = '' # The full version, including alpha/beta/rc tags. #release = '' # The language for content autogenerated by Sphinx. Refer to documentation # for a list of supported languages. #language = None # There are two options for replacing |today|: either, you set today to some # non-false value, then it is used: #today = '' # Else, today_fmt is used as the format for a strftime call. #today_fmt = '%B %d, %Y' # List of patterns, relative to source directory, that match files and # directories to ignore when looking for source files. exclude_patterns = [] # The reST default role (used for this markup: `text`) to use for all # documents. #default_role = None # If true, '()' will be appended to :func: etc. cross-reference text. #add_function_parentheses = True # If true, the current module name will be prepended to all description # unit titles (such as .. function::). #add_module_names = True # If true, sectionauthor and moduleauthor directives will be shown in the # output. They are ignored by default. show_authors = False # The name of the Pygments (syntax highlighting) style to use. pygments_style = 'sphinx' # A list of ignored prefixes for module index sorting. #modindex_common_prefix = [] # If true, keep warnings as "system message" paragraphs in the built documents. #keep_warnings = False # -- Options for HTML output ---------------------------------------------- # The theme to use for HTML and HTML Help pages. See the documentation for # a list of builtin themes. html_theme = 'default' # Theme options are theme-specific and customize the look and feel of a theme # further. For a list of options available for each theme, see the # documentation. #html_theme_options = {} # Add any paths that contain custom themes here, relative to this directory. #html_theme_path = [] # The name for this set of Sphinx documents. If None, it defaults to # "<project> v<release> documentation". #html_title = None # A shorter title for the navigation bar. Default is the same as html_title. #html_short_title = None # The name of an image file (relative to this directory) to place at the top # of the sidebar. #html_logo = None # The name of an image file (within the static path) to use as favicon of the # docs. This file should be a Windows icon file (.ico) being 16x16 or 32x32 # pixels large. #html_favicon = None # Add any paths that contain custom static files (such as style sheets) here, # relative to this directory. They are copied after the builtin static files, # so a file named "default.css" will overwrite the builtin "default.css". html_static_path = [] # Add any extra paths that contain custom files (such as robots.txt or # .htaccess) here, relative to this directory. These files are copied # directly to the root of the documentation. #html_extra_path = [] # If not '', a 'Last updated on:' timestamp is inserted at every page bottom, # using the given strftime format. #html_last_updated_fmt = '%b %d, %Y' # If true, SmartyPants will be used to convert quotes and dashes to # typographically correct entities. #html_use_smartypants = True # Custom sidebar templates, maps document names to template names. #html_sidebars = {} # Additional templates that should be rendered to pages, maps page names to # template names. #html_additional_pages = {} # If false, no module index is generated. #html_domain_indices = True # If false, no index is generated. #html_use_index = True # If true, the index is split into individual pages for each letter. #html_split_index = False # If true, links to the reST sources are added to the pages. #html_show_sourcelink = True # If true, "Created using Sphinx" is shown in the HTML footer. Default is True. #html_show_sphinx = True # If true, "(C) Copyright ..." is shown in the HTML footer. Default is True. #html_show_copyright = True # If true, an OpenSearch description file will be output, and all pages will # contain a <link> tag referring to it. The value of this option must be the # base URL from which the finished HTML is served. #html_use_opensearch = '' # This is the file name suffix for HTML files (e.g. ".xhtml"). #html_file_suffix = None # Output file base name for HTML help builder. htmlhelp_basename = 'Cartographerdoc' # -- Options for LaTeX output --------------------------------------------- latex_elements = { # The paper size ('letterpaper' or 'a4paper'). #'papersize': 'letterpaper', # The font size ('10pt', '11pt' or '12pt'). #'pointsize': '10pt', # Additional stuff for the LaTeX preamble. #'preamble': '', } # Grouping the document tree into LaTeX files. List of tuples # (source start file, target name, title, # author, documentclass [howto, manual, or own class]). latex_documents = [ ('index', 'Cartographer.tex', u'Cartographer Documentation', u'The Cartographer Authors', 'manual'), ] # The name of an image file (relative to this directory) to place at the top of # the title page. #latex_logo = None # For "manual" documents, if this is true, then toplevel headings are parts, # not chapters. #latex_use_parts = False # If true, show page references after internal links. #latex_show_pagerefs = False # If true, show URL addresses after external links. #latex_show_urls = False # Documents to append as an appendix to all manuals. #latex_appendices = [] # If false, no module index is generated. #latex_domain_indices = True # -- Options for manual page output --------------------------------------- # One entry per manual page. List of tuples # (source start file, name, description, authors, manual section). man_pages = [ ('index', 'cartographer', u'Cartographer Documentation', [u'The Cartographer Authors'], 1) ] # If true, show URL addresses after external links. #man_show_urls = False # -- Options for Texinfo output ------------------------------------------- # Grouping the document tree into Texinfo files. List of tuples # (source start file, target name, title, author, # dir menu entry, description, category) texinfo_documents = [ ('index', 'Cartographer', u'Cartographer Documentation', u'The Cartographer Authors', 'Cartographer', 'Cartographer is a system that provides real-time simultaneous ' 'localization and mapping (SLAM) in 2D and 3D across multiple platforms ' 'and sensor configurations.', 'Miscellaneous'), ] # Documents to append as an appendix to all manuals. #texinfo_appendices = [] # If false, no module index is generated. #texinfo_domain_indices = True # How to display URL addresses: 'footnote', 'no', or 'inline'. #texinfo_show_urls = 'footnote' # If true, do not generate a @detailmenu in the "Top" node's menu. #texinfo_no_detailmenu = False
"""Fixtures for component.""" from unittest.mock import patch from pyatv import conf, net import pytest from .common import MockPairingHandler, create_conf @pytest.fixture(autouse=True, name="mock_scan") def mock_scan_fixture(): """Mock pyatv.scan.""" with patch("homeassistant.components.apple_tv.config_flow.scan") as mock_scan: async def _scan(loop, timeout=5, identifier=None, protocol=None, hosts=None): if not mock_scan.hosts: mock_scan.hosts = hosts return mock_scan.result mock_scan.result = [] mock_scan.hosts = None mock_scan.side_effect = _scan yield mock_scan @pytest.fixture(name="dmap_pin") def dmap_pin_fixture(): """Mock pyatv.scan.""" with patch("homeassistant.components.apple_tv.config_flow.randrange") as mock_pin: mock_pin.side_effect = lambda start, stop: 1111 yield mock_pin @pytest.fixture def pairing(): """Mock pyatv.scan.""" with patch("homeassistant.components.apple_tv.config_flow.pair") as mock_pair: async def _pair(config, protocol, loop, session=None, **kwargs): handler = MockPairingHandler( await net.create_session(session), config.get_service(protocol) ) handler.always_fail = mock_pair.always_fail return handler mock_pair.always_fail = False mock_pair.side_effect = _pair yield mock_pair @pytest.fixture def pairing_mock(): """Mock pyatv.scan.""" with patch("homeassistant.components.apple_tv.config_flow.pair") as mock_pair: async def _pair(config, protocol, loop, session=None, **kwargs): return mock_pair async def _begin(): pass async def _close(): pass mock_pair.close.side_effect = _close mock_pair.begin.side_effect = _begin mock_pair.pin = lambda pin: None mock_pair.side_effect = _pair yield mock_pair @pytest.fixture def full_device(mock_scan, dmap_pin): """Mock pyatv.scan.""" mock_scan.result.append( create_conf( "127.0.0.1", "MRP Device", conf.MrpService("mrpid", 5555), conf.DmapService("dmapid", None, port=6666), conf.AirPlayService("airplayid", port=7777), ) ) yield mock_scan @pytest.fixture def mrp_device(mock_scan): """Mock pyatv.scan.""" mock_scan.result.append( create_conf("127.0.0.1", "MRP Device", conf.MrpService("mrpid", 5555)) ) yield mock_scan @pytest.fixture def dmap_device(mock_scan): """Mock pyatv.scan.""" mock_scan.result.append( create_conf( "127.0.0.1", "DMAP Device", conf.DmapService("dmapid", None, port=6666), ) ) yield mock_scan @pytest.fixture def dmap_device_with_credentials(mock_scan): """Mock pyatv.scan.""" mock_scan.result.append( create_conf( "127.0.0.1", "DMAP Device", conf.DmapService("dmapid", "dummy_creds", port=6666), ) ) yield mock_scan @pytest.fixture def airplay_device(mock_scan): """Mock pyatv.scan.""" mock_scan.result.append( create_conf( "127.0.0.1", "AirPlay Device", conf.AirPlayService("airplayid", port=7777) ) ) yield mock_scan
# Copyright (C) 2012 Zan Dobersek <zandobersek@gmail.com> # # Redistribution and use in source and binary forms, with or without # modification, are permitted provided that the following conditions are # met: # # * Redistributions of source code must retain the above copyright # notice, this list of conditions and the following disclaimer. # * Redistributions in binary form must reproduce the above # copyright notice, this list of conditions and the following disclaimer # in the documentation and/or other materials provided with the # distribution. # * Neither the name of Google Inc. nor the names of its # contributors may be used to endorse or promote products derived from # this software without specific prior written permission. # # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS # "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT # LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR # A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT # OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, # SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT # LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, # DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY # THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT # (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE # OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. import logging import unittest2 as unittest from webkitpy.common.system.filesystem_mock import MockFileSystem from webkitpy.common.system.executive_mock import MockExecutive2 from webkitpy.common.system.outputcapture import OutputCapture from webkitpy.common.system.systemhost_mock import MockSystemHost from webkitpy.port import Port from webkitpy.port.server_process_mock import MockServerProcess from webkitpy.port.xvfbdriver import XvfbDriver from webkitpy.tool.mocktool import MockOptions _log = logging.getLogger(__name__) class XvfbDriverTest(unittest.TestCase): def make_driver(self, worker_number=0, xorg_running=False, executive=None): port = Port(MockSystemHost(log_executive=True, executive=executive), 'xvfbdrivertestport', options=MockOptions(configuration='Release')) port._config.build_directory = lambda configuration: "/mock-build" port._server_process_constructor = MockServerProcess if xorg_running: port._executive._running_pids['Xorg'] = 108 driver = XvfbDriver(port, worker_number=worker_number, pixel_tests=True) driver._startup_delay_secs = 0 return driver def cleanup_driver(self, driver): # Setting _xvfb_process member to None is necessary as the Driver object is stopped on deletion, # killing the Xvfb process if present. Thus, this method should only be called from tests that do not # intend to test the behavior of XvfbDriver.stop. driver._xvfb_process = None def assertDriverStartSuccessful(self, driver, expected_logs, expected_display, pixel_tests=False): OutputCapture().assert_outputs(self, driver.start, [pixel_tests, []], expected_logs=expected_logs) self.assertTrue(driver._server_process.started) self.assertEqual(driver._server_process.env["DISPLAY"], expected_display) def test_start_no_pixel_tests(self): driver = self.make_driver() expected_logs = "MOCK run_command: ['ps', '-eo', 'comm,command'], cwd=None\nMOCK popen: ['Xvfb', ':0', '-screen', '0', '800x600x24', '-nolisten', 'tcp']\n" self.assertDriverStartSuccessful(driver, expected_logs=expected_logs, expected_display=":0") self.cleanup_driver(driver) def test_start_pixel_tests(self): driver = self.make_driver() expected_logs = "MOCK run_command: ['ps', '-eo', 'comm,command'], cwd=None\nMOCK popen: ['Xvfb', ':0', '-screen', '0', '800x600x24', '-nolisten', 'tcp']\n" self.assertDriverStartSuccessful(driver, expected_logs=expected_logs, expected_display=":0", pixel_tests=True) self.cleanup_driver(driver) def test_start_arbitrary_worker_number(self): driver = self.make_driver(worker_number=17) expected_logs = "MOCK run_command: ['ps', '-eo', 'comm,command'], cwd=None\nMOCK popen: ['Xvfb', ':0', '-screen', '0', '800x600x24', '-nolisten', 'tcp']\n" self.assertDriverStartSuccessful(driver, expected_logs=expected_logs, expected_display=":0", pixel_tests=True) self.cleanup_driver(driver) def test_next_free_display(self): output = "Xorg /usr/bin/X :0 -auth /var/run/lightdm/root/:0 -nolisten tcp vt7 -novtswitch -background none\nXvfb Xvfb :1 -screen 0 800x600x24 -nolisten tcp" executive = MockExecutive2(output) driver = self.make_driver(executive=executive) self.assertEqual(driver._next_free_display(), 2) self.cleanup_driver(driver) output = "X /usr/bin/X :0 vt7 -nolisten tcp -auth /var/run/xauth/A:0-8p7Ybb" executive = MockExecutive2(output) driver = self.make_driver(executive=executive) self.assertEqual(driver._next_free_display(), 1) self.cleanup_driver(driver) output = "Xvfb Xvfb :0 -screen 0 800x600x24 -nolisten tcp" executive = MockExecutive2(output) driver = self.make_driver(executive=executive) self.assertEqual(driver._next_free_display(), 1) self.cleanup_driver(driver) output = "Xvfb Xvfb :1 -screen 0 800x600x24 -nolisten tcp\nXvfb Xvfb :0 -screen 0 800x600x24 -nolisten tcp\nXvfb Xvfb :3 -screen 0 800x600x24 -nolisten tcp" executive = MockExecutive2(output) driver = self.make_driver(executive=executive) self.assertEqual(driver._next_free_display(), 2) self.cleanup_driver(driver) def test_start_next_worker(self): driver = self.make_driver() driver._next_free_display = lambda: 0 expected_logs = "MOCK popen: ['Xvfb', ':0', '-screen', '0', '800x600x24', '-nolisten', 'tcp']\n" self.assertDriverStartSuccessful(driver, expected_logs=expected_logs, expected_display=":0", pixel_tests=True) self.cleanup_driver(driver) driver = self.make_driver() driver._next_free_display = lambda: 3 expected_logs = "MOCK popen: ['Xvfb', ':3', '-screen', '0', '800x600x24', '-nolisten', 'tcp']\n" self.assertDriverStartSuccessful(driver, expected_logs=expected_logs, expected_display=":3", pixel_tests=True) self.cleanup_driver(driver) def test_stop(self): filesystem = MockFileSystem(files={'/tmp/.X42-lock': '1234\n'}) port = Port(MockSystemHost(log_executive=True, filesystem=filesystem), 'xvfbdrivertestport', options=MockOptions(configuration='Release')) port._executive.kill_process = lambda x: _log.info("MOCK kill_process pid: " + str(x)) driver = XvfbDriver(port, worker_number=0, pixel_tests=True) class FakeXvfbProcess(object): pid = 1234 driver._xvfb_process = FakeXvfbProcess() driver._lock_file = '/tmp/.X42-lock' expected_logs = "MOCK kill_process pid: 1234\n" OutputCapture().assert_outputs(self, driver.stop, [], expected_logs=expected_logs) self.assertIsNone(driver._xvfb_process) self.assertFalse(port._filesystem.exists(driver._lock_file))
"""Utilities for with-statement contexts. See PEP 343.""" import sys from functools import wraps __all__ = ["contextmanager", "nested", "closing"] class GeneratorContextManager(object): """Helper for @contextmanager decorator.""" def __init__(self, gen): self.gen = gen def __enter__(self): try: return self.gen.next() except StopIteration: raise RuntimeError("generator didn't yield") def __exit__(self, type, value, traceback): if type is None: try: self.gen.next() except StopIteration: return else: raise RuntimeError("generator didn't stop") else: if value is None: # Need to force instantiation so we can reliably # tell if we get the same exception back value = type() try: self.gen.throw(type, value, traceback) raise RuntimeError("generator didn't stop after throw()") except StopIteration, exc: # Suppress the exception *unless* it's the same exception that # was passed to throw(). This prevents a StopIteration # raised inside the "with" statement from being suppressed return exc is not value except: # only re-raise if it's *not* the exception that was # passed to throw(), because __exit__() must not raise # an exception unless __exit__() itself failed. But throw() # has to raise the exception to signal propagation, so this # fixes the impedance mismatch between the throw() protocol # and the __exit__() protocol. # if sys.exc_info()[1] is not value: raise def contextmanager(func): """@contextmanager decorator. Typical usage: @contextmanager def some_generator(<arguments>): <setup> try: yield <value> finally: <cleanup> This makes this: with some_generator(<arguments>) as <variable>: <body> equivalent to this: <setup> try: <variable> = <value> <body> finally: <cleanup> """ @wraps(func) def helper(*args, **kwds): return GeneratorContextManager(func(*args, **kwds)) return helper @contextmanager def nested(*managers): """Support multiple context managers in a single with-statement. Code like this: with nested(A, B, C) as (X, Y, Z): <body> is equivalent to this: with A as X: with B as Y: with C as Z: <body> """ exits = [] vars = [] exc = (None, None, None) try: for mgr in managers: exit = mgr.__exit__ enter = mgr.__enter__ vars.append(enter()) exits.append(exit) yield vars except: exc = sys.exc_info() finally: while exits: exit = exits.pop() try: if exit(*exc): exc = (None, None, None) except: exc = sys.exc_info() if exc != (None, None, None): # Don't rely on sys.exc_info() still containing # the right information. Another exception may # have been raised and caught by an exit method raise exc[0], exc[1], exc[2] class closing(object): """Context to automatically close something at the end of a block. Code like this: with closing(<module>.open(<arguments>)) as f: <block> is equivalent to this: f = <module>.open(<arguments>) try: <block> finally: f.close() """ def __init__(self, thing): self.thing = thing def __enter__(self): return self.thing def __exit__(self, *exc_info): self.thing.close()
#!/usr/bin/env python3 import os import argparse import configparser import logging import sys import logging import subprocess from datetime import datetime from pathlib import Path from Pegasus.api import * logging.basicConfig(level=logging.DEBUG) def parse_args(args=sys.argv[1:]): parser = argparse.ArgumentParser(description="Runtime Cluster Test Workflow") parser.add_argument( "pegasus_keg_path", help="abs path to pegasus-keg install (e.g '/usr/bin/pegasus-keg')", metavar="PEGASUS_KEG_PATH", ) parser.add_argument( "config_dir", help="name of test config dir (e.g. 'runtime-condorio', 'runtime-nonsharedfs'", ) return parser.parse_args(args) def write_sc(top_dir: Path, run_id: str): # get pegasus version cp = subprocess.run( ["pegasus-version"], stdout=subprocess.PIPE, stderr=subprocess.PIPE ) if cp.returncode != 0: raise RuntimeError( "unable to call pegasus-version: {}".format(cp.stderr.decode().strip()) ) REMOTE_PEGASUS_HOME = "/lizard/scratch-90-days/bamboo/installs/pegasus-{}".format( cp.stdout.decode().strip() ) sc = SiteCatalog() # --- cartman-data site ---------------------------------------------------- cartman_data = Site(name="cartman-data", arch=Arch.X86_64, os_type=OS.LINUX) cartman_data.add_directories( Directory( Directory.SHARED_SCRATCH, top_dir / "staging-site/scratch", ).add_file_servers( FileServer( "gsiftp://bamboo.isi.edu" + str(top_dir / "staging-site/scratch"), Operation.ALL, ) ) ) cartman_data.add_env(PEGASUS_HOME=REMOTE_PEGASUS_HOME) sc.add_sites(cartman_data) # --- condorpool site ------------------------------------------------------ condorpool = Site(name="condorpool", arch=Arch.X86_64, os_type=OS.LINUX) condorpool.add_condor_profile(universe="vanilla") condorpool.add_pegasus_profile(style="condor") sc.add_sites(condorpool) # --- sharedfs site -------------------------------------------------------- sharedfs = Site(name="sharedfs", arch=Arch.X86_64, os_type=OS.LINUX) sharedfs_dir1 = Directory( Directory.SHARED_STORAGE, Path("/lizard/scratch-90-days") / os.getenv("USER") / "storage/black-diamond-output" / run_id, ) sharedfs_dir1.add_file_servers( FileServer( "file://" + str( Path("/lizard/scratch-90-days") / os.getenv("USER") / "storage/black-diamond-output" / run_id ), Operation.ALL, ) ) sharedfs.add_directories(sharedfs_dir1) sharedfs_dir2 = Directory( Directory.SHARED_SCRATCH, Path("/lizard/scratch-90-days") / os.getenv("USER") / "scratch" / run_id, ) sharedfs_dir2.add_file_servers( FileServer( "file://" + str( Path("/lizard/scratch-90-days") / os.getenv("USER") / "scratch" / run_id ), Operation.ALL, ) ) sharedfs.add_directories(sharedfs_dir2) sharedfs.add_env(PEGASUS_HOME=REMOTE_PEGASUS_HOME) sharedfs.add_condor_profile( should_transfer_files="Yes", universe="vanilla", when_to_transfer_output="ON_EXIT", ) sharedfs.add_pegasus_profile(style="condor") sc.add_sites(sharedfs) # --- local site ----------------------------------------------------------- local_site_url = config.get("all", "local_site_url", fallback="") local = Site(name="local", arch=Arch.X86_64, os_type=OS.LINUX) local_dir1 = Directory(Directory.SHARED_STORAGE, top_dir / "outputs") local_dir1.add_file_servers( FileServer(local_site_url + str(top_dir / "outputs"), Operation.ALL) ) local.add_directories(local_dir1) local_dir2 = Directory(Directory.SHARED_SCRATCH, top_dir / "work") local_dir2.add_file_servers( FileServer(local_site_url + str(top_dir / "work"), Operation.ALL) ) local.add_directories(local_dir2) sc.add_sites(local) # write sc.write() def write_rc(config: configparser.ConfigParser): input_file = config.get("all", "input_file") if input_file == "": input_file = Path("f.a") else: # is a directory such as '/lizard/scratch-90-days' input_dir = Path(input_file) / os.getenv("USER") / "inputs" input_dir.mkdir(parents=True, exist_ok=True) input_file = input_dir / "f.a" with input_file.open("w") as f: f.write("This is sample input to KEG") rc = ReplicaCatalog() rc.add_replica( site=config.get("all", "file_site"), lfn="f.a", pfn=input_file.resolve() ) rc.write() def write_tc(config: configparser.ConfigParser, pegasus_keg_path: str): tc = TransformationCatalog() for i in range(1, 3): sleep = Transformation( namespace="cluster", name="level{}".format(i), version="1.0", site=config.get("all", "executable_site"), pfn=config.get("all", "executable_url") + pegasus_keg_path, is_stageable=True, os_type=OS.LINUX, arch=Arch.X86_64, ) sleep.add_pegasus_profile( clusters_size=config.get("all", "clusters_size"), clusters_max_runtime=config.get("all", "clusters_maxruntime"), ) tc.add_transformations(sleep) tc.write() if __name__ == "__main__": args = parse_args() TOP_DIR = Path().cwd().resolve() RUN_ID = datetime.now().strftime("%Y%m%d_%H%M%S") # --- validate test config dir --------------------------------------------- config_dir = Path(__file__).parent / args.config_dir if not config_dir.is_dir(): raise ValueError( "config_dir: {} does not a directory or does not exist".format(config_dir) ) config_file = config_dir / "test.config" if not config_file.is_file(): raise ValueError("{} does not contain required file: {}".format(config_file)) # --- general test config -------------------------------------------------- config = configparser.ConfigParser( { "input_file": "", "workflow_name": "horizontal-clustering-test", "clusters_size": "3", "clusters_maxruntime": "7", } ) config.read(str(config_file)) # --- catalogs ------------------------------------------------------------- write_sc(TOP_DIR, RUN_ID) write_rc(config) write_tc(config, args.pegasus_keg_path) # --- workflow ------------------------------------------------------------- wf = Workflow(config.get("all", "workflow_name")) input_file = File("f.a") # create 4 lvl1 jobs for i in range(4): job = ( Job(namespace="cluster", transformation="level1", version="1.0") .add_args("-a", "level1", "-T", i + 1, "-i", input_file) .add_inputs(input_file) .add_profiles(Namespace.PEGASUS, key="job.runtime", value=i + 1) ) wf.add_jobs(job) # for each lvl1 job, create 4 lvl2 children for j in range(4): child = ( Job(namespace="cluster", transformation="level2", version="1.0") .add_args("-a", "level2", "-T", ((j + 1) * 2)) .add_profiles(Namespace.PEGASUS, key="runtime", value=((j + 1) * 2)) ) wf.add_jobs(child) wf.add_dependency(job=job, children=[child]) # plan and run execution_site = config.get("all", "execution_site", fallback="local") staging_site = config.get("all", "staging_site", fallback="local") output_site = config.get("all", "output_site", fallback="local") top_pegasusrc = Path(__file__).parent / "pegasusrc" pegasusrc = config_dir / "pegasusrc" # include anything in __file__/pegasusrc in ./config_dir/pegasusrc with top_pegasusrc.open("r") as top_cfg, pegasusrc.open("a") as cfg: cfg.write(top_cfg.read()) try: wf.plan( conf=str(pegasusrc), sites=[execution_site], staging_sites={execution_site: staging_site}, output_sites=[output_site], dir="work/submit", cleanup="leaf", cluster=["horizontal"], verbose=3, submit=True, ).wait().analyze().statistics() except PegasusClientError as e: print(e) print(e.result.stdout)
from setuptools import setup, find_packages from phonenumber_field import __version__ setup( name="django-phonenumber-field", version=__version__, url='http://github.com/stefanfoulis/django-phonenumber-field', license='BSD', platforms=['OS Independent'], description="An international phone number field for django models.", install_requires=[ 'phonenumbers>=7.0.2', 'babel', ], long_description=open('README.rst').read(), author='Stefan Foulis', author_email='stefan.foulis@gmail.com', maintainer='Stefan Foulis', maintainer_email='stefan.foulis@gmail.com', packages=find_packages(), package_data = { 'phonenumber_field': [ 'locale/*/LC_MESSAGES/*', ], }, include_package_data=True, zip_safe=False, classifiers=[ 'Development Status :: 4 - Beta', 'Framework :: Django', 'Intended Audience :: Developers', 'License :: OSI Approved :: BSD License', 'Operating System :: OS Independent', 'Programming Language :: Python', 'Programming Language :: Python :: 2', 'Programming Language :: Python :: 2.7', 'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.3', 'Programming Language :: Python :: 3.4', 'Programming Language :: Python :: 3.5', 'Programming Language :: Python :: Implementation :: CPython', 'Programming Language :: Python :: Implementation :: PyPy', 'Topic :: Internet :: WWW/HTTP', ] )
#!/usr/bin/env python """ Get rid of lines containing duplicate copies of the same atom in the "Atoms" section of a LAMMPS data file. Duplicate lines which occur later are preserved and the earlier lines are erased. The file is read from sys.stdin. This program does not parse the entire data file. The text from the "Atoms" section of the LAMMPS file must be extracted in advance before it is sent to this program.) """ import sys def main(): in_stream = sys.stdin f = None fname = None if len(sys.argv) == 2: fname = sys.argv[1] f = open(fname, 'r') in_stream = f atom_ids_in_use = set([]) lines = in_stream.readlines() # Start at the end of the file and read backwards. # If duplicate lines exist, eliminate the ones that occur earlier in the file. i = len(lines) while i > 0: i -= 1 line_orig = lines[i] line = line_orig.rstrip('\n') if '#' in line_orig: ic = line.find('#') line = line_orig[:ic] tokens = line.strip().split() if len(tokens) > 0: atom_id = tokens[0] if atom_id in atom_ids_in_use: del lines[i] else: atom_ids_in_use.add(atom_id) else: del lines[i] for line in lines: sys.stdout.write(line) if f != None: f.close() return if __name__ == '__main__': main()
from django.contrib.contenttypes import generic from django.contrib.auth.models import User, Group from django.db import models from mezzanine.pages.models import Page, RichText,Displayable from mezzanine.core.fields import FileField, RichTextField from mezzanine.core.models import Ownable from mezzanine.generic.models import Keyword, Orderable from hs_core.models import AbstractResource from django.db.models.signals import post_save from datetime import date from uuid import uuid4 from django.db.models.signals import post_save,pre_save,post_init from django.contrib.auth.signals import user_logged_in from django.dispatch import receiver from django.core.exceptions import ObjectDoesNotExist,ValidationError from django.core.urlresolvers import reverse from .party import Party from .party_types import PartyEmailModel,PartyGeolocation,PartyPhoneModel,PartyLocationModel from .activities import ActivitiesModel from .person import Person from .organization import Organization __author__ = 'valentin' class GroupAssociation( ActivitiesModel): # object to handle a person being in one or more organizations #organization = models.ForeignKey(Organization) uniqueCode = models.CharField(max_length=64,default=lambda: str(uuid4()),verbose_name="A unique code for the record", help_text="A unique code for the record") group = models.ForeignKey(Group) #person = models.ForeignKey(Person) person = models.ForeignKey(Person) beginDate = models.DateField(null=True,blank=True,verbose_name="begin date of associate, Empty is not know.") endDate = models.DateField(null=True,blank=True, verbose_name="End date of association. Empty if still with group") positionName = models.CharField(verbose_name="Position, empty is not known", blank=True,max_length='100') def __unicode__(self): if (self.beginDate): if (self.endDate): range=u' [%s, %s]' % (self.beginDate,self.endDate) else: range=u' [%s]' % (self.beginDate) else: range='' if (self.jobTitle): title = ' ,' + self.jobTitle return u'%s (%s%s%s)' % (self.person.name, self.group.name,title,range) class Meta: app_label = 'hs_party'
#!/usr/bin/env python from gnuradio import gr from gnuradio import trellis, digital, filter, blocks from gnuradio import eng_notation import math import sys import random import fsm_utils try: from gnuradio import analog except ImportError: sys.stderr.write("Error: Program requires gr-analog.\n") sys.exit(1) def make_rx(tb,fo,fi,dimensionality,tot_constellation,K,interleaver,IT,Es,N0,type): metrics_in = trellis.metrics_f(fi.O(),dimensionality,tot_constellation,digital.TRELLIS_EUCLIDEAN) # data preprocessing to generate metrics for innner SISO scale = blocks.multiply_const_ff(1.0/N0) gnd = blocks.vector_source_f([0],True); inter=[] deinter=[] siso_in=[] siso_out=[] # generate all blocks for it in range(IT): inter.append( trellis.permutation(interleaver.K(),interleaver.INTER(),fi.I(),gr.sizeof_float) ) siso_in.append( trellis.siso_f(fi,K,0,-1,True,False,type) ) deinter.append( trellis.permutation(interleaver.K(),interleaver.DEINTER(),fi.I(),gr.sizeof_float) ) if it < IT-1: siso_out.append( trellis.siso_f(fo,K,0,-1,False,True,type) ) else: siso_out.append( trellis.viterbi_s(fo,K,0,-1) ) # no soft outputs needed # connect first stage tb.connect (gnd,inter[0]) tb.connect (metrics_in,scale) tb.connect (scale,(siso_in[0],1)) # connect the rest for it in range(IT): if it < IT-1: tb.connect (scale,(siso_in[it+1],1)) tb.connect (siso_in[it],deinter[it],(siso_out[it],1)) tb.connect (gnd,(siso_out[it],0)) tb.connect (siso_out[it],inter[it+1]) tb.connect (inter[it],(siso_in[it],0)) else: tb.connect (siso_in[it],deinter[it],siso_out[it]) tb.connect (inter[it],(siso_in[it],0)) return (metrics_in,siso_out[IT-1]) def run_test (fo,fi,interleaver,Kb,bitspersymbol,K,channel,modulation,dimensionality,tot_constellation,Es,N0,IT,seed): tb = gr.top_block () L = len(channel) # TX # this for loop is TOO slow in python!!! packet = [0]*(K) random.seed(seed) for i in range(len(packet)): packet[i] = random.randint(0, 2**bitspersymbol - 1) # random symbols src = blocks.vector_source_s(packet,False) enc_out = trellis.encoder_ss(fo,0) # initial state = 0 inter = trellis.permutation(interleaver.K(),interleaver.INTER(),1,gr.sizeof_short) mod = digital.chunks_to_symbols_sf(modulation[1],modulation[0]) # CHANNEL isi = filter.fir_filter_fff(1,channel) add = blocks.add_ff() noise = analog.noise_source_f(analog.GR_GAUSSIAN,math.sqrt(N0/2),seed) # RX (head,tail) = make_rx(tb,fo,fi,dimensionality,tot_constellation,K,interleaver,IT,Es,N0,trellis.TRELLIS_MIN_SUM) dst = blocks.vector_sink_s(); tb.connect (src,enc_out,inter,mod) tb.connect (mod,isi,(add,0)) tb.connect (noise,(add,1)) tb.connect (add,head) tb.connect (tail,dst) tb.run() data = dst.data() ntotal = len(data) nright=0 for i in range(ntotal): if packet[i]==data[i]: nright=nright+1 #else: #print "Error in ", i return (ntotal,ntotal-nright) def main(args): nargs = len (args) if nargs == 3: fname_out=args[0] esn0_db=float(args[1]) rep=int(args[2]) else: sys.stderr.write ('usage: test_turbo_equalization.py fsm_name_out Es/No_db repetitions\n') sys.exit (1) # system parameters Kb=64*16 # packet size in bits (multiple of 16) modulation = fsm_utils.pam4 # see fsm_utlis.py for available predefined modulations channel = fsm_utils.c_channel # see fsm_utlis.py for available predefined test channels fo=trellis.fsm(fname_out) # get the outer FSM specification from a file fi=trellis.fsm(len(modulation[1]),len(channel)) # generate the FSM automatically if fo.O() != fi.I(): sys.stderr.write ('Incompatible cardinality between outer and inner FSM.\n') sys.exit (1) bitspersymbol = int(round(math.log(fo.I())/math.log(2))) # bits per FSM input symbol K=Kb/bitspersymbol # packet size in trellis steps interleaver=trellis.interleaver(K,666) # construct a random interleaver tot_channel = fsm_utils.make_isi_lookup(modulation,channel,True) # generate the lookup table (normalize energy to 1) dimensionality = tot_channel[0] tot_constellation = tot_channel[1] if len(tot_constellation)/dimensionality != fi.O(): sys.stderr.write ('Incompatible FSM output cardinality and lookup table size.\n') sys.exit (1) N0=pow(10.0,-esn0_db/10.0); # noise variance IT = 3 # number of turbo iterations tot_s=0 # total number of transmitted shorts terr_s=0 # total number of shorts in error terr_p=0 # total number of packets in error for i in range(rep): (s,e)=run_test(fo,fi,interleaver,Kb,bitspersymbol,K,channel,modulation,dimensionality,tot_constellation,1,N0,IT,-long(666+i)) # run experiment with different seed to get different noise realizations tot_s=tot_s+s terr_s=terr_s+e terr_p=terr_p+(terr_s!=0) if ((i+1)%10==0) : # display progress print i+1,terr_p, '%.2e' % ((1.0*terr_p)/(i+1)),tot_s,terr_s, '%.2e' % ((1.0*terr_s)/tot_s) # estimate of the (short or bit) error rate print rep,terr_p, '%.2e' % ((1.0*terr_p)/(i+1)),tot_s,terr_s, '%.2e' % ((1.0*terr_s)/tot_s) if __name__ == '__main__': main (sys.argv[1:])
# -*- coding: utf-8 -*- from __future__ import unicode_literals from django.db import models, migrations import cms.models.fields import cms.test_utils.project.placeholderapp.models class Migration(migrations.Migration): dependencies = [ ('cms', '0002_auto_20140816_1918'), ] operations = [ migrations.CreateModel( name='DynamicPlaceholderSlotExample', fields=[ ('id', models.AutoField(primary_key=True, verbose_name='ID', auto_created=True, serialize=False)), ('char_1', models.CharField(max_length=255, verbose_name='char_1')), ('char_2', models.CharField(max_length=255, verbose_name='char_2')), ('placeholder_1', cms.models.fields.PlaceholderField(null=True, to='cms.Placeholder', slotname=cms.test_utils.project.placeholderapp.models.dynamic_placeholder_1, related_name='dynamic_pl_1', editable=False)), ('placeholder_2', cms.models.fields.PlaceholderField(null=True, to='cms.Placeholder', slotname=cms.test_utils.project.placeholderapp.models.dynamic_placeholder_2, related_name='dynamic_pl_2', editable=False)), ], options={ }, bases=(models.Model,), ), migrations.CreateModel( name='Example1', fields=[ ('id', models.AutoField(primary_key=True, verbose_name='ID', auto_created=True, serialize=False)), ('char_1', models.CharField(max_length=255, verbose_name='char_1')), ('char_2', models.CharField(max_length=255, verbose_name='char_2')), ('char_3', models.CharField(max_length=255, verbose_name='char_3')), ('char_4', models.CharField(max_length=255, verbose_name='char_4')), ('date_field', models.DateField(null=True)), ('placeholder', cms.models.fields.PlaceholderField(null=True, to='cms.Placeholder', slotname='placeholder', editable=False)), ], options={ }, bases=(models.Model,), ), migrations.CreateModel( name='MultilingualExample1', fields=[ ('id', models.AutoField(primary_key=True, verbose_name='ID', auto_created=True, serialize=False)), ('placeholder_1', cms.models.fields.PlaceholderField(null=True, to='cms.Placeholder', slotname='placeholder_1', editable=False)), ], options={ 'abstract': False, }, bases=(models.Model,), ), migrations.CreateModel( name='MultilingualExample1Translation', fields=[ ('id', models.AutoField(primary_key=True, verbose_name='ID', auto_created=True, serialize=False)), ('char_1', models.CharField(max_length=255, verbose_name='char_1')), ('char_2', models.CharField(max_length=255, verbose_name='char_2')), ('language_code', models.CharField(db_index=True, max_length=15)), ('master', models.ForeignKey(null=True, to='placeholderapp.MultilingualExample1', related_name='translations', editable=False)), ], options={ 'db_table': 'placeholderapp_multilingualexample1_translation', }, bases=(models.Model,), ), migrations.AlterUniqueTogether( name='multilingualexample1translation', unique_together=set([('language_code', 'master')]), ), migrations.CreateModel( name='TwoPlaceholderExample', fields=[ ('id', models.AutoField(primary_key=True, verbose_name='ID', auto_created=True, serialize=False)), ('char_1', models.CharField(max_length=255, verbose_name='char_1')), ('char_2', models.CharField(max_length=255, verbose_name='char_2')), ('char_3', models.CharField(max_length=255, verbose_name='char_3')), ('char_4', models.CharField(max_length=255, verbose_name='char_4')), ('placeholder_1', cms.models.fields.PlaceholderField(null=True, to='cms.Placeholder', slotname='placeholder_1', related_name='p1', editable=False)), ('placeholder_2', cms.models.fields.PlaceholderField(null=True, to='cms.Placeholder', slotname='placeholder_2', related_name='p2', editable=False)), ], options={ }, bases=(models.Model,), ), ]
# Copyright (c) 2008-2009 Aryeh Leib Taurog, http://www.aryehleib.com # All rights reserved. # # Modified from original contribution by Aryeh Leib Taurog, which was # released under the New BSD license. import unittest from django.contrib.gis.geos.mutable_list import ListMixin from django.utils import six class UserListA(ListMixin): _mytype = tuple def __init__(self, i_list, *args, **kwargs): self._list = self._mytype(i_list) super(UserListA, self).__init__(*args, **kwargs) def __len__(self): return len(self._list) def __str__(self): return str(self._list) def __repr__(self): return repr(self._list) def _set_list(self, length, items): # this would work: # self._list = self._mytype(items) # but then we wouldn't be testing length parameter itemList = ['x'] * length for i, v in enumerate(items): itemList[i] = v self._list = self._mytype(itemList) def _get_single_external(self, index): return self._list[index] class UserListB(UserListA): _mytype = list def _set_single(self, index, value): self._list[index] = value def nextRange(length): nextRange.start += 100 return range(nextRange.start, nextRange.start + length) nextRange.start = 0 class ListMixinTest(unittest.TestCase): """ Tests base class ListMixin by comparing a list clone which is a ListMixin subclass with a real Python list. """ limit = 3 listType = UserListA def lists_of_len(self, length=None): if length is None: length = self.limit pl = list(range(length)) return pl, self.listType(pl) def limits_plus(self, b): return range(-self.limit - b, self.limit + b) def step_range(self): return list(range(-1 - self.limit, 0)) + list(range(1, 1 + self.limit)) def test01_getslice(self): 'Slice retrieval' pl, ul = self.lists_of_len() for i in self.limits_plus(1): self.assertEqual(pl[i:], ul[i:], 'slice [%d:]' % (i)) self.assertEqual(pl[:i], ul[:i], 'slice [:%d]' % (i)) for j in self.limits_plus(1): self.assertEqual(pl[i:j], ul[i:j], 'slice [%d:%d]' % (i, j)) for k in self.step_range(): self.assertEqual(pl[i:j:k], ul[i:j:k], 'slice [%d:%d:%d]' % (i, j, k)) for k in self.step_range(): self.assertEqual(pl[i::k], ul[i::k], 'slice [%d::%d]' % (i, k)) self.assertEqual(pl[:i:k], ul[:i:k], 'slice [:%d:%d]' % (i, k)) for k in self.step_range(): self.assertEqual(pl[::k], ul[::k], 'slice [::%d]' % (k)) def test02_setslice(self): 'Slice assignment' def setfcn(x, i, j, k, L): x[i:j:k] = range(L) pl, ul = self.lists_of_len() for slen in range(self.limit + 1): ssl = nextRange(slen) ul[:] = ssl pl[:] = ssl self.assertEqual(pl, ul[:], 'set slice [:]') for i in self.limits_plus(1): ssl = nextRange(slen) ul[i:] = ssl pl[i:] = ssl self.assertEqual(pl, ul[:], 'set slice [%d:]' % (i)) ssl = nextRange(slen) ul[:i] = ssl pl[:i] = ssl self.assertEqual(pl, ul[:], 'set slice [:%d]' % (i)) for j in self.limits_plus(1): ssl = nextRange(slen) ul[i:j] = ssl pl[i:j] = ssl self.assertEqual(pl, ul[:], 'set slice [%d:%d]' % (i, j)) for k in self.step_range(): ssl = nextRange(len(ul[i:j:k])) ul[i:j:k] = ssl pl[i:j:k] = ssl self.assertEqual(pl, ul[:], 'set slice [%d:%d:%d]' % (i, j, k)) sliceLen = len(ul[i:j:k]) self.assertRaises(ValueError, setfcn, ul, i, j, k, sliceLen + 1) if sliceLen > 2: self.assertRaises(ValueError, setfcn, ul, i, j, k, sliceLen - 1) for k in self.step_range(): ssl = nextRange(len(ul[i::k])) ul[i::k] = ssl pl[i::k] = ssl self.assertEqual(pl, ul[:], 'set slice [%d::%d]' % (i, k)) ssl = nextRange(len(ul[:i:k])) ul[:i:k] = ssl pl[:i:k] = ssl self.assertEqual(pl, ul[:], 'set slice [:%d:%d]' % (i, k)) for k in self.step_range(): ssl = nextRange(len(ul[::k])) ul[::k] = ssl pl[::k] = ssl self.assertEqual(pl, ul[:], 'set slice [::%d]' % (k)) def test03_delslice(self): 'Delete slice' for Len in range(self.limit): pl, ul = self.lists_of_len(Len) del pl[:] del ul[:] self.assertEqual(pl[:], ul[:], 'del slice [:]') for i in range(-Len - 1, Len + 1): pl, ul = self.lists_of_len(Len) del pl[i:] del ul[i:] self.assertEqual(pl[:], ul[:], 'del slice [%d:]' % (i)) pl, ul = self.lists_of_len(Len) del pl[:i] del ul[:i] self.assertEqual(pl[:], ul[:], 'del slice [:%d]' % (i)) for j in range(-Len - 1, Len + 1): pl, ul = self.lists_of_len(Len) del pl[i:j] del ul[i:j] self.assertEqual(pl[:], ul[:], 'del slice [%d:%d]' % (i, j)) for k in list(range(-Len - 1, 0)) + list(range(1, Len)): pl, ul = self.lists_of_len(Len) del pl[i:j:k] del ul[i:j:k] self.assertEqual(pl[:], ul[:], 'del slice [%d:%d:%d]' % (i, j, k)) for k in list(range(-Len - 1, 0)) + list(range(1, Len)): pl, ul = self.lists_of_len(Len) del pl[:i:k] del ul[:i:k] self.assertEqual(pl[:], ul[:], 'del slice [:%d:%d]' % (i, k)) pl, ul = self.lists_of_len(Len) del pl[i::k] del ul[i::k] self.assertEqual(pl[:], ul[:], 'del slice [%d::%d]' % (i, k)) for k in list(range(-Len - 1, 0)) + list(range(1, Len)): pl, ul = self.lists_of_len(Len) del pl[::k] del ul[::k] self.assertEqual(pl[:], ul[:], 'del slice [::%d]' % (k)) def test04_get_set_del_single(self): 'Get/set/delete single item' pl, ul = self.lists_of_len() for i in self.limits_plus(0): self.assertEqual(pl[i], ul[i], 'get single item [%d]' % i) for i in self.limits_plus(0): pl, ul = self.lists_of_len() pl[i] = 100 ul[i] = 100 self.assertEqual(pl[:], ul[:], 'set single item [%d]' % i) for i in self.limits_plus(0): pl, ul = self.lists_of_len() del pl[i] del ul[i] self.assertEqual(pl[:], ul[:], 'del single item [%d]' % i) def test05_out_of_range_exceptions(self): 'Out of range exceptions' def setfcn(x, i): x[i] = 20 def getfcn(x, i): return x[i] def delfcn(x, i): del x[i] pl, ul = self.lists_of_len() for i in (-1 - self.limit, self.limit): self.assertRaises(IndexError, setfcn, ul, i) # 'set index %d' % i) self.assertRaises(IndexError, getfcn, ul, i) # 'get index %d' % i) self.assertRaises(IndexError, delfcn, ul, i) # 'del index %d' % i) def test06_list_methods(self): 'List methods' pl, ul = self.lists_of_len() pl.append(40) ul.append(40) self.assertEqual(pl[:], ul[:], 'append') pl.extend(range(50, 55)) ul.extend(range(50, 55)) self.assertEqual(pl[:], ul[:], 'extend') pl.reverse() ul.reverse() self.assertEqual(pl[:], ul[:], 'reverse') for i in self.limits_plus(1): pl, ul = self.lists_of_len() pl.insert(i, 50) ul.insert(i, 50) self.assertEqual(pl[:], ul[:], 'insert at %d' % i) for i in self.limits_plus(0): pl, ul = self.lists_of_len() self.assertEqual(pl.pop(i), ul.pop(i), 'popped value at %d' % i) self.assertEqual(pl[:], ul[:], 'after pop at %d' % i) pl, ul = self.lists_of_len() self.assertEqual(pl.pop(), ul.pop(i), 'popped value') self.assertEqual(pl[:], ul[:], 'after pop') pl, ul = self.lists_of_len() def popfcn(x, i): x.pop(i) self.assertRaises(IndexError, popfcn, ul, self.limit) self.assertRaises(IndexError, popfcn, ul, -1 - self.limit) pl, ul = self.lists_of_len() for val in range(self.limit): self.assertEqual(pl.index(val), ul.index(val), 'index of %d' % val) for val in self.limits_plus(2): self.assertEqual(pl.count(val), ul.count(val), 'count %d' % val) for val in range(self.limit): pl, ul = self.lists_of_len() pl.remove(val) ul.remove(val) self.assertEqual(pl[:], ul[:], 'after remove val %d' % val) def indexfcn(x, v): return x.index(v) def removefcn(x, v): return x.remove(v) self.assertRaises(ValueError, indexfcn, ul, 40) self.assertRaises(ValueError, removefcn, ul, 40) def test07_allowed_types(self): 'Type-restricted list' pl, ul = self.lists_of_len() ul._allowed = six.integer_types ul[1] = 50 ul[:2] = [60, 70, 80] def setfcn(x, i, v): x[i] = v self.assertRaises(TypeError, setfcn, ul, 2, 'hello') self.assertRaises(TypeError, setfcn, ul, slice(0, 3, 2), ('hello', 'goodbye')) def test08_min_length(self): 'Length limits' pl, ul = self.lists_of_len() ul._minlength = 1 def delfcn(x, i): del x[:i] def setfcn(x, i): x[:i] = [] for i in range(self.limit - ul._minlength + 1, self.limit + 1): self.assertRaises(ValueError, delfcn, ul, i) self.assertRaises(ValueError, setfcn, ul, i) del ul[:ul._minlength] ul._maxlength = 4 for i in range(0, ul._maxlength - len(ul)): ul.append(i) self.assertRaises(ValueError, ul.append, 10) def test09_iterable_check(self): 'Error on assigning non-iterable to slice' pl, ul = self.lists_of_len(self.limit + 1) def setfcn(x, i, v): x[i] = v self.assertRaises(TypeError, setfcn, ul, slice(0, 3, 2), 2) def test10_checkindex(self): 'Index check' pl, ul = self.lists_of_len() for i in self.limits_plus(0): if i < 0: self.assertEqual(ul._checkindex(i), i + self.limit, '_checkindex(neg index)') else: self.assertEqual(ul._checkindex(i), i, '_checkindex(pos index)') for i in (-self.limit - 1, self.limit): self.assertRaises(IndexError, ul._checkindex, i) def test_11_sorting(self): 'Sorting' pl, ul = self.lists_of_len() pl.insert(0, pl.pop()) ul.insert(0, ul.pop()) pl.sort() ul.sort() self.assertEqual(pl[:], ul[:], 'sort') mid = pl[len(pl) // 2] pl.sort(key=lambda x: (mid - x) ** 2) ul.sort(key=lambda x: (mid - x) ** 2) self.assertEqual(pl[:], ul[:], 'sort w/ key') pl.insert(0, pl.pop()) ul.insert(0, ul.pop()) pl.sort(reverse=True) ul.sort(reverse=True) self.assertEqual(pl[:], ul[:], 'sort w/ reverse') mid = pl[len(pl) // 2] pl.sort(key=lambda x: (mid - x) ** 2) ul.sort(key=lambda x: (mid - x) ** 2) self.assertEqual(pl[:], ul[:], 'sort w/ key') def test_12_arithmetic(self): 'Arithmetic' pl, ul = self.lists_of_len() al = list(range(10, 14)) self.assertEqual(list(pl + al), list(ul + al), 'add') self.assertEqual(type(ul), type(ul + al), 'type of add result') self.assertEqual(list(al + pl), list(al + ul), 'radd') self.assertEqual(type(al), type(al + ul), 'type of radd result') objid = id(ul) pl += al ul += al self.assertEqual(pl[:], ul[:], 'in-place add') self.assertEqual(objid, id(ul), 'in-place add id') for n in (-1, 0, 1, 3): pl, ul = self.lists_of_len() self.assertEqual(list(pl * n), list(ul * n), 'mul by %d' % n) self.assertEqual(type(ul), type(ul * n), 'type of mul by %d result' % n) self.assertEqual(list(n * pl), list(n * ul), 'rmul by %d' % n) self.assertEqual(type(ul), type(n * ul), 'type of rmul by %d result' % n) objid = id(ul) pl *= n ul *= n self.assertEqual(pl[:], ul[:], 'in-place mul by %d' % n) self.assertEqual(objid, id(ul), 'in-place mul by %d id' % n) pl, ul = self.lists_of_len() self.assertEqual(pl, ul, 'cmp for equal') self.assertNotEqual(ul, pl + [2], 'cmp for not equal') self.assertGreaterEqual(pl, ul, 'cmp for gte self') self.assertLessEqual(pl, ul, 'cmp for lte self') self.assertGreaterEqual(ul, pl, 'cmp for self gte') self.assertLessEqual(ul, pl, 'cmp for self lte') self.assertGreater(pl + [5], ul, 'cmp') self.assertGreaterEqual(pl + [5], ul, 'cmp') self.assertLess(pl, ul + [2], 'cmp') self.assertLessEqual(pl, ul + [2], 'cmp') self.assertGreater(ul + [5], pl, 'cmp') self.assertGreaterEqual(ul + [5], pl, 'cmp') self.assertLess(ul, pl + [2], 'cmp') self.assertLessEqual(ul, pl + [2], 'cmp') # Also works with a custom IndexError ul_longer = ul + [2] ul_longer._IndexError = TypeError ul._IndexError = TypeError self.assertNotEqual(ul_longer, pl) self.assertGreater(ul_longer, ul) pl[1] = 20 self.assertGreater(pl, ul, 'cmp for gt self') self.assertLess(ul, pl, 'cmp for self lt') pl[1] = -20 self.assertLess(pl, ul, 'cmp for lt self') self.assertGreater(ul, pl, 'cmp for gt self') class ListMixinTestSingle(ListMixinTest): listType = UserListB
# Copyright 2007 Google, Inc. All Rights Reserved. # Licensed to PSF under a Contributor Agreement. """Tests for the raise statement.""" from test import support import sys import types import unittest def get_tb(): try: raise OSError() except: return sys.exc_info()[2] class Context: def __enter__(self): return self def __exit__(self, exc_type, exc_value, exc_tb): return True class TestRaise(unittest.TestCase): def test_invalid_reraise(self): try: raise except RuntimeError as e: self.assertIn("No active exception", str(e)) else: self.fail("No exception raised") def test_reraise(self): try: try: raise IndexError() except IndexError as e: exc1 = e raise except IndexError as exc2: self.assertTrue(exc1 is exc2) else: self.fail("No exception raised") def test_except_reraise(self): def reraise(): try: raise TypeError("foo") except: try: raise KeyError("caught") except KeyError: pass raise self.assertRaises(TypeError, reraise) def test_finally_reraise(self): def reraise(): try: raise TypeError("foo") except: try: raise KeyError("caught") finally: raise self.assertRaises(KeyError, reraise) def test_nested_reraise(self): def nested_reraise(): raise def reraise(): try: raise TypeError("foo") except: nested_reraise() self.assertRaises(TypeError, reraise) def test_with_reraise1(self): def reraise(): try: raise TypeError("foo") except: with Context(): pass raise self.assertRaises(TypeError, reraise) def test_with_reraise2(self): def reraise(): try: raise TypeError("foo") except: with Context(): raise KeyError("caught") raise self.assertRaises(TypeError, reraise) def test_yield_reraise(self): def reraise(): try: raise TypeError("foo") except: yield 1 raise g = reraise() next(g) self.assertRaises(TypeError, lambda: next(g)) self.assertRaises(StopIteration, lambda: next(g)) def test_erroneous_exception(self): class MyException(Exception): def __init__(self): raise RuntimeError() try: raise MyException except RuntimeError: pass else: self.fail("No exception raised") def test_new_returns_invalid_instance(self): # See issue #11627. class MyException(Exception): def __new__(cls, *args): return object() with self.assertRaises(TypeError): raise MyException class TestCause(unittest.TestCase): def test_invalid_cause(self): try: raise IndexError from 5 except TypeError as e: self.assertIn("exception cause", str(e)) else: self.fail("No exception raised") def test_class_cause(self): try: raise IndexError from KeyError except IndexError as e: self.assertIsInstance(e.__cause__, KeyError) else: self.fail("No exception raised") def test_instance_cause(self): cause = KeyError() try: raise IndexError from cause except IndexError as e: self.assertTrue(e.__cause__ is cause) else: self.fail("No exception raised") def test_erroneous_cause(self): class MyException(Exception): def __init__(self): raise RuntimeError() try: raise IndexError from MyException except RuntimeError: pass else: self.fail("No exception raised") class TestTraceback(unittest.TestCase): def test_sets_traceback(self): try: raise IndexError() except IndexError as e: self.assertIsInstance(e.__traceback__, types.TracebackType) else: self.fail("No exception raised") def test_accepts_traceback(self): tb = get_tb() try: raise IndexError().with_traceback(tb) except IndexError as e: self.assertNotEqual(e.__traceback__, tb) self.assertEqual(e.__traceback__.tb_next, tb) else: self.fail("No exception raised") class TestContext(unittest.TestCase): def test_instance_context_instance_raise(self): context = IndexError() try: try: raise context except: raise OSError() except OSError as e: self.assertEqual(e.__context__, context) else: self.fail("No exception raised") def test_class_context_instance_raise(self): context = IndexError try: try: raise context except: raise OSError() except OSError as e: self.assertNotEqual(e.__context__, context) self.assertIsInstance(e.__context__, context) else: self.fail("No exception raised") def test_class_context_class_raise(self): context = IndexError try: try: raise context except: raise OSError except OSError as e: self.assertNotEqual(e.__context__, context) self.assertIsInstance(e.__context__, context) else: self.fail("No exception raised") def test_c_exception_context(self): try: try: 1/0 except: raise OSError except OSError as e: self.assertIsInstance(e.__context__, ZeroDivisionError) else: self.fail("No exception raised") def test_c_exception_raise(self): try: try: 1/0 except: xyzzy except NameError as e: self.assertIsInstance(e.__context__, ZeroDivisionError) else: self.fail("No exception raised") def test_noraise_finally(self): try: try: pass finally: raise OSError except OSError as e: self.assertTrue(e.__context__ is None) else: self.fail("No exception raised") def test_raise_finally(self): try: try: 1/0 finally: raise OSError except OSError as e: self.assertIsInstance(e.__context__, ZeroDivisionError) else: self.fail("No exception raised") def test_context_manager(self): class ContextManager: def __enter__(self): pass def __exit__(self, t, v, tb): xyzzy try: with ContextManager(): 1/0 except NameError as e: self.assertIsInstance(e.__context__, ZeroDivisionError) else: self.fail("No exception raised") def test_cycle_broken(self): # Self-cycles (when re-raising a caught exception) are broken try: try: 1/0 except ZeroDivisionError as e: raise e except ZeroDivisionError as e: self.assertTrue(e.__context__ is None, e.__context__) def test_reraise_cycle_broken(self): # Non-trivial context cycles (through re-raising a previous exception) # are broken too. try: try: xyzzy except NameError as a: try: 1/0 except ZeroDivisionError: raise a except NameError as e: self.assertTrue(e.__context__.__context__ is None) def test_3118(self): # deleting the generator caused the __context__ to be cleared def gen(): try: yield 1 finally: pass def f(): g = gen() next(g) try: try: raise ValueError except: del g raise KeyError except Exception as e: self.assertIsInstance(e.__context__, ValueError) f() def test_3611(self): # A re-raised exception in a __del__ caused the __context__ # to be cleared class C: def __del__(self): try: 1/0 except: raise def f(): x = C() try: try: x.x except AttributeError: del x raise TypeError except Exception as e: self.assertNotEqual(e.__context__, None) self.assertIsInstance(e.__context__, AttributeError) with support.captured_output("stderr"): f() class TestRemovedFunctionality(unittest.TestCase): def test_tuples(self): try: raise (IndexError, KeyError) # This should be a tuple! except TypeError: pass else: self.fail("No exception raised") def test_strings(self): try: raise "foo" except TypeError: pass else: self.fail("No exception raised") def test_main(): support.run_unittest(__name__) if __name__ == "__main__": unittest.main()
#!/usr/bin/env python # # Copyright 2013 The Chromium Authors. All rights reserved. # Use of this source code is governed by a BSD-style license that can be # found in the LICENSE file. # # Find the most recent tombstone file(s) on all connected devices # and prints their stacks. # # Assumes tombstone file was created with current symbols. import datetime import logging import multiprocessing import os import subprocess import sys import optparse from pylib import android_commands def _ListTombstones(adb): """List the tombstone files on the device. Args: adb: An instance of AndroidCommands. Yields: Tuples of (tombstone filename, date time of file on device). """ lines = adb.RunShellCommand('TZ=UTC su -c ls -a -l /data/tombstones') for line in lines: if 'tombstone' in line and not 'No such file or directory' in line: details = line.split() t = datetime.datetime.strptime(details[-3] + ' ' + details[-2], '%Y-%m-%d %H:%M') yield details[-1], t def _GetDeviceDateTime(adb): """Determine the date time on the device. Args: adb: An instance of AndroidCommands. Returns: A datetime instance. """ device_now_string = adb.RunShellCommand('TZ=UTC date') return datetime.datetime.strptime( device_now_string[0], '%a %b %d %H:%M:%S %Z %Y') def _GetTombstoneData(adb, tombstone_file): """Retrieve the tombstone data from the device Args: tombstone_file: the tombstone to retrieve Returns: A list of lines """ return adb.GetProtectedFileContents('/data/tombstones/' + tombstone_file) def _EraseTombstone(adb, tombstone_file): """Deletes a tombstone from the device. Args: tombstone_file: the tombstone to delete. """ return adb.RunShellCommandWithSU('rm /data/tombstones/' + tombstone_file) def _ResolveSymbols(tombstone_data, include_stack): """Run the stack tool for given tombstone input. Args: tombstone_data: a list of strings of tombstone data. include_stack: boolean whether to include stack data in output. Yields: A string for each line of resolved stack output. """ stack_tool = os.path.join(os.path.dirname(__file__), '..', '..', 'third_party', 'android_platform', 'development', 'scripts', 'stack') proc = subprocess.Popen(stack_tool, stdin=subprocess.PIPE, stdout=subprocess.PIPE) output = proc.communicate(input='\n'.join(tombstone_data))[0] for line in output.split('\n'): if not include_stack and 'Stack Data:' in line: break yield line def _ResolveTombstone(tombstone): lines = [] lines += [tombstone['file'] + ' created on ' + str(tombstone['time']) + ', about this long ago: ' + (str(tombstone['device_now'] - tombstone['time']) + ' Device: ' + tombstone['serial'])] print '\n'.join(lines) print 'Resolving...' lines += _ResolveSymbols(tombstone['data'], tombstone['stack']) return lines def _ResolveTombstones(jobs, tombstones): """Resolve a list of tombstones. Args: jobs: the number of jobs to use with multiprocess. tombstones: a list of tombstones. """ if not tombstones: print 'No device attached? Or no tombstones?' return if len(tombstones) == 1: data = _ResolveTombstone(tombstones[0]) else: pool = multiprocessing.Pool(processes=jobs) data = pool.map(_ResolveTombstone, tombstones) data = ['\n'.join(d) for d in data] print '\n'.join(data) def _GetTombstonesForDevice(adb, options): """Returns a list of tombstones on a given adb connection. Args: adb: An instance of Androidcommands. options: command line arguments from OptParse """ ret = [] all_tombstones = list(_ListTombstones(adb)) if not all_tombstones: print 'No device attached? Or no tombstones?' return ret # Sort the tombstones in date order, descending all_tombstones.sort(cmp=lambda a, b: cmp(b[1], a[1])) # Only resolve the most recent unless --all-tombstones given. tombstones = all_tombstones if options.all_tombstones else [all_tombstones[0]] device_now = _GetDeviceDateTime(adb) for tombstone_file, tombstone_time in tombstones: ret += [{'serial': adb.Adb().GetSerialNumber(), 'device_now': device_now, 'time': tombstone_time, 'file': tombstone_file, 'stack': options.stack, 'data': _GetTombstoneData(adb, tombstone_file)}] # Erase all the tombstones if desired. if options.wipe_tombstones: for tombstone_file, _ in all_tombstones: _EraseTombstone(adb, tombstone_file) return ret def main(): parser = optparse.OptionParser() parser.add_option('--device', help='The serial number of the device. If not specified ' 'will use all devices.') parser.add_option('-a', '--all-tombstones', action='store_true', help="""Resolve symbols for all tombstones, rather than just the most recent""") parser.add_option('-s', '--stack', action='store_true', help='Also include symbols for stack data') parser.add_option('-w', '--wipe-tombstones', action='store_true', help='Erase all tombstones from device after processing') parser.add_option('-j', '--jobs', type='int', default=4, help='Number of jobs to use when processing multiple ' 'crash stacks.') options, args = parser.parse_args() if options.device: devices = [options.device] else: devices = android_commands.GetAttachedDevices() tombstones = [] for device in devices: adb = android_commands.AndroidCommands(device) tombstones += _GetTombstonesForDevice(adb, options) _ResolveTombstones(options.jobs, tombstones) if __name__ == '__main__': sys.exit(main())
"""SCons.Tool.tar Tool-specific initialization for tar. There normally shouldn't be any need to import this module directly. It will usually be imported through the generic SCons.Tool.Tool() selection method. """ # # __COPYRIGHT__ # # Permission is hereby granted, free of charge, to any person obtaining # a copy of this software and associated documentation files (the # "Software"), to deal in the Software without restriction, including # without limitation the rights to use, copy, modify, merge, publish, # distribute, sublicense, and/or sell copies of the Software, and to # permit persons to whom the Software is furnished to do so, subject to # the following conditions: # # The above copyright notice and this permission notice shall be included # in all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY # KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE # WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND # NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE # LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION # OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION # WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. # __revision__ = "__FILE__ __REVISION__ __DATE__ __DEVELOPER__" import SCons.Action import SCons.Builder import SCons.Defaults import SCons.Node.FS import SCons.Util tars = ['tar', 'gtar'] TarAction = SCons.Action.Action('$TARCOM', '$TARCOMSTR') TarBuilder = SCons.Builder.Builder(action = TarAction, source_factory = SCons.Node.FS.Entry, source_scanner = SCons.Defaults.DirScanner, suffix = '$TARSUFFIX', multi = 1) def generate(env): """Add Builders and construction variables for tar to an Environment.""" try: bld = env['BUILDERS']['Tar'] except KeyError: bld = TarBuilder env['BUILDERS']['Tar'] = bld env['TAR'] = env.Detect(tars) or 'gtar' env['TARFLAGS'] = SCons.Util.CLVar('-c') env['TARCOM'] = '$TAR $TARFLAGS -f $TARGET $SOURCES' env['TARSUFFIX'] = '.tar' def exists(env): return env.Detect(tars)
""" Copyright (C) 2020 Google Inc. Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. """ """ A good transform function. @param {string} inJson @return {string} outJson """ import copy import json import sys import traceback def transform(event): """ Return a Dict or List of Dict Objects. Return None to discard """ event['new_key'] = 'new_value' # event = event return event def _handle_result(input_data): event_id = copy.deepcopy(input_data['id']) event = copy.deepcopy(input_data['event']) try: transformed_event = transform(event) if isinstance(transformed_event, list): for row in transformed_event: payload = json.dumps({'status': 'SUCCESS', 'id': event_id, 'event': row, 'error_message': None}) print(payload) else: payload = json.dumps({'status': 'SUCCESS', 'id': event_id, 'event': transformed_event, 'error_message': None}) print(payload) except Exception as e: stack_trace = traceback.format_exc() payload = json.dumps({'status': 'FAILED', 'id': event_id, 'event': event, 'error_message': stack_trace}) print(payload) if __name__ == '__main__': # TODO: How do we handle the case where there are no messages file_name = sys.argv[1] data = [] with open(file_name, "r") as data_file: for line in data_file: data.append(json.loads(line)) if isinstance(data, list): for event in data: _handle_result(event) else: event = data _handle_result(event) exit()
#! /usr/bin/env python import sys import ddlib # DeepDive python utility ARR_DELIM = '~^~' # For each input tuple for row in sys.stdin: parts = row.strip().split('\t') if len(parts) != 6: print >>sys.stderr, 'Failed to parse row:', row continue # Get all fields from a row words = parts[0].split(ARR_DELIM) relation_id = parts[1] p1_start, p1_length, p2_start, p2_length = [int(x) for x in parts[2:]] # Unpack input into tuples. span1 = ddlib.Span(begin_word_id=p1_start, length=p1_length) span2 = ddlib.Span(begin_word_id=p2_start, length=p2_length) # Features for this pair come in here features = set() # Feature 1: Bag of words between the two phrases words_between = ddlib.tokens_between_spans(words, span1, span2) for word in words_between.elements: features.add("word_between=" + word) # Feature 2: Number of words between the two phrases features.add("num_words_between=%s" % len(words_between.elements)) # Feature 3: Does the last word (last name) match? last_word_left = ddlib.materialize_span(words, span1)[-1] last_word_right = ddlib.materialize_span(words, span2)[-1] if (last_word_left == last_word_right): features.add("potential_last_name_match") for feature in features: print str(relation_id) + '\t' + feature
# Copyright 2006 Georg Brandl. # Licensed to PSF under a Contributor Agreement. """Fixer for intern(). intern(s) -> sys.intern(s)""" # Local imports from .. import pytree from .. import fixer_base from ..fixer_util import Name, Attr, touch_import class FixIntern(fixer_base.BaseFix): PATTERN = """ power< 'intern' trailer< lpar='(' ( not(arglist | argument<any '=' any>) obj=any | obj=arglist<(not argument<any '=' any>) any ','> ) rpar=')' > after=any* > """ def transform(self, node, results): syms = self.syms obj = results["obj"].clone() if obj.type == syms.arglist: newarglist = obj.clone() else: newarglist = pytree.Node(syms.arglist, [obj.clone()]) after = results["after"] if after: after = [n.clone() for n in after] new = pytree.Node(syms.power, Attr(Name(u"sys"), Name(u"intern")) + [pytree.Node(syms.trailer, [results["lpar"].clone(), newarglist, results["rpar"].clone()])] + after) new.prefix = node.prefix touch_import(None, u'sys', node) return new
# -*- coding: utf-8 -*- # # psutil documentation build configuration file, created by # sphinx-quickstart. # # This file is execfile()d with the current directory set to its # containing dir. # # Note that not all possible configuration values are present in this # autogenerated file. # # All configuration values have a default; values that are commented out # serve to show the default. import datetime import os PROJECT_NAME = "psutil" AUTHOR = "Giampaolo Rodola'" THIS_YEAR = str(datetime.datetime.now().year) HERE = os.path.abspath(os.path.dirname(__file__)) def get_version(): INIT = os.path.abspath(os.path.join(HERE, '../psutil/__init__.py')) with open(INIT, 'r') as f: for line in f: if line.startswith('__version__'): ret = eval(line.strip().split(' = ')[1]) assert ret.count('.') == 2, ret for num in ret.split('.'): assert num.isdigit(), ret return ret else: raise ValueError("couldn't find version string") VERSION = get_version() # If your documentation needs a minimal Sphinx version, state it here. needs_sphinx = '1.0' # Add any Sphinx extension module names here, as strings. They can be # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom # ones. extensions = ['sphinx.ext.autodoc', 'sphinx.ext.coverage', 'sphinx.ext.pngmath', 'sphinx.ext.viewcode', 'sphinx.ext.intersphinx'] # Add any paths that contain templates here, relative to this directory. templates_path = ['_template'] # The suffix of source filenames. source_suffix = '.rst' # The encoding of source files. # source_encoding = 'utf-8-sig' # The master toctree document. master_doc = 'index' # General information about the project. project = PROJECT_NAME copyright = '2009-%s, %s' % (THIS_YEAR, AUTHOR) # The version info for the project you're documenting, acts as replacement for # |version| and |release|, also used in various other places throughout the # built documents. # # The short X.Y version. version = VERSION # The language for content autogenerated by Sphinx. Refer to documentation # for a list of supported languages. # language = None # There are two options for replacing |today|: either, you set today to some # non-false value, then it is used: # today = '' # Else, today_fmt is used as the format for a strftime call. # today_fmt = '%B %d, %Y' # List of patterns, relative to source directory, that match files and # directories to ignore when looking for source files. exclude_patterns = ['_build'] # The reST default role (used for this markup: `text`) to use for all # documents. # default_role = None # If true, '()' will be appended to :func: etc. cross-reference text. add_function_parentheses = True # If true, the current module name will be prepended to all description # unit titles (such as .. function::). # add_module_names = True autodoc_docstring_signature = True # If true, sectionauthor and moduleauthor directives will be shown in the # output. They are ignored by default. # show_authors = False # The name of the Pygments (syntax highlighting) style to use. pygments_style = 'sphinx' # A list of ignored prefixes for module index sorting. # modindex_common_prefix = [] # -- Options for HTML output ------------------------------------------------- # The theme to use for HTML and HTML Help pages. See the documentation for # a list of builtin themes. # Theme options are theme-specific and customize the look and feel of a theme # further. For a list of options available for each theme, see the # documentation. html_theme = 'pydoctheme' html_theme_options = {'collapsiblesidebar': True} # Add any paths that contain custom themes here, relative to this directory. html_theme_path = ["_themes"] # The name for this set of Sphinx documents. If None, it defaults to # "<project> v<release> documentation". html_title = "{project} {version} documentation".format(**locals()) # A shorter title for the navigation bar. Default is the same as html_title. # html_short_title = None # The name of an image file (relative to this directory) to place at the top # of the sidebar. # html_logo = 'logo.png' # The name of an image file (within the static path) to use as favicon of the # docs. This file should be a Windows icon file (.ico) being 16x16 or 32x32 # pixels large. html_favicon = '_static/favicon.ico' # Add any paths that contain custom static files (such as style sheets) here, # relative to this directory. They are copied after the builtin static files, # so a file named "default.css" will overwrite the builtin "default.css". html_static_path = ['_static'] # If not '', a 'Last updated on:' timestamp is inserted at every page bottom, # using the given strftime format. html_last_updated_fmt = '%b %d, %Y' # If true, SmartyPants will be used to convert quotes and dashes to # typographically correct entities. html_use_smartypants = True # Custom sidebar templates, maps document names to template names. html_sidebars = { 'index': 'indexsidebar.html', '**': ['globaltoc.html', 'relations.html', 'sourcelink.html', 'searchbox.html'] } # Additional templates that should be rendered to pages, maps page names to # template names. # html_additional_pages = { # 'index': 'indexcontent.html', # } # If false, no module index is generated. html_domain_indices = False # If false, no index is generated. html_use_index = True # If true, the index is split into individual pages for each letter. # html_split_index = False # If true, links to the reST sources are added to the pages. # html_show_sourcelink = True # If true, "Created using Sphinx" is shown in the HTML footer. Default is True. # html_show_sphinx = True # If true, "(C) Copyright ..." is shown in the HTML footer. Default is True. # html_show_copyright = True # If true, an OpenSearch description file will be output, and all pages will # contain a <link> tag referring to it. The value of this option must be the # base URL from which the finished HTML is served. # html_use_opensearch = '' # This is the file name suffix for HTML files (e.g. ".xhtml"). # html_file_suffix = None # Output file base name for HTML help builder. htmlhelp_basename = '%s-doc' % PROJECT_NAME # -- Options for LaTeX output ------------------------------------------------ # The paper size ('letter' or 'a4'). # latex_paper_size = 'letter' # The font size ('10pt', '11pt' or '12pt'). # latex_font_size = '10pt' # Grouping the document tree into LaTeX files. List of tuples # (source start file, target name, title, author, documentclass # [howto/manual]). latex_documents = [ ('index', '%s.tex' % PROJECT_NAME, '%s documentation' % PROJECT_NAME, AUTHOR), ] # The name of an image file (relative to this directory) to place at # the top of the title page. # latex_logo = None # For "manual" documents, if this is true, then toplevel headings are parts, # not chapters. # latex_use_parts = False # If true, show page references after internal links. # latex_show_pagerefs = False # If true, show URL addresses after external links. # latex_show_urls = False # Additional stuff for the LaTeX preamble. # latex_preamble = '' # Documents to append as an appendix to all manuals. # latex_appendices = [] # If false, no module index is generated. # latex_domain_indices = True # -- Options for manual page output ------------------------------------------ # One entry per manual page. List of tuples # (source start file, name, description, authors, manual section). man_pages = [ ('index', PROJECT_NAME, '%s documentation' % PROJECT_NAME, [AUTHOR], 1) ] # If true, show URL addresses after external links. # man_show_urls = False
""" Creates permissions for all installed apps that need permissions. """ from django.contrib.auth import models as auth_app from django.db.models import get_models, signals def _get_permission_codename(action, opts): return u'%s_%s' % (action, opts.object_name.lower()) def _get_all_permissions(opts): "Returns (codename, name) for all permissions in the given opts." perms = [] for action in ('add', 'change', 'delete'): perms.append((_get_permission_codename(action, opts), u'Can %s %s' % (action, opts.verbose_name_raw))) return perms + list(opts.permissions) def create_permissions(app, created_models, verbosity, **kwargs): from django.contrib.contenttypes.models import ContentType app_models = get_models(app) # This will hold the permissions we're looking for as # (content_type, (codename, name)) searched_perms = list() # The codenames and ctypes that should exist. ctypes = set() for klass in app_models: ctype = ContentType.objects.get_for_model(klass) ctypes.add(ctype) for perm in _get_all_permissions(klass._meta): searched_perms.append((ctype, perm)) # Find all the Permissions that have a context_type for a model we're # looking for. We don't need to check for codenames since we already have # a list of the ones we're going to create. all_perms = set(auth_app.Permission.objects.filter( content_type__in=ctypes, ).values_list( "content_type", "codename" )) for ctype, (codename, name) in searched_perms: # If the permissions exists, move on. if (ctype.pk, codename) in all_perms: continue p = auth_app.Permission.objects.create( codename=codename, name=name, content_type=ctype ) if verbosity >= 2: print "Adding permission '%s'" % p def create_superuser(app, created_models, verbosity, **kwargs): from django.core.management import call_command if auth_app.User in created_models and kwargs.get('interactive', True): msg = ("\nYou just installed Django's auth system, which means you " "don't have any superusers defined.\nWould you like to create one " "now? (yes/no): ") confirm = raw_input(msg) while 1: if confirm not in ('yes', 'no'): confirm = raw_input('Please enter either "yes" or "no": ') continue if confirm == 'yes': call_command("createsuperuser", interactive=True) break signals.post_syncdb.connect(create_permissions, dispatch_uid = "django.contrib.auth.management.create_permissions") signals.post_syncdb.connect(create_superuser, sender=auth_app, dispatch_uid = "django.contrib.auth.management.create_superuser")
#!/usr/bin/env python from __future__ import division, absolute_import, print_function from numpy.testing import * from numpy.distutils.misc_util import appendpath, minrelpath, \ gpaths, get_shared_lib_extension from os.path import join, sep, dirname ajoin = lambda *paths: join(*((sep,)+paths)) class TestAppendpath(TestCase): def test_1(self): assert_equal(appendpath('prefix', 'name'), join('prefix', 'name')) assert_equal(appendpath('/prefix', 'name'), ajoin('prefix', 'name')) assert_equal(appendpath('/prefix', '/name'), ajoin('prefix', 'name')) assert_equal(appendpath('prefix', '/name'), join('prefix', 'name')) def test_2(self): assert_equal(appendpath('prefix/sub', 'name'), join('prefix', 'sub', 'name')) assert_equal(appendpath('prefix/sub', 'sup/name'), join('prefix', 'sub', 'sup', 'name')) assert_equal(appendpath('/prefix/sub', '/prefix/name'), ajoin('prefix', 'sub', 'name')) def test_3(self): assert_equal(appendpath('/prefix/sub', '/prefix/sup/name'), ajoin('prefix', 'sub', 'sup', 'name')) assert_equal(appendpath('/prefix/sub/sub2', '/prefix/sup/sup2/name'), ajoin('prefix', 'sub', 'sub2', 'sup', 'sup2', 'name')) assert_equal(appendpath('/prefix/sub/sub2', '/prefix/sub/sup/name'), ajoin('prefix', 'sub', 'sub2', 'sup', 'name')) class TestMinrelpath(TestCase): def test_1(self): n = lambda path: path.replace('/', sep) assert_equal(minrelpath(n('aa/bb')), n('aa/bb')) assert_equal(minrelpath('..'), '..') assert_equal(minrelpath(n('aa/..')), '') assert_equal(minrelpath(n('aa/../bb')), 'bb') assert_equal(minrelpath(n('aa/bb/..')), 'aa') assert_equal(minrelpath(n('aa/bb/../..')), '') assert_equal(minrelpath(n('aa/bb/../cc/../dd')), n('aa/dd')) assert_equal(minrelpath(n('.././..')), n('../..')) assert_equal(minrelpath(n('aa/bb/.././../dd')), n('dd')) class TestGpaths(TestCase): def test_gpaths(self): local_path = minrelpath(join(dirname(__file__), '..')) ls = gpaths('command/*.py', local_path) assert_(join(local_path, 'command', 'build_src.py') in ls, repr(ls)) f = gpaths('system_info.py', local_path) assert_(join(local_path, 'system_info.py')==f[0], repr(f)) class TestSharedExtension(TestCase): def test_get_shared_lib_extension(self): import sys ext = get_shared_lib_extension(is_python_ext=False) if sys.platform.startswith('linux'): assert_equal(ext, '.so') elif sys.platform.startswith('gnukfreebsd'): assert_equal(ext, '.so') elif sys.platform.startswith('darwin'): assert_equal(ext, '.dylib') elif sys.platform.startswith('win'): assert_equal(ext, '.dll') # just check for no crash assert_(get_shared_lib_extension(is_python_ext=True)) if __name__ == "__main__": run_module_suite()
# -*- coding: utf-8 -*- ############################################################################## # # OpenERP, Open Source Management Solution # Copyright (C) 2009 Sharoon Thomas # Copyright (C) 2010-Today OpenERP SA (<http://www.openerp.com>) # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program. If not, see <http://www.gnu.org/licenses/> # ############################################################################## import email_template_preview import mail_compose_message # vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
#!/usr/bin/env python """Python version of cairo-demo/cairo_snippets/cairo_snippets_pdf.c create a file for each example rather than one large file for all examples """ from __future__ import division from math import pi as M_PI # used by many snippets import sys import cairo if not cairo.HAS_PDF_SURFACE: raise SystemExit ('cairo was not compiled with PDF support') from snippets import snip_list, snippet_normalize width_in_inches, height_in_inches = 2, 2 width_in_points, height_in_points = width_in_inches * 72, height_in_inches * 72 width, height = width_in_points, height_in_points # used by snippet_normalize() def do_snippet (snippet): if verbose_mode: print('processing %s' % snippet) filename = 'snippets/%s.pdf' % snippet surface = cairo.PDFSurface (filename, width_in_points, height_in_points) cr = cairo.Context (surface) cr.save() try: fName = 'snippets/%s.py' % snippet code = open(fName).read() exec (code, globals(), locals()) except: exc_type, exc_value = sys.exc_info()[:2] print(exc_type, exc_value, file=sys.stderr) else: cr.restore() cr.show_page() surface.finish() if verbose_mode: print if __name__ == '__main__': verbose_mode = True if len(sys.argv) > 1 and sys.argv[1] == '-s': verbose_mode = False del sys.argv[1] if len(sys.argv) > 1: # do specified snippets snippet_list = sys.argv[1:] else: # do all snippets snippet_list = snip_list for s in snippet_list: do_snippet (s)
""" Definitions of the cost for the gated-autoencoder. """ from pylearn2.costs.cost import Cost, DefaultDataSpecsMixin from pylearn2.space import VectorSpace class SymmetricCost(DefaultDataSpecsMixin, Cost): """ Summary (Class representing the symmetric cost). Subclasses can define the type of data they will use. Mean reconstruction error is used for real valued data and cross-Entropy loss is used for binary. See Also -------- "Gradient-based learning of higher-order image features" """ @staticmethod def cost(x, y, rx, ry): """ Symmetric reconstruction cost. Parameters ---------- x : tensor_like Theano symbolic representing the first input minibatch. Assumed to be 2-tensors, with the first dimension indexing training examples and the second indexing data dimensions. y : tensor_like Theano symbolic representing the seconde input minibatch. Assumed to be 2-tensors, with the first dimension indexing training examples and the second indexing data dimensions. rx : tensor_like Reconstruction of the first minibatch by the model. ry: tensor_like Reconstruction of the second minibatch by the model. Returns ------- Cost: theano_like expression Representation of the cost """ raise NotImplementedError def expr(self, model, data, *args, **kwargs): """ Returns a theano expression for the cost function. Returns a symbolic expression for a cost function applied to the minibatch of data. Optionally, may return None. This represents that the cost function is intractable but may be optimized via the get_gradients method. Parameters ---------- model : a pylearn2 Model instance data : a batch in cost.get_data_specs() form kwargs : dict Optional extra arguments. Not used by the base class. """ self.get_data_specs(model)[0].validate(data) x, y = data input_space = model.get_input_space() if not isinstance(input_space.components[0], VectorSpace): conv = input_space.components[0] vec = VectorSpace(conv.get_total_dimension()) x = conv.format_as(x, vec) if not isinstance(input_space.components[1], VectorSpace): conv = input_space.components[1] vec = VectorSpace(conv.get_total_dimension()) y = conv.format_as(y, vec) rx, ry = model.reconstructXY((x, y)) return self.cost(x, y, rx, ry) class SymmetricMSRE(SymmetricCost): """ Summary (Symmetric cost for real valued data). See Also -------- "Gradient-based learning of higher-order image features" """ @staticmethod def cost(x, y, rx, ry): """ Summary (Definition of the cost). Mean squared reconstruction error. Parameters ---------- x : tensor_like Theano symbolic representing the first input minibatch. Assumed to be 2-tensors, with the first dimension indexing training examples and the second indexing data dimensions. y : tensor_like Theano symbolic representing the seconde input minibatch. Assumed to be 2-tensors, with the first dimension indexing training examples and the second indexing data dimensions. rx : tensor_like Reconstruction of the first minibatch by the model. ry: tensor_like Reconstruction of the second minibatch by the model. Returns ------- Cost: theano_like expression Representation of the cost Notes ----- Symmetric reconstruction cost as defined by Memisevic in: "Gradient-based learning of higher-order image features". This function only works with real valued data. """ return ( ((0.5*((x - rx)**2)) + (0.5*((y - ry)**2)))).sum(axis=1).mean() class NormalizedSymmetricMSRE(SymmetricCost): """ Summary (Normalized Symmetric cost for real valued data). Notes ----- Value used to observe the percentage of reconstruction. """ @staticmethod def cost(x, y, rx, ry): """ Summary (Definition of the cost). Normalized Mean squared reconstruction error. Values between 0 and 1. Parameters ---------- x : tensor_like Theano symbolic representing the first input minibatch. Assumed to be 2-tensors, with the first dimension indexing training examples and the second indexing data dimensions. y : tensor_like Theano symbolic representing the seconde input minibatch. Assumed to be 2-tensors, with the first dimension indexing training examples and the second indexing data dimensions. rx : tensor_like Reconstruction of the first minibatch by the model. ry: tensor_like Reconstruction of the second minibatch by the model. Returns ------- Cost: theano_like expression Representation of the cost Notes ----- Do not use this function to train, only to monitor the average percentage of reconstruction achieved when training on real valued data. """ num = (((0.5*((x - rx)**2)) + (0.5*((y - ry)**2)))).sum(axis=1).mean() den = ((0.5*(x.norm(2, 1)**2)) + (0.5*(y.norm(2, 1)**2))).mean() return num/den
import os import unittest import urllib2 import json import wptserve from base import TestUsingServer, doc_root class TestFileHandler(TestUsingServer): def test_not_handled(self): with self.assertRaises(urllib2.HTTPError) as cm: resp = self.request("/not_existing") self.assertEquals(cm.exception.code, 404) class TestRewriter(TestUsingServer): def test_rewrite(self): @wptserve.handlers.handler def handler(request, response): return request.request_path route = ("GET", "/test/rewritten", handler) self.server.rewriter.register("GET", "/test/original", route[1]) self.server.router.register(*route) resp = self.request("/test/original") self.assertEquals(200, resp.getcode()) self.assertEquals("/test/rewritten", resp.read()) class TestRequestHandler(TestUsingServer): def test_exception(self): @wptserve.handlers.handler def handler(request, response): raise Exception route = ("GET", "/test/raises", handler) self.server.router.register(*route) with self.assertRaises(urllib2.HTTPError) as cm: resp = self.request("/test/raises") self.assertEquals(cm.exception.code, 500) if __name__ == "__main__": unittest.main()
from __future__ import unicode_literals from .common import InfoExtractor from ..utils import ( int_or_none, unified_strdate, ) class ExpoTVIE(InfoExtractor): _VALID_URL = r'https?://www\.expotv\.com/videos/[^?#]*/(?P<id>[0-9]+)($|[?#])' _TEST = { 'url': 'http://www.expotv.com/videos/reviews/3/40/NYX-Butter-lipstick/667916', 'md5': 'fe1d728c3a813ff78f595bc8b7a707a8', 'info_dict': { 'id': '667916', 'ext': 'mp4', 'title': 'NYX Butter Lipstick Little Susie', 'description': 'Goes on like butter, but looks better!', 'thumbnail': 're:^https?://.*\.jpg$', 'uploader': 'Stephanie S.', 'upload_date': '20150520', 'view_count': int, } } def _real_extract(self, url): video_id = self._match_id(url) webpage = self._download_webpage(url, video_id) player_key = self._search_regex( r'<param name="playerKey" value="([^"]+)"', webpage, 'player key') config = self._download_json( 'http://client.expotv.com/video/config/%s/%s' % (video_id, player_key), video_id, 'Downloading video configuration') formats = [] for fcfg in config['sources']: media_url = fcfg.get('file') if not media_url: continue if fcfg.get('type') == 'm3u8': formats.extend(self._extract_m3u8_formats( media_url, video_id, 'mp4', entry_protocol='m3u8_native', m3u8_id='hls')) else: formats.append({ 'url': media_url, 'height': int_or_none(fcfg.get('height')), 'format_id': fcfg.get('label'), 'ext': self._search_regex( r'filename=.*\.([a-z0-9_A-Z]+)&', media_url, 'file extension', default=None) or fcfg.get('type'), }) self._sort_formats(formats) title = self._og_search_title(webpage) description = self._og_search_description(webpage) thumbnail = config.get('image') view_count = int_or_none(self._search_regex( r'<h5>Plays: ([0-9]+)</h5>', webpage, 'view counts')) uploader = self._search_regex( r'<div class="reviewer">\s*<img alt="([^"]+)"', webpage, 'uploader', fatal=False) upload_date = unified_strdate(self._search_regex( r'<h5>Reviewed on ([0-9/.]+)</h5>', webpage, 'upload date', fatal=False), day_first=False) return { 'id': video_id, 'formats': formats, 'title': title, 'description': description, 'view_count': view_count, 'thumbnail': thumbnail, 'uploader': uploader, 'upload_date': upload_date, }
__author__ = 'gambit' import boto from boto.iam.connection import IAMConnection from boto.s3.key import Key import datetime import time import smtplib import os class IdentityAccessManagement(): admin_access_key = "XXXXXXXXXXXXXXXXXXXXXXX" admin_secret_key = "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX" def create_user(self, s3_user): connect = IAMConnection(self.admin_access_key, self.admin_secret_key) user = connect.get_all_users() users = user['list_users_response']['list_users_result']['users'] for user in users: if s3_user in user['user_name']: return False connect.create_user(s3_user) return True def access_key(self, s3_user): connect = IAMConnection(self.admin_access_key, self.admin_secret_key) key = connect.create_access_key(s3_user) access_key = key['create_access_key_response'][u'create_access_key_result'][u'access_key'][u'access_key_id'] secret_key = key['create_access_key_response'][u'create_access_key_result'][u'access_key'][u'secret_access_key'] return s3_user, access_key, secret_key def attach_policy(self, S3_User, bucket_name): policy = '''{ "Version": "2012-10-17", "Statement": [ { "Action": [ "s3:ListAllMyBuckets" ], "Effect": "Allow", "Resource": "arn:aws:s3:::*" }, { "Action": "s3:*", "Effect": "Allow", "Resource": [ "arn:aws:s3:::%s*", "arn:aws:s3:::%s*/*" ] } ] }''' % (bucket_name, bucket_name) print policy # # Attach Policy to acces s3 bucket connect = IAMConnection(self.admin_access_key, self.admin_secret_key) connect.put_user_policy(S3_User, bucket_name, policy) def create_s3_bucket(self, bucket_name): s3 = boto.connect_s3(self.admin_access_key, self.admin_secret_key) all_bucket = s3.get_all_buckets() for bucket in all_bucket: name = bucket.name if bucket_name not in name: s3.create_bucket(bucket_name) return True else: return False
# Copyright 2013 Dean Gardiner <gardiner91@gmail.com> # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. import copy from logr import Logr GROUP_MATCHES = ['identifier'] class CaperNode(object): def __init__(self, closure, parent=None, match=None): """ :type parent: CaperNode :type weight: float """ #: :type: caper.objects.CaperClosure self.closure = closure #: :type: CaperNode self.parent = parent #: :type: CaptureMatch self.match = match #: :type: list of CaptureGroup self.finished_groups = [] def next(self): raise NotImplementedError() def captured(self): cur = self if cur.match: yield cur.match.tag, cur.match.result while cur.parent: cur = cur.parent if cur.match: yield cur.match.tag, cur.match.result class CaperRootNode(CaperNode): def __init__(self, closure): """ :type closure: caper.objects.CaperClosure or list of caper.objects.CaperClosure """ super(CaperRootNode, self).__init__(closure) def next(self): return self.closure class CaperClosureNode(CaperNode): def __init__(self, closure, parent=None, match=None): """ :type closure: caper.objects.CaperClosure or list of caper.objects.CaperClosure """ super(CaperClosureNode, self).__init__(closure, parent, match) def next(self): if not self.closure: return None if self.match: # Jump to next closure if we have a match return self.closure.right elif len(self.closure.fragments) > 0: # Otherwise parse the fragments return self.closure.fragments[0] return None def __str__(self): return "<CaperClosureNode match: %s>" % repr(self.match) def __repr__(self): return self.__str__() class CaperFragmentNode(CaperNode): def __init__(self, closure, fragments, parent=None, match=None): """ :type closure: caper.objects.CaperClosure :type fragments: list of caper.objects.CaperFragment """ super(CaperFragmentNode, self).__init__(closure, parent, match) #: :type: caper.objects.CaperFragment or list of caper.objects.CaperFragment self.fragments = fragments def next(self): if len(self.fragments) > 0 and self.fragments[-1] and self.fragments[-1].right: return self.fragments[-1].right if self.closure.right: return self.closure.right return None def __str__(self): return "<CaperFragmentNode match: %s>" % repr(self.match) def __repr__(self): return self.__str__() class CaperResult(object): def __init__(self): #: :type: list of CaperNode self.heads = [] self.chains = [] def build(self): max_matched = 0 for head in self.heads: for chain in self.combine_chain(head): if chain.num_matched > max_matched: max_matched = chain.num_matched self.chains.append(chain) for chain in self.chains: chain.weights.append(chain.num_matched / float(max_matched or chain.num_matched or 1)) chain.finish() self.chains.sort(key=lambda chain: chain.weight, reverse=True) for chain in self.chains: Logr.debug("chain weight: %.02f", chain.weight) Logr.debug("\tInfo: %s", chain.info) Logr.debug("\tWeights: %s", chain.weights) Logr.debug("\tNumber of Fragments Matched: %s", chain.num_matched) def combine_chain(self, subject, chain=None): nodes = subject if type(subject) is list else [subject] if chain is None: chain = CaperResultChain() result = [] for x, node in enumerate(nodes): node_chain = chain if x == len(nodes) - 1 else chain.copy() if not node.parent: result.append(node_chain) continue node_chain.update(node) result.extend(self.combine_chain(node.parent, node_chain)) return result class CaperResultChain(object): def __init__(self): #: :type: float self.weight = None self.info = {} self.num_matched = 0 self.weights = [] def update(self, subject): """ :type subject: CaperFragmentNode """ if not subject.match or not subject.match.success: return # TODO this should support closure nodes if type(subject) is CaperFragmentNode: self.num_matched += len(subject.fragments) if subject.fragments is not None else 0 self.weights.append(subject.match.weight) if subject.match: if subject.match.tag not in self.info: self.info[subject.match.tag] = [] self.info[subject.match.tag].insert(0, subject.match.result) def finish(self): self.weight = sum(self.weights) / len(self.weights) def copy(self): chain = CaperResultChain() chain.weight = self.weight chain.info = copy.deepcopy(self.info) chain.num_matched = self.num_matched chain.weights = copy.copy(self.weights) return chain
#!/usr/bin/python # -*- coding: utf-8 -*- # (c) 2015, Billy Kimble <basslines@gmail.com> # # This file is part of Ansible # # Ansible is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Ansible is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Ansible. If not, see <http://www.gnu.org/licenses/>. DOCUMENTATION = """ module: hall short_description: Send notification to Hall description: - "The M(hall) module connects to the U(https://hall.com) messaging API and allows you to deliver notication messages to rooms." version_added: "2.0" author: Billy Kimble (@bkimble) <basslines@gmail.com> options: room_token: description: - "Room token provided to you by setting up the Ansible room integation on U(https://hall.com)" required: true msg: description: - The message you wish to deliver as a notifcation required: true title: description: - The title of the message required: true picture: description: - "The full URL to the image you wish to use for the Icon of the message. Defaults to U(http://cdn2.hubspot.net/hub/330046/file-769078210-png/Official_Logos/ansible_logo_black_square_small.png?t=1421076128627)" required: false """ EXAMPLES = """ - name: Send Hall notifiation local_action: module: hall room_token: <hall room integration token> title: Nginx msg: Created virtual host file on {{ inventory_hostname }} - name: Send Hall notification if EC2 servers were created. when: ec2.instances|length > 0 local_action: module: hall room_token: <hall room integration token> title: Server Creation msg: "Created EC2 instance {{ item.id }} of type {{ item.instance_type }}.\\nInstance can be reached at {{ item.public_ip }} in the {{ item.region }} region." with_items: ec2.instances """ HALL_API_ENDPOINT = 'https://hall.com/api/1/services/generic/%s' def send_request_to_hall(module, room_token, payload): headers = {'Content-Type': 'application/json'} payload=module.jsonify(payload) api_endpoint = HALL_API_ENDPOINT % (room_token) response, info = fetch_url(module, api_endpoint, data=payload, headers=headers) if info['status'] != 200: secure_url = HALL_API_ENDPOINT % ('[redacted]') module.fail_json(msg=" failed to send %s to %s: %s" % (payload, secure_url, info['msg'])) def main(): module = AnsibleModule( argument_spec = dict( room_token = dict(type='str', required=True), msg = dict(type='str', required=True), title = dict(type='str', required=True), picture = dict(type='str', default='http://cdn2.hubspot.net/hub/330046/file-769078210-png/Official_Logos/ansible_logo_black_square_small.png?t=1421076128627'), ) ) room_token = module.params['room_token'] message = module.params['msg'] title = module.params['title'] picture = module.params['picture'] payload = {'title': title, 'message': message, 'picture': picture} send_request_to_hall(module, room_token, payload) module.exit_json(msg="OK") from ansible.module_utils.basic import * from ansible.module_utils.urls import * main()
from __future__ import print_function import os, string, tempfile, shutil from subprocess import Popen from ase.io import write from ase.units import Bohr class Bader: '''class for running bader analysis and extracting data from it. The class runs bader, extracts the charge density and outputs it to a cube file. Then you call different functions of the class to extract the charges, volumes, etc... ACF.dat contains the coordinates of each atom, the charge associated with it according to Bader partitioning, percentage of the whole according to Bader partitioning and the minimum distance to the surface. This distance should be compared to maximum cut-off radius for the core region if pseudo potentials have been used. BCF.dat contains the coordinates of each Bader maxima, the charge within that volume, the nearest atom and the distance to that atom. AtomVolumes.dat contains the number of each volume that has been assigned to each atom. These numbers correspond to the number of the BvAtxxxx.dat files. The options for the executable are:: bader [ -c bader | voronoi ] [ -n bader | voronoi ] [ -b neargrid | ongrid ] [ -r refine_edge_iterations ] [ -ref reference_charge ] [ -p all_atom | all_bader ] [ -p sel_atom | sel_bader ] [volume list] [ -p atom_index | bader_index ] [ -i cube | chgcar ] [ -h ] [ -v ] chargefile References: G. Henkelman, A. Arnaldsson, and H. Jonsson, A fast and robust algorithm for Bader decomposition of charge density, Comput. Mater. Sci. 36 254-360 (2006). E. Sanville, S. D. Kenny, R. Smith, and G. Henkelman An improved grid-based algorithm for Bader charge allocation, J. Comp. Chem. 28 899-908 (2007). W. Tang, E. Sanville, and G. Henkelman A grid-based Bader analysis algorithm without lattice bias, J. Phys.: Condens. Matter 21 084204 (2009). ''' def __init__(self, atoms): ''' ''' self.atoms = atoms #get density and write cube file calc = atoms.get_calculator() ncfile = calc.get_nc() base, ext = os.path.splitext(ncfile) x, y, z, density = calc.get_charge_density() cubefile = base + '_charge_density.cube' self.densityfile = cubefile if not os.path.exists(cubefile): write(cubefile, atoms, data=density * Bohr ** 3) #cmd to run for bader analysis. check if output exists so we #don't run this too often. acf_file = base + '_ACF.dat' if not os.path.exists(acf_file): #mk tempdir tempdir = tempfile.mkdtemp() cwd = os.getcwd() abscubefile = os.path.abspath(cubefile) os.chdir(tempdir) cmd = 'bader %s' % abscubefile process = Popen(cmd) status = Popen.wait() if status != 0: print(process) shutil.copy2('ACF.dat', os.path.join(cwd, acf_file)) os.chdir(cwd) shutil.rmtree(tempdir) self.charges = [] self.volumes = [] #now parse the output f = open(acf_file, 'r') #skip 2 lines f.readline() f.readline() for i, atom in enumerate(self.atoms): line = f.readline() fields = line.split() n = int(fields[0]) x = float(fields[1]) y = float(fields[2]) z = float(fields[3]) chg = float(fields[4]) mindist = float(fields[5]) vol = float(fields[6]) self.charges.append(chg) self.volumes.append(vol) f.close() def get_bader_charges(self): return self.charges def get_bader_volumes(self): 'return volumes in Ang**3' return [x * Bohr ** 3 for x in self.volumes] def write_atom_volume(self, atomlist): '''write bader atom volumes to cube files. atomlist = [0,2] #for example -p sel_atom Write the selected atomic volumes, read from the subsequent list of volumes. ''' alist = string.join([str(x) for x in atomlist], ' ') cmd = 'bader -p sel_atom %s %s' % (alist, self.densityfile) print(cmd) os.system(cmd) def write_bader_volume(self, atomlist): """write bader atom volumes to cube files. :: atomlist = [0,2] # for example -p sel_bader Write the selected Bader volumes, read from the subsequent list of volumes. """ alist = string.join([str(x) for x in atomlist], ' ') cmd = 'bader -p sel_bader %s %s' % (alist, self.densityfile) print(cmd) os.system(cmd) def write_atom_index(self): ''' -p atom_index Write the atomic volume index to a charge density file. ''' cmd = 'bader -p atom_index %s' % (self.densityfile) print(cmd) os.system(cmd) def write_bader_index(self): ''' -p bader_index Write the Bader volume index to a charge density file. ''' cmd = 'bader -p bader_index %s' % (self.densityfile) print(cmd) os.system(cmd) def write_all_atom(self): ''' -p all_atom Combine all volumes associated with an atom and write to file. This is done for all atoms and written to files named BvAtxxxx.dat. The volumes associated with atoms are those for which the maximum in charge density within the volume is closest to the atom. ''' cmd = 'bader -p all_atom %s' % (self.densityfile) print(cmd) os.system(cmd) def write_all_bader(self): ''' -p all_bader Write all Bader volumes (containing charge above threshold of 0.0001) to a file. The charge distribution in each volume is written to a separate file, named Bvolxxxx.dat. It will either be of a CHGCAR format or a CUBE file format, depending on the format of the initial charge density file. These files can be quite large, so this option should be used with caution. ''' cmd = 'bader -p all_bader %s' % (self.densityfile) print(cmd) os.system(cmd) if __name__ == '__main__': from ase.calculators.jacapo import Jacapo atoms = Jacapo.read_atoms('ethylene.nc') b = Bader(atoms) print(b.get_bader_charges()) print(b.get_bader_volumes()) b.write_atom_volume([3, 4])
# vim: tabstop=4 shiftwidth=4 softtabstop=4 # Copyright (c) 2010 OpenStack, LLC. # Copyright 2010 United States Government as represented by the # Administrator of the National Aeronautics and Space Administration. # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. """ Chance (Random) Scheduler implementation """ import random from manila import exception from manila.scheduler import driver from oslo.config import cfg CONF = cfg.CONF class ChanceScheduler(driver.Scheduler): """Implements Scheduler as a random node selector.""" def _filter_hosts(self, request_spec, hosts, **kwargs): """Filter a list of hosts based on request_spec.""" filter_properties = kwargs.get('filter_properties', {}) ignore_hosts = filter_properties.get('ignore_hosts', []) hosts = [host for host in hosts if host not in ignore_hosts] return hosts def _schedule(self, context, topic, request_spec, **kwargs): """Picks a host that is up at random.""" elevated = context.elevated() hosts = self.hosts_up(elevated, topic) if not hosts: msg = _("Is the appropriate service running?") raise exception.NoValidHost(reason=msg) hosts = self._filter_hosts(request_spec, hosts, **kwargs) if not hosts: msg = _("Could not find another host") raise exception.NoValidHost(reason=msg) return hosts[int(random.random() * len(hosts))] def schedule_create_share(self, context, request_spec, filter_properties): """Picks a host that is up at random.""" topic = CONF.share_topic host = self._schedule(context, topic, request_spec, filter_properties=filter_properties) share_id = request_spec['share_id'] snapshot_id = request_spec['snapshot_id'] updated_share = driver.share_update_db(context, share_id, host) self.share_rpcapi.create_share(context, updated_share, host, request_spec, filter_properties, snapshot_id)
#!/usr/bin/python2.4 # # Copyright 2011 Google Inc. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. """Handler for assisting with the machine install process.""" # Disable 'Import not at top of file' lint error. # pylint: disable-msg=C6204, C6205, W0611 import logging from django.utils import simplejson from google.appengine.api import memcache from google.appengine.ext import db from google.appengine.ext import deferred from google.appengine.ext import webapp from google.appengine.ext.webapp.util import run_wsgi_app from common import ec2_manager from common import enum from handlers import base from handlers import launch_tasks from models import client_machine INIT_START = '/init/start' INSTALL_FAILED = '/init/install_failed' INSTALL_SUCEEDED = '/init/install_succeeded' class InitializationStart(base.BaseHandler): """Handler to acknowledge a machine starting initialization.""" # Disable 'Invalid method name' lint error. # pylint: disable-msg=C6409 def get(self): """Updates the status of a machine starting initialization.""" instance_id = self.GetRequiredParameter('instance_id') instance = db.GqlQuery('SELECT * FROM ClientMachine WHERE client_id = :1', instance_id).get() if not instance: logging.error('The given instance id "%s" does not match any machines.', instance_id) self.error(500) return if instance.status != enum.MACHINE_STATUS.PROVISIONED: logging.error('The machine with instance id "%s" was in an unexpected ' 'state for initialization: "%s"', instance_id, enum.MACHINE_STATUS.LookupKey(instance.status)) instance.status = enum.MACHINE_STATUS.INITIALIZING instance.put() self.response.out.write('Initialization acknowledged.') class InstallFailed(base.BaseHandler): """Handler to deal with a machine that fails to properly setup and install.""" # Disable 'Invalid method name' lint error. # pylint: disable-msg=C6409 def post(self): """Updates the status of a machine that failed with initialization.""" instance_id = self.GetRequiredParameter('instance_id') log = self.GetOptionalParameter('log', None) old_instance = db.GqlQuery( 'SELECT * FROM ClientMachine WHERE client_id = :1', instance_id).get() if not old_instance: logging.error('The given instance id "%s" does not match any machines.', instance_id) self.error(500) return if old_instance.status != enum.MACHINE_STATUS.INITIALIZING: logging.error('The machine with instance id "%s" was in an unexpected ' 'state for initialization: "%s"', instance_id, enum.MACHINE_STATUS.LookupKey(old_instance.status)) old_instance.status = enum.MACHINE_STATUS.FAILED if log: old_instance.initialization_log = log old_instance.put() if old_instance.retry_count >= client_machine.MAX_RETRIES: logging.error('Reached the maximum number of retries for starting this ' 'machine: %s.', str(old_instance.key())) logging.info('Terminating the failed instance.') deferred.defer(launch_tasks.TerminateFailedMachine, instance_id, _countdown=launch_tasks.DEFAULT_COUNTDOWN, _queue=launch_tasks.DEFAULT_QUEUE) self.error(500) return logging.info('Rebooting the failed instance.') deferred.defer(launch_tasks.RebootMachine, instance_id, _countdown=launch_tasks.DEFAULT_COUNTDOWN, _queue=launch_tasks.DEFAULT_QUEUE) self.response.out.write('Initialization failure acknowledged.') class InstallSucceeded(base.BaseHandler): """Handler to deal with a machine that installs successfully.""" # Disable 'Invalid method name' lint error. # pylint: disable-msg=C6409 def post(self): """Updates the status of a machine that succeeded with initialization.""" instance_id = self.GetRequiredParameter('instance_id') log = self.GetOptionalParameter('log', None) instance = db.GqlQuery('SELECT * FROM ClientMachine WHERE client_id = :1', instance_id).get() if not instance: logging.error('The given instance id "%s" does not match any machines.', instance_id) self.error(500) return if instance.status != enum.MACHINE_STATUS.INITIALIZING: logging.error('The machine with instance id "%s" was in an unexpected ' 'state for initialization: "%s"', instance_id, enum.MACHINE_STATUS.LookupKey(instance.status)) instance.status = enum.MACHINE_STATUS.RUNNING if log: instance.initialization_log = log instance.put() self.response.out.write('Initialization success acknowledged.') application = webapp.WSGIApplication( [(INIT_START, InitializationStart), (INSTALL_FAILED, InstallFailed), (INSTALL_SUCEEDED, InstallSucceeded)], debug=True) def main(): run_wsgi_app(application) if __name__ == '__main__': main()
'''CTS: Cluster Testing System: AIS dependent modules... ''' __copyright__ = ''' Copyright (C) 2007 Andrew Beekhof <andrew@suse.de> ''' # # This program is free software; you can redistribute it and/or # modify it under the terms of the GNU General Public License # as published by the Free Software Foundation; either version 2 # of the License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program; if not, write to the Free Software # Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA. from cts.CTSvars import * from cts.CM_lha import crm_lha from cts.CTS import Process from cts.patterns import PatternSelector ####################################################################### # # LinuxHA v2 dependent modules # ####################################################################### class crm_ais(crm_lha): ''' The crm version 3 cluster manager class. It implements the things we need to talk to and manipulate crm clusters running on top of openais ''' def __init__(self, Environment, randseed=None, name=None): if not name: name="crm-ais" crm_lha.__init__(self, Environment, randseed=randseed, name=name) self.fullcomplist = {} self.templates = PatternSelector(self.name) def NodeUUID(self, node): return node def ais_components(self, extra={}): complist = [] if not len(self.fullcomplist.keys()): for c in ["cib", "lrmd", "crmd", "attrd" ]: self.fullcomplist[c] = Process( self, c, pats = self.templates.get_component(self.name, c), badnews_ignore = self.templates.get_component(self.name, "%s-ignore" % c), common_ignore = self.templates.get_component(self.name, "common-ignore")) # pengine uses dc_pats instead of pats self.fullcomplist["pengine"] = Process( self, "pengine", dc_pats = self.templates.get_component(self.name, "pengine"), badnews_ignore = self.templates.get_component(self.name, "pengine-ignore"), common_ignore = self.templates.get_component(self.name, "common-ignore")) # stonith-ng's process name is different from its component name self.fullcomplist["stonith-ng"] = Process( self, "stonith-ng", process="stonithd", pats = self.templates.get_component(self.name, "stonith"), badnews_ignore = self.templates.get_component(self.name, "stonith-ignore"), common_ignore = self.templates.get_component(self.name, "common-ignore")) # add (or replace) any extra components passed in self.fullcomplist.update(extra) # Processes running under valgrind can't be shot with "killall -9 processname", # so don't include them in the returned list vgrind = self.Env["valgrind-procs"].split() for key in list(self.fullcomplist.keys()): if self.Env["valgrind-tests"]: if key in vgrind: self.log("Filtering %s from the component list as it is being profiled by valgrind" % key) continue if key == "stonith-ng" and not self.Env["DoFencing"]: continue complist.append(self.fullcomplist[key]) return complist class crm_cs_v0(crm_ais): ''' The crm version 3 cluster manager class. It implements the things we need to talk to and manipulate crm clusters running against version 0 of our plugin ''' def __init__(self, Environment, randseed=None, name=None): if not name: name="crm-plugin-v0" crm_ais.__init__(self, Environment, randseed=randseed, name=name) def Components(self): extra = {} extra["corosync"] = Process( self, "corosync", pats = self.templates.get_component(self.name, "corosync"), badnews_ignore = self.templates.get_component(self.name, "corosync-ignore"), common_ignore = self.templates.get_component(self.name, "common-ignore") ) return self.ais_components(extra=extra) class crm_cs_v1(crm_cs_v0): ''' The crm version 3 cluster manager class. It implements the things we need to talk to and manipulate crm clusters running on top of version 1 of our plugin ''' def __init__(self, Environment, randseed=None, name=None): if not name: name="crm-plugin-v1" crm_cs_v0.__init__(self, Environment, randseed=randseed, name=name) class crm_mcp(crm_cs_v0): ''' The crm version 4 cluster manager class. It implements the things we need to talk to and manipulate crm clusters running on top of native corosync (no plugins) ''' def __init__(self, Environment, randseed=None, name=None): if not name: name="crm-mcp" crm_cs_v0.__init__(self, Environment, randseed=randseed, name=name) if self.Env["have_systemd"]: self.update({ # When systemd is in use, we can look for this instead "Pat:We_stopped" : "%s.*Corosync Cluster Engine exiting normally", }) class crm_cman(crm_cs_v0): ''' The crm version 3 cluster manager class. It implements the things we need to talk to and manipulate crm clusters running on top of openais ''' def __init__(self, Environment, randseed=None, name=None): if not name: name="crm-cman" crm_cs_v0.__init__(self, Environment, randseed=randseed, name=name)
# from yowsup.layers.protocol_media import mediacipher import tempfile import os class DownloadableMediaMessageBuilder(object): def __init__(self, downloadbleMediaMessageClass, jid, filepath): self.jid = jid self.filepath = filepath self.encryptedFilepath = None self.cls = downloadbleMediaMessageClass self.mediaKey = None self.attributes = {} self.mediaType = self.cls.__name__.split("DownloadableMediaMessageProtocolEntity")[0].lower() #ugly ? # def encrypt(self): # fd, encpath = tempfile.mkstemp() # mediaKey = os.urandom(112) # keys = mediacipher.getDerivedKeys(mediaKey) # out = mediacipher.encryptImage(self.filepath, keys) # with open(encImagePath, 'w') as outF: # outF.write(out) # # self.mediaKey = mediaKey # self.encryptedFilepath = encpath # def decrypt(self): # self.mediaKey = None # self.encryptedFilePath = None def setEncryptionData(self, mediaKey, encryptedFilepath): self.mediaKey = mediaKey self.encryptedFilepath = encryptedFilepath def isEncrypted(self): return self.encryptedFilepath is not None def getFilepath(self): return self.encryptedFilepath or self.filepath def getOriginalFilepath(self): return self.filepath def set(self, key, val): self.attributes[key] = val def get(self, key, default = None): if key in self.attributes and self.attributes[key] is not None: return self.attributes[key] return default def getOrSet(self, key, func): if not self.get(key): self.set(key, func()) def build(self, url = None, ip = None): if url: self.set("url", url) if ip: self.set("ip", ip) return self.cls.fromBuilder(self)
# -*- coding: utf-8 -*- ## ## This file is part of Invenio. ## Copyright (C) 2012 CERN. ## ## Invenio is free software; you can redistribute it and/or ## modify it under the terms of the GNU General Public License as ## published by the Free Software Foundation; either version 2 of the ## License, or (at your option) any later version. ## ## Invenio is distributed in the hope that it will be useful, but ## WITHOUT ANY WARRANTY; without even the implied warranty of ## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU ## General Public License for more details. ## ## You should have received a copy of the GNU General Public License ## along with Invenio; if not, write to the Free Software Foundation, Inc., ## 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA. from invenio.dbquery import run_sql depends_on = ['invenio_release_1_1_0'] def info(): return "New selfcite tables" def do_upgrade(): run_sql(""" CREATE TABLE IF NOT EXISTS `rnkRECORDSCACHE` ( `id_bibrec` int(10) unsigned NOT NULL, `authorid` bigint(10) NOT NULL, PRIMARY KEY (`id_bibrec`,`authorid`) ) ENGINE=MyISAM""") run_sql(""" CREATE TABLE IF NOT EXISTS `rnkEXTENDEDAUTHORS` ( `id` int(10) unsigned NOT NULL, `authorid` bigint(10) NOT NULL, PRIMARY KEY (`id`,`authorid`) ) ENGINE=MyISAM""") run_sql(""" CREATE TABLE IF NOT EXISTS `rnkSELFCITES` ( `id_bibrec` int(10) unsigned NOT NULL, `count` int(10) unsigned NOT NULL, `references` text NOT NULL, `last_updated` datetime NOT NULL, PRIMARY KEY (`id_bibrec`) ) ENGINE=MyISAM""") def estimate(): return 1
""" Extra HTML Widget classes """ from django.newforms.widgets import Widget, Select from django.utils.dates import MONTHS import datetime __all__ = ('SelectDateWidget',) class SelectDateWidget(Widget): """ A Widget that splits date input into three <select> boxes. This also serves as an example of a Widget that has more than one HTML element and hence implements value_from_datadict. """ month_field = '%s_month' day_field = '%s_day' year_field = '%s_year' def __init__(self, attrs=None, years=None): # years is an optional list/tuple of years to use in the "year" select box. self.attrs = attrs or {} if years: self.years = years else: this_year = datetime.date.today().year self.years = range(this_year, this_year+10) def render(self, name, value, attrs=None): try: value = datetime.date(*map(int, value.split('-'))) year_val, month_val, day_val = value.year, value.month, value.day except (AttributeError, TypeError, ValueError): year_val = month_val = day_val = None output = [] month_choices = MONTHS.items() month_choices.sort() select_html = Select(choices=month_choices).render(self.month_field % name, month_val) output.append(select_html) day_choices = [(i, i) for i in range(1, 32)] select_html = Select(choices=day_choices).render(self.day_field % name, day_val) output.append(select_html) year_choices = [(i, i) for i in self.years] select_html = Select(choices=year_choices).render(self.year_field % name, year_val) output.append(select_html) return u'\n'.join(output) def value_from_datadict(self, data, name): y, m, d = data.get(self.year_field % name), data.get(self.month_field % name), data.get(self.day_field % name) if y and m and d: return '%s-%s-%s' % (y, m, d) return None
from MySQLdb.constants import FIELD_TYPE from django.contrib.gis.gdal import OGRGeomType from django.db.backends.mysql.introspection import DatabaseIntrospection class MySQLIntrospection(DatabaseIntrospection): # Updating the data_types_reverse dictionary with the appropriate # type for Geometry fields. data_types_reverse = DatabaseIntrospection.data_types_reverse.copy() data_types_reverse[FIELD_TYPE.GEOMETRY] = 'GeometryField' def get_geometry_type(self, table_name, geo_col): cursor = self.connection.cursor() try: # In order to get the specific geometry type of the field, # we introspect on the table definition using `DESCRIBE`. cursor.execute('DESCRIBE %s' % self.connection.ops.quote_name(table_name)) # Increment over description info until we get to the geometry # column. for column, typ, null, key, default, extra in cursor.fetchall(): if column == geo_col: # Using OGRGeomType to convert from OGC name to Django field. # MySQL does not support 3D or SRIDs, so the field params # are empty. field_type = OGRGeomType(typ).django field_params = {} break finally: cursor.close() return field_type, field_params
''' Debian and other distributions "unbundle" requests' vendored dependencies, and rewrite all imports to use the global versions of ``urllib3`` and ``chardet``. The problem with this is that not only requests itself imports those dependencies, but third-party code outside of the distros' control too. In reaction to these problems, the distro maintainers replaced ``requests.packages`` with a magical "stub module" that imports the correct modules. The implementations were varying in quality and all had severe problems. For example, a symlink (or hardlink) that links the correct modules into place introduces problems regarding object identity, since you now have two modules in `sys.modules` with the same API, but different identities:: requests.packages.urllib3 is not urllib3 With version ``2.5.2``, requests started to maintain its own stub, so that distro-specific breakage would be reduced to a minimum, even though the whole issue is not requests' fault in the first place. See https://github.com/kennethreitz/requests/pull/2375 for the corresponding pull request. ''' from __future__ import absolute_import import sys try: from . import urllib3 except ImportError: import urllib3 sys.modules['%s.urllib3' % __name__] = urllib3 try: from . import chardet except ImportError: import chardet sys.modules['%s.chardet' % __name__] = chardet try: from . import idna except ImportError: import idna sys.modules['%s.idna' % __name__] = idna
"""The tests for local file camera component.""" from unittest import mock from homeassistant.components.local_file.const import DOMAIN, SERVICE_UPDATE_FILE_PATH from homeassistant.setup import async_setup_component from tests.common import mock_registry async def test_loading_file(hass, hass_client): """Test that it loads image from disk.""" mock_registry(hass) with mock.patch("os.path.isfile", mock.Mock(return_value=True)), mock.patch( "os.access", mock.Mock(return_value=True) ): await async_setup_component( hass, "camera", { "camera": { "name": "config_test", "platform": "local_file", "file_path": "mock.file", } }, ) await hass.async_block_till_done() client = await hass_client() m_open = mock.mock_open(read_data=b"hello") with mock.patch( "homeassistant.components.local_file.camera.open", m_open, create=True ): resp = await client.get("/api/camera_proxy/camera.config_test") assert resp.status == 200 body = await resp.text() assert body == "hello" async def test_file_not_readable(hass, caplog): """Test a warning is shown setup when file is not readable.""" mock_registry(hass) with mock.patch("os.path.isfile", mock.Mock(return_value=True)), mock.patch( "os.access", mock.Mock(return_value=False) ): await async_setup_component( hass, "camera", { "camera": { "name": "config_test", "platform": "local_file", "file_path": "mock.file", } }, ) await hass.async_block_till_done() assert "Could not read" in caplog.text assert "config_test" in caplog.text assert "mock.file" in caplog.text async def test_camera_content_type(hass, hass_client): """Test local_file camera content_type.""" cam_config_jpg = { "name": "test_jpg", "platform": "local_file", "file_path": "/path/to/image.jpg", } cam_config_png = { "name": "test_png", "platform": "local_file", "file_path": "/path/to/image.png", } cam_config_svg = { "name": "test_svg", "platform": "local_file", "file_path": "/path/to/image.svg", } cam_config_noext = { "name": "test_no_ext", "platform": "local_file", "file_path": "/path/to/image", } await async_setup_component( hass, "camera", {"camera": [cam_config_jpg, cam_config_png, cam_config_svg, cam_config_noext]}, ) await hass.async_block_till_done() client = await hass_client() image = "hello" m_open = mock.mock_open(read_data=image.encode()) with mock.patch( "homeassistant.components.local_file.camera.open", m_open, create=True ): resp_1 = await client.get("/api/camera_proxy/camera.test_jpg") resp_2 = await client.get("/api/camera_proxy/camera.test_png") resp_3 = await client.get("/api/camera_proxy/camera.test_svg") resp_4 = await client.get("/api/camera_proxy/camera.test_no_ext") assert resp_1.status == 200 assert resp_1.content_type == "image/jpeg" body = await resp_1.text() assert body == image assert resp_2.status == 200 assert resp_2.content_type == "image/png" body = await resp_2.text() assert body == image assert resp_3.status == 200 assert resp_3.content_type == "image/svg+xml" body = await resp_3.text() assert body == image # default mime type assert resp_4.status == 200 assert resp_4.content_type == "image/jpeg" body = await resp_4.text() assert body == image async def test_update_file_path(hass): """Test update_file_path service.""" # Setup platform mock_registry(hass) with mock.patch("os.path.isfile", mock.Mock(return_value=True)), mock.patch( "os.access", mock.Mock(return_value=True) ): camera_1 = {"platform": "local_file", "file_path": "mock/path.jpg"} camera_2 = { "platform": "local_file", "name": "local_file_camera_2", "file_path": "mock/path_2.jpg", } await async_setup_component(hass, "camera", {"camera": [camera_1, camera_2]}) await hass.async_block_till_done() # Fetch state and check motion detection attribute state = hass.states.get("camera.local_file") assert state.attributes.get("friendly_name") == "Local File" assert state.attributes.get("file_path") == "mock/path.jpg" service_data = {"entity_id": "camera.local_file", "file_path": "new/path.jpg"} await hass.services.async_call(DOMAIN, SERVICE_UPDATE_FILE_PATH, service_data) await hass.async_block_till_done() state = hass.states.get("camera.local_file") assert state.attributes.get("file_path") == "new/path.jpg" # Check that local_file_camera_2 file_path is still as configured state = hass.states.get("camera.local_file_camera_2") assert state.attributes.get("file_path") == "mock/path_2.jpg"
from django.contrib.auth.models import User from concepts.importer import ConceptsImporter, ValidationLogger from concepts.validation_messages import OPENMRS_NAMES_EXCEPT_SHORT_MUST_BE_UNIQUE, OPENMRS_MUST_HAVE_EXACTLY_ONE_PREFERRED_NAME, \ OPENMRS_SHORT_NAME_CANNOT_BE_PREFERRED, OPENMRS_PREFERRED_NAME_UNIQUE_PER_SOURCE_LOCALE, \ OPENMRS_AT_LEAST_ONE_FULLY_SPECIFIED_NAME, OPENMRS_FULLY_SPECIFIED_NAME_UNIQUE_PER_SOURCE_LOCALE from concepts.models import Concept, ConceptVersion from concepts.tests import ConceptBaseTest from integration_tests.models import TestStream from mappings.importer import MappingsImporter from mappings.models import Mapping from mappings.models import MappingVersion from mappings.tests import MappingBaseTest from sources.models import SourceVersion from oclapi.models import CUSTOM_VALIDATION_SCHEMA_OPENMRS, LOOKUP_CONCEPT_CLASSES from test_helper.base import create_source, create_user, create_concept class BulkConceptImporterTest(ConceptBaseTest): def setUp(self): super(BulkConceptImporterTest, self).setUp() User.objects.create( username='superuser', password='superuser', email='superuser@test.com', last_name='Super', first_name='User', is_superuser=True ) def test_import_single_concept_without_fully_specified_name(self): self.testfile = open('./integration_tests/fixtures/concept_without_fully_specified_name.json', 'rb') stderr_stub = TestStream() source = create_source(self.user1, validation_schema=CUSTOM_VALIDATION_SCHEMA_OPENMRS) importer = ConceptsImporter(source, self.testfile, 'test', TestStream(), stderr_stub, save_validation_errors=False) importer.import_concepts(total=1) self.assertTrue(OPENMRS_AT_LEAST_ONE_FULLY_SPECIFIED_NAME in stderr_stub.getvalue()) def test_import_concepts_with_invalid_records(self): self.testfile = open('./integration_tests/fixtures/valid_invalid_concepts.json', 'rb') stderr_stub = TestStream() source = create_source(self.user1, validation_schema=CUSTOM_VALIDATION_SCHEMA_OPENMRS) importer = ConceptsImporter(source, self.testfile, 'test', TestStream(), stderr_stub, save_validation_errors=False) importer.import_concepts(total=7) self.assertTrue(OPENMRS_AT_LEAST_ONE_FULLY_SPECIFIED_NAME in stderr_stub.getvalue()) self.assertTrue(OPENMRS_FULLY_SPECIFIED_NAME_UNIQUE_PER_SOURCE_LOCALE in stderr_stub.getvalue()) self.assertEquals(5, Concept.objects.exclude(concept_class__in=LOOKUP_CONCEPT_CLASSES).count()) self.assertEquals(5, ConceptVersion.objects.exclude(concept_class__in=LOOKUP_CONCEPT_CLASSES).count()) def test_update_concept_with_invalid_record(self): (concept, _) = create_concept(mnemonic='1', user=self.user1, source=self.source1, names=[self.name]) self.testfile = open('./integration_tests/fixtures/concept_without_fully_specified_name.json', 'rb') stderr_stub = TestStream() source = create_source(self.user1, validation_schema=CUSTOM_VALIDATION_SCHEMA_OPENMRS) importer = ConceptsImporter(source, self.testfile, 'test', TestStream(), stderr_stub, save_validation_errors=False) importer.import_concepts(total=1) self.assertTrue(OPENMRS_AT_LEAST_ONE_FULLY_SPECIFIED_NAME in stderr_stub.getvalue()) self.assertEquals(1, Concept.objects.exclude(concept_class__in=LOOKUP_CONCEPT_CLASSES).count()) self.assertEquals(1, ConceptVersion.objects.exclude(concept_class__in=LOOKUP_CONCEPT_CLASSES).count()) def test_import_concepts_into_openmrs_validated_source_with_valid_records(self): test_file = open('./integration_tests/fixtures/concepts_for_openmrs_validation.json', 'rb') stderr_stub = TestStream() user = create_user() source = create_source(user, validation_schema=CUSTOM_VALIDATION_SCHEMA_OPENMRS) importer = ConceptsImporter(source, test_file, 'test', TestStream(), stderr_stub, save_validation_errors=False) importer.import_concepts(total=5) self.assertTrue(OPENMRS_MUST_HAVE_EXACTLY_ONE_PREFERRED_NAME in stderr_stub.getvalue()) self.assertTrue(OPENMRS_SHORT_NAME_CANNOT_BE_PREFERRED in stderr_stub.getvalue()) self.assertTrue(OPENMRS_SHORT_NAME_CANNOT_BE_PREFERRED in stderr_stub.getvalue()) self.assertTrue(OPENMRS_NAMES_EXCEPT_SHORT_MUST_BE_UNIQUE in stderr_stub.getvalue()) self.assertEquals(2, Concept.objects.exclude(concept_class__in=LOOKUP_CONCEPT_CLASSES).count()) self.assertEquals(2, ConceptVersion.objects.exclude(concept_class__in=LOOKUP_CONCEPT_CLASSES).count()) def test_validation_error_file_output(self): self.testfile = open('./integration_tests/fixtures/valid_invalid_concepts.json', 'rb') stderr_stub = TestStream() logger = ValidationLogger(output=TestStream()) source = create_source(self.user1, validation_schema=CUSTOM_VALIDATION_SCHEMA_OPENMRS) importer = ConceptsImporter(source, self.testfile, 'test', TestStream(), stderr_stub, validation_logger=logger) importer.import_concepts(total=7) self.assertTrue('MNEMONIC;ERROR;JSON' in logger.output.getvalue()) self.assertTrue('4;%s' % OPENMRS_AT_LEAST_ONE_FULLY_SPECIFIED_NAME in logger.output.getvalue()) self.assertTrue('7;%s' % OPENMRS_FULLY_SPECIFIED_NAME_UNIQUE_PER_SOURCE_LOCALE in logger.output.getvalue()) def test_validation_error_file_exists(self): self.testfile = open('./integration_tests/fixtures/valid_invalid_concepts.json', 'rb') stderr_stub = TestStream() output_file_name = 'test_file.csv' logger = ValidationLogger(output_file_name=output_file_name) importer = ConceptsImporter(create_source(user=self.user1, validation_schema=CUSTOM_VALIDATION_SCHEMA_OPENMRS), self.testfile, 'test', TestStream(), stderr_stub, validation_logger=logger) importer.import_concepts(total=7) from os import path, remove self.assertTrue(path.exists(output_file_name)) remove(output_file_name) class ConceptImporterTest(ConceptBaseTest): def setUp(self): super(ConceptImporterTest, self).setUp() User.objects.create( username='superuser', password='superuser', email='superuser@test.com', last_name='Super', first_name='User', is_superuser=True ) self.testfile = open('./integration_tests/fixtures/one_concept.json', 'rb') def test_import_job_for_one_record(self): stdout_stub = TestStream() importer = ConceptsImporter(self.source1, self.testfile, 'test', stdout_stub, TestStream(), save_validation_errors=False) importer.import_concepts(total=1) self.assertTrue('Created new concept: 1 = Diagnosis' in stdout_stub.getvalue()) self.assertTrue('Finished importing concepts!' in stdout_stub.getvalue()) inserted_concept = Concept.objects.get(mnemonic='1') self.assertEquals(inserted_concept.parent, self.source1) inserted_concept_version = ConceptVersion.objects.get(versioned_object_id=inserted_concept.id) source_version_latest = SourceVersion.get_latest_version_of(self.source1) self.assertEquals(source_version_latest.concepts, [inserted_concept_version.id]) def test_import_job_for_change_in_data(self): stdout_stub = TestStream() create_concept(mnemonic='1', user=self.user1, source=self.source1) importer = ConceptsImporter(self.source1, self.testfile, 'test', stdout_stub, TestStream(), save_validation_errors=False) importer.import_concepts(total=1) all_concept_versions = ConceptVersion.objects.exclude(concept_class__in=LOOKUP_CONCEPT_CLASSES) self.assertEquals(len(all_concept_versions), 2) latest_concept_version = [version for version in all_concept_versions if version.previous_version][0] self.assertEquals(len(latest_concept_version.names), 4) self.assertTrue(('Updated concept, replacing version ID ' + latest_concept_version.previous_version.id) in stdout_stub.getvalue()) self.assertTrue('**** Processed 1 out of 1 concepts - 1 updated, ****' in stdout_stub.getvalue()) class MappingImporterTest(MappingBaseTest): def setUp(self): super(MappingImporterTest, self).setUp() User.objects.create( username='superuser', password='superuser', email='superuser@test.com', last_name='Super', first_name='User', is_superuser=True ) self.testfile = open('./integration_tests/fixtures/one_mapping.json', 'rb') def test_import_job_for_one_record(self): stdout_stub = TestStream() stderr_stub = TestStream() importer = MappingsImporter(self.source1, self.testfile, stdout_stub, stderr_stub, 'test') importer.import_mappings(total=1) self.assertTrue('Created new mapping:' in stdout_stub.getvalue()) self.assertTrue('/users/user1/sources/source1/:413532003' in stdout_stub.getvalue()) inserted_mapping = Mapping.objects.get(to_concept_code='413532003') self.assertEquals(inserted_mapping.to_source, self.source1) self.assertEquals(inserted_mapping.from_source, self.source2) mapping_ids = SourceVersion.get_latest_version_of(self.source1).mappings mapping_version = MappingVersion.objects.get(versioned_object_id=inserted_mapping.id, is_latest_version=True) self.assertEquals(mapping_ids[0], mapping_version.id) def test_import_job_for_one_invalid_record(self): stdout_stub = TestStream() stderr_stub = TestStream() invalid_json_file = open('./integration_tests/fixtures/one_invalid_mapping.json', 'rb') importer = MappingsImporter(self.source1, invalid_json_file, stdout_stub, stderr_stub, 'test') importer.import_mappings(total=1) self.assertTrue('Cannot map concept to itself.' in stderr_stub.getvalue()) def test_import_job_for_change_in_data(self): stdout_stub = TestStream() stderr_stub = TestStream() mapping = Mapping( parent=self.source1, map_type='SAME-AS', from_concept=self.concept3, to_source=self.source1, to_concept_code='413532003', external_id='junk' ) kwargs = { 'parent_resource': self.source1, } Mapping.persist_new(mapping, self.user1, **kwargs) source_version = SourceVersion.get_latest_version_of(self.source1) source_version.mappings = [mapping.id] source_version.save() importer = MappingsImporter(self.source1, self.testfile, stdout_stub, stderr_stub, 'test') importer.import_mappings(total=1) self.assertTrue('**** Processed 1 out of 1 mappings - 1 updated, ****' in stdout_stub.getvalue()) self.assertTrue(('Updated mapping with ID ' + mapping.id) in stdout_stub.getvalue()) updated_mapping = Mapping.objects.get(to_concept_code='413532003') self.assertTrue(updated_mapping.retired) self.assertEquals(updated_mapping.external_id, '70279ABBBBBBBBBBBBBBBBBBBBBBBBBBBBBB') def test_update_mapping_with_invalid_record(self): mapping = Mapping( parent=self.source1, map_type='SAME-AS', from_concept=self.concept3, to_concept=self.concept1 ) kwargs = { 'parent_resource': self.source1, } Mapping.persist_new(mapping, self.user1, **kwargs) source_version = SourceVersion.get_latest_version_of(self.source1) source_version.mappings = [mapping.id] source_version.save() stderr_stub = TestStream() invalid_json_file = open('./integration_tests/fixtures/one_internal_invalid_mapping.json', 'rb') importer = MappingsImporter(self.source1, invalid_json_file, TestStream(), stderr_stub, 'test') importer.import_mappings(total=1) self.assertTrue( "Must specify either 'to_concept' or 'to_source' & 'to_concept_code'. Cannot specify both." in stderr_stub.getvalue()) def test_import_valid_invalid_mappings(self): stdout_stub = TestStream() stderr_stub = TestStream() invalid_json_file = open('./integration_tests/fixtures/valid_invalid_mapping.json', 'rb') importer = MappingsImporter(self.source1, invalid_json_file, stdout_stub, stderr_stub, 'test') importer.import_mappings(total=5) self.assertTrue('Cannot map concept to itself.' in stderr_stub.getvalue()) self.assertTrue("Must specify either 'to_concept' or 'to_source' & " in stderr_stub.getvalue()) self.assertEquals(3, Mapping.objects.count()) self.assertEquals(3, MappingVersion.objects.count())
from django.core.exceptions import FieldError from django.test import TestCase from models import (SelfRefer, Tag, TagCollection, Entry, SelfReferChild, SelfReferChildSibling, Worksheet) class M2MRegressionTests(TestCase): def assertRaisesErrorWithMessage(self, error, message, callable, *args, **kwargs): self.assertRaises(error, callable, *args, **kwargs) try: callable(*args, **kwargs) except error, e: self.assertEqual(message, str(e)) def test_multiple_m2m(self): # Multiple m2m references to model must be distinguished when # accessing the relations through an instance attribute. s1 = SelfRefer.objects.create(name='s1') s2 = SelfRefer.objects.create(name='s2') s3 = SelfRefer.objects.create(name='s3') s1.references.add(s2) s1.related.add(s3) e1 = Entry.objects.create(name='e1') t1 = Tag.objects.create(name='t1') t2 = Tag.objects.create(name='t2') e1.topics.add(t1) e1.related.add(t2) self.assertQuerysetEqual(s1.references.all(), ["<SelfRefer: s2>"]) self.assertQuerysetEqual(s1.related.all(), ["<SelfRefer: s3>"]) self.assertQuerysetEqual(e1.topics.all(), ["<Tag: t1>"]) self.assertQuerysetEqual(e1.related.all(), ["<Tag: t2>"]) def test_internal_related_name_not_in_error_msg(self): # The secret internal related names for self-referential many-to-many # fields shouldn't appear in the list when an error is made. self.assertRaisesErrorWithMessage(FieldError, "Cannot resolve keyword 'porcupine' into field. Choices are: id, name, references, related, selfreferchild, selfreferchildsibling", lambda: SelfRefer.objects.filter(porcupine='fred') ) def test_m2m_inheritance_symmetry(self): # Test to ensure that the relationship between two inherited models # with a self-referential m2m field maintains symmetry sr_child = SelfReferChild(name="Hanna") sr_child.save() sr_sibling = SelfReferChildSibling(name="Beth") sr_sibling.save() sr_child.related.add(sr_sibling) self.assertQuerysetEqual(sr_child.related.all(), ["<SelfRefer: Beth>"]) self.assertQuerysetEqual(sr_sibling.related.all(), ["<SelfRefer: Hanna>"]) def test_m2m_pk_field_type(self): # Regression for #11311 - The primary key for models in a m2m relation # doesn't have to be an AutoField w = Worksheet(id='abc') w.save() w.delete() def test_add_m2m_with_base_class(self): # Regression for #11956 -- You can add an object to a m2m with the # base class without causing integrity errors t1 = Tag.objects.create(name='t1') t2 = Tag.objects.create(name='t2') c1 = TagCollection.objects.create(name='c1') c1.tags = [t1,t2] c1 = TagCollection.objects.get(name='c1') self.assertQuerysetEqual(c1.tags.all(), ["<Tag: t1>", "<Tag: t2>"]) self.assertQuerysetEqual(t1.tag_collections.all(), ["<TagCollection: c1>"])
""" Tests for courseware middleware """ from django.http import Http404 from django.test.client import RequestFactory from nose.plugins.attrib import attr from lms.djangoapps.courseware.exceptions import Redirect from lms.djangoapps.courseware.middleware import RedirectMiddleware from xmodule.modulestore.tests.django_utils import SharedModuleStoreTestCase from xmodule.modulestore.tests.factories import CourseFactory @attr(shard=1) class CoursewareMiddlewareTestCase(SharedModuleStoreTestCase): """Tests that courseware middleware is correctly redirected""" @classmethod def setUpClass(cls): super(CoursewareMiddlewareTestCase, cls).setUpClass() cls.course = CourseFactory.create() def test_process_404(self): """A 404 should not trigger anything""" request = RequestFactory().get("dummy_url") response = RedirectMiddleware().process_exception( request, Http404() ) self.assertIsNone(response) def test_redirect_exceptions(self): """ Unit tests for handling of Redirect exceptions. """ request = RequestFactory().get("dummy_url") test_url = '/test_url' exception = Redirect(test_url) response = RedirectMiddleware().process_exception( request, exception ) self.assertEqual(response.status_code, 302) target_url = response._headers['location'][1] self.assertTrue(target_url.endswith(test_url))
# -*- coding: utf-8 -*- ############################################################################## # # Copyright (C) 2016 Compassion CH (http://www.compassion.ch) # Releasing children from poverty in Jesus' name # @author: Emanuel Cino <ecino@compassion.ch> # # The licence is in the file __manifest__.py # ############################################################################## from odoo import api, models, fields class Email(models.Model): """ Add relation to communication configuration to track generated e-mails. """ _inherit = 'mail.mail' ########################################################################## # FIELDS # ########################################################################## communication_config_id = fields.Many2one('partner.communication.config') @api.multi def send(self, auto_commit=False, raise_exception=False): """ Create communication for partner, if not already existing. """ comm_obj = self.env['partner.communication.job'].with_context( {}).with_context(no_print=True) config = self.env.ref( 'partner_communication.default_communication') for email in self.exists().filtered( lambda e: e.mail_message_id.model != 'partner.communication.job'): communication = comm_obj.search([('email_id', '=', email.id)]) if not communication: for partner in email.recipient_ids.filtered( lambda p: not p.user_ids or reduce( lambda u1, u2: u1 and u2, p.user_ids.mapped('share'))): comm_obj.create({ 'config_id': config.id, 'partner_id': partner.id, 'user_id': email.author_id.user_ids.id, 'object_ids': email.recipient_ids.ids, 'state': 'done', 'auto_send': False, 'email_id': email.id, 'sent_date': fields.Datetime.now(), 'body_html': email.body_html, 'subject': email.subject, 'ir_attachment_ids': [(6, 0, email.attachment_ids.ids)] }) return super(Email, self).send(auto_commit, raise_exception)
# ---------------------------------------------------------------------- # Numenta Platform for Intelligent Computing (NuPIC) # Copyright (C) 2013, Numenta, Inc. Unless you have an agreement # with Numenta, Inc., for a separate license for this software code, the # following terms and conditions apply: # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero Public License version 3 as # published by the Free Software Foundation. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. # See the GNU Affero Public License for more details. # # You should have received a copy of the GNU Affero Public License # along with this program. If not, see http://www.gnu.org/licenses. # # http://numenta.org/licenses/ # ---------------------------------------------------------------------- import copy # TODO: Note the functions 'rUpdate' are duplicated in # the swarming.hypersearch.utils.py module class DictObj(dict): """Dictionary that allows attribute-like access to its elements. Attributes are read-only.""" def __getattr__(self, name): if name == '__deepcopy__': return super(DictObj, self).__getattribute__("__deepcopy__") return self[name] def __setstate__(self, state): for k, v in state.items(): self[k] = v def rUpdate(original, updates): """Recursively updates the values in original with the values from updates.""" # Keep a list of the sub-dictionaries that need to be updated to avoid having # to use recursion (which could fail for dictionaries with a lot of nesting. dictPairs = [(original, updates)] while len(dictPairs) > 0: original, updates = dictPairs.pop() for k, v in updates.iteritems(): if k in original and isinstance(original[k], dict) and isinstance(v, dict): dictPairs.append((original[k], v)) else: original[k] = v def rApply(d, f): """Recursively applies f to the values in dict d. Args: d: The dict to recurse over. f: A function to apply to values in d that takes the value and a list of keys from the root of the dict to the value. """ remainingDicts = [(d, ())] while len(remainingDicts) > 0: current, prevKeys = remainingDicts.pop() for k, v in current.iteritems(): keys = prevKeys + (k,) if isinstance(v, dict): remainingDicts.insert(0, (v, keys)) else: f(v, keys) def find(d, target): remainingDicts = [d] while len(remainingDicts) > 0: current = remainingDicts.pop() for k, v in current.iteritems(): if k == target: return v if isinstance(v, dict): remainingDicts.insert(0, v) return None def get(d, keys): for key in keys: d = d[key] return d def set(d, keys, value): for key in keys[:-1]: d = d[key] d[keys[-1]] = value def dictDiffAndReport(da, db): """ Compares two python dictionaries at the top level and report differences, if any, to stdout da: first dictionary db: second dictionary Returns: The same value as returned by dictDiff() for the given args """ differences = dictDiff(da, db) if not differences: return differences if differences['inAButNotInB']: print ">>> inAButNotInB: %s" % differences['inAButNotInB'] if differences['inBButNotInA']: print ">>> inBButNotInA: %s" % differences['inBButNotInA'] for key in differences['differentValues']: print ">>> da[%s] != db[%s]" % (key, key) print "da[%s] = %r" % (key, da[key]) print "db[%s] = %r" % (key, db[key]) return differences def dictDiff(da, db): """ Compares two python dictionaries at the top level and return differences da: first dictionary db: second dictionary Returns: None if dictionaries test equal; otherwise returns a dictionary as follows: { 'inAButNotInB': <sequence of keys that are in da but not in db> 'inBButNotInA': <sequence of keys that are in db but not in da> 'differentValues': <sequence of keys whose corresponding values differ between da and db> } """ different = False resultDict = dict() resultDict['inAButNotInB'] = set(da) - set(db) if resultDict['inAButNotInB']: different = True resultDict['inBButNotInA'] = set(db) - set(da) if resultDict['inBButNotInA']: different = True resultDict['differentValues'] = [] for key in (set(da) - resultDict['inAButNotInB']): comparisonResult = da[key] == db[key] if isinstance(comparisonResult, bool): isEqual = comparisonResult else: # This handles numpy arrays (but only at the top level) isEqual = comparisonResult.all() if not isEqual: resultDict['differentValues'].append(key) different = True assert (((resultDict['inAButNotInB'] or resultDict['inBButNotInA'] or resultDict['differentValues']) and different) or not different) return resultDict if different else None
import unittest from ncclient.devices.junos import * import ncclient.transport from mock import patch import paramiko import sys xml = '''<xsl:stylesheet version="1.0" xmlns:xsl="http://www.w3.org/1999/XSL/Transform"> <xsl:output method="xml" indent="no"/> <xsl:template match="/|comment()|processing-instruction()"> <xsl:copy> <xsl:apply-templates/> </xsl:copy> </xsl:template> <xsl:template match="*"> <xsl:element name="{local-name()}"> <xsl:apply-templates select="@*|node()"/> </xsl:element> </xsl:template> <xsl:template match="@*"> <xsl:attribute name="{local-name()}"> <xsl:value-of select="."/> </xsl:attribute> </xsl:template> </xsl:stylesheet> ''' xml2 = """<rpc-reply xmlns:junos="http://xml.juniper.net/junos/12.1X46/junos"> <routing-engine> <name>reX</name> <commit-success/> <ok/> </rpc-reply>""" xml3 = """<rpc-reply xmlns:junos="http://xml.juniper.net/junos/12.1X46/junos"> <routing-engine> <name>reX</name> <commit-success/> <routing-engine/> <ok/> </rpc-reply>""" class TestJunosDevice(unittest.TestCase): def setUp(self): self.obj = JunosDeviceHandler({'name': 'junos'}) @patch('paramiko.Channel.exec_command') @patch('paramiko.Transport.__init__') @patch('paramiko.Transport.open_channel') def test_handle_connection_exceptions( self, mock_open, mock_init, mock_channel): session = ncclient.transport.SSHSession(self.obj) session._channel_id = 100 mock_init.return_value = None session._transport = paramiko.Transport() channel = paramiko.Channel(100) mock_open.return_value = channel self.obj.handle_connection_exceptions(session) self.assertEqual(channel._name, "netconf-command-100") self.assertEqual( mock_channel.call_args_list[0][0][0], "xml-mode netconf need-trailer") def test_additional_operations(self): dict = {} dict["rpc"] = ExecuteRpc dict["get_configuration"] = GetConfiguration dict["load_configuration"] = LoadConfiguration dict["compare_configuration"] = CompareConfiguration dict["command"] = Command dict["reboot"] = Reboot dict["halt"] = Halt dict["commit"] = Commit self.assertEqual(dict, self.obj.add_additional_operations()) def test_transform_reply(self): if sys.version >= '3': reply = xml.encode('utf-8') else: reply = xml self.assertEqual(self.obj.transform_reply(), reply) def test_perform_quality_check(self): self.assertFalse(self.obj.perform_qualify_check())
############################################################################## # Copyright (c) 2017, Los Alamos National Security, LLC # Produced at the Los Alamos National Laboratory. # # This file is part of Spack. # Created by Todd Gamblin, tgamblin@llnl.gov, All rights reserved. # LLNL-CODE-647188 # # For details, see https://github.com/spack/spack # Please also see the NOTICE and LICENSE files for our notice and the LGPL. # # This program is free software; you can redistribute it and/or modify # it under the terms of the GNU Lesser General Public License (as # published by the Free Software Foundation) version 2.1, February 1999. # # This program is distributed in the hope that it will be useful, but # WITHOUT ANY WARRANTY; without even the IMPLIED WARRANTY OF # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the terms and # conditions of the GNU Lesser General Public License for more details. # # You should have received a copy of the GNU Lesser General Public # License along with this program; if not, write to the Free Software # Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA ############################################################################## from spack import * import glob class Sw4lite(MakefilePackage): """Sw4lite is a bare bone version of SW4 intended for testing performance optimizations in a few important numerical kernels of SW4.""" tags = ['proxy-app', 'ecp-proxy-app'] homepage = "https://geodynamics.org/cig/software/sw4" url = "https://github.com/geodynamics/sw4lite/archive/v1.0.zip" git = "https://github.com/geodynamics/sw4lite.git" version('develop', branch='master') version('1.0', '3d911165f4f2ff6d5f9c1bd56ab6723f') variant('openmp', default=True, description='Build with OpenMP support') variant('precision', default='double', values=('float', 'double'), multi=False, description='Floating point precision') variant('ckernel', default=False, description='C or Fortran kernel') depends_on('blas') depends_on('lapack') depends_on('mpi') parallel = False @property def build_targets(self): targets = [] spec = self.spec if spec.variants['precision'].value == 'double': cxxflags = ['-I../src', '-I../src/double'] else: cxxflags = ['-I../src', '-I../src/float'] cflags = [] fflags = [] if '+openmp' in self.spec: cflags.append('-DSW4_OPENMP') cflags.append(self.compiler.openmp_flag) cxxflags.append('-DSW4_OPENMP') cxxflags.append(self.compiler.openmp_flag) fflags.append(self.compiler.openmp_flag) if spec.variants['ckernel'].value is True: cxxflags.append('-DSW4_CROUTINES') targets.append('ckernel=yes') targets.append('FC=' + spec['mpi'].mpifc) targets.append('CXX=' + spec['mpi'].mpicxx) targets.append('CFLAGS={0}'.format(' '.join(cflags))) targets.append('CXXFLAGS={0}'.format(' '.join(cxxflags))) targets.append('FFLAGS={0}'.format(' '.join(fflags))) targets.append('EXTRA_CXX_FLAGS=') targets.append('EXTRA_FORT_FLAGS=') lapack_blas = spec['lapack'].libs + spec['blas'].libs if spec.satisfies('%gcc'): targets.append('EXTRA_LINK_FLAGS={0} -lgfortran' .format(lapack_blas.ld_flags)) else: targets.append('EXTRA_LINK_FLAGS={0}'.format(lapack_blas.ld_flags)) return targets def install(self, spec, prefix): mkdir(prefix.bin) exe_name = glob.glob('*/sw4lite')[0] install(exe_name, prefix.bin) install_tree('tests', prefix.tests)
from highfive.runner import Configuration, Response from highfive.api_provider.interface import APIProvider, CONTRIBUTORS_STORE_KEY, DEFAULTS from handler_tests import TestStore from datetime import datetime from dateutil.parser import parse as datetime_parse from unittest import TestCase def create_config(): config = Configuration() config.name = 'test_app' config.imgur_client_id = None return config class APIProviderTests(TestCase): def test_api_init(self): '''The default interface will only initialize the app name and payload.''' config = Configuration() config.name = 'test_app' api = APIProvider(config=config, payload={}) self.assertEqual(api.name, 'test_app') self.assertEqual(api.payload, {}) self.assertEqual(api.config, config) for attr in DEFAULTS: self.assertTrue(getattr(api, attr) is None) def test_api_issue_payload(self): ''' If the payload is related to an issue (or an issue comment in an issue/PR), then this should've initialized the commonly used issue-related stuff. ''' payload = { 'issue': { 'user': { 'login': 'Foobar' }, 'state': 'open', 'labels': [ { 'name': 'Foo' }, { 'name': 'Bar' } ], 'number': 200, 'updated_at': '1970-01-01T00:00:00Z' }, } api = APIProvider(config=create_config(), payload=payload) self.assertEqual(api.payload, payload) self.assertFalse(api.is_pull) self.assertTrue(api.is_open) self.assertEqual(api.creator, 'foobar') self.assertEqual(api.last_updated, payload['issue']['updated_at']) self.assertEqual(api.number, '200') self.assertTrue(api.pull_url is None) self.assertEqual(api.labels, ['foo', 'bar']) def test_api_pr_payload(self): ''' If the payload is related to a PR, then the commonly used PR attributes should've been initialized. ''' payload = { 'pull_request': { 'user': { 'login': 'Foobar' }, 'assignee': { 'login': 'Baz' }, 'state': 'open', 'number': 50, 'url': 'some url', 'updated_at': '1970-01-01T00:00:00Z' } } api = APIProvider(config=create_config(), payload=payload) self.assertEqual(api.payload, payload) self.assertTrue(api.is_open) self.assertTrue(api.is_pull) self.assertEqual(api.creator, 'foobar') self.assertEqual(api.assignee, 'baz') self.assertEqual(api.last_updated, payload['pull_request']['updated_at']) self.assertEqual(api.number, '50') self.assertEqual(api.pull_url, 'some url') def test_api_other_events(self): '''Test for payload belonging to other events such as comment, label, etc.''' payload = { # This is a hypothetical payload just for tests 'sender': { 'login': 'Someone' }, 'label': { 'name': 'Label' }, 'repository': { 'owner': { 'login': 'foo' }, 'name': 'bar' }, 'comment': { 'body': 'Hello, world!', }, 'issue': { 'pull_request': {}, 'labels': [], 'user': { 'login': 'Foobar' }, 'state': 'open', 'number': 200, } } api = APIProvider(config=create_config(), payload=payload) self.assertTrue(api.is_pull) self.assertEqual(api.sender, 'someone') self.assertEqual(api.comment, 'Hello, world!') self.assertEqual(api.current_label, 'label') self.assertEqual(api.owner, 'foo') self.assertEqual(api.repo, 'bar') def test_api_imgur_upload(self): '''Test Imgur API upload''' config = create_config() api = APIProvider(config=config, payload={}) resp = api.post_image_to_imgur('some data') self.assertTrue(resp is None) # No client ID - returns None config.imgur_client_id = 'foobar' def test_valid_request(method, url, data, headers): self.assertEqual(headers['Authorization'], 'Client-ID foobar') self.assertEqual(method, 'POST') self.assertEqual(url, 'https://api.imgur.com/3/image') self.assertEqual(data, {'image': 'some data'}) return Response(data={'data': {'link': 'hello'}}) tests = [ (test_valid_request, 'hello'), (lambda method, url, data, headers: Response(data='', code=400), None), (lambda method, url, data, headers: Response(data=''), None) ] for func, expected in tests: resp = api.post_image_to_imgur('some data', json_request=func) self.assertEqual(resp, expected) def test_contributors_update(self): ''' Contributors list (cache) live only for an hour (by default). Once it's outdated, the next call to `get_contributors` calls `fetch_contributors`, writes it to the store and returns the list. Any calls within the next hour will return the existing contributors without calling the API. ''' class TestAPI(APIProvider): fetched = False def fetch_contributors(self): self.fetched = True return [] config = create_config() api = TestAPI(config=config, payload={}, store=None) self.assertFalse(api.fetched) api.get_contributors() # No store. This will always call the API. self.assertTrue(api.fetched) store = TestStore() api = TestAPI(config=config, payload={}, store=store) self.assertFalse(api.fetched) now = datetime.now() api.get_contributors() data = store.get_object(CONTRIBUTORS_STORE_KEY) updated_time = datetime_parse(data['last_update_time']) # Store doesn't have contributors. It's been updated for the first time. self.assertTrue(updated_time >= now) self.assertTrue(api.fetched) store = TestStore() store.write_object(CONTRIBUTORS_STORE_KEY, { 'last_update_time': str(now), 'list': ['booya'] }) api = TestAPI(config=config, payload={}, store=store) self.assertFalse(api.fetched) api.get_contributors() data = store.get_object(CONTRIBUTORS_STORE_KEY) updated_time = datetime_parse(data['last_update_time']) # Called within a cycle - no fetch occurs. self.assertEqual(updated_time, now) self.assertFalse(api.fetched) store = TestStore() store.write_object(CONTRIBUTORS_STORE_KEY, { 'last_update_time': str(now), 'list': ['booya'] }) api = TestAPI(config=config, payload={}, store=store) self.assertFalse(api.fetched) api.get_contributors(fetch=True) # When `fetch` is enabled, API is called regardless. self.assertTrue(api.fetched) data = store.get_object(CONTRIBUTORS_STORE_KEY) updated_time = datetime_parse(data['last_update_time']) self.assertTrue(updated_time > now)
import sys import random read_file = open("data/user_profile.txt", 'r') write_file = open("data/mini_user_profile.txt", 'w') number_of_lines = int(sys.argv[1]) number_of_items = int(sys.argv[2]) #record number of lines count = 0 random_num_list = [] # loop through the file to get number of lines in the file for line in read_file: count += 1 print "generating random numbers" # generating a list of random lines to read from for i in range(0, number_of_lines): random_num_list.append(random.randint(0, count)) #get rid of any duplicates no_duplicate_list = list(set(random_num_list)) #sort the list no_duplicate_list.sort() #print no_duplicate_list #go to file begining read_file.seek(0) count = 0 index = 0 user_id_list = [] print "getting lines from user_profile" for line in read_file: if count == no_duplicate_list[index]: write_file.write(line) index += 1 user_id_list.append(int(line.split()[0])) if index == len(no_duplicate_list): break count += 1 #user_id_list is sorted user_id_list = map(str, user_id_list) user_id_list.sort() #print user_id_list print "user_id finished" print "getting lines from item" read_file = open("data/item.txt", 'r') write_file = open("data/mini_item.txt", 'w') count = 0 random_num_list = [] for line in read_file: count += 1 for i in range(0, number_of_items): random_num_list.append(random.randint(0, count)) #no duplicate random_num_list = list(set(random_num_list)) random_num_list.sort() read_file.seek(0) count = 0 index = 0 item_id_list = [] for line in read_file: if count == random_num_list[index]: write_file.write(line) index += 1 item_id_list.append(int(line.split()[0])) if index == len(random_num_list): break count += 1 print "item finished" print "getting mini user_key_word" read_file = open("data/user_key_word.txt", 'r') write_file = open("data/mini_user_key_word.txt", 'w') #record number of lines count = 0 index = 0 # loop through the file to get number of lines in the file for line in read_file: if line.split()[0] == user_id_list[index]: write_file.write(line) index += 1 if index == len(user_id_list): #print "break" break print "user keyword finished" #go to file begining #getting the user_sns_small print "getting user sns" #print user_id_list read_file = open("data/user_sns.txt", 'r') #write_file = open("data/mini_user_sns_small.txt", 'w') user_sns_list = [] index = 0 met = False count = 0 for line in read_file: count += 1 #print count #Same user multiple following if met: if line.split()[0] != user_id_list[index]: index += 1 met = False if index == len(user_id_list): break if line.split()[0] == user_id_list[index]: #print "here" user_sns_list.append(line) met = True # if the current line's user is greater than the user list, that means # the user doesn't follow or are following, then we move to next user if line.split()[0] > user_id_list[index]: index += 1 if index == len(user_id_list): break #print user_sns_list write_file = open("data/mini_user_sns.txt",'w') for line in user_sns_list: for user_id in user_id_list: if line.split()[1] == user_id: write_file.write(line) break print "sns got" print "getting user action" #for line in write_file: read_file = open("data/user_action.txt", 'r') user_action_list = [] index = 0 met = False count = 0 for line in read_file: count += 1 #print count if met: if line.split()[0] != user_id_list[index]: index += 1 met = False if index == len(user_id_list): break if line.split()[0] == user_id_list[index]: #print "here" user_action_list.append(line) met = True if line.split()[0] > user_id_list[index]: index += 1 if index == len(user_id_list): break #print user_action_list write_file = open("data/mini_user_action.txt",'w') for line in user_action_list: for user_id in user_id_list: if line.split()[1] == user_id: write_file.write(line) break print "user action got" print "getting rec_log_train" user_set = set(user_id_list) item_set = set(item_id_list) read_file = open("data/rec_log_train.txt", 'r') write_file = open("data/mini_rec_log_train.txt",'w') count = 0 #for item in item_set: # print type(item) #for user in user_set: # print type(user) for line in read_file: words = line.split() # if words[0] in user_set and (words[1] in user_set or words[1] in item_set): if words[0] in user_set and words[1] in item_set: write_file.write(line) print count count += 1 print "Done"
from Plugins.Extensions.CutListEditor.plugin import CutListEditor from Components.ServiceEventTracker import ServiceEventTracker from enigma import iPlayableService, iServiceInformation from Tools.Directories import fileExists class TitleCutter(CutListEditor): def __init__(self, session, t): CutListEditor.__init__(self, session, t.source) self.skin = CutListEditor.skin self.session = session self.t = t self.__event_tracker = ServiceEventTracker(screen=self, eventmap= { iPlayableService.evUpdatedInfo: self.getPMTInfo, iPlayableService.evCuesheetChanged: self.refillList }) self.onExecBegin.remove(self.showTutorial) def getPMTInfo(self): service = self.session.nav.getCurrentService() audio = service and service.audioTracks() n = audio and audio.getNumberOfTracks() or 0 if n > 0: from Title import ConfigFixedText from Project import iso639language from Components.config import config, ConfigSubsection, ConfigSubList, ConfigSelection, ConfigYesNo self.t.properties.audiotracks = ConfigSubList() for x in range(n): i = audio.getTrackInfo(x) DVB_lang = i.getLanguage() description = i.getDescription() pid = str(i.getPID()) if description == "MPEG": description = "MP2" print "[audiotrack] pid:", pid, "description:", description, "language:", DVB_lang, "count:", x, "active:", (x < 8) self.t.properties.audiotracks.append(ConfigSubsection()) self.t.properties.audiotracks[-1].active = ConfigYesNo(default = (x < 8)) self.t.properties.audiotracks[-1].format = ConfigFixedText(description) choicelist = iso639language.getChoices() determined_language = iso639language.determineLanguage(DVB_lang) self.t.properties.audiotracks[-1].language = ConfigSelection(choices = choicelist, default=determined_language) self.t.properties.audiotracks[-1].pid = ConfigFixedText(pid) self.t.properties.audiotracks[-1].DVB_lang = ConfigFixedText(DVB_lang) sAspect = service.info().getInfo(iServiceInformation.sAspect) if sAspect in ( 1, 2, 5, 6, 9, 0xA, 0xD, 0xE ): aspect = "4:3" else: aspect = "16:9" self.t.properties.aspect.setValue(aspect) self.t.VideoType = service.info().getInfo(iServiceInformation.sVideoType) self.t.VideoPID = service.info().getInfo(iServiceInformation.sVideoPID) xres = service.info().getInfo(iServiceInformation.sVideoWidth) yres = service.info().getInfo(iServiceInformation.sVideoHeight) self.t.resolution = (xres, yres) self.t.framerate = service.info().getInfo(iServiceInformation.sFrameRate) self.t.progressive = service.info().getInfo(iServiceInformation.sProgressive) def checkAndGrabThumb(self): if not fileExists(self.t.inputfile.rsplit('.',1)[0] + ".png"): CutListEditor.grabFrame(self) def exit(self): if self.t.VideoType == -1: self.getPMTInfo() self.checkAndGrabThumb() self.session.nav.stopService() self.close(self.cut_list[:]) class CutlistReader(TitleCutter): skin = """ <screen position="0,0" size="720,576"> <eLabel position="0,0" size="720,576" zPosition="1" backgroundColor="#000000" /> <widget name="Video" position="0,0" size="100,75" /> <widget name="SeekState" position="0,0" /> <widget source="cutlist" position="0,0" render="Listbox" > <convert type="TemplatedMultiContent"> {"template": [ MultiContentEntryText(text = 1), MultiContentEntryText(text = 2) ], "fonts": [gFont("Regular", 18)], "itemHeight": 20 } </convert> </widget> <widget name="Timeline" position="0,0" /> </screen>""" def __init__(self, session, t): TitleCutter.__init__(self, session, t) self.skin = CutlistReader.skin def getPMTInfo(self): TitleCutter.getPMTInfo(self) TitleCutter.checkAndGrabThumb(self) self.close(self.cut_list[:])
#!/usr/bin/env python __author__ = 'saguinag' + '@' + 'nd.edu' __version__ = "0.1.0" ## ## fname "b2CliqueTreeRules.py" ## ## TODO: some todo list ## VersionLog: import net_metrics as metrics import pandas as pd import argparse, traceback import os, sys import networkx as nx import re from collections import deque, defaultdict, Counter import tree_decomposition as td import PHRG as phrg import probabilistic_cfg as pcfg import exact_phrg as xphrg import a1_hrg_cliq_tree as nfld from a1_hrg_cliq_tree import load_edgelist DEBUG = False def get_parser (): parser = argparse.ArgumentParser(description='b2CliqueTreeRules.py: given a tree derive grammar rules') parser.add_argument('-t', '--treedecomp', required=True, help='input tree decomposition (dimacs file format)') parser.add_argument('--version', action='version', version=__version__) return parser def dimacs_td_ct (tdfname): """ tree decomp to clique-tree """ print '... input file:', tdfname fname = tdfname graph_name = os.path.basename(fname) gname = graph_name.split('.')[0] gfname = "datasets/out." + gname tdh = os.path.basename(fname).split('.')[1] # tree decomp heuristic tfname = gname+"."+tdh G = load_edgelist(gfname) if DEBUG: print nx.info(G) print with open(fname, 'r') as f: # read tree decomp from inddgo lines = f.readlines() lines = [x.rstrip('\r\n') for x in lines] cbags = {} bags = [x.split() for x in lines if x.startswith('B')] for b in bags: cbags[int(b[1])] = [int(x) for x in b[3:]] # what to do with bag size? edges = [x.split()[1:] for x in lines if x.startswith('e')] edges = [[int(k) for k in x] for x in edges] tree = defaultdict(set) for s, t in edges: tree[frozenset(cbags[s])].add(frozenset(cbags[t])) if DEBUG: print '.. # of keys in `tree`:', len(tree.keys()) if DEBUG: print tree.keys() root = list(tree)[0] if DEBUG: print '.. Root:', root root = frozenset(cbags[1]) if DEBUG: print '.. Root:', root T = td.make_rooted(tree, root) if DEBUG: print '.. T rooted:', len(T) # nfld.unfold_2wide_tuple(T) # lets me display the tree's frozen sets T = phrg.binarize(T) prod_rules = {} td.new_visit(T, G, prod_rules) if DEBUG: print "--------------------" if DEBUG: print "- Production Rules -" if DEBUG: print "--------------------" for k in prod_rules.iterkeys(): if DEBUG: print k s = 0 for d in prod_rules[k]: s += prod_rules[k][d] for d in prod_rules[k]: prod_rules[k][d] = float(prod_rules[k][d]) / float(s) # normailization step to create probs not counts. if DEBUG: print '\t -> ', d, prod_rules[k][d] rules = [] id = 0 for k, v in prod_rules.iteritems(): sid = 0 for x in prod_rules[k]: rhs = re.findall("[^()]+", x) rules.append(("r%d.%d" % (id, sid), "%s" % re.findall("[^()]+", k)[0], rhs, prod_rules[k][x])) if DEBUG: print ("r%d.%d" % (id, sid), "%s" % re.findall("[^()]+", k)[0], rhs, prod_rules[k][x]) sid += 1 id += 1 df = pd.DataFrame(rules) outdf_fname = "./ProdRules/"+tfname+".prules" if not os.path.isfile(outdf_fname+".bz2"): print '...',outdf_fname, "written" df.to_csv(outdf_fname+".bz2", compression="bz2") else: print '...', outdf_fname, "file exists" return def main (): parser = get_parser() args = vars(parser.parse_args()) dimacs_td_ct(args['treedecomp']) # gen synth graph if __name__ == '__main__': try: main() except Exception, e: print str(e) traceback.print_exc() sys.exit(1) sys.exit(0)
import copy from django.core.exceptions import FieldError from django.db.models.constants import LOOKUP_SEP from django.db.models.fields import FieldDoesNotExist class SQLEvaluator(object): def __init__(self, expression, query, allow_joins=True, reuse=None): self.expression = expression self.opts = query.get_meta() self.reuse = reuse self.cols = [] self.expression.prepare(self, query, allow_joins) def relabeled_clone(self, change_map): clone = copy.copy(self) clone.cols = [] for node, col in self.cols: if hasattr(col, 'relabeled_clone'): clone.cols.append((node, col.relabeled_clone(change_map))) else: clone.cols.append((node, (change_map.get(col[0], col[0]), col[1]))) return clone def get_cols(self): cols = [] for node, col in self.cols: if hasattr(node, 'get_cols'): cols.extend(node.get_cols()) elif isinstance(col, tuple): cols.append(col) return cols def prepare(self): return self def as_sql(self, qn, connection): return self.expression.evaluate(self, qn, connection) ##################################################### # Vistor methods for initial expression preparation # ##################################################### def prepare_node(self, node, query, allow_joins): for child in node.children: if hasattr(child, 'prepare'): child.prepare(self, query, allow_joins) def prepare_leaf(self, node, query, allow_joins): if not allow_joins and LOOKUP_SEP in node.name: raise FieldError("Joined field references are not permitted in this query") field_list = node.name.split(LOOKUP_SEP) if node.name in query.aggregates: self.cols.append((node, query.aggregate_select[node.name])) else: try: field, sources, opts, join_list, path = query.setup_joins( field_list, query.get_meta(), query.get_initial_alias(), self.reuse) targets, _, join_list = query.trim_joins(sources, join_list, path) if self.reuse is not None: self.reuse.update(join_list) for t in targets: self.cols.append((node, (join_list[-1], t.column))) except FieldDoesNotExist: raise FieldError("Cannot resolve keyword %r into field. " "Choices are: %s" % (self.name, [f.name for f in self.opts.fields])) ################################################## # Vistor methods for final expression evaluation # ################################################## def evaluate_node(self, node, qn, connection): expressions = [] expression_params = [] for child in node.children: if hasattr(child, 'evaluate'): sql, params = child.evaluate(self, qn, connection) else: sql, params = '%s', (child,) if len(getattr(child, 'children', [])) > 1: format = '(%s)' else: format = '%s' if sql: expressions.append(format % sql) expression_params.extend(params) return connection.ops.combine_expression(node.connector, expressions), expression_params def evaluate_leaf(self, node, qn, connection): col = None for n, c in self.cols: if n is node: col = c break if col is None: raise ValueError("Given node not found") if hasattr(col, 'as_sql'): return col.as_sql(qn, connection) else: return '%s.%s' % (qn(col[0]), qn(col[1])), [] def evaluate_date_modifier_node(self, node, qn, connection): timedelta = node.children.pop() sql, params = self.evaluate_node(node, qn, connection) if (timedelta.days == timedelta.seconds == timedelta.microseconds == 0): return sql, params return connection.ops.date_interval_sql(sql, node.connector, timedelta), params
import unittest from ctypes import * class StructFieldsTestCase(unittest.TestCase): # Structure/Union classes must get 'finalized' sooner or # later, when one of these things happen: # # 1. _fields_ is set. # 2. An instance is created. # 3. The type is used as field of another Structure/Union. # 4. The type is subclassed # # When they are finalized, assigning _fields_ is no longer allowed. def test_1_A(self): class X(Structure): pass self.assertEqual(sizeof(X), 0) # not finalized X._fields_ = [] # finalized self.assertRaises(AttributeError, setattr, X, "_fields_", []) def test_1_B(self): class X(Structure): _fields_ = [] # finalized self.assertRaises(AttributeError, setattr, X, "_fields_", []) def test_2(self): class X(Structure): pass X() self.assertRaises(AttributeError, setattr, X, "_fields_", []) def test_3(self): class X(Structure): pass class Y(Structure): _fields_ = [("x", X)] # finalizes X self.assertRaises(AttributeError, setattr, X, "_fields_", []) def test_4(self): class X(Structure): pass class Y(X): pass self.assertRaises(AttributeError, setattr, X, "_fields_", []) Y._fields_ = [] self.assertRaises(AttributeError, setattr, X, "_fields_", []) if __name__ == "__main__": unittest.main()
# run doom process on a series of maps # can be used for regression testing, or to fetch media # keeps a log of each run ( see getLogfile ) # currently uses a basic stdout activity timeout to decide when to move on # using a periodic check of /proc/<pid>/status SleepAVG # when the sleep average is reaching 0, issue a 'quit' to stdout # keeps serialized run status in runner.pickle # NOTE: can be used to initiate runs on failed maps only for instance etc. # TODO: use the serialized and not the logs to sort the run order # TODO: better logging. Use idLogger? # TODO: configurable event when the process is found interactive # instead of emitting a quit, perform some warning action? import sys, os, commands, string, time, traceback, pickle from twisted.application import internet, service from twisted.internet import protocol, reactor, utils, defer from twisted.internet.task import LoopingCall class doomClientProtocol( protocol.ProcessProtocol ): # ProcessProtocol API def connectionMade( self ): self.logfile.write( 'connectionMade\n' ) def outReceived( self, data ): print data self.logfile.write( data ) def errReceived( self, data ): print 'stderr: ' + data self.logfile.write( 'stderr: ' + data ) def inConnectionLost( self ): self.logfile.write( 'inConnectionLost\n' ) def outConnectionLost( self ): self.logfile.write( 'outConnectionLost\n' ) def errConnectionLost( self ): self.logfile.write( 'errConnectionLost\n' ) def processEnded( self, status_object ): self.logfile.write( 'processEnded %s\n' % repr( status_object ) ) self.logfile.write( time.strftime( '%H:%M:%S', time.localtime( time.time() ) ) + '\n' ) self.logfile.close() self.deferred.callback( None ) # mac management def __init__( self, logfilename, deferred ): self.logfilename = logfilename self.logfile = open( logfilename, 'a' ) self.logfile.write( time.strftime( '%H:%M:%S', time.localtime( time.time() ) ) + '\n' ) self.deferred = deferred class doomService( service.Service ): # current monitoring state # 0: nothing running # 1: we have a process running, we're monitoring it's CPU usage # 2: we issued a 'quit' to the process's stdin # either going to get a processEnded, or a timeout # 3: we forced a kill because of error, timeout etc. state = 0 # load check period check_period = 10 # pickled status file pickle_file = 'runner.pickle' # stores status indexed by filename # { 'mapname' : ( state, last_update ), .. } status = {} # start the maps as multiplayer server multiplayer = 0 def __init__( self, bin, cmdline, maps, sort = 0, multiplayer = 0, blank_run = 0 ): self.p_transport = None self.multiplayer = multiplayer self.blank_run = blank_run if ( self.multiplayer ): print 'Operate in multiplayer mode' self.bin = os.path.abspath( bin ) if ( type( cmdline ) is type( '' ) ): self.cmdline = string.split( cmdline, ' ' ) else: self.cmdline = cmdline self.maps = maps if ( os.path.exists( self.pickle_file ) ): print 'Loading pickled status %s' % self.pickle_file handle = open( self.pickle_file, 'r' ) self.status = pickle.load( handle ) handle.close() if ( sort ): print 'Sorting maps oldest runs first' maps_sorted = [ ] for i in self.maps: i_log = self.getLogfile( i ) if ( os.path.exists( i_log ) ): maps_sorted.append( ( i, os.path.getmtime( i_log ) ) ) else: maps_sorted.append( ( i, 0 ) ) maps_sorted.sort( lambda x,y : cmp( x[1], y[1] ) ) self.maps = [ ] if ( blank_run ): self.maps.append( 'blankrun' ) for i in maps_sorted: self.maps.append( i[ 0 ] ) print 'Sorted as: %s\n' % repr( self.maps ) def getLogfile( self, name ): return 'logs/' + string.translate( name, string.maketrans( '/', '-' ) ) + '.log' # deferred call when child process dies def processEnded( self, val ): print 'child has died - state %d' % self.state self.status[ self.maps[ self.i_map ] ] = ( self.state, time.time() ) self.i_map += 1 if ( self.i_map >= len( self.maps ) ): reactor.stop() else: self.nextMap() def processTimeout( self ): self.p_transport.signalProcess( "KILL" ) def sleepAVGReply( self, val ): try: s = val[10:][:-2] print 'sleepAVGReply %s%%' % s if ( s == '0' ): # need twice in a row if ( self.state == 2 ): print 'child process is interactive' self.p_transport.write( 'quit\n' ) else: self.state = 2 else: self.state = 1 # else: # reactor.callLater( self.check_period, self.checkCPU ) except: print traceback.format_tb( sys.exc_info()[2] ) print sys.exc_info()[0] print 'exception raised in sleepAVGReply - killing process' self.state = 3 self.p_transport.signalProcess( 'KILL' ) def sleepAVGTimeout( self ): print 'sleepAVGTimeout - killing process' self.state = 3 self.p_transport.signalProcess( 'KILL' ) # called at regular intervals to monitor the sleep average of the child process # when sleep reaches 0, it means the map is loaded and interactive def checkCPU( self ): if ( self.state == 0 or self.p_transport is None or self.p_transport.pid is None ): print 'checkCPU: no child process atm' return defer = utils.getProcessOutput( '/bin/bash', [ '-c', 'cat /proc/%d/status | grep SleepAVG' % self.p_transport.pid ] ) defer.addCallback( self.sleepAVGReply ) defer.setTimeout( 2, self.sleepAVGTimeout ) def nextMap( self ): self.state = 0 name = self.maps[ self.i_map ] print 'Starting map: ' + name logfile = self.getLogfile( name ) print 'Logging to: ' + logfile if ( self.multiplayer ): cmdline = [ self.bin ] + self.cmdline + [ '+set', 'si_map', name ] if ( name != 'blankrun' ): cmdline.append( '+spawnServer' ) else: cmdline = [ self.bin ] + self.cmdline if ( name != 'blankrun' ): cmdline += [ '+devmap', name ] print 'Command line: ' + repr( cmdline ) self.deferred = defer.Deferred() self.deferred.addCallback( self.processEnded ) self.p_transport = reactor.spawnProcess( doomClientProtocol( logfile, self.deferred ), self.bin, cmdline , path = os.path.dirname( self.bin ), env = os.environ ) self.state = 1 # # setup the CPU usage loop # reactor.callLater( self.check_period, self.checkCPU ) def startService( self ): print 'doomService startService' loop = LoopingCall( self.checkCPU ) loop.start( self.check_period ) self.i_map = 0 self.nextMap() def stopService( self ): print 'doomService stopService' if ( not self.p_transport.pid is None ): self.p_transport.signalProcess( 'KILL' ) # serialize print 'saving status to %s' % self.pickle_file handle = open( self.pickle_file, 'w+' ) pickle.dump( self.status, handle ) handle.close()
# -*- coding: utf-8 -*- import inspect import re import urlparse from module.network.HTTPRequest import BadHeader from ..captcha.ReCaptcha import ReCaptcha from ..internal.Addon import Addon from ..internal.misc import parse_html_header def plugin_id(plugin): return ("<%(plugintype)s %(pluginname)s%(id)s>" % {'plugintype': plugin.__type__.upper(), 'pluginname': plugin.__name__, 'id': "[%s]" % plugin.pyfile.id if plugin.pyfile else ""}) def is_simple_plugin(obj): return any(k.__name__ in ("SimpleHoster", "SimpleCrypter") for k in inspect.getmro(type(obj))) def get_plugin_last_header(plugin): # @NOTE: req can be a HTTPRequest or a Browser object return plugin.req.http.header if hasattr(plugin.req, "http") else plugin.req.header class CloudFlare(object): @staticmethod def handle_function(addon_plugin, owner_plugin, func_name, orig_func, args): addon_plugin.log_debug("Calling %s() of %s" % (func_name, plugin_id(owner_plugin))) try: data = orig_func(*args[0], **args[1]) addon_plugin.log_debug("%s() returned successfully" % func_name) return data except BadHeader, e: addon_plugin.log_debug("%s(): got BadHeader exception %s" % (func_name, e.code)) header = parse_html_header(e.header) if "cloudflare" in header.get('server', ""): if e.code == 403: data = CloudFlare._solve_cf_security_check(addon_plugin, owner_plugin, e.content) elif e.code == 503: for _i in range(3): try: data = CloudFlare._solve_cf_ddos_challenge(addon_plugin, owner_plugin, e.content) break except BadHeader, e: #: Possibly we got another ddos challenge addon_plugin.log_debug("%s(): got BadHeader exception %s" % (func_name, e.code)) header = parse_html_header(e.header) if e.code == 503 and "cloudflare" in header.get('server', ""): continue #: Yes, it's a ddos challenge again.. else: data = None # Tell the exception handler to re-throw the exception break else: addon_plugin.log_error("%s(): Max solve retries reached" % func_name) data = None # Tell the exception handler to re-throw the exception else: addon_plugin.log_warning(_("Unknown CloudFlare response code %s") % e.code) raise if data is None: raise e else: return data else: raise @staticmethod def _solve_cf_ddos_challenge(addon_plugin, owner_plugin, data): try: addon_plugin.log_info(_("Detected CloudFlare's DDoS protection page")) # Cloudflare requires a delay before solving the challenge wait_time = (int(re.search('submit\(\);\r?\n\s*},\s*([0-9]+)', data).group(1)) + 999) / 1000 owner_plugin.set_wait(wait_time) last_url = owner_plugin.req.lastEffectiveURL urlp = urlparse.urlparse(last_url) domain = urlp.netloc submit_url = "%s://%s/cdn-cgi/l/chk_jschl" % (urlp.scheme, domain) get_params = {} try: get_params['jschl_vc'] = re.search(r'name="jschl_vc" value="(\w+)"', data).group(1) get_params['pass'] = re.search(r'name="pass" value="(.+?)"', data).group(1) get_params['s'] = re.search(r'name="s" value="(.+?)"', data).group(1) # Extract the arithmetic operation js = re.search(r'setTimeout\(function\(\){\s+(var s,t,o,p,b,r,e,a,k,i,n,g,f.+?\r?\n[\s\S]+?a\.value =.+?)\r?\n', data).group(1) js = re.sub(r'a\.value = (.+\.toFixed\(10\);).+', r'\1', js) solution_name = re.search(r's,t,o,p,b,r,e,a,k,i,n,g,f,\s*(.+)\s*=', js).group(1) g = re.search(r'(.*};)\n\s*(t\s*=(.+))\n\s*(;%s.*)' % (solution_name), js, re.M | re.I | re.S).groups() js = g[0] + g[-1] js = re.sub(r"[\n\\']", "", js) except Exception: # Something is wrong with the page. # This may indicate CloudFlare has changed their anti-bot # technique. owner_plugin.log_error(_("Unable to parse CloudFlare's DDoS protection page")) return None # Tell the exception handler to re-throw the exception if "toFixed" not in js: owner_plugin.log_error(_("Unable to parse CloudFlare's DDoS protection page")) return None # Tell the exception handler to re-throw the exception atob = 'var atob = function(str) {return Buffer.from(str, "base64").toString("binary");}' try: k = re.search(r'k\s*=\s*\'(.+?)\';', data).group(1) v = re.search(r'<div(?:.*)id="%s"(?:.*)>(.*)</div>' % k, data).group(1) doc = 'var document= {getElementById: function(x) { return {innerHTML:"%s"};}}' % v except (AttributeError, IndexError): doc = '' js = '%s;%s;var t="%s";%s' % (doc, atob, domain, js) # Safely evaluate the Javascript expression res = owner_plugin.js.eval(js) try: get_params['jschl_answer'] = str(float(res)) except ValueError: owner_plugin.log_error(_("Unable to parse CloudFlare's DDoS protection page")) return None # Tell the exception handler to re-throw the exception owner_plugin.wait() # Do the actual wait return owner_plugin.load(submit_url, get=get_params, ref=last_url) except BadHeader, e: raise e #: Huston, we have a BadHeader! except Exception, e: addon_plugin.log_error(e) return None # Tell the exception handler to re-throw the exception @staticmethod def _solve_cf_security_check(addon_plugin, owner_plugin, data): try: last_url = owner_plugin.req.lastEffectiveURL captcha = ReCaptcha(owner_plugin.pyfile) captcha_key = captcha.detect_key(data) if captcha_key: addon_plugin.log_info(_("Detected CloudFlare's security check page")) response, challenge = captcha.challenge(captcha_key, data) return owner_plugin.load(owner_plugin.fixurl("/cdn-cgi/l/chk_captcha"), get={'g-recaptcha-response': response}, ref=last_url) else: addon_plugin.log_warning(_("Got unexpected CloudFlare html page")) return None # Tell the exception handler to re-throw the exception except Exception, e: addon_plugin.log_error(e) return None # Tell the exception handler to re-throw the exception class PreloadStub(object): def __init__(self, addon_plugin, owner_plugin): self.addon_plugin = addon_plugin self.owner_plugin = owner_plugin self.old_preload = owner_plugin._preload def my_preload(self, *args, **kwargs): data = CloudFlare.handle_function(self.addon_plugin, self.owner_plugin, "_preload", self.old_preload, (args, kwargs)) if data is not None: self.owner_plugin.data = data def __repr__(self): return "<PreloadStub object at %s>" % hex(id(self)) class CloudFlareDdos(Addon): __name__ = "CloudFlareDdos" __type__ = "hook" __version__ = "0.16" __status__ = "testing" __config__ = [("activated", "bool", "Activated", False)] __description__ = """CloudFlare DDoS protection support""" __license__ = "GPLv3" __authors__ = [("GammaC0de", "nitzo2001[AT]yahoo[DOT]com")] def activate(self): self.stubs = {} self._override_get_url() def deactivate(self): while len(self.stubs): stub = next(self.stubs.itervalues()) self._unoverride_preload(stub.owner_plugin) self._unoverride_get_url() def _unoverride_preload(self, plugin): if id(plugin) in self.stubs: self.log_debug("Unoverriding _preload() for %s" % plugin_id(plugin)) stub = self.stubs.pop(id(plugin)) stub.owner_plugin._preload = stub.old_preload else: self.log_warning(_("No _preload() override found for %s, cannot un-override>") % plugin_id(plugin)) def _override_preload(self, plugin): if id(plugin) not in self.stubs: stub = PreloadStub(self, plugin) self.stubs[id(plugin)] = stub self.log_debug("Overriding _preload() for %s" % plugin_id(plugin)) plugin._preload = stub.my_preload else: self.log_warning(_("Already overrided _preload() for %s") % plugin_id(plugin)) def _override_get_url(self): self.log_debug("Overriding get_url()") self.old_get_url = self.pyload.requestFactory.getURL self.pyload.requestFactory.getURL = self.my_get_url def _unoverride_get_url(self): self.log_debug("Unoverriding get_url()") self.pyload.requestFactory.getURL = self.old_get_url def _find_owner_plugin(self): """ Walk the callstack until we find SimpleHoster or SimpleCrypter class Dirty but works. """ f = frame = inspect.currentframe() try: while True: if f is None: return None elif 'self' in f.f_locals and is_simple_plugin(f.f_locals['self']): return f.f_locals['self'] else: f = f.f_back finally: del frame def download_preparing(self, pyfile): #: Only SimpleHoster and SimpleCrypter based plugins are supported if not is_simple_plugin(pyfile.plugin): self.log_debug("Skipping plugin %s" % plugin_id(pyfile.plugin)) return attr = getattr(pyfile.plugin, "_preload", None) if not attr and not callable(attr): self.log_error(_("%s is missing _preload() function, cannot override!") % plugin_id(pyfile.plugin)) return self._override_preload(pyfile.plugin) def download_processed(self, pyfile): if id(pyfile.plugin) in self.stubs: self._unoverride_preload(pyfile.plugin) def my_get_url(self, *args, **kwargs): owner_plugin = self._find_owner_plugin() if owner_plugin is None: self.log_warning(_("Owner plugin not found, cannot process")) return self.old_get_url(*args, **kwargs) else: #@NOTE: Better use owner_plugin.load() instead of get_url() so cookies are saved and so captcha credits #@NOTE: Also that way we can use 'owner_plugin.req.header' to get the headers, otherwise we cannot get them res = CloudFlare.handle_function(self, owner_plugin, "get_url", owner_plugin.load, (args, kwargs)) if kwargs.get('just_header', False): # @NOTE: SimpleHoster/SimpleCrypter returns a dict while get_url() returns raw headers string, # make sure we return a string for get_url('just_header'=True) res = get_plugin_last_header(owner_plugin) return res
# -*- coding: utf-8 -*- """ Tests for student profile views. """ from django.conf import settings from django.core.urlresolvers import reverse from django.test import TestCase from django.test.client import RequestFactory from util.testing import UrlResetMixin from student.tests.factories import UserFactory from student_profile.views import learner_profile_context class LearnerProfileViewTest(UrlResetMixin, TestCase): """ Tests for the student profile view. """ USERNAME = "username" PASSWORD = "password" CONTEXT_DATA = [ 'default_public_account_fields', 'accounts_api_url', 'preferences_api_url', 'account_settings_page_url', 'has_preferences_access', 'own_profile', 'country_options', 'language_options', 'account_settings_data', 'preferences_data', ] def setUp(self): super(LearnerProfileViewTest, self).setUp() self.user = UserFactory.create(username=self.USERNAME, password=self.PASSWORD) self.client.login(username=self.USERNAME, password=self.PASSWORD) def test_context(self): """ Verify learner profile page context data. """ request = RequestFactory().get('/url') request.user = self.user context = learner_profile_context(request, self.USERNAME, self.user.is_staff) self.assertEqual( context['data']['default_public_account_fields'], settings.ACCOUNT_VISIBILITY_CONFIGURATION['public_fields'] ) self.assertEqual( context['data']['accounts_api_url'], reverse("accounts_api", kwargs={'username': self.user.username}) ) self.assertEqual( context['data']['preferences_api_url'], reverse('preferences_api', kwargs={'username': self.user.username}) ) self.assertEqual( context['data']['profile_image_upload_url'], reverse("profile_image_upload", kwargs={'username': self.user.username}) ) self.assertEqual( context['data']['profile_image_remove_url'], reverse('profile_image_remove', kwargs={'username': self.user.username}) ) self.assertEqual( context['data']['profile_image_max_bytes'], settings.PROFILE_IMAGE_MAX_BYTES ) self.assertEqual( context['data']['profile_image_min_bytes'], settings.PROFILE_IMAGE_MIN_BYTES ) self.assertEqual(context['data']['account_settings_page_url'], reverse('account_settings')) for attribute in self.CONTEXT_DATA: self.assertIn(attribute, context['data']) def test_view(self): """ Verify learner profile page view. """ profile_path = reverse('learner_profile', kwargs={'username': self.USERNAME}) response = self.client.get(path=profile_path) for attribute in self.CONTEXT_DATA: self.assertIn(attribute, response.content) def test_undefined_profile_page(self): """ Verify that a 404 is returned for a non-existent profile page. """ profile_path = reverse('learner_profile', kwargs={'username': "no_such_user"}) response = self.client.get(path=profile_path) self.assertEqual(404, response.status_code)
# encoding: utf-8 # module PyKDE4.kdeui # from /usr/lib/python3/dist-packages/PyKDE4/kdeui.cpython-34m-x86_64-linux-gnu.so # by generator 1.135 # no doc # imports import PyKDE4.kdecore as __PyKDE4_kdecore import PyQt4.QtCore as __PyQt4_QtCore import PyQt4.QtGui as __PyQt4_QtGui import PyQt4.QtSvg as __PyQt4_QtSvg class KShortcutWidget(__PyQt4_QtGui.QWidget): # no doc def applyStealShortcut(self, *args, **kwargs): # real signature unknown pass def clearShortcut(self, *args, **kwargs): # real signature unknown pass def isModifierlessAllowed(self, *args, **kwargs): # real signature unknown pass def setCheckActionCollections(self, *args, **kwargs): # real signature unknown pass def setCheckActionList(self, *args, **kwargs): # real signature unknown pass def setClearButtonsShown(self, *args, **kwargs): # real signature unknown pass def setModifierlessAllowed(self, *args, **kwargs): # real signature unknown pass def setShortcut(self, *args, **kwargs): # real signature unknown pass def shortcutChanged(self, *args, **kwargs): # real signature unknown pass def __init__(self, *args, **kwargs): # real signature unknown pass
# Copyright (c) 2013 Rackspace Hosting # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. """ Tests For Scheduler Utils """ import contextlib import uuid import mock from mox3 import mox from oslo_config import cfg from nova.compute import flavors from nova.compute import utils as compute_utils from nova import db from nova import exception from nova import objects from nova import rpc from nova.scheduler import utils as scheduler_utils from nova import test from nova.tests.unit import fake_instance from nova.tests.unit.objects import test_flavor CONF = cfg.CONF class SchedulerUtilsTestCase(test.NoDBTestCase): """Test case for scheduler utils methods.""" def setUp(self): super(SchedulerUtilsTestCase, self).setUp() self.context = 'fake-context' @mock.patch('nova.objects.Flavor.get_by_flavor_id') def test_build_request_spec_without_image(self, mock_get): image = None instance = {'uuid': 'fake-uuid'} instance_type = objects.Flavor(**test_flavor.fake_flavor) mock_get.return_value = objects.Flavor(extra_specs={}) self.mox.StubOutWithMock(flavors, 'extract_flavor') flavors.extract_flavor(mox.IgnoreArg()).AndReturn(instance_type) self.mox.ReplayAll() request_spec = scheduler_utils.build_request_spec(self.context, image, [instance]) self.assertEqual({}, request_spec['image']) def test_build_request_spec_with_object(self): instance_type = objects.Flavor() instance = fake_instance.fake_instance_obj(self.context) with mock.patch.object(instance, 'get_flavor') as mock_get: mock_get.return_value = instance_type request_spec = scheduler_utils.build_request_spec(self.context, None, [instance]) mock_get.assert_called_once_with() self.assertIsInstance(request_spec['instance_properties'], dict) @mock.patch.object(rpc, 'get_notifier', return_value=mock.Mock()) @mock.patch.object(compute_utils, 'add_instance_fault_from_exc') @mock.patch.object(objects.Instance, 'save') def test_set_vm_state_and_notify(self, mock_save, mock_add, mock_get): expected_uuid = 'fake-uuid' request_spec = dict(instance_properties=dict(uuid='other-uuid')) updates = dict(vm_state='fake-vm-state') service = 'fake-service' method = 'fake-method' exc_info = 'exc_info' payload = dict(request_spec=request_spec, instance_properties=request_spec.get( 'instance_properties', {}), instance_id=expected_uuid, state='fake-vm-state', method=method, reason=exc_info) event_type = '%s.%s' % (service, method) scheduler_utils.set_vm_state_and_notify(self.context, expected_uuid, service, method, updates, exc_info, request_spec, db) mock_save.assert_called_once_with() mock_add.assert_called_once_with(self.context, mock.ANY, exc_info, mock.ANY) self.assertIsInstance(mock_add.call_args[0][1], objects.Instance) self.assertIsInstance(mock_add.call_args[0][3], tuple) mock_get.return_value.error.assert_called_once_with(self.context, event_type, payload) def _test_populate_filter_props(self, host_state_obj=True, with_retry=True, force_hosts=None, force_nodes=None): if force_hosts is None: force_hosts = [] if force_nodes is None: force_nodes = [] if with_retry: if ((len(force_hosts) == 1 and len(force_nodes) <= 1) or (len(force_nodes) == 1 and len(force_hosts) <= 1)): filter_properties = dict(force_hosts=force_hosts, force_nodes=force_nodes) elif len(force_hosts) > 1 or len(force_nodes) > 1: filter_properties = dict(retry=dict(hosts=[]), force_hosts=force_hosts, force_nodes=force_nodes) else: filter_properties = dict(retry=dict(hosts=[])) else: filter_properties = dict() if host_state_obj: class host_state(object): host = 'fake-host' nodename = 'fake-node' limits = 'fake-limits' else: host_state = dict(host='fake-host', nodename='fake-node', limits='fake-limits') scheduler_utils.populate_filter_properties(filter_properties, host_state) enable_retry_force_hosts = not force_hosts or len(force_hosts) > 1 enable_retry_force_nodes = not force_nodes or len(force_nodes) > 1 if with_retry or enable_retry_force_hosts or enable_retry_force_nodes: # So we can check for 2 hosts scheduler_utils.populate_filter_properties(filter_properties, host_state) if force_hosts: expected_limits = None else: expected_limits = 'fake-limits' self.assertEqual(expected_limits, filter_properties.get('limits')) if (with_retry and enable_retry_force_hosts and enable_retry_force_nodes): self.assertEqual([['fake-host', 'fake-node'], ['fake-host', 'fake-node']], filter_properties['retry']['hosts']) else: self.assertNotIn('retry', filter_properties) def test_populate_filter_props(self): self._test_populate_filter_props() def test_populate_filter_props_host_dict(self): self._test_populate_filter_props(host_state_obj=False) def test_populate_filter_props_no_retry(self): self._test_populate_filter_props(with_retry=False) def test_populate_filter_props_force_hosts_no_retry(self): self._test_populate_filter_props(force_hosts=['force-host']) def test_populate_filter_props_force_nodes_no_retry(self): self._test_populate_filter_props(force_nodes=['force-node']) def test_populate_filter_props_multi_force_hosts_with_retry(self): self._test_populate_filter_props(force_hosts=['force-host1', 'force-host2']) def test_populate_filter_props_multi_force_nodes_with_retry(self): self._test_populate_filter_props(force_nodes=['force-node1', 'force-node2']) @mock.patch.object(scheduler_utils, '_max_attempts') def test_populate_retry_exception_at_max_attempts(self, _max_attempts): _max_attempts.return_value = 2 msg = 'The exception text was preserved!' filter_properties = dict(retry=dict(num_attempts=2, hosts=[], exc=[msg])) nvh = self.assertRaises(exception.NoValidHost, scheduler_utils.populate_retry, filter_properties, 'fake-uuid') # make sure 'msg' is a substring of the complete exception text self.assertIn(msg, nvh.message) def _check_parse_options(self, opts, sep, converter, expected): good = scheduler_utils.parse_options(opts, sep=sep, converter=converter) for item in expected: self.assertIn(item, good) def test_parse_options(self): # check normal self._check_parse_options(['foo=1', 'bar=-2.1'], '=', float, [('foo', 1.0), ('bar', -2.1)]) # check convert error self._check_parse_options(['foo=a1', 'bar=-2.1'], '=', float, [('bar', -2.1)]) # check separator missing self._check_parse_options(['foo', 'bar=-2.1'], '=', float, [('bar', -2.1)]) # check key missing self._check_parse_options(['=5', 'bar=-2.1'], '=', float, [('bar', -2.1)]) def test_validate_filters_configured(self): self.flags(scheduler_default_filters='FakeFilter1,FakeFilter2') self.assertTrue(scheduler_utils.validate_filter('FakeFilter1')) self.assertTrue(scheduler_utils.validate_filter('FakeFilter2')) self.assertFalse(scheduler_utils.validate_filter('FakeFilter3')) def _create_server_group(self, policy='anti-affinity'): instance = fake_instance.fake_instance_obj(self.context, params={'host': 'hostA'}) group = objects.InstanceGroup() group.name = 'pele' group.uuid = str(uuid.uuid4()) group.members = [instance.uuid] group.policies = [policy] return group def _get_group_details(self, group, policy=None): group_hosts = ['hostB'] with contextlib.nested( mock.patch.object(objects.InstanceGroup, 'get_by_instance_uuid', return_value=group), mock.patch.object(objects.InstanceGroup, 'get_hosts', return_value=['hostA']), ) as (get_group, get_hosts): scheduler_utils._SUPPORTS_ANTI_AFFINITY = None scheduler_utils._SUPPORTS_AFFINITY = None group_info = scheduler_utils._get_group_details( self.context, 'fake_uuid', group_hosts) self.assertEqual( (set(['hostA', 'hostB']), [policy]), group_info) def test_get_group_details(self): for policy in ['affinity', 'anti-affinity']: group = self._create_server_group(policy) self._get_group_details(group, policy=policy) def test_get_group_details_with_no_affinity_filters(self): self.flags(scheduler_default_filters=['fake']) scheduler_utils._SUPPORTS_ANTI_AFFINITY = None scheduler_utils._SUPPORTS_AFFINITY = None group_info = scheduler_utils._get_group_details(self.context, 'fake-uuid') self.assertIsNone(group_info) def test_get_group_details_with_no_instance_uuid(self): self.flags(scheduler_default_filters=['fake']) scheduler_utils._SUPPORTS_ANTI_AFFINITY = None scheduler_utils._SUPPORTS_AFFINITY = None group_info = scheduler_utils._get_group_details(self.context, None) self.assertIsNone(group_info) def _get_group_details_with_filter_not_configured(self, policy): wrong_filter = { 'affinity': 'ServerGroupAntiAffinityFilter', 'anti-affinity': 'ServerGroupAffinityFilter', } self.flags(scheduler_default_filters=[wrong_filter[policy]]) instance = fake_instance.fake_instance_obj(self.context, params={'host': 'hostA'}) group = objects.InstanceGroup() group.uuid = str(uuid.uuid4()) group.members = [instance.uuid] group.policies = [policy] with contextlib.nested( mock.patch.object(objects.InstanceGroup, 'get_by_instance_uuid', return_value=group), mock.patch.object(objects.InstanceGroup, 'get_hosts', return_value=['hostA']), ) as (get_group, get_hosts): scheduler_utils._SUPPORTS_ANTI_AFFINITY = None scheduler_utils._SUPPORTS_AFFINITY = None self.assertRaises(exception.UnsupportedPolicyException, scheduler_utils._get_group_details, self.context, 'fake-uuid') def test_get_group_details_with_filter_not_configured(self): policies = ['anti-affinity', 'affinity'] for policy in policies: self._get_group_details_with_filter_not_configured(policy) @mock.patch.object(scheduler_utils, '_get_group_details') def test_setup_instance_group_in_filter_properties(self, mock_ggd): mock_ggd.return_value = scheduler_utils.GroupDetails( hosts=set(['hostA', 'hostB']), policies=['policy']) spec = {'instance_properties': {'uuid': 'fake-uuid'}} filter_props = {'group_hosts': ['hostC']} scheduler_utils.setup_instance_group(self.context, spec, filter_props) mock_ggd.assert_called_once_with(self.context, 'fake-uuid', ['hostC']) expected_filter_props = {'group_updated': True, 'group_hosts': set(['hostA', 'hostB']), 'group_policies': ['policy']} self.assertEqual(expected_filter_props, filter_props) @mock.patch.object(scheduler_utils, '_get_group_details') def test_setup_instance_group_with_no_group(self, mock_ggd): mock_ggd.return_value = None spec = {'instance_properties': {'uuid': 'fake-uuid'}} filter_props = {'group_hosts': ['hostC']} scheduler_utils.setup_instance_group(self.context, spec, filter_props) mock_ggd.assert_called_once_with(self.context, 'fake-uuid', ['hostC']) self.assertNotIn('group_updated', filter_props) self.assertNotIn('group_policies', filter_props) self.assertEqual(['hostC'], filter_props['group_hosts']) @mock.patch.object(scheduler_utils, '_get_group_details') def test_setup_instance_group_with_filter_not_configured(self, mock_ggd): mock_ggd.side_effect = exception.NoValidHost(reason='whatever') spec = {'instance_properties': {'uuid': 'fake-uuid'}} filter_props = {'group_hosts': ['hostC']} self.assertRaises(exception.NoValidHost, scheduler_utils.setup_instance_group, self.context, spec, filter_props)
# -*- coding: utf-8 -*- # # Copyright (c) 2011 # Per Ove Ringdal # # Copyright (C) 2004 # Wido Depping, <widod@users.sourceforge.net> # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 2 of the License, or # (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program. If not, see http://www.gnu.org/licenses/ import os.path import copy import PyQt4 from PyQt4.QtCore import QString, pyqtSlot from PyQt4.QtGui import QWizard from .gui.AddAttributeWizardDesign import Ui_AddAttributeWizardDesign from base.backend.ObjectClassAttributeInfo import ObjectClassAttributeInfo from base.util.IconTheme import pixmapFromTheme class AddAttributeWizard(QWizard, Ui_AddAttributeWizardDesign): def __init__(self, parent = None, flags = PyQt4.QtCore.Qt.Widget): QWizard.__init__(self, parent, flags) self.setupUi(self) # need to initialize the pages before connecting signals self.restart() attributePixmap = pixmapFromTheme( "addattribute", ":/icons/64/add-attribute") objectclassPixmap = pixmapFromTheme( "objectclass", ":/icons/64/objectclass") self.imageLabel.setPixmap(attributePixmap) self.objectclassLabel.setPixmap(objectclassPixmap) self.enableAllBox.toggled.connect(self.initAttributeBox) self.attributeBox.activated[str].connect(self.newSelection) self.classBox.itemSelectionChanged.connect(self.classSelection) # attribute values of the current ldap object self.OBJECTVALUES = None # schema information for the ldap server self.SCHEMAINFO = None # set of attributes which are possible with the current objectclasses self.possibleAttributes = None # set of all attributes which are supported by the server self.allPossibleAttributes = None ############################################################################### def setData(self, smartObject): """ Sets the current object data, schema information and initializes the attribute box and wizard buttons. """ self.smartObject = smartObject self.SCHEMAINFO = ObjectClassAttributeInfo(self.smartObject.getServerMeta()) self.processData() self.initAttributeBox() currentPageWidget = self.page(0) #self.button(QWizard.FinishButton).setDisabled(False) #self.button(QWizard.NextButton).setDisabled(True) ############################################################################### def processData(self): """ Compute all attributes which can be added according to the data of the object. Single values which are already given are sorted out. """ possibleMust, possibleMay = self.smartObject.getPossibleAttributes() # attributes used by the current objectClass #usedAttributes = set(objectAttributes).difference(set(['objectClass'])) usedAttributes = self.smartObject.getAttributeList() # set of attribute which are used and have to be single singleAttributes = set(filter(self.SCHEMAINFO.isSingle, usedAttributes)) # create a set of attributes which may be added self.possibleAttributes = (possibleMust.union(possibleMay)).difference(singleAttributes) self.possibleAttributes = map(lambda x: x.lower(), self.possibleAttributes) # create a set of attributes which are supported by the server self.allPossibleAttributes = set(self.SCHEMAINFO.attributeDict.keys()).difference(singleAttributes) ############################################################################### def initAttributeBox(self): self.attributeBox.clear() currentPageWidget = self.currentPage() showAll = self.enableAllBox.isChecked() currentPageWidget.setFinalPage(True) currentPageWidget.setCommitPage(False) #self.button(QWizard.FinishButton).setDisabled(False) tmpList = None if showAll: tmpList = copy.deepcopy(self.allPossibleAttributes) else: tmpList = copy.deepcopy(self.possibleAttributes) structuralClass = self.smartObject.getStructuralClasses() # only show attributes whose objectclass combinations don't violate # the objectclass chain (not two structural classes) if len(structuralClass) > 0: classList = filter(lambda x: not self.SCHEMAINFO.isStructural(x), self.SCHEMAINFO.getObjectClasses()) for x in structuralClass: classList += self.SCHEMAINFO.getParents(x) for x in self.smartObject.getObjectClasses(): if not (x in classList): classList.append(x) mustAttributes, mayAttributes = self.SCHEMAINFO.getAllAttributes(classList) attributeList = mustAttributes.union(mayAttributes) cleanList = filter(lambda x: x.lower() in tmpList, attributeList) tmpList = cleanList else: self.enableAllBox.setChecked(True) self.enableAllBox.setEnabled(False) tmpList = sorted(self.allPossibleAttributes) tmpList.sort() tmpList = filter(lambda x: not (x.lower() == "objectclass"), tmpList) map(self.attributeBox.addItem, tmpList) self.newSelection(self.attributeBox.currentText()) ############################################################################### @pyqtSlot(int) def newSelection(self, attribute): pass @pyqtSlot("QString") def newSelection(self, attribute): attribute = str(attribute).lower() currentPageWidget = self.currentPage() mustSet, maySet = self.SCHEMAINFO.getAllObjectclassesForAttr(attribute) tmpSet = mustSet.union(maySet) if (attribute in self.possibleAttributes) or (len(tmpSet) == 0): currentPageWidget.setFinalPage(True) #self.button(QWizard.FinishButton).setDisabled(False) self.button(QWizard.NextButton).setDisabled(True) else: currentPageWidget.setFinalPage(False) #self.button(QWizard.FinishButton).setDisabled(True) self.button(QWizard.NextButton).setDisabled(False) ############################################################################### def initClassPage(self): currentPageWidget = self.currentPage() #self.button(QWizard.FinishButton).setDisabled(True) self.classBox.clear() self.mustAttributeBox.clear() attribute = str(self.attributeBox.currentText()) mustSet, maySet = self.SCHEMAINFO.getAllObjectclassesForAttr(attribute) classList = mustSet.union(maySet) if self.smartObject.hasStructuralClass(): structList = filter(lambda x: self.SCHEMAINFO.isStructural(x), classList) classList = filter(lambda x: not self.SCHEMAINFO.isStructural(x), classList) for x in structList: for y in self.smartObject.getObjectClasses(): if self.SCHEMAINFO.sameObjectClassChain(x, y): classList.append(x) else: classList = sorted(classList) classList.sort() map(self.classBox.addItem, classList) self.classBox.setCurrentRow(0) ############################################################################### def classSelection(self): self.mustAttributeBox.clear() objectclass = str(self.classBox.currentItem().text()) mustAttributes = self.SCHEMAINFO.getAllMusts([objectclass]) attribute = set([str(self.attributeBox.currentText())]) map(self.mustAttributeBox.addItem, mustAttributes.difference(attribute)) currentPageWidget = self.currentPage() #self.button(QWizard.FinishButton).setDisabled(False) ############################################################################### def initializePage(self, id): if id == 1: self.initClassPage() # vim: tabstop=4 expandtab shiftwidth=4 softtabstop=4
#!/usr/bin/env python3 # Copyright (c) 2014-2017 The Bitcoin Core developers # Distributed under the MIT software license, see the accompanying # file COPYING or http://www.opensource.org/licenses/mit-license.php. """Test the zapwallettxes functionality. - start two bitcoind nodes - create two transactions on node 0 - one is confirmed and one is unconfirmed. - restart node 0 and verify that both the confirmed and the unconfirmed transactions are still available. - restart node 0 with zapwallettxes and persistmempool, and verify that both the confirmed and the unconfirmed transactions are still available. - restart node 0 with just zapwallettxes and verify that the confirmed transactions are still available, but that the unconfirmed transaction has been zapped. """ from test_framework.test_framework import BitcoinTestFramework from test_framework.util import ( assert_equal, assert_raises_rpc_error, wait_until, ) class ZapWalletTXesTest (BitcoinTestFramework): def set_test_params(self): self.setup_clean_chain = True self.num_nodes = 2 def run_test(self): self.log.info("Mining blocks...") self.nodes[0].generate(1) self.sync_all() self.nodes[1].generate(101) self.sync_all() assert_equal(self.nodes[0].getbalance(), 250) # This transaction will be confirmed txid1 = self.nodes[0].sendtoaddress(self.nodes[1].getnewaddress(), 10) self.sync_all() self.nodes[0].generate(1) self.sync_all() # This transaction will not be confirmed txid2 = self.nodes[0].sendtoaddress(self.nodes[1].getnewaddress(), 20) # Confirmed and unconfirmed transactions are now in the wallet. assert_equal(self.nodes[0].gettransaction(txid1)['txid'], txid1) assert_equal(self.nodes[0].gettransaction(txid2)['txid'], txid2) # Stop-start node0. Both confirmed and unconfirmed transactions remain in the wallet. self.stop_node(0) self.start_node(0) assert_equal(self.nodes[0].gettransaction(txid1)['txid'], txid1) assert_equal(self.nodes[0].gettransaction(txid2)['txid'], txid2) # Stop node0 and restart with zapwallettxes and persistmempool. The unconfirmed # transaction is zapped from the wallet, but is re-added when the mempool is reloaded. self.stop_node(0) self.start_node(0, ["-zapwallettxes=2"]) # tx1 is still be available because it was confirmed assert_equal(self.nodes[0].gettransaction(txid1)['txid'], txid1) # This will raise an exception because the unconfirmed transaction has been zapped assert_raises_rpc_error(-5, 'Invalid or non-wallet transaction id', self.nodes[0].gettransaction, txid2) if __name__ == '__main__': ZapWalletTXesTest().main()
#import os import sys import time import xmltodict import pprint pp = pprint.PrettyPrinter(indent=4,stream=sys.stderr) testing = False # def poll_condor(jonbr, bagnr): def poll_condor(filename): # filename = "hist-%d-%d.xml" % ( jobnr, bagnr ) # command = "condor_history -constraint 'HtcJob == %d && HtcBag == %d' -xml > %s" % ( jobnr, bagnr, filename ) # os.system( command ) tries = 0 poll_dict = {} while tries < 4: tries += 1 _trystr = "Try %d (%s) :" % (tries, filename) xml = open(filename).read() xmldict = xmltodict.parse(xml) print >> sys.stderr, "type(xmldict) = ", type(xmldict) if not ( type(xmldict) == dict and xmldict.has_key('classads') ): print >> sys.stderr, _trystr, "No classads, wait a little until the first results come in" time.sleep(2) continue print >> sys.stderr, "type(xmldict['classads']) = ", type(xmldict['classads']) if not ( type(xmldict['classads']) == dict and xmldict['classads'].has_key('c') ) : print >> sys.stderr, _trystr, "No classads <c> entries, wait a little until the first results come in" time.sleep(2) continue print >> sys.stderr, "type(xmldict['classads']['c']) = ", type(xmldict['classads']['c']) if not ( type(xmldict['classads']['c']) == list and xmldict['classads']['c'][0].has_key('a') ) : print >> sys.stderr, _trystr, "No classads attributes, wait a little until the first results come in" time.sleep(2) continue poll_dict = get_poll_dict(xmldict) break # if poll_dict['CompletedTasks'] == poll_dict['TotalTask']: #pp.pprint(xmldict) return poll_dict def get_poll_dict(xmldict): if testing: print >> sys.stderr, "selecting info from file %s, job %s, bag %s" % (filename, jobnr, bagnr) res_dict = {} # print >> sys.stderr, xml # print "----" # jobid = 0 for c in xmldict['classads']['c']: tempdict = {} # pp.pprint(c) attrs=c['a'] # pp.pprint(attrs) for d in attrs: v = None k = d['@n'].encode('ascii', 'ignore') # get rid of unicode from xmltodict # handle float if d.has_key('r'): v=float( d['r'].encode('ascii', 'ignore') ) # get rid of unicode from xmltodict # handle int if d.has_key('i'): v=int( d['i'].encode('ascii', 'ignore') ) # get rid of unicode from xmltodict # handle string if d.has_key('s'): # pp.pprint(d) if d['s'] == None: v = 'None' else: v= d['s'].encode('ascii', 'ignore') # get rid of unicode from xmltodict # handle boolean if d.has_key('b'): # pp.pprint(d) v= 'True' if d['b']['@v'] == 't' else 'False' # handle expression if d.has_key('e'): v= d['e'].encode('ascii', 'ignore') # get rid of unicode from xmltodict if v != None: tempdict[k] = v else: print "unknown datatype in " pp.pprint(d) attrdict = {} for k in [ 'HtcJob', 'HtcBag', 'HtcTask', 'RemoteWallClockTime', 'Cmd', 'MATCH_EXP_MachineCloudMachineType' ]: if tempdict.has_key(k): attrdict[k] = tempdict[k] #print kl # cur_jobnr = "%(HtcJob)s" % tempdict # if not ( jobnr == None or jobnr == cur_jobnr): # continue # cur_bagnr = "%(HtcBag)s" % tempdict # if not ( bagnr == None or bagnr == cur_bagnr): # continue # tasknr = "%(HtcTask)s" % taskdict taskid = "%(HtcJob)s.%(HtcBag)s.%(HtcTask)s" % tempdict #jobid += 1 # print "----" if res_dict.has_key(taskid): res_dict[taskid].append ( attrdict ) else: res_dict[taskid] = [ attrdict ] if testing: print >> sys.stderr, "====== res_dict ======" pp.pprint(res_dict) print >> sys.stderr, "------ res_dict ------" return res_dict """ { 'tasks': { taskid: [ { attr1: val1, attrn: valn, }, { attr1: val1, attrn: valn, } ] } } """ def do_test(filename): poll_dict = poll_condor(filename) completed_tasks = 0 for _ in poll_dict.keys(): completed_tasks += len(poll_dict[_]) completed_task_sets = poll_dict.keys().__len__() print >> sys.stderr, "Found %d completed tasks in %d sets" % (completed_tasks, completed_task_sets) if False: pp.pprint(poll_dict) if __name__ == "__main__": pp = pprint.PrettyPrinter(indent=4,stream=sys.stderr) testing = True usage = "usage : %s ClassAd_XML_file [ jobnr [ bagnr ] ]" % sys.argv[0] argc = len(sys.argv) jobnr = None bagnr = None print "%d args" % argc if argc <= 1: print usage filename = "test3.xml" if argc >= 2: filename = sys.argv[1] print "file = %s" % filename if argc >= 3: jobnr = sys.argv[2] print "job = %s" % jobnr if argc >= 4: bagnr = sys.argv[3] print "bag = %s" % bagnr for _ in [ "test1.xml", "test2.xml", "test3.xml", "test4.xml" ] : do_test( _ )
# Copyright (C) 2003-2007 Robey Pointer <robey@lag.net> # # This file is part of paramiko. # # Paramiko is free software; you can redistribute it and/or modify it under the # terms of the GNU Lesser General Public License as published by the Free # Software Foundation; either version 2.1 of the License, or (at your option) # any later version. # # Paramiko is distrubuted in the hope that it will be useful, but WITHOUT ANY # WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR # A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more # details. # # You should have received a copy of the GNU Lesser General Public License # along with Paramiko; if not, write to the Free Software Foundation, Inc., # 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA. import socket import sys # windows does not have termios... try: import termios import tty has_termios = True except ImportError: has_termios = False def interactive_shell(chan): if has_termios: posix_shell(chan) else: windows_shell(chan) def posix_shell(chan): import select oldtty = termios.tcgetattr(sys.stdin) try: tty.setraw(sys.stdin.fileno()) tty.setcbreak(sys.stdin.fileno()) chan.settimeout(0.0) while True: r, w, e = select.select([chan, sys.stdin], [], []) if chan in r: try: x = chan.recv(1024) if len(x) == 0: print '\r\n*** EOF\r\n', break sys.stdout.write(x) sys.stdout.flush() except socket.timeout: pass if sys.stdin in r: x = sys.stdin.read(1) if len(x) == 0: break chan.send(x) finally: termios.tcsetattr(sys.stdin, termios.TCSADRAIN, oldtty) # thanks to Mike Looijmans for this code def windows_shell(chan): import threading sys.stdout.write("Line-buffered terminal emulation. Press F6 or ^Z to send EOF.\r\n\r\n") def writeall(sock): while True: data = sock.recv(256) if not data: sys.stdout.write('\r\n*** EOF ***\r\n\r\n') sys.stdout.flush() break sys.stdout.write(data) sys.stdout.flush() writer = threading.Thread(target=writeall, args=(chan,)) writer.start() try: while True: d = sys.stdin.read(1) if not d: break chan.send(d) except EOFError: # user hit ^Z or F6 pass
from typing import Any from zerver.lib.actions import bulk_add_subscriptions, do_create_realm, do_create_user from zerver.lib.management import ZulipBaseCommand from zerver.lib.onboarding import send_initial_realm_messages from zerver.models import Realm, UserProfile class Command(ZulipBaseCommand): help = """Add a new realm and initial user for manual testing of the onboarding process.""" def handle(self, **options: Any) -> None: string_id = "realm{:02}".format(Realm.objects.filter(string_id__startswith="realm").count()) realm = do_create_realm(string_id, string_id) name = "{:02}-user".format(UserProfile.objects.filter(email__contains="user@").count()) user = do_create_user( f"{name}@{string_id}.zulip.com", "password", realm, name, role=UserProfile.ROLE_REALM_ADMINISTRATOR, acting_user=None, ) assert realm.signup_notifications_stream is not None bulk_add_subscriptions(realm, [realm.signup_notifications_stream], [user], acting_user=None) send_initial_realm_messages(realm)
# SchedGui.py - Python extension for perf script, basic GUI code for # traces drawing and overview. # # Copyright (C) 2010 by Frederic Weisbecker <fweisbec@gmail.com> # # This software is distributed under the terms of the GNU General # Public License ("GPL") version 2 as published by the Free Software # Foundation. try: import wx except ImportError: raise ImportError, "You need to install the wxpython lib for this script" class RootFrame(wx.Frame): Y_OFFSET = 100 RECT_HEIGHT = 100 RECT_SPACE = 50 EVENT_MARKING_WIDTH = 5 def __init__(self, sched_tracer, title, parent = None, id = -1): wx.Frame.__init__(self, parent, id, title) (self.screen_width, self.screen_height) = wx.GetDisplaySize() self.screen_width -= 10 self.screen_height -= 10 self.zoom = 0.5 self.scroll_scale = 20 self.sched_tracer = sched_tracer self.sched_tracer.set_root_win(self) (self.ts_start, self.ts_end) = sched_tracer.interval() self.update_width_virtual() self.nr_rects = sched_tracer.nr_rectangles() + 1 self.height_virtual = RootFrame.Y_OFFSET + (self.nr_rects * (RootFrame.RECT_HEIGHT + RootFrame.RECT_SPACE)) # whole window panel self.panel = wx.Panel(self, size=(self.screen_width, self.screen_height)) # scrollable container self.scroll = wx.ScrolledWindow(self.panel) self.scroll.SetScrollbars(self.scroll_scale, self.scroll_scale, self.width_virtual / self.scroll_scale, self.height_virtual / self.scroll_scale) self.scroll.EnableScrolling(True, True) self.scroll.SetFocus() # scrollable drawing area self.scroll_panel = wx.Panel(self.scroll, size=(self.screen_width - 15, self.screen_height / 2)) self.scroll_panel.Bind(wx.EVT_PAINT, self.on_paint) self.scroll_panel.Bind(wx.EVT_KEY_DOWN, self.on_key_press) self.scroll_panel.Bind(wx.EVT_LEFT_DOWN, self.on_mouse_down) self.scroll.Bind(wx.EVT_PAINT, self.on_paint) self.scroll.Bind(wx.EVT_KEY_DOWN, self.on_key_press) self.scroll.Bind(wx.EVT_LEFT_DOWN, self.on_mouse_down) self.scroll.Fit() self.Fit() self.scroll_panel.SetDimensions(-1, -1, self.width_virtual, self.height_virtual, wx.SIZE_USE_EXISTING) self.txt = None self.Show(True) def us_to_px(self, val): return val / (10 ** 3) * self.zoom def px_to_us(self, val): return (val / self.zoom) * (10 ** 3) def scroll_start(self): (x, y) = self.scroll.GetViewStart() return (x * self.scroll_scale, y * self.scroll_scale) def scroll_start_us(self): (x, y) = self.scroll_start() return self.px_to_us(x) def paint_rectangle_zone(self, nr, color, top_color, start, end): offset_px = self.us_to_px(start - self.ts_start) width_px = self.us_to_px(end - self.ts_start) offset_py = RootFrame.Y_OFFSET + (nr * (RootFrame.RECT_HEIGHT + RootFrame.RECT_SPACE)) width_py = RootFrame.RECT_HEIGHT dc = self.dc if top_color is not None: (r, g, b) = top_color top_color = wx.Colour(r, g, b) brush = wx.Brush(top_color, wx.SOLID) dc.SetBrush(brush) dc.DrawRectangle(offset_px, offset_py, width_px, RootFrame.EVENT_MARKING_WIDTH) width_py -= RootFrame.EVENT_MARKING_WIDTH offset_py += RootFrame.EVENT_MARKING_WIDTH (r ,g, b) = color color = wx.Colour(r, g, b) brush = wx.Brush(color, wx.SOLID) dc.SetBrush(brush) dc.DrawRectangle(offset_px, offset_py, width_px, width_py) def update_rectangles(self, dc, start, end): start += self.ts_start end += self.ts_start self.sched_tracer.fill_zone(start, end) def on_paint(self, event): dc = wx.PaintDC(self.scroll_panel) self.dc = dc width = min(self.width_virtual, self.screen_width) (x, y) = self.scroll_start() start = self.px_to_us(x) end = self.px_to_us(x + width) self.update_rectangles(dc, start, end) def rect_from_ypixel(self, y): y -= RootFrame.Y_OFFSET rect = y / (RootFrame.RECT_HEIGHT + RootFrame.RECT_SPACE) height = y % (RootFrame.RECT_HEIGHT + RootFrame.RECT_SPACE) if rect < 0 or rect > self.nr_rects - 1 or height > RootFrame.RECT_HEIGHT: return -1 return rect def update_summary(self, txt): if self.txt: self.txt.Destroy() self.txt = wx.StaticText(self.panel, -1, txt, (0, (self.screen_height / 2) + 50)) def on_mouse_down(self, event): (x, y) = event.GetPositionTuple() rect = self.rect_from_ypixel(y) if rect == -1: return t = self.px_to_us(x) + self.ts_start self.sched_tracer.mouse_down(rect, t) def update_width_virtual(self): self.width_virtual = self.us_to_px(self.ts_end - self.ts_start) def __zoom(self, x): self.update_width_virtual() (xpos, ypos) = self.scroll.GetViewStart() xpos = self.us_to_px(x) / self.scroll_scale self.scroll.SetScrollbars(self.scroll_scale, self.scroll_scale, self.width_virtual / self.scroll_scale, self.height_virtual / self.scroll_scale, xpos, ypos) self.Refresh() def zoom_in(self): x = self.scroll_start_us() self.zoom *= 2 self.__zoom(x) def zoom_out(self): x = self.scroll_start_us() self.zoom /= 2 self.__zoom(x) def on_key_press(self, event): key = event.GetRawKeyCode() if key == ord("+"): self.zoom_in() return if key == ord("-"): self.zoom_out() return key = event.GetKeyCode() (x, y) = self.scroll.GetViewStart() if key == wx.WXK_RIGHT: self.scroll.Scroll(x + 1, y) elif key == wx.WXK_LEFT: self.scroll.Scroll(x - 1, y) elif key == wx.WXK_DOWN: self.scroll.Scroll(x, y + 1) elif key == wx.WXK_UP: self.scroll.Scroll(x, y - 1)
# All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from rally.benchmark import context from rally.common.i18n import _ from rally.common import log as logging from rally.common import utils as rutils from rally import objects from rally import osclients LOG = logging.getLogger(__name__) # NOTE(boris-42): This context should be hidden for now and used only by # benchmark engine. In future during various refactoring of # validation system and rally CI testing we will make it public @context.context(name="existing_users", order=99, hidden=True) class ExistingUsers(context.Context): """This context supports using existing users in Rally. It uses information about deployment to properly initialize context["users"] and context["tenants"] So there won't be big difference between usage of "users" and "existing_users" context. """ # NOTE(boris-42): We don't need to check config schema because # this is used only by benchmark engine CONFIG_SCHEMA = {} def __init__(self, ctx): super(ExistingUsers, self).__init__(ctx) self.context["users"] = [] self.context["tenants"] = {} @rutils.log_task_wrapper(LOG.info, _("Enter context: `existing_users`")) def setup(self): for user in self.config: user_endpoint = objects.Endpoint(**user) user_kclient = osclients.Clients(user_endpoint).keystone() if user_kclient.tenant_id not in self.context["tenants"]: self.context["tenants"][user_kclient.tenant_id] = { "id": user_kclient.tenant_id, "name": user_kclient.tenant_name } self.context["users"].append({ "endpoint": user_endpoint, "id": user_kclient.user_id, "tenant_id": user_kclient.tenant_id }) @rutils.log_task_wrapper(LOG.info, _("Exit context: `existing_users`")) def cleanup(self): """These users are not managed by Rally, so don't touch them."""
# -*- coding: utf-8 -*- import xbmc import xbmcaddon ADDON = xbmcaddon.Addon(id='screensaver.weather') ADDON_ID = ADDON.getAddonInfo('id') # Common logging module def log(txt, loglevel=xbmc.LOGDEBUG): if (ADDON.getSetting("logEnabled") == "true") or (loglevel != xbmc.LOGDEBUG): if isinstance(txt, str): txt = txt.decode("utf-8") message = u'%s: %s' % (ADDON_ID, txt) xbmc.log(msg=message.encode("utf-8"), level=loglevel) ############################## # Stores Various Settings ############################## class Settings(): DIM_LEVEL = ( '00000000', '11000000', '22000000', '33000000', '44000000', '55000000', '66000000', '77000000', '88000000', '99000000', 'AA000000', 'BB000000', 'CC000000', 'DD000000', 'EE000000' ) @staticmethod def getDimValue(): # The actual dim level (Hex) is one of # Where 00000000 is not changed # So that is a total of 15 different options # FF000000 would be completely black, so we do not use that one if ADDON.getSetting("dimLevel"): return Settings.DIM_LEVEL[int(ADDON.getSetting("dimLevel"))] else: return '00000000'
# -*- coding: utf-8 -*- from __future__ import unicode_literals from django.db import models, migrations class Migration(migrations.Migration): dependencies = [ ] operations = [ migrations.CreateModel( name='OpenIDNonce', fields=[ ('id', models.AutoField(primary_key=True, verbose_name='ID', serialize=False, auto_created=True)), ('server_url', models.CharField(max_length=255)), ('timestamp', models.IntegerField()), ('salt', models.CharField(max_length=255)), ('date_created', models.DateTimeField(auto_now_add=True)), ], ), migrations.CreateModel( name='OpenIDStore', fields=[ ('id', models.AutoField(primary_key=True, verbose_name='ID', serialize=False, auto_created=True)), ('server_url', models.CharField(max_length=255)), ('handle', models.CharField(max_length=255)), ('secret', models.TextField()), ('issued', models.IntegerField()), ('lifetime', models.IntegerField()), ('assoc_type', models.TextField()), ], ), ]
''' Author: Jon Tsai Created: May 29 2016 ''' import numpy as np import theano from time import sleep import sys def progress_bar(percent, speed): i = int(percent)/2 sys.stdout.write('\r') # the exact output you're looking for: sys.stdout.write("[%-50s] %d%% %f instances/s" % ('='*i, percent, speed)) sys.stdout.flush() def combine_sents(sent_set): ''' parameter: sent_set ==> 2D sentences set ==> type: list[list[list]] return: sents1D ==> 1D sentences set ==> type: list[list] This function will combine 2D sentence set into 1D sentence set. e.g. [ [[sent1], [sent2], [sent3], ..., [sentn]] ... [[sent1], [sent2], [sent3], ..., [sentn]] ] ==> [ [sentences1], ... [sentencesn] ] ''' sents1D = [] for doc in sent_set: combine_sent = np.array([]) for sent in doc: combine_sent = np.concatenate((combine_sent,sent)) sents1D.append(combine_sent) return sents1D def shuffle_index(length_of_indices_ls): ''' ---------- parameter: ---------- length_of_indices_ls: type = int ---------- return: ---------- a shuffled numpy array of indices ''' ls = np.arange(length_of_indices_ls) np.random.shuffle(ls) return ls def padding(batch_input_list): ''' ---------- parameter: ---------- batch_input_list: type = list(list) ---------- return: ---------- numpy.ndarray: shape == (n_batch, max_time_step) ''' n_batch = len(batch_input_list) max_time_step = max([len(batch_input_list[i]) for i in range(n_batch)]) padding_result = np.zeros((n_batch, max_time_step)) for batch in range(n_batch): padding_result[batch] = np.concatenate((np.asarray(batch_input_list[batch]), np.zeros(max_time_step - len(batch_input_list[batch])))) return padding_result.astype('int64') def mask_generator(indices_matrix): ''' ---------- parameter: ---------- indices_matrix: type = list[list] ---------- return: ---------- mask : type = np.ndarray a mask matrix of a batch of varied length instances ''' n_batch = len(indices_matrix) len_ls = [len(sent) for sent in indices_matrix] max_len = max(len_ls) mask = np.zeros((n_batch, max_len)) for i in range(n_batch): for j in range(len(indices_matrix[i])): mask[i][j] = 1 return mask def mlp_mask_generator(indices_matrix, wemb_size): ''' ---------- parameter: ---------- indices_matrix: type = list[list] ---------- return: ---------- mask : type = np.ndarray mask.shape = (n_batch, wemb_size) ''' n_batch = len(indices_matrix) len_ls = [len(sent) for sent in indices_matrix] mask = np.ones((n_batch, wemb_size)) for i in range(n_batch): mask[i] = mask[i] * len_ls[i] return mask def fake_input_generator(max_index, batch_number, length_range): ''' ---------- parameter: ---------- max_index: type = int batch_number: type = int length_range: tuple(int), len(length_range) = 2 e.g. (50, 70) ---------- return: ---------- fake_data: type = list[list] format: fake_data.shape[0] = batch_number length_range[0] <= len(fake_data[i]) <= length_range[1] 0 <= fake_data[i][j] <= max_index ''' max_time_step = length_range[0] + np.random.randint(length_range[1] - length_range[0] + 1) fake_data = np.zeros((batch_number, max_time_step)) mask = np.zeros((batch_number, max_time_step)).astype(theano.config.floatX) len_range = max_time_step - length_range[0] assert len_range >= 0 #pick a row to be the max length row row = np.random.randint(batch_number) fake_data[row] = np.random.randint(max_index+1, size = (max_time_step,)) mask[row] = np.ones(max_time_step) for batch in range(batch_number): if batch == row: continue length = length_range[0]+np.random.randint(len_range) fake_data[batch] = np.concatenate((np.random.randint(max_index+1 ,size = (length,)), np.zeros(max_time_step - length))) mask[batch] = np.concatenate((np.ones(length), np.zeros(max_time_step - length))) return (fake_data.astype('int32'), mask) def fake_data(max_index, batch_number, max_time_step, min_time_step): fake_data = np.zeros((batch_number, max_time_step)) mask = np.zeros((batch_number, max_time_step)).astype(theano.config.floatX) len_range = max_time_step - min_time_step assert len_range >= 0 #pick a row to be the max length row row = np.random.randint(batch_number) fake_data[row] = np.random.randint(max_index+1, size = (max_time_step,)) mask[row] = np.ones(max_time_step) for batch in range(batch_number): if batch == row: continue length = min_time_step+np.random.randint(len_range) fake_data[batch] = np.concatenate((np.random.randint(max_index+1 ,size = (length,)), np.zeros(max_time_step - length))) mask[batch] = np.concatenate((np.ones(length), np.zeros(max_time_step - length))) return (fake_data.astype('int32'), mask)
from common import * from editorcommon import * import weakref class KPEditorObject(KPEditorItem): SNAP_TO = (24,24) def __init__(self, obj, layer): KPEditorItem.__init__(self) obj.qtItem = self self._objRef = weakref.ref(obj) self._layerRef = weakref.ref(layer) self._updatePosition() self._updateSize() self.setAcceptHoverEvents(True) self.resizing = None if not hasattr(KPEditorObject, 'SELECTION_PEN'): KPEditorObject.SELECTION_PEN = QtGui.QPen(Qt.green, 1, Qt.DotLine) # I don't bother setting the ZValue because it doesn't quite matter: # only one layer's objects are ever clickable, and drawBackground takes # care of the layered drawing def _updatePosition(self): self.ignoreMovement = True x,y = self._objRef().position self.setPos(x*24, y*24) self.ignoreMovement = False def _updateSize(self): self.prepareGeometryChange() obj = self._objRef() w,h = obj.size self._boundingRect = QtCore.QRectF(0, 0, w*24, h*24) self._selectionRect = QtCore.QRectF(0, 0, w*24-1, h*24-1) self._resizerEndXY = (w*24-5, h*24-5) def paint(self, painter, option, widget): if self.isSelected(): painter.setPen(self.SELECTION_PEN) painter.drawRect(self._selectionRect) def hoverMoveEvent(self, event): if self._layerRef() != KP.mapScene.currentLayer: self.setCursor(Qt.ArrowCursor) return pos = event.pos() bit = self.resizerPortionAt(pos.x(), pos.y()) if bit == 1 or bit == 4: self.setCursor(Qt.SizeFDiagCursor) elif bit == 2 or bit == 3: self.setCursor(Qt.SizeBDiagCursor) elif bit == 7 or bit == 8: self.setCursor(Qt.SizeHorCursor) elif bit == 5 or bit == 6: self.setCursor(Qt.SizeVerCursor) else: self.setCursor(Qt.ArrowCursor) def mousePressEvent(self, event): if event.button() == Qt.LeftButton: pos = event.pos() bit = self.resizerPortionAt(pos.x(), pos.y()) if self._layerRef() == KP.mapScene.currentLayer and bit: # if bit: event.accept() x, xSide, y, ySide = False, None, False, None if bit == 1 or bit == 7 or bit == 3: x, xSide = True, 1 elif bit == 2 or bit == 4 or bit == 8: x, xSide = True, 0 if bit == 1 or bit == 2 or bit == 5: y, ySide = True, 1 elif bit == 3 or bit == 4 or bit == 6: y, ySide = True, 0 self.resizing = (x, xSide, y, ySide) return KPEditorItem.mousePressEvent(self, event) def _tryAndResize(self, obj, axisIndex, mousePosition, stationarySide): objPosition = obj.position[axisIndex] objSize = obj.size[axisIndex] if stationarySide == 0: # Resize the right/bottom side relativeMousePosition = mousePosition - objPosition newSize = relativeMousePosition + 1 if newSize == objSize or newSize < 1: return False if axisIndex == 1: obj.size = (obj.size[0], newSize) else: obj.size = (newSize, obj.size[1]) else: # Resize the left/top side rightSide = objPosition + objSize - 1 newLeftSide = mousePosition newPosition = newLeftSide newSize = rightSide - newLeftSide + 1 if newSize < 1: return False if newPosition == objPosition and newSize == objSize: return False if axisIndex == 1: obj.position = (obj.position[0], newPosition) obj.size = (obj.size[0], newSize) else: obj.position = (newPosition, obj.position[1]) obj.size = (newSize, obj.size[1]) return True def mouseMoveEvent(self, event): if self.resizing: obj = self._objRef() scenePos = event.scenePos() hasChanged = False resizeX, xSide, resizeY, ySide = self.resizing if resizeX: hasChanged |= self._tryAndResize(obj, 0, int(scenePos.x() / 24), xSide) if resizeY: hasChanged |= self._tryAndResize(obj, 1, int(scenePos.y() / 24), ySide) if hasChanged: obj.updateCache() self._layerRef().updateCache() self._updatePosition() self._updateSize() else: KPEditorItem.mouseMoveEvent(self, event) def mouseReleaseEvent(self, event): if self.resizing and event.button() == Qt.LeftButton: self.resizing = None else: KPEditorItem.mouseReleaseEvent(self, event) def _itemMoved(self, oldX, oldY, newX, newY): obj = self._objRef() obj.position = (newX/24, newY/24) self._layerRef().updateCache() def remove(self, withItem=False): obj = self._objRef() layer = self._layerRef() layer.objects.remove(obj) layer.updateCache() if withItem: self.scene().removeItem(self)
#!/usr/bin/env python """ Render Django templates. Useful for generating fixtures for the JavaScript unit test suite. Usage: python render_templates.py path/to/templates.json where "templates.json" is a JSON file of the form: [ { "template": "openassessmentblock/oa_base.html", "context": { "title": "Lorem", "question": "Ipsum?" }, "output": "oa_base.html" }, ... ] The rendered templates are saved to "output" relative to the templates.json file's directory. """ import sys import os.path import json import re import dateutil.parser import pytz # This is a bit of a hack to ensure that the root repo directory # is in the Python path, so Django can find the settings module. sys.path.append(os.path.dirname(os.path.dirname(__file__))) from django.template.context import Context from django.template.loader import get_template USAGE = u"{prog} TEMPLATE_DESC" DATETIME_REGEX = re.compile("^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}$") def parse_dates(context): """ Transform datetime strings into Python datetime objects. JSON does not provide a standard way to serialize datetime objects, but some of the templates expect that the context contains Python datetime objects. This (somewhat hacky) solution recursively searches the context for formatted datetime strings of the form "2014-01-02T12:34" and converts them to Python datetime objects with the timezone set to UTC. Args: context (JSON-serializable): The context (or part of the context) that will be passed to the template. Dictionaries and lists will be recursively searched and transformed. Returns: JSON-serializable of the same type as the `context` argument. """ if isinstance(context, dict): return { key: parse_dates(value) for key, value in context.iteritems() } elif isinstance(context, list): return [ parse_dates(item) for item in context ] elif isinstance(context, basestring): if DATETIME_REGEX.match(context) is not None: return dateutil.parser.parse(context).replace(tzinfo=pytz.utc) return context def render_templates(root_dir, template_json): """ Create rendered templates. Args: root_dir (str): The directory in which to write the rendered templates. template_json (dict): Description of which templates to render. Must be a list of dicts, each containing keys "template" (str), "context" (dict), and "output" (str). Returns: None """ for template_dict in template_json: template = get_template(template_dict['template']) context = parse_dates(template_dict['context']) rendered = template.render(Context(context)) output_path = os.path.join(root_dir, template_dict['output']) try: with open(output_path, 'w') as output_file: output_file.write(rendered.encode('utf-8')) except IOError: print "Could not write rendered template to file: {}".format(output_path) sys.exit(1) def main(): """ Main entry point for the script. """ if len(sys.argv) < 2: print USAGE.format(sys.argv[0]) sys.exit(1) try: with open(sys.argv[1]) as template_json: root_dir = os.path.dirname(sys.argv[1]) render_templates(root_dir, json.load(template_json)) except IOError as ex: print u"Could not open template description file: {}".format(sys.argv[1]) print(ex) sys.exit(1) except ValueError as ex: print u"Could not parse template description as JSON: {}".format(sys.argv[1]) print(ex) sys.exit(1) if __name__ == '__main__': main()
#!/usr/bin/python # Copyright (c) 2010 Alon Swartz <alon@turnkeylinux.org> # # This program is free software; you can redistribute it and/or # modify it under the terms of the GNU General Public License as # published by the Free Software Foundation; either version 2 of # the License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. """EBS Mount - manually mount EBS device (simulates udev add trigger) Arguments: device EBS device to mount (e.g., /dev/xvdf, /dev/vda) Options: --format=FS Format device prior to mount (e.g., --format=ext3) """ import re import os import sys import getopt import ebsmount import executil from utils import config, is_mounted def usage(e=None): if e: print >> sys.stderr, "error: " + str(e) print >> sys.stderr, "Syntax: %s [-opts] <device>" % sys.argv[0] print >> sys.stderr, __doc__.strip() sys.exit(1) def fatal(s): print >> sys.stderr, "error: " + str(s) sys.exit(1) def _expected_devpath(devname, devpaths): """ugly hack to test expected structure of devpath""" raw_output = executil.getoutput('udevadm info -a -n %s' % devname) for line in raw_output.splitlines(): line = line.strip() m = re.match("^looking at parent device '(.*)':", line) if m: devpath = m.group(1) for pattern in devpaths: if re.search(pattern, devpath): return True return False def main(): try: opts, args = getopt.gnu_getopt(sys.argv[1:], 'h', ['format=']) except getopt.GetoptError, e: usage(e) filesystem = None for opt, val in opts: if opt == '-h': usage() if opt == '--format': filesystem = val if not len(args) == 1: usage() devname = args[0] if not os.path.exists(devname): fatal("%s does not exist" % devname) if not _expected_devpath(devname, config.devpaths.split()): fatal("devpath not of expected structure, or failed lookup") if filesystem: if is_mounted(devname): fatal("%s is mounted" % devname) if not filesystem in config.filesystems.split(): fatal("%s is not supported in %s" % (filesystem, config.CONF_FILE)) executil.system("mkfs." + filesystem, "-q", devname) ebsmount.ebsmount_add(devname, config.mountdir) if __name__=="__main__": main()
# Copyright (c) 2012 Giorgos Verigakis <verigak@gmail.com> # # Permission to use, copy, modify, and distribute this software for any # purpose with or without fee is hereby granted, provided that the above # copyright notice and this permission notice appear in all copies. # # THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES # WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF # MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR # ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES # WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN # ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF # OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE. from __future__ import division from collections import deque from datetime import timedelta from math import ceil from sys import stderr from time import time __version__ = '1.2' class Infinite(object): file = stderr sma_window = 10 def __init__(self, *args, **kwargs): self.index = 0 self.start_ts = time() self._ts = self.start_ts self._dt = deque(maxlen=self.sma_window) for key, val in kwargs.items(): setattr(self, key, val) def __getitem__(self, key): if key.startswith('_'): return None return getattr(self, key, None) @property def avg(self): return sum(self._dt) / len(self._dt) if self._dt else 0 @property def elapsed(self): return int(time() - self.start_ts) @property def elapsed_td(self): return timedelta(seconds=self.elapsed) def update(self): pass def start(self): pass def finish(self): pass def next(self, n=1): if n > 0: now = time() dt = (now - self._ts) / n self._dt.append(dt) self._ts = now self.index = self.index + n self.update() def iter(self, it): for x in it: yield x self.next() self.finish() class Progress(Infinite): def __init__(self, *args, **kwargs): super(Progress, self).__init__(*args, **kwargs) self.max = kwargs.get('max', 100) @property def eta(self): return int(ceil(self.avg * self.remaining)) @property def eta_td(self): return timedelta(seconds=self.eta) @property def percent(self): return self.progress * 100 @property def progress(self): return min(1, self.index / self.max) @property def remaining(self): return max(self.max - self.index, 0) def start(self): self.update() def goto(self, index): incr = index - self.index self.next(incr) def iter(self, it): try: self.max = len(it) except TypeError: pass for x in it: yield x self.next() self.finish()
#!/usr/bin/env python # InkCutter, Plot HPGL directly from Inkscape. # device.py # # Copyright 2010 Jairus Martin <frmdstryr@gmail.com> # Copyright 2013 Shaun Landis <slandis@gmail.com> # # This program is free software; you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation; either version 2 of the License, or # (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program; if not, write to the Free Software # Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, # MA 02110-1301, USA. import serial from lxml import etree import os if os.name != 'nt': import cups class Device: def __init__(self,config={}): #self.xml = etree.parse(filename).getroot() conf = {'width':0,'length':0,'name':'','interface':'serial','serial':{'port':'/dev/ttyUSB0','baud':9600}} conf.update(config) self.width = conf['width'] self.length = conf['length'] self.name = conf['name'] self.interface = conf['interface'] self.serial = conf['serial'] def getPrinters(self): con = cups.Connection() printers = con.getPrinters() self.printers = printers def save(self,id,attribs): # save settings to xml dev = self.xml.find('device[@id="%s"]'%id) err = [] # delete if exists? if len(dev): del dev[0] else: dev = etree.SubElement(self.xml,'device') dev.set('id',id) iface = etree.SubElement(d, "interface") for key,value in attribs.iteritems(): iface.set(key,value) def plot(self,filename): def toSerial(data,settings): assert type(data) == str, "input data must be a str type" import serial # set default settings set = {'baud':9600} set.update(settings); #create serial and set settings ser = serial.Serial() ser.baudrate = set['baud'] ser.port = set['port'] ser.open() if ser.isOpen(): #send data & return bits sent bits = ser.write(data); ser.close(); return True; else: return False; def toPrinter(data,printer): assert type(data) == str, "input data must be a str type" assert type(printer) == str, "printer name must be a string" printer = os.popen('lpr -P %s'%(printer),'w') printer.write(data) printer.close() return True; def toUSBPrinter(data,printer): assert type(data) == str, "input data must be a str type" assert type(printer) == str, "printer name must be a string" p = open(printer, 'w+') p.write(data) p.close() return True; f=open(filename,'r') if self.interface=='printer': toPrinter(f.read(),self.name) elif self.interface=='usb printer': toUSBPrinter(f.read(),self.name) elif self.interface=='serial': toSerial(f.read(),self.serial) else: raise AssertionError('Invalid interface type, only printers and serial connections are supported.')
from django.db.models import Q, Sum from django.db.models.deletion import ProtectedError from django.db.utils import IntegrityError from django.forms.models import modelform_factory from django.test import TestCase, skipIfDBFeature from .models import ( A, B, C, D, Address, Board, CharLink, Company, Contact, Content, Developer, Guild, HasLinkThing, Link, Node, Note, OddRelation1, OddRelation2, Organization, Person, Place, Related, Restaurant, Tag, Team, TextLink, ) class GenericRelationTests(TestCase): def test_inherited_models_content_type(self): """ Test that GenericRelations on inherited classes use the correct content type. """ p = Place.objects.create(name="South Park") r = Restaurant.objects.create(name="Chubby's") l1 = Link.objects.create(content_object=p) l2 = Link.objects.create(content_object=r) self.assertEqual(list(p.links.all()), [l1]) self.assertEqual(list(r.links.all()), [l2]) def test_reverse_relation_pk(self): """ Test that the correct column name is used for the primary key on the originating model of a query. See #12664. """ p = Person.objects.create(account=23, name='Chef') Address.objects.create(street='123 Anywhere Place', city='Conifer', state='CO', zipcode='80433', content_object=p) qs = Person.objects.filter(addresses__zipcode='80433') self.assertEqual(1, qs.count()) self.assertEqual('Chef', qs[0].name) def test_charlink_delete(self): oddrel = OddRelation1.objects.create(name='clink') CharLink.objects.create(content_object=oddrel) oddrel.delete() def test_textlink_delete(self): oddrel = OddRelation2.objects.create(name='tlink') TextLink.objects.create(content_object=oddrel) oddrel.delete() def test_q_object_or(self): """ Tests that SQL query parameters for generic relations are properly grouped when OR is used. Test for bug http://code.djangoproject.com/ticket/11535 In this bug the first query (below) works while the second, with the query parameters the same but in reverse order, does not. The issue is that the generic relation conditions do not get properly grouped in parentheses. """ note_contact = Contact.objects.create() org_contact = Contact.objects.create() Note.objects.create(note='note', content_object=note_contact) org = Organization.objects.create(name='org name') org.contacts.add(org_contact) # search with a non-matching note and a matching org name qs = Contact.objects.filter(Q(notes__note__icontains=r'other note') | Q(organizations__name__icontains=r'org name')) self.assertIn(org_contact, qs) # search again, with the same query parameters, in reverse order qs = Contact.objects.filter( Q(organizations__name__icontains=r'org name') | Q(notes__note__icontains=r'other note')) self.assertIn(org_contact, qs) def test_join_reuse(self): qs = Person.objects.filter( addresses__street='foo' ).filter( addresses__street='bar' ) self.assertEqual(str(qs.query).count('JOIN'), 2) def test_generic_relation_ordering(self): """ Test that ordering over a generic relation does not include extraneous duplicate results, nor excludes rows not participating in the relation. """ p1 = Place.objects.create(name="South Park") p2 = Place.objects.create(name="The City") c = Company.objects.create(name="Chubby's Intl.") Link.objects.create(content_object=p1) Link.objects.create(content_object=c) places = list(Place.objects.order_by('links__id')) def count_places(place): return len([p for p in places if p.id == place.id]) self.assertEqual(len(places), 2) self.assertEqual(count_places(p1), 1) self.assertEqual(count_places(p2), 1) def test_target_model_is_unsaved(self): """Test related to #13085""" # Fails with another, ORM-level error dev1 = Developer(name='Joe') note = Note(note='Deserves promotion', content_object=dev1) self.assertRaises(IntegrityError, note.save) def test_target_model_len_zero(self): """Test for #13085 -- __len__() returns 0""" team1 = Team.objects.create(name='Backend devs') try: note = Note(note='Deserve a bonus', content_object=team1) except Exception as e: if (issubclass(type(e), Exception) and str(e) == 'Impossible arguments to GFK.get_content_type!'): self.fail("Saving model with GenericForeignKey to model instance whose " "__len__ method returns 0 shouldn't fail.") raise e note.save() def test_target_model_nonzero_false(self): """Test related to #13085""" # __nonzero__() returns False -- This actually doesn't currently fail. # This test validates that g1 = Guild.objects.create(name='First guild') note = Note(note='Note for guild', content_object=g1) note.save() @skipIfDBFeature('interprets_empty_strings_as_nulls') def test_gfk_to_model_with_empty_pk(self): """Test related to #13085""" # Saving model with GenericForeignKey to model instance with an # empty CharField PK b1 = Board.objects.create(name='') tag = Tag(label='VP', content_object=b1) tag.save() def test_ticket_20378(self): # Create a couple of extra HasLinkThing so that the autopk value # isn't the same for Link and HasLinkThing. hs1 = HasLinkThing.objects.create() hs2 = HasLinkThing.objects.create() hs3 = HasLinkThing.objects.create() hs4 = HasLinkThing.objects.create() l1 = Link.objects.create(content_object=hs3) l2 = Link.objects.create(content_object=hs4) self.assertQuerysetEqual( HasLinkThing.objects.filter(links=l1), [hs3], lambda x: x) self.assertQuerysetEqual( HasLinkThing.objects.filter(links=l2), [hs4], lambda x: x) self.assertQuerysetEqual( HasLinkThing.objects.exclude(links=l2), [hs1, hs2, hs3], lambda x: x, ordered=False) self.assertQuerysetEqual( HasLinkThing.objects.exclude(links=l1), [hs1, hs2, hs4], lambda x: x, ordered=False) def test_ticket_20564(self): b1 = B.objects.create() b2 = B.objects.create() b3 = B.objects.create() c1 = C.objects.create(b=b1) c2 = C.objects.create(b=b2) c3 = C.objects.create(b=b3) A.objects.create(flag=None, content_object=b1) A.objects.create(flag=True, content_object=b2) self.assertQuerysetEqual( C.objects.filter(b__a__flag=None), [c1, c3], lambda x: x ) self.assertQuerysetEqual( C.objects.exclude(b__a__flag=None), [c2], lambda x: x ) def test_ticket_20564_nullable_fk(self): b1 = B.objects.create() b2 = B.objects.create() b3 = B.objects.create() d1 = D.objects.create(b=b1) d2 = D.objects.create(b=b2) d3 = D.objects.create(b=b3) d4 = D.objects.create() A.objects.create(flag=None, content_object=b1) A.objects.create(flag=True, content_object=b1) A.objects.create(flag=True, content_object=b2) self.assertQuerysetEqual( D.objects.exclude(b__a__flag=None), [d2], lambda x: x ) self.assertQuerysetEqual( D.objects.filter(b__a__flag=None), [d1, d3, d4], lambda x: x ) self.assertQuerysetEqual( B.objects.filter(a__flag=None), [b1, b3], lambda x: x ) self.assertQuerysetEqual( B.objects.exclude(a__flag=None), [b2], lambda x: x ) def test_extra_join_condition(self): # A crude check that content_type_id is taken in account in the # join/subquery condition. self.assertIn("content_type_id", str(B.objects.exclude(a__flag=None).query).lower()) # No need for any joins - the join from inner query can be trimmed in # this case (but not in the above case as no a objects at all for given # B would then fail). self.assertNotIn(" join ", str(B.objects.exclude(a__flag=True).query).lower()) self.assertIn("content_type_id", str(B.objects.exclude(a__flag=True).query).lower()) def test_annotate(self): hs1 = HasLinkThing.objects.create() hs2 = HasLinkThing.objects.create() HasLinkThing.objects.create() b = Board.objects.create(name=str(hs1.pk)) Link.objects.create(content_object=hs2) l = Link.objects.create(content_object=hs1) Link.objects.create(content_object=b) qs = HasLinkThing.objects.annotate(Sum('links')).filter(pk=hs1.pk) # If content_type restriction isn't in the query's join condition, # then wrong results are produced here as the link to b will also match # (b and hs1 have equal pks). self.assertEqual(qs.count(), 1) self.assertEqual(qs[0].links__sum, l.id) l.delete() # Now if we don't have proper left join, we will not produce any # results at all here. # clear cached results qs = qs.all() self.assertEqual(qs.count(), 1) # Note - 0 here would be a nicer result... self.assertIs(qs[0].links__sum, None) # Finally test that filtering works. self.assertEqual(qs.filter(links__sum__isnull=True).count(), 1) self.assertEqual(qs.filter(links__sum__isnull=False).count(), 0) def test_filter_targets_related_pk(self): HasLinkThing.objects.create() hs2 = HasLinkThing.objects.create() l = Link.objects.create(content_object=hs2) self.assertNotEqual(l.object_id, l.pk) self.assertQuerysetEqual( HasLinkThing.objects.filter(links=l.pk), [hs2], lambda x: x) def test_editable_generic_rel(self): GenericRelationForm = modelform_factory(HasLinkThing, fields='__all__') form = GenericRelationForm() self.assertIn('links', form.fields) form = GenericRelationForm({'links': None}) self.assertTrue(form.is_valid()) form.save() links = HasLinkThing._meta.get_field('links') self.assertEqual(links.save_form_data_calls, 1) def test_ticket_22998(self): related = Related.objects.create() content = Content.objects.create(related_obj=related) Node.objects.create(content=content) # deleting the Related cascades to the Content cascades to the Node, # where the pre_delete signal should fire and prevent deletion. with self.assertRaises(ProtectedError): related.delete() def test_ticket_22982(self): place = Place.objects.create(name='My Place') self.assertIn('GenericRelatedObjectManager', str(place.links))
############################################################################## # # Copyright (c) 2003 Zope Foundation and Contributors. # All Rights Reserved. # # This software is subject to the provisions of the Zope Public License, # Version 2.1 (ZPL). A copy of the ZPL should accompany this distribution. # THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL EXPRESS OR IMPLIED # WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED # WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS # FOR A PARTICULAR PURPOSE. # ############################################################################## """Test Element meta-class. """ import unittest from zope.interface.interface import Element class TestElement(unittest.TestCase): def test_taggedValues(self): """Test that we can update tagged values of more than one element """ e1 = Element("foo") e2 = Element("bar") e1.setTaggedValue("x", 1) e2.setTaggedValue("x", 2) self.assertEqual(e1.getTaggedValue("x"), 1) self.assertEqual(e2.getTaggedValue("x"), 2) def test_suite(): suite = unittest.TestSuite() suite.addTest(unittest.makeSuite(TestElement)) return suite if __name__ == '__main__': unittest.main(defaultTest='test_suite')
#!/usr/bin/python # -*- coding: utf-8 -*- """ Extractor to validate a METS file and check the existence and content of the files linked from each fileSec/fileGrp/file/FLocat tag, assumed to contain an MD5 checksum. The "md5sum" utility is required. """ # # (C) Federico Leva and Fondazione BEIC, 2018 # # Distributed under the terms of the MIT license. # __version__ = '0.1.0' from lxml import etree import os import subprocess # http://lxml.de/validation.html parser = etree.XMLParser(dtd_validation=True) digest = open('mets.md5sum', 'w') for dirpath, dirnames, filenames in os.walk('.'): for filename in [ each for each in filenames if each.endswith('.xml') ]: xml = os.path.join(dirpath, filename) try: mets = etree.parse(open(xml, 'r')) files = mets.xpath('//*[local-name()="file"]') for item in files: content = item.xpath( './*[local-name()="FLocat"]/@xlink:href', namespaces={"xlink": "http://www.w3.org/1999/xlink"} )[0] checksum = item.xpath('./@CHECKSUM')[0] digest.write("%s %s\n" % (checksum, os.path.normpath(os.path.join(dirpath, content)) ) ) except: pass check = subprocess.call(["md5sum", "-c", "--status", "mets.md5sum"]) if check == 0: print("SUCCESS: The METS content has been verified correctly.") else: print("ERROR: The checksum validation has failed.")
import pytest import sql_query_dict def test_escape_string_with_single_quote(): assert sql_query_dict.quote_string("'a") == '"\'a"' def test_escape_string_with_double_quote(): assert sql_query_dict.quote_string('"a') == "'\"a'" def test_escape_string_with_single_and_double_quote(): assert sql_query_dict.quote_string(""" '" """) == """' \\\'" '""" def test_escape_string(): assert sql_query_dict.quote_string('a') == "'a'" def test_split_key_compare(): assert sql_query_dict._split_key_compare('key|=') == ('key', '|=') def test_mysql_list_esc(): assert sql_query_dict.mysql_list_esc([1, 2]) == "1,2" def test_mysql_list_esc_string_numbers(): assert sql_query_dict.mysql_list_esc(["1", "2"]) == "'1','2'" def test_mysql_list_esc_string(): assert sql_query_dict.mysql_list_esc(["a", "b"]) == "'a','b'" def test_mysql_list_with_or_equals(): assert sql_query_dict._mysql_clause('x|=', [1, 2, 3], '%s') == \ " (x IN (1,2,3)) " def test_mysql_list_with_none(): assert sql_query_dict._mysql_clause('x', [None, False], '%s') == \ ' ((x IS NULL) OR (x IN (False)) ) ' def test_mysql_list_with_generator(): assert sql_query_dict._mysql_clause( 'x', (x for x in [1, 2, 3]), '%s' ) == " (x IN (1,2,3)) " def test_parse_tablename(): assert sql_query_dict._parse_tablename('xyz') == 'xyz' def test_parse_tablename_err(): with pytest.raises(TypeError): sql_query_dict._parse_tablename(1) def test_parse_tablename_set(): assert sql_query_dict._parse_tablename(set(['xyz', 'abc'])) in ( 'xyz,abc', 'abc,xyz' ) def test_mysql_with_gt_lt(): SQL, vals = sql_query_dict.select( 't', 'z', {'x><': (10, 30), 'y': 1} ) # easiest way to handle both orderings of the clauses assert SQL in ( "SELECT z FROM t WHERE (y = %s) AND ((x > %s) AND (x < %s)) ", "SELECT z FROM t WHERE ((x > %s) AND (x < %s)) AND (y = %s) ", ) assert vals in ( [1, 10, 30], [10, 30, 1], ) def test_mysql_string_value(): assert sql_query_dict._mysql_clause('x', 'the', '%s') == \ " (x = %s) " def test_mysql_like(): assert sql_query_dict._mysql_clause('x~', 'the %', '%s') == \ " (x LIKE %s) " def test_mysql_not_like(): assert sql_query_dict._mysql_clause('x!~', 'the %', '%s') == \ " (x NOT LIKE %s) " def test_mysql_not_in(): assert sql_query_dict._mysql_clause('x!=', [1, 2, 3], '%s') == \ " (x NOT IN (1,2,3)) " def test_mysql_list_compare_with_none(): assert sql_query_dict._mysql_clause( 'x!=', [None, 1, 2, 3], '%s' ) == " ((x IS NOT NULL) AND (x NOT IN (1,2,3)) ) " assert sql_query_dict._mysql_clause( 'x', [None, 1, 2, 3], '%s' ) == " ((x IS NULL) OR (x IN (1,2,3)) ) "
#!/usr/bin/env python3 """Reddit bot for updating user flairs via PM requests""" import sys import re import os import time import logging import logging.handlers import praw import OAuth2Util from config import cfg def setup_logging(): """Configure logging module for rotating logs and console output""" rotate_cfg = { "filename": cfg["log_file"], "maxBytes": 1024*1000, "backupCount": 5 } rotate_fmt = "%(asctime)s %(levelname)-8s %(message)s" console_fmt = "%(levelname)-8s %(message)s" if cfg["debug"]: level = logging.DEBUG else: level = logging.INFO logger = logging.getLogger() logger.setLevel(level) rotate = logging.handlers.RotatingFileHandler(**rotate_cfg) rotate.setFormatter(logging.Formatter(rotate_fmt)) logger.addHandler(rotate) console = logging.StreamHandler() console.setLevel(level) console.setFormatter(logging.Formatter(console_fmt)) logger.addHandler(console) def parse_wiki_flairs(content): regex = re.compile(cfg["wiki_format"]) matches = [] for line in content.splitlines(): match = regex.match(line) if match is not None: flair = match.groups() matches.append(flair[0]) return matches class FlairBot: def __init__(self): user_agent = cfg["user_agent"] % (cfg["version"], cfg["subreddit"]) self.r = praw.Reddit(user_agent=user_agent) self.o = OAuth2Util.OAuth2Util(self.r) self.processed = 0 self.flairs = [] self.login() def login(self): """Start a new reddit session""" logging.info("Logging in...") try: self.o.refresh() except: logging.exception("Login failed") sys.exit(1) def get_requests(self): """Fetch and return all new PMs matching configured subject""" logging.info("Fetching new messages...") pending = [] try: msgs = self.r.get_unread(limit=None) except: logging.exception("Failed to get new messages") return for msg in msgs: logging.debug(msg) if str(msg.subject) == cfg["subject"]: pending.append(msg) if not cfg["limit_read"]: msg.mark_as_read() pending.reverse() logging.info("Got %i new requests", len(pending)) return pending def process_request(self, subreddit, msg): """Read flair request message and set if possible""" user = str(msg.author) flair = str(msg.body) if user in cfg["blacklist"]: logging.warning("Skipping blacklisted user: %s", user) return if flair in self.flairs: try: subreddit.set_flair(user, "", flair) except: logging.exception("Error setting flair to %s for %s", flair, user) return self.processed += 1 logging.info("Flair changed to %s for %s", flair, user) try: self.r.send_message(user, cfg["msg_subject"], cfg["msg_success"] % (flair)) except: logging.exception("Error messaging user") else: logging.warning("Flair %s requested by %s doesn't exist", flair, user) wiki = "https://www.reddit.com/r/%s/wiki/%s" % (cfg["subreddit"], cfg["wiki_page"]) try: self.r.send_message(user, cfg["msg_subject"], cfg["msg_failure"] % (flair, wiki)) except: logging.exception("Error messaging user") if cfg["limit_read"]: msg.mark_as_read() def get_wiki_page(self, subreddit): logging.info("Fetching wiki page...") if not os.path.exists(cfg["cache_file"]): logging.warning("No cache file found") modified = 0 else: stat = os.stat(cfg["cache_file"]) modified = int(stat.st_mtime) now = int(time.time()) if modified > 0 and now - modified < cfg["cache_time"]: cache = open(cfg["cache_file"], "r") logging.debug("Using valid cache") wiki_page = cache.read() cache.close() return wiki_page try: logging.debug("Updating cache") wiki_page = subreddit.get_wiki_page(cfg["wiki_page"]).content_md except (praw.errors.NotFound): logging.error("Wiki page %s doesn't exist", cfg["wiki_page"]) return cache = open(cfg["cache_file"], "w") logging.debug("Writing cache") cache.write(wiki_page) cache.close() return wiki_page def run(self): """Process all new flair requests""" try: requests = self.get_requests() except (praw.errors.HTTPException): logging.error("OAuth access is invalid") return subreddit = self.r.get_subreddit(cfg["subreddit"]) wiki_page = self.get_wiki_page(subreddit) if wiki_page is None: return self.flairs = parse_wiki_flairs(wiki_page) logging.debug(self.flairs) if requests is None: logging.info("No new messages to process") return for msg in requests: self.process_request(subreddit, msg) setup_logging() if __name__ == "__main__": flair_bot = FlairBot() logging.info("Starting new run...") flair_bot.run() logging.info("Run complete! Processed %i requests.", flair_bot.processed)
# Copyright 2016 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== """Tests for sparse_feature_column.py (deprecated). This module and all its submodules are deprecated. To UPDATE or USE linear optimizers, please check its latest version in core: tensorflow_estimator/python/estimator/canned/linear_optimizer/. """ from __future__ import absolute_import from __future__ import division from __future__ import print_function from tensorflow.contrib.linear_optimizer.python.ops.sparse_feature_column import SparseFeatureColumn from tensorflow.python.framework import ops from tensorflow.python.framework.test_util import TensorFlowTestCase from tensorflow.python.platform import googletest class SparseFeatureColumnTest(TensorFlowTestCase): """Tests for SparseFeatureColumn. """ def testBasic(self): expected_example_indices = [1, 1, 1, 2] expected_feature_indices = [0, 1, 2, 0] sfc = SparseFeatureColumn(expected_example_indices, expected_feature_indices, None) self.assertTrue(isinstance(sfc.example_indices, ops.Tensor)) self.assertTrue(isinstance(sfc.feature_indices, ops.Tensor)) self.assertEqual(sfc.feature_values, None) with self.cached_session(): self.assertAllEqual(expected_example_indices, sfc.example_indices.eval()) self.assertAllEqual(expected_feature_indices, sfc.feature_indices.eval()) expected_feature_values = [1.0, 2.0, 3.0, 4.0] sfc = SparseFeatureColumn([1, 1, 1, 2], [0, 1, 2, 0], expected_feature_values) with self.cached_session(): self.assertAllEqual(expected_feature_values, sfc.feature_values.eval()) if __name__ == '__main__': googletest.main()
import nose import angr from angr.calling_conventions import SimCCSystemVAMD64 import logging l = logging.getLogger("angr.tests.test_rol") import os test_location = str(os.path.join(os.path.dirname(os.path.realpath(__file__)), '../../binaries/tests')) def test_rol_x86_64(): binary_path = test_location + "/x86_64/test_rol.exe" proj = angr.Project(binary_path) initial_state = proj.factory.blank_state(addr=0x401000) r_rax = initial_state.se.BVS('rax', 64) initial_state.regs.rax = r_rax pg = proj.factory.simgr(initial_state, immutable=False) pg.explore(find=0x401013, avoid=0x401010) found_state = pg.found[0] result = found_state.se.eval(r_rax) nose.tools.assert_equal(result, 0x37B7AB70) def test_rol_i386(): binary_path = test_location + "/i386/test_rol.exe" proj = angr.Project(binary_path) initial_state = proj.factory.blank_state(addr=0x401000) r_eax = initial_state.se.BVS('eax', 32) initial_state.regs.eax = r_eax pg = proj.factory.simgr(initial_state, immutable=False) pg.explore(find=0x401013, avoid=0x401010) found_state = pg.found[0] result = found_state.se.eval(r_eax) nose.tools.assert_equal(result, 0x37B7AB70) def test_all(): test_rol_x86_64() test_rol_i386() if __name__ == "__main__": test_all()
# -*- coding: utf-8 -*- ############################################################################## # # OpenERP, Open Source Management Solution # Copyright (C) 2012-today OpenERP SA (<http://www.openerp.com>) # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/> # ############################################################################## import logging import werkzeug import openerp from openerp.addons.auth_signup.res_users import SignupError from openerp.addons.web.controllers.main import ensure_db from openerp import http from openerp.http import request from openerp.tools.translate import _ _logger = logging.getLogger(__name__) class AuthSignupHome(openerp.addons.web.controllers.main.Home): @http.route() def web_login(self, *args, **kw): ensure_db() response = super(AuthSignupHome, self).web_login(*args, **kw) response.qcontext.update(self.get_auth_signup_config()) if request.httprequest.method == 'GET' and request.session.uid and request.params.get('redirect'): # Redirect if already logged in and redirect param is present return http.redirect_with_hash(request.params.get('redirect')) return response @http.route('/web/signup', type='http', auth='public', website=True) def web_auth_signup(self, *args, **kw): qcontext = self.get_auth_signup_qcontext() if not qcontext.get('token') and not qcontext.get('signup_enabled'): raise werkzeug.exceptions.NotFound() if 'error' not in qcontext and request.httprequest.method == 'POST': try: self.do_signup(qcontext) return super(AuthSignupHome, self).web_login(*args, **kw) except (SignupError, AssertionError), e: qcontext['error'] = _(e.message) return request.render('auth_signup.signup', qcontext) @http.route('/web/reset_password', type='http', auth='public', website=True) def web_auth_reset_password(self, *args, **kw): qcontext = self.get_auth_signup_qcontext() if not qcontext.get('token') and not qcontext.get('reset_password_enabled'): raise werkzeug.exceptions.NotFound() if 'error' not in qcontext and request.httprequest.method == 'POST': try: if qcontext.get('token'): self.do_signup(qcontext) return super(AuthSignupHome, self).web_login(*args, **kw) else: login = qcontext.get('login') assert login, "No login provided." res_users = request.registry.get('res.users') res_users.reset_password(request.cr, openerp.SUPERUSER_ID, login) qcontext['message'] = _("An email has been sent with credentials to reset your password") except SignupError: qcontext['error'] = _("Could not reset your password") _logger.exception('error when resetting password') except Exception, e: qcontext['error'] = _(e.message) return request.render('auth_signup.reset_password', qcontext) def get_auth_signup_config(self): """retrieve the module config (which features are enabled) for the login page""" icp = request.registry.get('ir.config_parameter') return { 'signup_enabled': icp.get_param(request.cr, openerp.SUPERUSER_ID, 'auth_signup.allow_uninvited') == 'True', 'reset_password_enabled': icp.get_param(request.cr, openerp.SUPERUSER_ID, 'auth_signup.reset_password') == 'True', } def get_auth_signup_qcontext(self): """ Shared helper returning the rendering context for signup and reset password """ qcontext = request.params.copy() qcontext.update(self.get_auth_signup_config()) if qcontext.get('token'): try: # retrieve the user info (name, login or email) corresponding to a signup token res_partner = request.registry.get('res.partner') token_infos = res_partner.signup_retrieve_info(request.cr, openerp.SUPERUSER_ID, qcontext.get('token')) for k, v in token_infos.items(): qcontext.setdefault(k, v) except: qcontext['error'] = _("Invalid signup token") return qcontext def do_signup(self, qcontext): """ Shared helper that creates a res.partner out of a token """ values = dict((key, qcontext.get(key)) for key in ('login', 'name', 'password')) assert any([k for k in values.values()]), "The form was not properly filled in." assert values.get('password') == qcontext.get('confirm_password'), "Passwords do not match; please retype them." self._signup_with_values(qcontext.get('token'), values) request.cr.commit() def _signup_with_values(self, token, values): db, login, password = request.registry['res.users'].signup(request.cr, openerp.SUPERUSER_ID, values, token) request.cr.commit() # as authenticate will use its own cursor we need to commit the current transaction uid = request.session.authenticate(db, login, password) if not uid: raise SignupError(_('Authentification Failed.')) # vim:expandtab:tabstop=4:softtabstop=4:shiftwidth=4:
# -*- coding: utf-8 -*- # # Copyright 2012-2015 Spotify AB # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # import datetime import json import luigi from luigi.contrib.esindex import CopyToIndex class FakeDocuments(luigi.Task): """ Generates a local file containing 5 elements of data in JSON format. """ #: the date parameter. date = luigi.DateParameter(default=datetime.date.today()) def run(self): """ Writes data in JSON format into the task's output target. The data objects have the following attributes: * `_id` is the default Elasticsearch id field, * `text`: the text, * `date`: the day when the data was created. """ today = datetime.date.today() with self.output().open('w') as output: for i in range(5): output.write(json.dumps({'_id': i, 'text': 'Hi %s' % i, 'date': str(today)})) output.write('\n') def output(self): """ Returns the target output for this task. In this case, a successful execution of this task will create a file on the local filesystem. :return: the target output for this task. :rtype: object (:py:class:`luigi.target.Target`) """ return luigi.LocalTarget(path='/tmp/_docs-%s.ldj' % self.date) class IndexDocuments(CopyToIndex): """ This task loads JSON data contained in a :py:class:`luigi.target.Target` into an ElasticSearch index. This task's input will the target returned by :py:meth:`~.FakeDocuments.output`. This class uses :py:meth:`luigi.contrib.esindex.CopyToIndex.run`. After running this task you can run: .. code-block:: console $ curl "localhost:9200/example_index/_search?pretty" to see the indexed documents. To see the update log, run .. code-block:: console $ curl "localhost:9200/update_log/_search?q=target_index:example_index&pretty" To cleanup both indexes run: .. code-block:: console $ curl -XDELETE "localhost:9200/example_index" $ curl -XDELETE "localhost:9200/update_log/_query?q=target_index:example_index" """ #: date task parameter (default = today) date = luigi.DateParameter(default=datetime.date.today()) #: the name of the index in ElasticSearch to be updated. index = 'example_index' #: the name of the document type. doc_type = 'greetings' #: the host running the ElasticSearch service. host = 'localhost' #: the port used by the ElasticSearch service. port = 9200 def requires(self): """ This task's dependencies: * :py:class:`~.FakeDocuments` :return: object (:py:class:`luigi.task.Task`) """ return FakeDocuments() if __name__ == "__main__": luigi.run(['--task', 'IndexDocuments'])
#!/usr/bin/python # Copyright 2018 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== """Generates YAML configuration file for allreduce-based distributed TensorFlow. The workers will be run in a Kubernetes (k8s) container cluster. """ from __future__ import absolute_import from __future__ import division from __future__ import print_function import argparse import sys import k8s_generate_yaml_lib # Note: It is intentional that we do not import tensorflow in this script. The # machine that launches a TensorFlow k8s cluster does not have to have the # Python package of TensorFlow installed on it. DEFAULT_DOCKER_IMAGE = 'tensorflow/tensorflow:latest-devel' DEFAULT_PORT = 22 DEFAULT_CONFIG_MAP = 'k8s-config-map' DEFAULT_DEPLOYMENT = 'k8s-ml-deployment' def main(): """Do arg parsing.""" parser = argparse.ArgumentParser() parser.add_argument( '--docker_image', type=str, default=DEFAULT_DOCKER_IMAGE, help='Override default docker image for the TensorFlow') parser.add_argument( '--num_containers', type=int, default=0, help='How many docker containers to launch') parser.add_argument( '--config_map', type=str, default=DEFAULT_CONFIG_MAP, help='Override default config map') parser.add_argument( '--deployment', type=str, default=DEFAULT_DEPLOYMENT, help='Override default deployment') parser.add_argument( '--ssh_port', type=int, default=DEFAULT_PORT, help='Override default ssh port (Default: %d)' % DEFAULT_PORT) parser.add_argument( '--use_hostnet', type=int, default=0, help='Used to enable host network mode (Default: 0)') parser.add_argument( '--use_shared_volume', type=int, default=0, help='Used to mount shared volume (Default: 0)') args = parser.parse_args() if args.num_containers <= 0: sys.stderr.write('--num_containers must be greater than 0; received %d\n' % args.num_containers) sys.exit(1) # Generate contents of yaml config yaml_config = k8s_generate_yaml_lib.GenerateConfig( args.docker_image, args.num_containers, args.config_map, args.deployment, args.ssh_port, args.use_hostnet, args.use_shared_volume) print(yaml_config) # pylint: disable=superfluous-parens if __name__ == '__main__': main()
from collections import MutableMapping class OrderedDict(MutableMapping): '''OrderedDict is a mapping object that allows for ordered access and insertion of keys. With the exception of the key_index, insert, and reorder_keys methods behavior is identical to stock dictionary objects.''' def __init__(self, items=None): '''OrderedDict accepts an optional iterable of two-tuples indicating keys and values.''' self._d = dict() self._keys = [] if items is None: return for key, value in items: self[key] = value def __len__(self): return len(self._d) def __iter__(self): for key in self._keys: yield key def __setitem__(self, key, value): if key not in self._keys: self._keys.append(key) self._d[key] = value def __getitem__(self, key): return self._d[key] def __delitem__(self, key): self._keys.remove(key) del self._d[key] def key_index(self, key): '''Accepts a parameter, :key:, and returns an integer value representing its index in the ordered list of keys.''' return self._keys.index(key) def insert(self, key, value, index): '''Accepts a :key:, :value:, and :index: parameter and inserts a new key, value member at the desired index. Note: Inserting with a negative index will have the following behavior: >>> l = [1, 2, 3, 4] >>> l.insert(-1, 5) >>> l [1, 2, 3, 5, 4] ''' if key in self._keys: self._keys.remove(key) self._keys.insert(index, key) self._d[key] = value def reorder_keys(self, keys): '''Accepts a :keys: parameter, an iterable of keys in the desired new order. The :keys: parameter must contain all existing keys.''' if len(keys) != len(self._keys): raise ValueError('The supplied number of keys does not match.') if set(keys) != set(self._d.keys()): raise ValueError('The supplied keys do not match the current set of keys.') self._keys = keys def __repr__(self): return str([(key, self[key]) for key in self]) def __eq__(self, other): if not isinstance(other, OrderedDict): return False return self.items() == other.items() def keys(self): """Return a copy of the _keys list instead of iterating over it as the MutableMapping does by default. """ return list(self._keys)
import os from django.contrib.auth import validators from django.contrib.auth.models import User from django.contrib.auth.password_validation import ( CommonPasswordValidator, MinimumLengthValidator, NumericPasswordValidator, UserAttributeSimilarityValidator, get_default_password_validators, get_password_validators, password_changed, password_validators_help_text_html, password_validators_help_texts, validate_password, ) from django.core.exceptions import ValidationError from django.db import models from django.test import TestCase, override_settings from django.test.utils import isolate_apps @override_settings(AUTH_PASSWORD_VALIDATORS=[ {'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator'}, {'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator', 'OPTIONS': { 'min_length': 12, }}, ]) class PasswordValidationTest(TestCase): def test_get_default_password_validators(self): validators = get_default_password_validators() self.assertEqual(len(validators), 2) self.assertEqual(validators[0].__class__.__name__, 'CommonPasswordValidator') self.assertEqual(validators[1].__class__.__name__, 'MinimumLengthValidator') self.assertEqual(validators[1].min_length, 12) def test_get_password_validators_custom(self): validator_config = [{'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator'}] validators = get_password_validators(validator_config) self.assertEqual(len(validators), 1) self.assertEqual(validators[0].__class__.__name__, 'CommonPasswordValidator') self.assertEqual(get_password_validators([]), []) def test_validate_password(self): self.assertIsNone(validate_password('sufficiently-long')) msg_too_short = 'This password is too short. It must contain at least 12 characters.' with self.assertRaises(ValidationError) as cm: validate_password('django4242') self.assertEqual(cm.exception.messages, [msg_too_short]) self.assertEqual(cm.exception.error_list[0].code, 'password_too_short') with self.assertRaises(ValidationError) as cm: validate_password('password') self.assertEqual(cm.exception.messages, ['This password is too common.', msg_too_short]) self.assertEqual(cm.exception.error_list[0].code, 'password_too_common') self.assertIsNone(validate_password('password', password_validators=[])) def test_password_changed(self): self.assertIsNone(password_changed('password')) def test_password_validators_help_texts(self): help_texts = password_validators_help_texts() self.assertEqual(len(help_texts), 2) self.assertIn('12 characters', help_texts[1]) self.assertEqual(password_validators_help_texts(password_validators=[]), []) def test_password_validators_help_text_html(self): help_text = password_validators_help_text_html() self.assertEqual(help_text.count('<li>'), 2) self.assertIn('12 characters', help_text) @override_settings(AUTH_PASSWORD_VALIDATORS=[]) def test_empty_password_validator_help_text_html(self): self.assertEqual(password_validators_help_text_html(), '') class MinimumLengthValidatorTest(TestCase): def test_validate(self): expected_error = "This password is too short. It must contain at least %d characters." self.assertIsNone(MinimumLengthValidator().validate('12345678')) self.assertIsNone(MinimumLengthValidator(min_length=3).validate('123')) with self.assertRaises(ValidationError) as cm: MinimumLengthValidator().validate('1234567') self.assertEqual(cm.exception.messages, [expected_error % 8]) self.assertEqual(cm.exception.error_list[0].code, 'password_too_short') with self.assertRaises(ValidationError) as cm: MinimumLengthValidator(min_length=3).validate('12') self.assertEqual(cm.exception.messages, [expected_error % 3]) def test_help_text(self): self.assertEqual( MinimumLengthValidator().get_help_text(), "Your password must contain at least 8 characters." ) class UserAttributeSimilarityValidatorTest(TestCase): def test_validate(self): user = User.objects.create_user( username='testclient', password='password', email='testclient@example.com', first_name='Test', last_name='Client', ) expected_error = "The password is too similar to the %s." self.assertIsNone(UserAttributeSimilarityValidator().validate('testclient')) with self.assertRaises(ValidationError) as cm: UserAttributeSimilarityValidator().validate('testclient', user=user), self.assertEqual(cm.exception.messages, [expected_error % "username"]) self.assertEqual(cm.exception.error_list[0].code, 'password_too_similar') with self.assertRaises(ValidationError) as cm: UserAttributeSimilarityValidator().validate('example.com', user=user), self.assertEqual(cm.exception.messages, [expected_error % "email address"]) with self.assertRaises(ValidationError) as cm: UserAttributeSimilarityValidator( user_attributes=['first_name'], max_similarity=0.3, ).validate('testclient', user=user) self.assertEqual(cm.exception.messages, [expected_error % "first name"]) # max_similarity=1 doesn't allow passwords that are identical to the # attribute's value. with self.assertRaises(ValidationError) as cm: UserAttributeSimilarityValidator( user_attributes=['first_name'], max_similarity=1, ).validate(user.first_name, user=user) self.assertEqual(cm.exception.messages, [expected_error % "first name"]) # max_similarity=0 rejects all passwords. with self.assertRaises(ValidationError) as cm: UserAttributeSimilarityValidator( user_attributes=['first_name'], max_similarity=0, ).validate('XXX', user=user) self.assertEqual(cm.exception.messages, [expected_error % "first name"]) # Passes validation. self.assertIsNone( UserAttributeSimilarityValidator(user_attributes=['first_name']).validate('testclient', user=user) ) @isolate_apps('auth_tests') def test_validate_property(self): class TestUser(models.Model): pass @property def username(self): return 'foobar' with self.assertRaises(ValidationError) as cm: UserAttributeSimilarityValidator().validate('foobar', user=TestUser()), self.assertEqual(cm.exception.messages, ['The password is too similar to the username.']) def test_help_text(self): self.assertEqual( UserAttributeSimilarityValidator().get_help_text(), "Your password can't be too similar to your other personal information." ) class CommonPasswordValidatorTest(TestCase): def test_validate(self): expected_error = "This password is too common." self.assertIsNone(CommonPasswordValidator().validate('a-safe-password')) with self.assertRaises(ValidationError) as cm: CommonPasswordValidator().validate('godzilla') self.assertEqual(cm.exception.messages, [expected_error]) def test_validate_custom_list(self): path = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'common-passwords-custom.txt') validator = CommonPasswordValidator(password_list_path=path) expected_error = "This password is too common." self.assertIsNone(validator.validate('a-safe-password')) with self.assertRaises(ValidationError) as cm: validator.validate('from-my-custom-list') self.assertEqual(cm.exception.messages, [expected_error]) self.assertEqual(cm.exception.error_list[0].code, 'password_too_common') def test_help_text(self): self.assertEqual( CommonPasswordValidator().get_help_text(), "Your password can't be a commonly used password." ) class NumericPasswordValidatorTest(TestCase): def test_validate(self): expected_error = "This password is entirely numeric." self.assertIsNone(NumericPasswordValidator().validate('a-safe-password')) with self.assertRaises(ValidationError) as cm: NumericPasswordValidator().validate('42424242') self.assertEqual(cm.exception.messages, [expected_error]) self.assertEqual(cm.exception.error_list[0].code, 'password_entirely_numeric') def test_help_text(self): self.assertEqual( NumericPasswordValidator().get_help_text(), "Your password can't be entirely numeric." ) class UsernameValidatorsTests(TestCase): def test_unicode_validator(self): valid_usernames = ['joe', 'Ren', '', ''] invalid_usernames = [ "o'connell", " ", "zerowidth\u200Bspace", "nonbreaking\u00A0space", "en\u2013dash", ] v = validators.UnicodeUsernameValidator() for valid in valid_usernames: with self.subTest(valid=valid): v(valid) for invalid in invalid_usernames: with self.subTest(invalid=invalid): with self.assertRaises(ValidationError): v(invalid) def test_ascii_validator(self): valid_usernames = ['glenn', 'GLEnN', 'jean-marc'] invalid_usernames = ["o'connell", 'ric', 'jean marc', ""] v = validators.ASCIIUsernameValidator() for valid in valid_usernames: with self.subTest(valid=valid): v(valid) for invalid in invalid_usernames: with self.subTest(invalid=invalid): with self.assertRaises(ValidationError): v(invalid)
# -*- encoding: utf-8 -*- # # Copyright  2013 Julien Danjou # # Author: Julien Danjou <julien@danjou.info> # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import abc import itertools from oslo.config import cfg from stevedore import dispatch from ceilometer.logger import logger from ceilometer.openstack.common import context from ceilometer.openstack.common import log from ceilometer import pipeline LOG = log.getLogger(__name__) class PollingTask(object): """Polling task for polling counters and inject into pipeline A polling task can be invoked periodically or only once""" def __init__(self, agent_manager): self.manager = agent_manager self.pollsters = set() self.publish_context = pipeline.PublishContext( agent_manager.context, cfg.CONF.counter_source) def add(self, pollster, pipelines): self.publish_context.add_pipelines(pipelines) self.pollsters.update([pollster]) @abc.abstractmethod def poll_and_publish(self): """Polling counter and publish into pipeline.""" class AgentManager(object): def __init__(self, extension_manager): logger.debug("AGENT MANAGER") publisher_manager = dispatch.NameDispatchExtensionManager( namespace=pipeline.PUBLISHER_NAMESPACE, check_func=lambda x: True, invoke_on_load=True, ) self.pipeline_manager = pipeline.setup_pipeline(publisher_manager) self.pollster_manager = extension_manager self.context = context.RequestContext('admin', 'admin', is_admin=True) logger.debug("pipeline manager:{}".format(self.pipeline_manager)) logger.debug("pollster manager:{}".format(self.pollster_manager)) @abc.abstractmethod def create_polling_task(self): """Create an empty polling task.""" def setup_polling_tasks(self): polling_tasks = {} logger.debug("setup polling tasks") for pipeline, pollster in itertools.product( self.pipeline_manager.pipelines, self.pollster_manager.extensions): logger.debug("pipeline:{},pollster:{}".format(pipeline,pollster)) for counter in pollster.obj.get_counter_names(): logger.debug("counter:{}".format(counter)) if pipeline.support_counter(counter): logger.debug("pipeline support counter:{}".format(counter)) polling_task = polling_tasks.get(pipeline.interval, None) if not polling_task: polling_task = self.create_polling_task() polling_tasks[pipeline.interval] = polling_task polling_task.add(pollster, [pipeline]) break logger.debug("POLLINGTASK:{}".format([p.name for p in polling_task.pollsters])) return polling_tasks def initialize_service_hook(self, service): logger.debug("service:{}".format(service)) self.service = service for interval, task in self.setup_polling_tasks().iteritems(): logger.debug("intervak:{} task:{}".format(interval,task)) #openstack.common.threadgroup.py self.service.tg.add_timer(interval, self.interval_task, task=task) def interval_task(self, task): task.poll_and_publish()
from __future__ import unicode_literals from django import forms, http from django.conf import settings from django.db import models from django.test import TestCase from django.template.response import TemplateResponse from django.utils.importlib import import_module from django.contrib.auth.models import User from django.contrib.formtools.wizard.views import (WizardView, SessionWizardView, CookieWizardView) class DummyRequest(http.HttpRequest): def __init__(self, POST=None): super(DummyRequest, self).__init__() self.method = POST and "POST" or "GET" if POST is not None: self.POST.update(POST) self.session = {} self._dont_enforce_csrf_checks = True def get_request(*args, **kwargs): request = DummyRequest(*args, **kwargs) engine = import_module(settings.SESSION_ENGINE) request.session = engine.SessionStore(None) return request class Step1(forms.Form): name = forms.CharField() class Step2(forms.Form): name = forms.CharField() class Step3(forms.Form): data = forms.CharField() class CustomKwargsStep1(Step1): def __init__(self, test=None, *args, **kwargs): self.test = test return super(CustomKwargsStep1, self).__init__(*args, **kwargs) class TestModel(models.Model): name = models.CharField(max_length=100) class Meta: app_label = 'formtools' class TestModelForm(forms.ModelForm): class Meta: model = TestModel TestModelFormSet = forms.models.modelformset_factory(TestModel, form=TestModelForm, extra=2) class TestWizard(WizardView): storage_name = 'django.contrib.formtools.wizard.storage.session.SessionStorage' def dispatch(self, request, *args, **kwargs): response = super(TestWizard, self).dispatch(request, *args, **kwargs) return response, self def get_form_kwargs(self, step, *args, **kwargs): kwargs = super(TestWizard, self).get_form_kwargs(step, *args, **kwargs) if step == 'kwargs_test': kwargs['test'] = True return kwargs class FormTests(TestCase): def test_form_init(self): testform = TestWizard.get_initkwargs([Step1, Step2]) self.assertEqual(testform['form_list'], {'0': Step1, '1': Step2}) testform = TestWizard.get_initkwargs([('start', Step1), ('step2', Step2)]) self.assertEqual( testform['form_list'], {'start': Step1, 'step2': Step2}) testform = TestWizard.get_initkwargs([Step1, Step2, ('finish', Step3)]) self.assertEqual( testform['form_list'], {'0': Step1, '1': Step2, 'finish': Step3}) def test_first_step(self): request = get_request() testform = TestWizard.as_view([Step1, Step2]) response, instance = testform(request) self.assertEqual(instance.steps.current, '0') testform = TestWizard.as_view([('start', Step1), ('step2', Step2)]) response, instance = testform(request) self.assertEqual(instance.steps.current, 'start') def test_persistence(self): testform = TestWizard.as_view([('start', Step1), ('step2', Step2)]) request = get_request({'test_wizard-current_step': 'start', 'name': 'data1'}) response, instance = testform(request) self.assertEqual(instance.steps.current, 'start') instance.storage.current_step = 'step2' testform2 = TestWizard.as_view([('start', Step1), ('step2', Step2)]) request.POST = {'test_wizard-current_step': 'step2'} response, instance = testform2(request) self.assertEqual(instance.steps.current, 'step2') def test_form_condition(self): request = get_request() testform = TestWizard.as_view( [('start', Step1), ('step2', Step2), ('step3', Step3)], condition_dict={'step2': True}) response, instance = testform(request) self.assertEqual(instance.get_next_step(), 'step2') testform = TestWizard.as_view( [('start', Step1), ('step2', Step2), ('step3', Step3)], condition_dict={'step2': False}) response, instance = testform(request) self.assertEqual(instance.get_next_step(), 'step3') def test_form_kwargs(self): request = get_request() testform = TestWizard.as_view([('start', Step1), ('kwargs_test', CustomKwargsStep1)]) response, instance = testform(request) self.assertEqual(instance.get_form_kwargs('start'), {}) self.assertEqual(instance.get_form_kwargs('kwargs_test'), {'test': True}) self.assertEqual(instance.get_form('kwargs_test').test, True) def test_form_prefix(self): request = get_request() testform = TestWizard.as_view([('start', Step1), ('step2', Step2)]) response, instance = testform(request) self.assertEqual(instance.get_form_prefix(), 'start') self.assertEqual(instance.get_form_prefix('another'), 'another') def test_form_initial(self): request = get_request() testform = TestWizard.as_view([('start', Step1), ('step2', Step2)], initial_dict={'start': {'name': 'value1'}}) response, instance = testform(request) self.assertEqual(instance.get_form_initial('start'), {'name': 'value1'}) self.assertEqual(instance.get_form_initial('step2'), {}) def test_form_instance(self): request = get_request() the_instance = TestModel() testform = TestWizard.as_view([('start', TestModelForm), ('step2', Step2)], instance_dict={'start': the_instance}) response, instance = testform(request) self.assertEqual( instance.get_form_instance('start'), the_instance) self.assertEqual( instance.get_form_instance('non_exist_instance'), None) def test_formset_instance(self): request = get_request() the_instance1, created = TestModel.objects.get_or_create( name='test object 1') the_instance2, created = TestModel.objects.get_or_create( name='test object 2') testform = TestWizard.as_view([('start', TestModelFormSet), ('step2', Step2)], instance_dict={'start': TestModel.objects.filter(name='test object 1')}) response, instance = testform(request) self.assertEqual(list(instance.get_form_instance('start')), [the_instance1]) self.assertEqual(instance.get_form_instance('non_exist_instance'), None) self.assertEqual(instance.get_form().initial_form_count(), 1) def test_done(self): request = get_request() testform = TestWizard.as_view([('start', Step1), ('step2', Step2)]) response, instance = testform(request) self.assertRaises(NotImplementedError, instance.done, None) def test_revalidation(self): request = get_request() testform = TestWizard.as_view([('start', Step1), ('step2', Step2)]) response, instance = testform(request) instance.render_done(None) self.assertEqual(instance.storage.current_step, 'start') class SessionFormTests(TestCase): def test_init(self): request = get_request() testform = SessionWizardView.as_view([('start', Step1)]) self.assertTrue(isinstance(testform(request), TemplateResponse)) class CookieFormTests(TestCase): def test_init(self): request = get_request() testform = CookieWizardView.as_view([('start', Step1)]) self.assertTrue(isinstance(testform(request), TemplateResponse))
# Copyright (c) 2011 Advanced Micro Devices, Inc. # All rights reserved. # # Redistribution and use in source and binary forms, with or without # modification, are permitted provided that the following conditions are # met: redistributions of source code must retain the above copyright # notice, this list of conditions and the following disclaimer; # redistributions in binary form must reproduce the above copyright # notice, this list of conditions and the following disclaimer in the # documentation and/or other materials provided with the distribution; # neither the name of the copyright holders nor the names of its # contributors may be used to endorse or promote products derived from # this software without specific prior written permission. # # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS # "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT # LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR # A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT # OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, # SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT # LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, # DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY # THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT # (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE # OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. # # Authors: Steve Reinhardt # Brad Beckmann from m5.params import * from ClockedObject import ClockedObject class BasicRouter(ClockedObject): type = 'BasicRouter' cxx_header = "mem/ruby/network/BasicRouter.hh" router_id = Param.Int("ID in relation to other routers")
from django.core.management.base import BaseCommand, CommandError from django.conf import settings from django.db import connection from build.management.commands.base_build import Command as BaseBuild from protein.models import (Protein, ProteinConformation, ProteinSequenceType, ProteinSegment, ProteinConformationTemplateStructure) from structure.models import Structure from common.alignment import Alignment from cProfile import Profile class Command(BaseBuild): help = 'Goes though all protein records and finds the best structural templates for the whole 7TM domain, and ' \ + 'each helix individually' def add_arguments(self, parser): parser.add_argument('-p', '--proc', type=int, action='store', dest='proc', default=1, help='Number of processes to run') parser.add_argument('-r', '--profile', action='store_true', dest='profile', default=False, help='Profile the script with cProfile') # segments segments = ProteinSegment.objects.filter(slug__in=settings.REFERENCE_POSITIONS) # fetch representative (inactive) structures FIXME add active structure?? structures = Structure.objects.filter(representative=True, protein_conformation__state__slug=settings.DEFAULT_PROTEIN_STATE).prefetch_related( 'protein_conformation__protein__parent__family') # fetch all protein conformations pconfs = ProteinConformation.objects.all().prefetch_related('protein__family', 'template_structure') def handle(self, *args, **options): # run with profiling if options['profile']: profiler = Profile() profiler.runcall(self._handle, *args, **options) profiler.print_stats() # run without profiling else: self._handle(*args, **options) def _handle(self, *args, **options): try: self.logger.info('ASSIGNING STRUCTURE TEMPLATES FOR PROTEINS') self.prepare_input(options['proc'], self.pconfs) self.logger.info('COMPLETED ASSIGNING STRUCTURE TEMPLATES FOR PROTEINS') except Exception as msg: print(msg) self.logger.error(msg) def main_func(self, positions, iteration): # pconfs if not positions[1]: pconfs = self.pconfs[positions[0]:] else: pconfs = self.pconfs[positions[0]:positions[1]] # find templates for pconf in pconfs: # filter structure sequence queryset to include only sequences from within the same class pconf_class = pconf.protein.family.slug[:3] class_sps = self.structures.filter( protein_conformation__protein__parent__family__slug__startswith=pconf_class) sps = [] sps_str = [] if class_sps.exists(): for structure in class_sps: sps.append(structure.protein_conformation.protein.parent) # use the wild-type sequence for main tpl sps_str.append(structure.protein_conformation.protein) # use the structure sequence for segment tpl else: for structure in self.structures: sps.append(structure.protein_conformation.protein.parent) sps_str.append(structure.protein_conformation.protein) # overall template = self.find_segment_template(pconf, sps, self.segments) template_structure = self.fetch_template_structure(self.structures, template.protein.entry_name) pconf.template_structure = template_structure pconf.save() self.logger.info("Assigned {} as overall template for {}".format(template_structure, pconf)) # for each segment for segment in self.segments: template = self.find_segment_template(pconf, sps_str, [segment]) template_structure = self.fetch_template_structure(self.structures, template.protein.parent.entry_name) pcts, created = ProteinConformationTemplateStructure.objects.get_or_create(protein_conformation=pconf, protein_segment=segment, defaults={'structure': template_structure}) if pcts.structure != template_structure: pcts.structure = template_structure pcts.save() self.logger.info("Assigned {} as {} template for {}".format(template_structure, segment, pconf)) def find_segment_template(self, pconf, sconfs, segments): a = Alignment() a.load_reference_protein(pconf.protein) a.load_proteins(sconfs) a.load_segments(segments) a.build_alignment() a.calculate_similarity() return a.proteins[1] def fetch_template_structure(self, structures, template_protein): for structure in structures: structure_protein = structure.protein_conformation.protein.parent.entry_name if structure_protein == template_protein: return structure
#!/usr/bin/python # -*- coding: utf-8 -*- # (c) 2016, Loic Blot <loic.blot@unix-experience.fr> # Sponsored by Infopro Digital. http://www.infopro-digital.com/ # Sponsored by E.T.A.I. http://www.etai.fr/ # # GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt) from __future__ import absolute_import, division, print_function __metaclass__ = type ANSIBLE_METADATA = {'metadata_version': '1.1', 'status': ['preview'], 'supported_by': 'community'} DOCUMENTATION = r''' --- module: mongodb_parameter short_description: Change an administrative parameter on a MongoDB server description: - Change an administrative parameter on a MongoDB server. version_added: "2.1" options: login_user: description: - The MongoDB username used to authenticate with. type: str login_password: description: - The login user's password used to authenticate with. type: str login_host: description: - The host running the database. type: str default: localhost login_port: description: - The MongoDB port to connect to. default: 27017 type: int login_database: description: - The database where login credentials are stored. type: str replica_set: description: - Replica set to connect to (automatically connects to primary for writes). type: str ssl: description: - Whether to use an SSL connection when connecting to the database. type: bool default: no param: description: - MongoDB administrative parameter to modify. type: str required: true value: description: - MongoDB administrative parameter value to set. type: str required: true param_type: description: - Define the type of parameter value. default: str type: str choices: [int, str] notes: - Requires the pymongo Python package on the remote host, version 2.4.2+. - This can be installed using pip or the OS package manager. - See also U(http://api.mongodb.org/python/current/installation.html) requirements: [ "pymongo" ] author: "Loic Blot (@nerzhul)" ''' EXAMPLES = r''' - name: Set MongoDB syncdelay to 60 (this is an int) mongodb_parameter: param: syncdelay value: 60 param_type: int ''' RETURN = r''' before: description: value before modification returned: success type: str after: description: value after modification returned: success type: str ''' import os import traceback try: from pymongo.errors import ConnectionFailure from pymongo.errors import OperationFailure from pymongo import version as PyMongoVersion from pymongo import MongoClient except ImportError: try: # for older PyMongo 2.2 from pymongo import Connection as MongoClient except ImportError: pymongo_found = False else: pymongo_found = True else: pymongo_found = True from ansible.module_utils.basic import AnsibleModule, missing_required_lib from ansible.module_utils.six.moves import configparser from ansible.module_utils._text import to_native # ========================================= # MongoDB module specific support methods. # def load_mongocnf(): config = configparser.RawConfigParser() mongocnf = os.path.expanduser('~/.mongodb.cnf') try: config.readfp(open(mongocnf)) creds = dict( user=config.get('client', 'user'), password=config.get('client', 'pass') ) except (configparser.NoOptionError, IOError): return False return creds # ========================================= # Module execution. # def main(): module = AnsibleModule( argument_spec=dict( login_user=dict(default=None), login_password=dict(default=None, no_log=True), login_host=dict(default='localhost'), login_port=dict(default=27017, type='int'), login_database=dict(default=None), replica_set=dict(default=None), param=dict(required=True), value=dict(required=True), param_type=dict(default="str", choices=['str', 'int']), ssl=dict(default=False, type='bool'), ) ) if not pymongo_found: module.fail_json(msg=missing_required_lib('pymongo')) login_user = module.params['login_user'] login_password = module.params['login_password'] login_host = module.params['login_host'] login_port = module.params['login_port'] login_database = module.params['login_database'] replica_set = module.params['replica_set'] ssl = module.params['ssl'] param = module.params['param'] param_type = module.params['param_type'] value = module.params['value'] # Verify parameter is coherent with specified type try: if param_type == 'int': value = int(value) except ValueError: module.fail_json(msg="value '%s' is not %s" % (value, param_type)) try: if replica_set: client = MongoClient(login_host, int(login_port), replicaset=replica_set, ssl=ssl) else: client = MongoClient(login_host, int(login_port), ssl=ssl) if login_user is None and login_password is None: mongocnf_creds = load_mongocnf() if mongocnf_creds is not False: login_user = mongocnf_creds['user'] login_password = mongocnf_creds['password'] elif login_password is None or login_user is None: module.fail_json(msg='when supplying login arguments, both login_user and login_password must be provided') if login_user is not None and login_password is not None: client.admin.authenticate(login_user, login_password, source=login_database) except ConnectionFailure as e: module.fail_json(msg='unable to connect to database: %s' % to_native(e), exception=traceback.format_exc()) db = client.admin try: after_value = db.command("setParameter", **{param: value}) except OperationFailure as e: module.fail_json(msg="unable to change parameter: %s" % to_native(e), exception=traceback.format_exc()) if "was" not in after_value: module.exit_json(changed=True, msg="Unable to determine old value, assume it changed.") else: module.exit_json(changed=(value != after_value["was"]), before=after_value["was"], after=value) if __name__ == '__main__': main()
"""TestCases for multi-threaded access to a DB. """ import os import sys import time import errno import shutil import tempfile from pprint import pprint from whrandom import random try: True, False except NameError: True = 1 False = 0 DASH = '-' try: from threading import Thread, currentThread have_threads = True except ImportError: have_threads = False import unittest from test_all import verbose try: # For Python 2.3 from bsddb import db, dbutils except ImportError: # For earlier Pythons w/distutils pybsddb from bsddb3 import db, dbutils #---------------------------------------------------------------------- class BaseThreadedTestCase(unittest.TestCase): dbtype = db.DB_UNKNOWN # must be set in derived class dbopenflags = 0 dbsetflags = 0 envflags = 0 def setUp(self): if verbose: dbutils._deadlock_VerboseFile = sys.stdout homeDir = os.path.join(os.path.dirname(sys.argv[0]), 'db_home') self.homeDir = homeDir try: os.mkdir(homeDir) except OSError, e: if e.errno <> errno.EEXIST: raise self.env = db.DBEnv() self.setEnvOpts() self.env.open(homeDir, self.envflags | db.DB_CREATE) self.filename = self.__class__.__name__ + '.db' self.d = db.DB(self.env) if self.dbsetflags: self.d.set_flags(self.dbsetflags) self.d.open(self.filename, self.dbtype, self.dbopenflags|db.DB_CREATE) def tearDown(self): self.d.close() self.env.close() shutil.rmtree(self.homeDir) def setEnvOpts(self): pass def makeData(self, key): return DASH.join([key] * 5) #---------------------------------------------------------------------- class ConcurrentDataStoreBase(BaseThreadedTestCase): dbopenflags = db.DB_THREAD envflags = db.DB_THREAD | db.DB_INIT_CDB | db.DB_INIT_MPOOL readers = 0 # derived class should set writers = 0 records = 1000 def test01_1WriterMultiReaders(self): if verbose: print '\n', '-=' * 30 print "Running %s.test01_1WriterMultiReaders..." % \ self.__class__.__name__ threads = [] for x in range(self.writers): wt = Thread(target = self.writerThread, args = (self.d, self.records, x), name = 'writer %d' % x, )#verbose = verbose) threads.append(wt) for x in range(self.readers): rt = Thread(target = self.readerThread, args = (self.d, x), name = 'reader %d' % x, )#verbose = verbose) threads.append(rt) for t in threads: t.start() for t in threads: t.join() def writerThread(self, d, howMany, writerNum): #time.sleep(0.01 * writerNum + 0.01) name = currentThread().getName() start = howMany * writerNum stop = howMany * (writerNum + 1) - 1 if verbose: print "%s: creating records %d - %d" % (name, start, stop) for x in range(start, stop): key = '%04d' % x dbutils.DeadlockWrap(d.put, key, self.makeData(key), max_retries=12) if verbose and x % 100 == 0: print "%s: records %d - %d finished" % (name, start, x) if verbose: print "%s: finished creating records" % name ## # Each write-cursor will be exclusive, the only one that can update the DB... ## if verbose: print "%s: deleting a few records" % name ## c = d.cursor(flags = db.DB_WRITECURSOR) ## for x in range(10): ## key = int(random() * howMany) + start ## key = '%04d' % key ## if d.has_key(key): ## c.set(key) ## c.delete() ## c.close() if verbose: print "%s: thread finished" % name def readerThread(self, d, readerNum): time.sleep(0.01 * readerNum) name = currentThread().getName() for loop in range(5): c = d.cursor() count = 0 rec = c.first() while rec: count += 1 key, data = rec self.assertEqual(self.makeData(key), data) rec = c.next() if verbose: print "%s: found %d records" % (name, count) c.close() time.sleep(0.05) if verbose: print "%s: thread finished" % name class BTreeConcurrentDataStore(ConcurrentDataStoreBase): dbtype = db.DB_BTREE writers = 2 readers = 10 records = 1000 class HashConcurrentDataStore(ConcurrentDataStoreBase): dbtype = db.DB_HASH writers = 2 readers = 10 records = 1000 #---------------------------------------------------------------------- class SimpleThreadedBase(BaseThreadedTestCase): dbopenflags = db.DB_THREAD envflags = db.DB_THREAD | db.DB_INIT_MPOOL | db.DB_INIT_LOCK readers = 5 writers = 3 records = 1000 def setEnvOpts(self): self.env.set_lk_detect(db.DB_LOCK_DEFAULT) def test02_SimpleLocks(self): if verbose: print '\n', '-=' * 30 print "Running %s.test02_SimpleLocks..." % self.__class__.__name__ threads = [] for x in range(self.writers): wt = Thread(target = self.writerThread, args = (self.d, self.records, x), name = 'writer %d' % x, )#verbose = verbose) threads.append(wt) for x in range(self.readers): rt = Thread(target = self.readerThread, args = (self.d, x), name = 'reader %d' % x, )#verbose = verbose) threads.append(rt) for t in threads: t.start() for t in threads: t.join() def writerThread(self, d, howMany, writerNum): name = currentThread().getName() start = howMany * writerNum stop = howMany * (writerNum + 1) - 1 if verbose: print "%s: creating records %d - %d" % (name, start, stop) # create a bunch of records for x in xrange(start, stop): key = '%04d' % x dbutils.DeadlockWrap(d.put, key, self.makeData(key), max_retries=12) if verbose and x % 100 == 0: print "%s: records %d - %d finished" % (name, start, x) # do a bit or reading too if random() <= 0.05: for y in xrange(start, x): key = '%04d' % x data = dbutils.DeadlockWrap(d.get, key, max_retries=12) self.assertEqual(data, self.makeData(key)) # flush them try: dbutils.DeadlockWrap(d.sync, max_retries=12) except db.DBIncompleteError, val: if verbose: print "could not complete sync()..." # read them back, deleting a few for x in xrange(start, stop): key = '%04d' % x data = dbutils.DeadlockWrap(d.get, key, max_retries=12) if verbose and x % 100 == 0: print "%s: fetched record (%s, %s)" % (name, key, data) self.assertEqual(data, self.makeData(key)) if random() <= 0.10: dbutils.DeadlockWrap(d.delete, key, max_retries=12) if verbose: print "%s: deleted record %s" % (name, key) if verbose: print "%s: thread finished" % name def readerThread(self, d, readerNum): time.sleep(0.01 * readerNum) name = currentThread().getName() for loop in range(5): c = d.cursor() count = 0 rec = dbutils.DeadlockWrap(c.first, max_retries=10) while rec: count += 1 key, data = rec self.assertEqual(self.makeData(key), data) rec = dbutils.DeadlockWrap(c.next, max_retries=10) if verbose: print "%s: found %d records" % (name, count) c.close() time.sleep(0.05) if verbose: print "%s: thread finished" % name class BTreeSimpleThreaded(SimpleThreadedBase): dbtype = db.DB_BTREE class HashSimpleThreaded(SimpleThreadedBase): dbtype = db.DB_HASH #---------------------------------------------------------------------- class ThreadedTransactionsBase(BaseThreadedTestCase): dbopenflags = db.DB_THREAD | db.DB_AUTO_COMMIT envflags = (db.DB_THREAD | db.DB_INIT_MPOOL | db.DB_INIT_LOCK | db.DB_INIT_LOG | db.DB_INIT_TXN ) readers = 0 writers = 0 records = 2000 txnFlag = 0 def setEnvOpts(self): #self.env.set_lk_detect(db.DB_LOCK_DEFAULT) pass def test03_ThreadedTransactions(self): if verbose: print '\n', '-=' * 30 print "Running %s.test03_ThreadedTransactions..." % \ self.__class__.__name__ threads = [] for x in range(self.writers): wt = Thread(target = self.writerThread, args = (self.d, self.records, x), name = 'writer %d' % x, )#verbose = verbose) threads.append(wt) for x in range(self.readers): rt = Thread(target = self.readerThread, args = (self.d, x), name = 'reader %d' % x, )#verbose = verbose) threads.append(rt) dt = Thread(target = self.deadlockThread) dt.start() for t in threads: t.start() for t in threads: t.join() self.doLockDetect = False dt.join() def doWrite(self, d, name, start, stop): finished = False while not finished: try: txn = self.env.txn_begin(None, self.txnFlag) for x in range(start, stop): key = '%04d' % x d.put(key, self.makeData(key), txn) if verbose and x % 100 == 0: print "%s: records %d - %d finished" % (name, start, x) txn.commit() finished = True except (db.DBLockDeadlockError, db.DBLockNotGrantedError), val: if verbose: print "%s: Aborting transaction (%s)" % (name, val[1]) txn.abort() time.sleep(0.05) def writerThread(self, d, howMany, writerNum): name = currentThread().getName() start = howMany * writerNum stop = howMany * (writerNum + 1) - 1 if verbose: print "%s: creating records %d - %d" % (name, start, stop) step = 100 for x in range(start, stop, step): self.doWrite(d, name, x, min(stop, x+step)) if verbose: print "%s: finished creating records" % name if verbose: print "%s: deleting a few records" % name finished = False while not finished: try: recs = [] txn = self.env.txn_begin(None, self.txnFlag) for x in range(10): key = int(random() * howMany) + start key = '%04d' % key data = d.get(key, None, txn, db.DB_RMW) if data is not None: d.delete(key, txn) recs.append(key) txn.commit() finished = True if verbose: print "%s: deleted records %s" % (name, recs) except (db.DBLockDeadlockError, db.DBLockNotGrantedError), val: if verbose: print "%s: Aborting transaction (%s)" % (name, val[1]) txn.abort() time.sleep(0.05) if verbose: print "%s: thread finished" % name def readerThread(self, d, readerNum): time.sleep(0.01 * readerNum + 0.05) name = currentThread().getName() for loop in range(5): finished = False while not finished: try: txn = self.env.txn_begin(None, self.txnFlag) c = d.cursor(txn) count = 0 rec = c.first() while rec: count += 1 key, data = rec self.assertEqual(self.makeData(key), data) rec = c.next() if verbose: print "%s: found %d records" % (name, count) c.close() txn.commit() finished = True except (db.DBLockDeadlockError, db.DBLockNotGrantedError), val: if verbose: print "%s: Aborting transaction (%s)" % (name, val[1]) c.close() txn.abort() time.sleep(0.05) time.sleep(0.05) if verbose: print "%s: thread finished" % name def deadlockThread(self): self.doLockDetect = True while self.doLockDetect: time.sleep(0.5) try: aborted = self.env.lock_detect( db.DB_LOCK_RANDOM, db.DB_LOCK_CONFLICT) if verbose and aborted: print "deadlock: Aborted %d deadlocked transaction(s)" \ % aborted except db.DBError: pass class BTreeThreadedTransactions(ThreadedTransactionsBase): dbtype = db.DB_BTREE writers = 3 readers = 5 records = 2000 class HashThreadedTransactions(ThreadedTransactionsBase): dbtype = db.DB_HASH writers = 1 readers = 5 records = 2000 class BTreeThreadedNoWaitTransactions(ThreadedTransactionsBase): dbtype = db.DB_BTREE writers = 3 readers = 5 records = 2000 txnFlag = db.DB_TXN_NOWAIT class HashThreadedNoWaitTransactions(ThreadedTransactionsBase): dbtype = db.DB_HASH writers = 1 readers = 5 records = 2000 txnFlag = db.DB_TXN_NOWAIT #---------------------------------------------------------------------- def test_suite(): suite = unittest.TestSuite() if have_threads: suite.addTest(unittest.makeSuite(BTreeConcurrentDataStore)) suite.addTest(unittest.makeSuite(HashConcurrentDataStore)) suite.addTest(unittest.makeSuite(BTreeSimpleThreaded)) suite.addTest(unittest.makeSuite(HashSimpleThreaded)) suite.addTest(unittest.makeSuite(BTreeThreadedTransactions)) suite.addTest(unittest.makeSuite(HashThreadedTransactions)) suite.addTest(unittest.makeSuite(BTreeThreadedNoWaitTransactions)) suite.addTest(unittest.makeSuite(HashThreadedNoWaitTransactions)) else: print "Threads not available, skipping thread tests." return suite if __name__ == '__main__': unittest.main(defaultTest='test_suite')
import sys from gunicorn import six PY26 = (sys.version_info[:2] == (2, 6)) PY33 = (sys.version_info >= (3, 3)) def _check_if_pyc(fname): """Return True if the extension is .pyc, False if .py and None if otherwise""" from imp import find_module from os.path import realpath, dirname, basename, splitext # Normalize the file-path for the find_module() filepath = realpath(fname) dirpath = dirname(filepath) module_name = splitext(basename(filepath))[0] # Validate and fetch try: fileobj, fullpath, (_, _, pytype) = find_module(module_name, [dirpath]) except ImportError: raise IOError("Cannot find config file. " "Path maybe incorrect! : {0}".format(filepath)) return pytype, fileobj, fullpath def _get_codeobj(pyfile): """ Returns the code object, given a python file """ from imp import PY_COMPILED, PY_SOURCE result, fileobj, fullpath = _check_if_pyc(pyfile) # WARNING: # fp.read() can blowup if the module is extremely large file. # Lookout for overflow errors. try: data = fileobj.read() finally: fileobj.close() # This is a .pyc file. Treat accordingly. if result is PY_COMPILED: # .pyc format is as follows: # 0 - 4 bytes: Magic number, which changes with each create of .pyc file. # First 2 bytes change with each marshal of .pyc file. Last 2 bytes is "\r\n". # 4 - 8 bytes: Datetime value, when the .py was last changed. # 8 - EOF: Marshalled code object data. # So to get code object, just read the 8th byte onwards till EOF, and # UN-marshal it. import marshal code_obj = marshal.loads(data[8:]) elif result is PY_SOURCE: # This is a .py file. code_obj = compile(data, fullpath, 'exec') else: # Unsupported extension raise Exception("Input file is unknown format: {0}".format(fullpath)) # Return code object return code_obj if six.PY3: def execfile_(fname, *args): if fname.endswith(".pyc"): code = _get_codeobj(fname) else: code = compile(open(fname, 'rb').read(), fname, 'exec') return six.exec_(code, *args) def bytes_to_str(b): if isinstance(b, six.text_type): return b return str(b, 'latin1') import urllib.parse def unquote_to_wsgi_str(string): return _unquote_to_bytes(string).decode('latin-1') _unquote_to_bytes = urllib.parse.unquote_to_bytes else: def execfile_(fname, *args): """ Overriding PY2 execfile() implementation to support .pyc files """ if fname.endswith(".pyc"): return six.exec_(_get_codeobj(fname), *args) return execfile(fname, *args) def bytes_to_str(s): if isinstance(s, unicode): return s.encode('utf-8') return s import urllib unquote_to_wsgi_str = urllib.unquote # The following code adapted from trollius.py33_exceptions def _wrap_error(exc, mapping, key): if key not in mapping: return new_err_cls = mapping[key] new_err = new_err_cls(*exc.args) # raise a new exception with the original traceback if hasattr(exc, '__traceback__'): traceback = exc.__traceback__ else: traceback = sys.exc_info()[2] six.reraise(new_err_cls, new_err, traceback) if PY33: import builtins BlockingIOError = builtins.BlockingIOError BrokenPipeError = builtins.BrokenPipeError ChildProcessError = builtins.ChildProcessError ConnectionRefusedError = builtins.ConnectionRefusedError ConnectionResetError = builtins.ConnectionResetError InterruptedError = builtins.InterruptedError ConnectionAbortedError = builtins.ConnectionAbortedError PermissionError = builtins.PermissionError FileNotFoundError = builtins.FileNotFoundError ProcessLookupError = builtins.ProcessLookupError def wrap_error(func, *args, **kw): return func(*args, **kw) else: import errno import select import socket class BlockingIOError(OSError): pass class BrokenPipeError(OSError): pass class ChildProcessError(OSError): pass class ConnectionRefusedError(OSError): pass class InterruptedError(OSError): pass class ConnectionResetError(OSError): pass class ConnectionAbortedError(OSError): pass class PermissionError(OSError): pass class FileNotFoundError(OSError): pass class ProcessLookupError(OSError): pass _MAP_ERRNO = { errno.EACCES: PermissionError, errno.EAGAIN: BlockingIOError, errno.EALREADY: BlockingIOError, errno.ECHILD: ChildProcessError, errno.ECONNABORTED: ConnectionAbortedError, errno.ECONNREFUSED: ConnectionRefusedError, errno.ECONNRESET: ConnectionResetError, errno.EINPROGRESS: BlockingIOError, errno.EINTR: InterruptedError, errno.ENOENT: FileNotFoundError, errno.EPERM: PermissionError, errno.EPIPE: BrokenPipeError, errno.ESHUTDOWN: BrokenPipeError, errno.EWOULDBLOCK: BlockingIOError, errno.ESRCH: ProcessLookupError, } def wrap_error(func, *args, **kw): """ Wrap socket.error, IOError, OSError, select.error to raise new specialized exceptions of Python 3.3 like InterruptedError (PEP 3151). """ try: return func(*args, **kw) except (socket.error, IOError, OSError) as exc: if hasattr(exc, 'winerror'): _wrap_error(exc, _MAP_ERRNO, exc.winerror) # _MAP_ERRNO does not contain all Windows errors. # For some errors like "file not found", exc.errno should # be used (ex: ENOENT). _wrap_error(exc, _MAP_ERRNO, exc.errno) raise except select.error as exc: if exc.args: _wrap_error(exc, _MAP_ERRNO, exc.args[0]) raise if PY26: from urlparse import ( _parse_cache, MAX_CACHE_SIZE, clear_cache, _splitnetloc, SplitResult, scheme_chars, ) def urlsplit(url, scheme='', allow_fragments=True): """Parse a URL into 5 components: <scheme>://<netloc>/<path>?<query>#<fragment> Return a 5-tuple: (scheme, netloc, path, query, fragment). Note that we don't break the components up in smaller bits (e.g. netloc is a single string) and we don't expand % escapes.""" allow_fragments = bool(allow_fragments) key = url, scheme, allow_fragments, type(url), type(scheme) cached = _parse_cache.get(key, None) if cached: return cached if len(_parse_cache) >= MAX_CACHE_SIZE: # avoid runaway growth clear_cache() netloc = query = fragment = '' i = url.find(':') if i > 0: if url[:i] == 'http': # optimize the common case scheme = url[:i].lower() url = url[i+1:] if url[:2] == '//': netloc, url = _splitnetloc(url, 2) if (('[' in netloc and ']' not in netloc) or (']' in netloc and '[' not in netloc)): raise ValueError("Invalid IPv6 URL") if allow_fragments and '#' in url: url, fragment = url.split('#', 1) if '?' in url: url, query = url.split('?', 1) v = SplitResult(scheme, netloc, url, query, fragment) _parse_cache[key] = v return v for c in url[:i]: if c not in scheme_chars: break else: # make sure "url" is not actually a port number (in which case # "scheme" is really part of the path) rest = url[i+1:] if not rest or any(c not in '0123456789' for c in rest): # not a port number scheme, url = url[:i].lower(), rest if url[:2] == '//': netloc, url = _splitnetloc(url, 2) if (('[' in netloc and ']' not in netloc) or (']' in netloc and '[' not in netloc)): raise ValueError("Invalid IPv6 URL") if allow_fragments and '#' in url: url, fragment = url.split('#', 1) if '?' in url: url, query = url.split('?', 1) v = SplitResult(scheme, netloc, url, query, fragment) _parse_cache[key] = v return v else: from gunicorn.six.moves.urllib.parse import urlsplit
# Licensed to the Apache Software Foundation (ASF) under one # or more contributor license agreements. See the NOTICE file # distributed with this work for additional information # regarding copyright ownership. The ASF licenses this file # to you under the Apache License, Version 2.0 (the # "License"); you may not use this file except in compliance # with the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, # software distributed under the License is distributed on an # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY # KIND, either express or implied. See the License for the # specific language governing permissions and limitations # under the License. import json import os import re import unittest from unittest import mock import pytest from google.auth import exceptions from google.auth.environment_vars import CREDENTIALS from airflow.providers.google.common.utils.id_token_credentials import ( IDTokenCredentialsAdapter, get_default_id_token_credentials, ) class TestIDTokenCredentialsAdapter(unittest.TestCase): def test_should_use_id_token_from_parent_credentials(self): parent_credentials = mock.MagicMock() type(parent_credentials).id_token = mock.PropertyMock(side_effect=["ID_TOKEN1", "ID_TOKEN2"]) creds = IDTokenCredentialsAdapter(credentials=parent_credentials) assert creds.token == "ID_TOKEN1" request_adapter = mock.MagicMock() creds.refresh(request_adapter) assert creds.token == "ID_TOKEN2" class TestGetDefaultIdTokenCredentials(unittest.TestCase): @mock.patch.dict("os.environ") @mock.patch( "google.auth._cloud_sdk.get_application_default_credentials_path", return_value="/tmp/INVALID_PATH.json", ) @mock.patch( "google.auth.compute_engine._metadata.ping", return_value=False, ) def test_should_raise_exception(self, mock_metadata_ping, mock_gcloud_sdk_path): if CREDENTIALS in os.environ: del os.environ[CREDENTIALS] with pytest.raises( exceptions.DefaultCredentialsError, match=re.escape( "Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS " "or explicitly create credentials and re-run the application. For more information, please " "see https://cloud.google.com/docs/authentication/getting-started" ), ): get_default_id_token_credentials(target_audience="example.org") @mock.patch.dict("os.environ") @mock.patch( "google.auth._cloud_sdk.get_application_default_credentials_path", return_value="/tmp/INVALID_PATH.json", ) @mock.patch( "google.auth.compute_engine._metadata.ping", return_value=True, ) @mock.patch( "google.auth.compute_engine.IDTokenCredentials", ) def test_should_support_metadata_credentials(self, credentials, mock_metadata_ping, mock_gcloud_sdk_path): if CREDENTIALS in os.environ: del os.environ[CREDENTIALS] assert credentials.return_value == get_default_id_token_credentials(target_audience="example.org") @mock.patch.dict("os.environ") @mock.patch( "airflow.providers.google.common.utils.id_token_credentials.open", mock.mock_open( read_data=json.dumps( { "client_id": "CLIENT_ID", "client_secret": "CLIENT_SECRET", "refresh_token": "REFRESH_TOKEN", "type": "authorized_user", } ) ), ) @mock.patch("google.auth._cloud_sdk.get_application_default_credentials_path", return_value=__file__) def test_should_support_user_credentials_from_gcloud(self, mock_gcloud_sdk_path): if CREDENTIALS in os.environ: del os.environ[CREDENTIALS] credentials = get_default_id_token_credentials(target_audience="example.org") assert isinstance(credentials, IDTokenCredentialsAdapter) assert credentials.credentials.client_secret == "CLIENT_SECRET" @mock.patch.dict("os.environ") @mock.patch( "airflow.providers.google.common.utils.id_token_credentials.open", mock.mock_open( read_data=json.dumps( { "type": "service_account", "project_id": "PROJECT_ID", "private_key_id": "PRIVATE_KEY_ID", "private_key": "PRIVATE_KEY", "client_email": "CLIENT_EMAIL", "client_id": "CLIENT_ID", "auth_uri": "https://accounts.google.com/o/oauth2/auth", "token_uri": "https://oauth2.googleapis.com/token", "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs", "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/CERT", } ) ), ) @mock.patch("google.auth._service_account_info.from_dict", return_value="SIGNER") @mock.patch("google.auth._cloud_sdk.get_application_default_credentials_path", return_value=__file__) def test_should_support_service_account_from_gcloud(self, mock_gcloud_sdk_path, mock_from_dict): if CREDENTIALS in os.environ: del os.environ[CREDENTIALS] credentials = get_default_id_token_credentials(target_audience="example.org") assert credentials.service_account_email == "CLIENT_EMAIL" @mock.patch.dict("os.environ") @mock.patch( "airflow.providers.google.common.utils.id_token_credentials.open", mock.mock_open( read_data=json.dumps( { "type": "service_account", "project_id": "PROJECT_ID", "private_key_id": "PRIVATE_KEY_ID", "private_key": "PRIVATE_KEY", "client_email": "CLIENT_EMAIL", "client_id": "CLIENT_ID", "auth_uri": "https://accounts.google.com/o/oauth2/auth", "token_uri": "https://oauth2.googleapis.com/token", "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs", "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/CERT", } ) ), ) @mock.patch("google.auth._service_account_info.from_dict", return_value="SIGNER") def test_should_support_service_account_from_env(self, mock_gcloud_sdk_path): os.environ[CREDENTIALS] = __file__ credentials = get_default_id_token_credentials(target_audience="example.org") assert credentials.service_account_email == "CLIENT_EMAIL"
# sqlalchemy/__init__.py # Copyright (C) 2005-2016 the SQLAlchemy authors and contributors # <see AUTHORS file> # # This module is part of SQLAlchemy and is released under # the MIT License: http://www.opensource.org/licenses/mit-license.php from .sql import ( alias, all_, and_, any_, asc, between, bindparam, case, cast, collate, column, delete, desc, distinct, except_, except_all, exists, extract, false, func, funcfilter, insert, intersect, intersect_all, join, lateral, literal, literal_column, modifier, not_, null, or_, outerjoin, outparam, over, select, subquery, table, tablesample, text, true, tuple_, type_coerce, union, union_all, update, within_group, ) from .types import ( ARRAY, BIGINT, BINARY, BLOB, BOOLEAN, BigInteger, Binary, Boolean, CHAR, CLOB, DATE, DATETIME, DECIMAL, Date, DateTime, Enum, FLOAT, Float, INT, INTEGER, Integer, Interval, JSON, LargeBinary, NCHAR, NVARCHAR, NUMERIC, Numeric, PickleType, REAL, SMALLINT, SmallInteger, String, TEXT, TIME, TIMESTAMP, Text, Time, TypeDecorator, Unicode, UnicodeText, VARBINARY, VARCHAR, ) from .schema import ( CheckConstraint, Column, ColumnDefault, Constraint, DefaultClause, FetchedValue, ForeignKey, ForeignKeyConstraint, Index, MetaData, PassiveDefault, PrimaryKeyConstraint, Sequence, Table, ThreadLocalMetaData, UniqueConstraint, DDL, BLANK_SCHEMA ) from .inspection import inspect from .engine import create_engine, engine_from_config __version__ = '1.1.4' def __go(lcls): global __all__ from . import events from . import util as _sa_util import inspect as _inspect __all__ = sorted(name for name, obj in lcls.items() if not (name.startswith('_') or _inspect.ismodule(obj))) _sa_util.dependencies.resolve_all("sqlalchemy") __go(locals())
import re from django.utils.text import compress_string from django.utils.cache import patch_vary_headers re_accepts_gzip = re.compile(r'\bgzip\b') class GZipMiddleware(object): """ This middleware compresses content if the browser allows gzip compression. It sets the Vary header accordingly, so that caches will base their storage on the Accept-Encoding header. """ def process_response(self, request, response): # It's not worth compressing non-OK or really short responses. if response.status_code != 200 or len(response.content) < 200: return response patch_vary_headers(response, ('Accept-Encoding',)) # Avoid gzipping if we've already got a content-encoding. if response.has_header('Content-Encoding'): return response # MSIE have issues with gzipped respones of various content types. if "msie" in request.META.get('HTTP_USER_AGENT', '').lower(): ctype = response.get('Content-Type', '').lower() if not ctype.startswith("text/") or "javascript" in ctype: return response ae = request.META.get('HTTP_ACCEPT_ENCODING', '') if not re_accepts_gzip.search(ae): return response response.content = compress_string(response.content) response['Content-Encoding'] = 'gzip' response['Content-Length'] = str(len(response.content)) return response
# coding=utf-8 # Copyright 2014 Pants project contributors (see CONTRIBUTORS.md). # Licensed under the Apache License, Version 2.0 (see LICENSE). from __future__ import (absolute_import, division, generators, nested_scopes, print_function, unicode_literals, with_statement) import collections import os from netrc import netrc as NetrcDb from netrc import NetrcParseError class Netrc(object): """Fetches username and password from ~/.netrc for logged in user.""" class NetrcError(Exception): """Raised to indicate Netrc errors""" def __init__(self, *args, **kwargs): super(Netrc.NetrcError, self).__init__(*args, **kwargs) def __init__(self): self._login = collections.defaultdict(lambda: None) self._password = collections.defaultdict(lambda: None) def getusername(self, repository): self._ensure_loaded() return self._login[repository] def getpassword(self, repository): self._ensure_loaded() return self._password[repository] def _ensure_loaded(self): if not self._login and not self._password: db = os.path.expanduser('~/.netrc') if not os.path.exists(db): raise self.NetrcError('A ~/.netrc file is required to authenticate') try: db = NetrcDb(db) for host, value in db.hosts.items(): auth = db.authenticators(host) if auth: login, _, password = auth self._login[host] = login self._password[host] = password if len(self._login) == 0: raise self.NetrcError('Found no usable authentication blocks in ~/.netrc') except NetrcParseError as e: raise self.NetrcError('Problem parsing ~/.netrc: {}'.format(e))
# -*- coding: utf-8 -*- ## # TRACK 7 # TOO BLUE # Brian Foo (brianfoo.com) # This file builds the sequence file for use with ChucK from the data supplied ## # Library dependancies import csv import json import math import os import pprint import time # Config BPM = 100 # Beats per minute, e.g. 60, 75, 100, 120, 150 DIVISIONS_PER_BEAT = 4 # e.g. 4 = quarter notes, 8 = eighth notes, etc VARIANCE_MS = 20 # +/- milliseconds an instrument note should be off by to give it a little more "natural" feel GAIN = 0.4 # base gain TEMPO = 1.0 # base tempo MS_PER_YEAR = 7200 # Files INSTRUMENTS_INPUT_FILE = 'data/instruments.csv' LAND_LOSS_INPUT_FILE = 'data/land_loss.json' SUMMARY_OUTPUT_FILE = 'data/report_summary.csv' SUMMARY_SEQUENCE_OUTPUT_FILE = 'data/report_sequence.csv' INSTRUMENTS_OUTPUT_FILE = 'data/ck_instruments.csv' SEQUENCE_OUTPUT_FILE = 'data/ck_sequence.csv' INSTRUMENTS_DIR = 'instruments/' # Output options WRITE_SEQUENCE = True WRITE_REPORT = True # Calculations BEAT_MS = round(60.0 / BPM * 1000) ROUND_TO_NEAREST = round(BEAT_MS / DIVISIONS_PER_BEAT) BEATS_PER_YEAR = round(MS_PER_YEAR / BEAT_MS) GROUPS_PER_YEAR = int(BEATS_PER_YEAR) GROUP_MS = MS_PER_YEAR / GROUPS_PER_YEAR # Init years = [] instruments = [] sequence = [] hindex = 0 # For creating pseudo-random numbers def halton(index, base): result = 0.0 f = 1.0 / base i = 1.0 * index while(i > 0): result += f * (i % base) i = math.floor(i / base) f = f / base return result # floor {n} to nearest {nearest} def floorToNearest(n, nearest): return 1.0 * math.floor(1.0*n/nearest) * nearest # round {n} to nearest {nearest} def roundToNearest(n, nearest): return 1.0 * round(1.0*n/nearest) * nearest # Read instruments from file with open(INSTRUMENTS_INPUT_FILE, 'rb') as f: r = csv.reader(f, delimiter=',') next(r, None) # remove header for file,min_loss,max_loss,min_c_loss,max_c_loss,from_gain,to_gain,from_tempo,to_tempo,tempo_offset,interval_phase,interval,interval_offset,active in r: if int(active): index = len(instruments) # build instrument object _beat_ms = int(round(BEAT_MS/TEMPO)) instrument = { 'index': index, 'file': INSTRUMENTS_DIR + file, 'min_loss': float(min_loss), 'max_loss': float(max_loss), 'min_c_loss': float(min_c_loss), 'max_c_loss': float(max_c_loss), 'from_gain': float(from_gain) * GAIN, 'to_gain': float(to_gain) * GAIN, 'from_tempo': float(from_tempo) * TEMPO, 'to_tempo': float(to_tempo) * TEMPO, 'tempo_offset': float(tempo_offset), 'interval_ms': int(int(interval_phase)*_beat_ms), 'interval': int(interval), 'interval_offset': int(interval_offset), 'from_beat_ms': int(round(BEAT_MS/(float(from_tempo)*TEMPO))), 'to_beat_ms': int(round(BEAT_MS/(float(to_tempo)*TEMPO))), 'beat_ms': _beat_ms } # add instrument to instruments instruments.append(instrument) # Read countries from file with open(LAND_LOSS_INPUT_FILE) as data_file: years = json.load(data_file) # Break years up into groups for i, year in enumerate(years): total_loss = len(year['losses']) years[i]['total_loss'] = total_loss years[i]['loss_per_group'] = 1.0 * total_loss / GROUPS_PER_YEAR # Normalize groups all_years_loss = 1.0 * sum([y['total_loss'] for y in years]) min_group_value = min([y['loss_per_group'] for y in years]) max_group_value = max([y['loss_per_group'] for y in years]) for i, year in enumerate(years): years[i]['loss_per_group_n'] = year['loss_per_group'] / all_years_loss years[i]['group_loss'] = (1.0 * year['loss_per_group'] - min_group_value) / (max_group_value - min_group_value) # Calculate total time total_ms = len(years) * MS_PER_YEAR total_seconds = int(1.0*total_ms/1000) print('Main sequence time: '+time.strftime('%M:%S', time.gmtime(total_seconds)) + ' (' + str(total_seconds) + 's)') print('Ms per beat: ' + str(BEAT_MS)) print('Beats per year: ' + str(BEATS_PER_YEAR)) # Multiplier based on sine curve def getMultiplier(percent_complete, rad=1.0): radians = percent_complete * (math.pi * rad) multiplier = math.sin(radians) if multiplier < 0: multiplier = 0.0 elif multiplier > 1: multplier = 1.0 return multiplier # Retrieve gain based on current beat def getGain(instrument, percent_complete): multiplier = getMultiplier(percent_complete) from_gain = instrument['from_gain'] to_gain = instrument['to_gain'] min_gain = min(from_gain, to_gain) gain = multiplier * (to_gain - from_gain) + from_gain gain = max(min_gain, round(gain, 2)) return gain # Get beat duration in ms based on current point in time def getBeatMs(instrument, percent_complete, round_to): multiplier = getMultiplier(percent_complete) from_beat_ms = instrument['from_beat_ms'] to_beat_ms = instrument['to_beat_ms'] ms = multiplier * (to_beat_ms - from_beat_ms) + from_beat_ms ms = int(roundToNearest(ms, round_to)) return ms # Return if the instrument should be played in the given interval def isValidInterval(instrument, elapsed_ms): interval_ms = instrument['interval_ms'] interval = instrument['interval'] interval_offset = instrument['interval_offset'] return int(math.floor(1.0*elapsed_ms/interval_ms)) % interval == interval_offset # Add beats to sequence def addBeatsToSequence(instrument, duration, ms, round_to): global sequence global hindex beat_ms = int(roundToNearest(instrument['beat_ms'], round_to)) offset_ms = int(instrument['tempo_offset'] * instrument['from_beat_ms']) ms += offset_ms previous_ms = int(ms) from_beat_ms = instrument['from_beat_ms'] to_beat_ms = instrument['to_beat_ms'] min_ms = min(from_beat_ms, to_beat_ms) remaining_duration = int(duration) elapsed_duration = offset_ms while remaining_duration >= min_ms: elapsed_ms = int(ms) elapsed_beat = int((elapsed_ms-previous_ms) / beat_ms) percent_complete = 1.0 * elapsed_duration / duration this_beat_ms = getBeatMs(instrument, percent_complete, round_to) # add to sequence if in valid interval if isValidInterval(instrument, elapsed_ms): h = halton(hindex, 3) variance = int(h * VARIANCE_MS * 2 - VARIANCE_MS) sequence.append({ 'instrument_index': instrument['index'], 'instrument': instrument, 'position': 0, 'rate': 1, 'gain': getGain(instrument, percent_complete), 'elapsed_ms': max([elapsed_ms + variance, 0]), 'duration': min([this_beat_ms, MS_PER_YEAR]) }) hindex += 1 remaining_duration -= this_beat_ms elapsed_duration += this_beat_ms ms += this_beat_ms # Build sequence for instrument in instruments: ms = None queue_duration = 0 c_loss = 0 current_ms = 0 # Go through each year for year in years: for g in range(GROUPS_PER_YEAR): c_loss += year['loss_per_group_n'] is_valid = year['group_loss'] >= instrument['min_loss'] and year['group_loss'] < instrument['max_loss'] and c_loss >= instrument['min_c_loss'] and c_loss < instrument['max_c_loss'] # If not valid, add it queue to sequence if not is_valid and queue_duration > 0 and ms != None or is_valid and ms != None and current_ms > (ms+queue_duration): addBeatsToSequence(instrument.copy(), queue_duration, ms, ROUND_TO_NEAREST) ms = None queue_duration = 0 # If valid, add time to queue if is_valid: if ms==None: ms = current_ms queue_duration += GROUP_MS current_ms += GROUP_MS # Add remaining queue to sequence if queue_duration > 0 and ms != None: addBeatsToSequence(instrument.copy(), queue_duration, ms, ROUND_TO_NEAREST) # Sort sequence sequence = sorted(sequence, key=lambda k: k['elapsed_ms']) # Add milliseconds to sequence elapsed = 0 for i, step in enumerate(sequence): sequence[i]['milliseconds'] = step['elapsed_ms'] - elapsed elapsed = step['elapsed_ms'] # Write instruments to file if WRITE_SEQUENCE and len(instruments) > 0: with open(INSTRUMENTS_OUTPUT_FILE, 'wb') as f: w = csv.writer(f) for index, instrument in enumerate(instruments): w.writerow([index]) w.writerow([instrument['file']]) f.seek(-2, os.SEEK_END) # remove newline f.truncate() print('Successfully wrote instruments to file: '+INSTRUMENTS_OUTPUT_FILE) # Write sequence to file if WRITE_SEQUENCE and len(sequence) > 0: with open(SEQUENCE_OUTPUT_FILE, 'wb') as f: w = csv.writer(f) for step in sequence: w.writerow([step['instrument_index']]) w.writerow([step['position']]) w.writerow([step['gain']]) w.writerow([step['rate']]) w.writerow([step['milliseconds']]) f.seek(-2, os.SEEK_END) # remove newline f.truncate() print('Successfully wrote sequence to file: '+SEQUENCE_OUTPUT_FILE) # Write summary files if WRITE_REPORT: with open(SUMMARY_OUTPUT_FILE, 'wb') as f: w = csv.writer(f) header = ['Time', 'Year Start', 'Year End', 'Group', 'Loss', 'Loss Cum'] w.writerow(header) start_ms = 0 cumulative_loss = 0 for y in years: # cumulative_loss += y['loss'] for g in range(GROUPS_PER_YEAR): cumulative_loss += y['loss_per_group_n'] elapsed = start_ms elapsed_f = time.strftime('%M:%S', time.gmtime(int(elapsed/1000))) ms = int(elapsed % 1000) elapsed_f += '.' + str(ms) row = [elapsed_f, y['year_start'], y['year_end'], g, y['group_loss'], cumulative_loss] w.writerow(row) start_ms += GROUP_MS print('Successfully wrote summary file: '+SUMMARY_OUTPUT_FILE) if len(sequence) > 0: with open(SUMMARY_SEQUENCE_OUTPUT_FILE, 'wb') as f: w = csv.writer(f) w.writerow(['Time', 'Instrument', 'Gain']) for step in sequence: instrument = instruments[step['instrument_index']] elapsed = step['elapsed_ms'] elapsed_f = time.strftime('%M:%S', time.gmtime(int(elapsed/1000))) ms = int(elapsed % 1000) elapsed_f += '.' + str(ms) w.writerow([elapsed_f, instrument['file'], step['gain']]) f.seek(-2, os.SEEK_END) # remove newline f.truncate() print('Successfully wrote sequence report to file: '+SUMMARY_SEQUENCE_OUTPUT_FILE)
# Copyright 2017 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== """Platform-specific code for checking the integrity of the TensorFlow build.""" from __future__ import absolute_import from __future__ import division from __future__ import print_function import os try: from tensorflow.python.platform import build_info except ImportError: raise ImportError("Could not import tensorflow. Do not import tensorflow " "from its source directory; change directory to outside " "the TensorFlow source tree, and relaunch your Python " "interpreter from there.") def preload_check(): """Raises an exception if the environment is not correctly configured. Raises: ImportError: If the check detects that the environment is not correctly configured, and attempting to load the TensorFlow runtime will fail. """ if os.name == "nt": # Attempt to load any DLLs that the Python extension depends on before # we load the Python extension, so that we can raise an actionable error # message if they are not found. import ctypes # pylint: disable=g-import-not-at-top if hasattr(build_info, "msvcp_dll_name"): try: ctypes.WinDLL(build_info.msvcp_dll_name) except OSError: raise ImportError( "Could not find %r. TensorFlow requires that this DLL be " "installed in a directory that is named in your %%PATH%% " "environment variable. You may install this DLL by downloading " "Visual C++ 2015 Redistributable Update 3 from this URL: " "https://www.microsoft.com/en-us/download/details.aspx?id=53587" % build_info.msvcp_dll_name) else: # TODO(mrry): Consider adding checks for the Linux and Mac OS X builds. pass
# ACTION_CHECKBOX_NAME is unused, but should stay since its import from here # has been referenced in documentation. from django.contrib.admin.decorators import register from django.contrib.admin.filters import ( AllValuesFieldListFilter, BooleanFieldListFilter, ChoicesFieldListFilter, DateFieldListFilter, FieldListFilter, ListFilter, RelatedFieldListFilter, RelatedOnlyFieldListFilter, SimpleListFilter, ) from django.contrib.admin.helpers import ACTION_CHECKBOX_NAME from django.contrib.admin.options import ( HORIZONTAL, VERTICAL, ModelAdmin, StackedInline, TabularInline, ) from django.contrib.admin.sites import AdminSite, site from django.utils.module_loading import autodiscover_modules __all__ = [ "register", "ACTION_CHECKBOX_NAME", "ModelAdmin", "HORIZONTAL", "VERTICAL", "StackedInline", "TabularInline", "AdminSite", "site", "ListFilter", "SimpleListFilter", "FieldListFilter", "BooleanFieldListFilter", "RelatedFieldListFilter", "ChoicesFieldListFilter", "DateFieldListFilter", "AllValuesFieldListFilter", "RelatedOnlyFieldListFilter", "autodiscover", ] def autodiscover(): autodiscover_modules('admin', register_to=site) default_app_config = 'django.contrib.admin.apps.AdminConfig'
from django.test import TestCase from .models import Article, Bar, Base, Child, Foo, Whiz class StringLookupTests(TestCase): def test_string_form_referencing(self): """ Regression test for #1661 and #1662 String form referencing of models works, both as pre and post reference, on all RelatedField types. """ f1 = Foo(name="Foo1") f1.save() f2 = Foo(name="Foo2") f2.save() w1 = Whiz(name="Whiz1") w1.save() b1 = Bar(name="Bar1", normal=f1, fwd=w1, back=f2) b1.save() self.assertEqual(b1.normal, f1) self.assertEqual(b1.fwd, w1) self.assertEqual(b1.back, f2) base1 = Base(name="Base1") base1.save() child1 = Child(name="Child1", parent=base1) child1.save() self.assertEqual(child1.parent, base1) def test_unicode_chars_in_queries(self): """ Regression tests for #3937 make sure we can use unicode characters in queries. If these tests fail on MySQL, it's a problem with the test setup. A properly configured UTF-8 database can handle this. """ fx = Foo(name='Bjorn', friend='Franois') fx.save() self.assertEqual(Foo.objects.get(friend__contains='\xe7'), fx) def test_queries_on_textfields(self): """ Regression tests for #5087 make sure we can perform queries on TextFields. """ a = Article(name='Test', text='The quick brown fox jumps over the lazy dog.') a.save() self.assertEqual(Article.objects.get(text__exact='The quick brown fox jumps over the lazy dog.'), a) self.assertEqual(Article.objects.get(text__contains='quick brown fox'), a) def test_ipaddress_on_postgresql(self): """ Regression test for #708 "like" queries on IP address fields require casting with HOST() (on PostgreSQL). """ a = Article(name='IP test', text='The body', submitted_from='192.0.2.100') a.save() self.assertSequenceEqual(Article.objects.filter(submitted_from__contains='192.0.2'), [a]) # The searches do not match the subnet mask (/32 in this case) self.assertEqual(Article.objects.filter(submitted_from__contains='32').count(), 0)
import guardian from django.contrib.auth.models import AnonymousUser from django.contrib.auth.models import Group from django.contrib.auth.models import Permission from django.contrib.auth.models import User from django.contrib.contenttypes.models import ContentType from django.core.exceptions import ValidationError from django.test import TestCase from guardian.backends import ObjectPermissionBackend from guardian.exceptions import GuardianError from guardian.exceptions import NotUserNorGroup from guardian.exceptions import ObjectNotPersisted from guardian.exceptions import WrongAppError from guardian.models import GroupObjectPermission from guardian.models import UserObjectPermission from itertools import chain class UserPermissionTests(TestCase): fixtures = ['tests.json'] def setUp(self): self.user = User.objects.get(username='jack') self.ctype = ContentType.objects.create(name='foo', model='bar', app_label='fake-for-guardian-tests') self.obj1 = ContentType.objects.create(name='ct1', model='foo', app_label='guardian-tests') self.obj2 = ContentType.objects.create(name='ct2', model='bar', app_label='guardian-tests') def test_assignement(self): self.assertFalse(self.user.has_perm('change_contenttype', self.ctype)) UserObjectPermission.objects.assign('change_contenttype', self.user, self.ctype) self.assertTrue(self.user.has_perm('change_contenttype', self.ctype)) self.assertTrue(self.user.has_perm('contenttypes.change_contenttype', self.ctype)) def test_assignement_and_remove(self): UserObjectPermission.objects.assign('change_contenttype', self.user, self.ctype) self.assertTrue(self.user.has_perm('change_contenttype', self.ctype)) UserObjectPermission.objects.remove_perm('change_contenttype', self.user, self.ctype) self.assertFalse(self.user.has_perm('change_contenttype', self.ctype)) def test_ctypes(self): UserObjectPermission.objects.assign('change_contenttype', self.user, self.obj1) self.assertTrue(self.user.has_perm('change_contenttype', self.obj1)) self.assertFalse(self.user.has_perm('change_contenttype', self.obj2)) UserObjectPermission.objects.remove_perm('change_contenttype', self.user, self.obj1) UserObjectPermission.objects.assign('change_contenttype', self.user, self.obj2) self.assertTrue(self.user.has_perm('change_contenttype', self.obj2)) self.assertFalse(self.user.has_perm('change_contenttype', self.obj1)) UserObjectPermission.objects.assign('change_contenttype', self.user, self.obj1) UserObjectPermission.objects.assign('change_contenttype', self.user, self.obj2) self.assertTrue(self.user.has_perm('change_contenttype', self.obj2)) self.assertTrue(self.user.has_perm('change_contenttype', self.obj1)) UserObjectPermission.objects.remove_perm('change_contenttype', self.user, self.obj1) UserObjectPermission.objects.remove_perm('change_contenttype', self.user, self.obj2) self.assertFalse(self.user.has_perm('change_contenttype', self.obj2)) self.assertFalse(self.user.has_perm('change_contenttype', self.obj1)) def test_get_for_object(self): perms = UserObjectPermission.objects.get_for_object(self.user, self.ctype) self.assertEqual(perms.count(), 0) to_assign = sorted([ 'delete_contenttype', 'change_contenttype', ]) for perm in to_assign: UserObjectPermission.objects.assign(perm, self.user, self.ctype) perms = UserObjectPermission.objects.get_for_object(self.user, self.ctype) codenames = sorted(chain(*perms.values_list('permission__codename'))) self.assertEqual(to_assign, codenames) def test_assign_validation(self): self.assertRaises(Permission.DoesNotExist, UserObjectPermission.objects.assign, 'change_group', self.user, self.user) group = Group.objects.create(name='test_group_assign_validation') ctype = ContentType.objects.get_for_model(group) perm = Permission.objects.get(codename='change_user') create_info = dict( permission = perm, user = self.user, content_type = ctype, object_pk = group.pk ) self.assertRaises(ValidationError, UserObjectPermission.objects.create, **create_info) def test_unicode(self): obj_perm = UserObjectPermission.objects.assign("change_user", self.user, self.user) self.assertTrue(isinstance(obj_perm.__unicode__(), unicode)) def test_errors(self): not_saved_user = User(username='not_saved_user') self.assertRaises(ObjectNotPersisted, UserObjectPermission.objects.assign, "change_user", self.user, not_saved_user) self.assertRaises(ObjectNotPersisted, UserObjectPermission.objects.remove_perm, "change_user", self.user, not_saved_user) self.assertRaises(ObjectNotPersisted, UserObjectPermission.objects.get_for_object, "change_user", not_saved_user) class GroupPermissionTests(TestCase): fixtures = ['tests.json'] def setUp(self): self.user = User.objects.get(username='jack') self.group, created = Group.objects.get_or_create(name='jackGroup') self.user.groups.add(self.group) self.ctype = ContentType.objects.create(name='foo', model='bar', app_label='fake-for-guardian-tests') self.obj1 = ContentType.objects.create(name='ct1', model='foo', app_label='guardian-tests') self.obj2 = ContentType.objects.create(name='ct2', model='bar', app_label='guardian-tests') def test_assignement(self): self.assertFalse(self.user.has_perm('change_contenttype', self.ctype)) self.assertFalse(self.user.has_perm('contenttypes.change_contenttype', self.ctype)) GroupObjectPermission.objects.assign('change_contenttype', self.group, self.ctype) self.assertTrue(self.user.has_perm('change_contenttype', self.ctype)) self.assertTrue(self.user.has_perm('contenttypes.change_contenttype', self.ctype)) def test_assignement_and_remove(self): GroupObjectPermission.objects.assign('change_contenttype', self.group, self.ctype) self.assertTrue(self.user.has_perm('change_contenttype', self.ctype)) GroupObjectPermission.objects.remove_perm('change_contenttype', self.group, self.ctype) self.assertFalse(self.user.has_perm('change_contenttype', self.ctype)) def test_ctypes(self): GroupObjectPermission.objects.assign('change_contenttype', self.group, self.obj1) self.assertTrue(self.user.has_perm('change_contenttype', self.obj1)) self.assertFalse(self.user.has_perm('change_contenttype', self.obj2)) GroupObjectPermission.objects.remove_perm('change_contenttype', self.group, self.obj1) GroupObjectPermission.objects.assign('change_contenttype', self.group, self.obj2) self.assertTrue(self.user.has_perm('change_contenttype', self.obj2)) self.assertFalse(self.user.has_perm('change_contenttype', self.obj1)) GroupObjectPermission.objects.assign('change_contenttype', self.group, self.obj1) GroupObjectPermission.objects.assign('change_contenttype', self.group, self.obj2) self.assertTrue(self.user.has_perm('change_contenttype', self.obj2)) self.assertTrue(self.user.has_perm('change_contenttype', self.obj1)) GroupObjectPermission.objects.remove_perm('change_contenttype', self.group, self.obj1) GroupObjectPermission.objects.remove_perm('change_contenttype', self.group, self.obj2) self.assertFalse(self.user.has_perm('change_contenttype', self.obj2)) self.assertFalse(self.user.has_perm('change_contenttype', self.obj1)) def test_get_for_object(self): group = Group.objects.create(name='get_group_perms_for_object') self.user.groups.add(group) perms = GroupObjectPermission.objects.get_for_object(group, self.ctype) self.assertEqual(perms.count(), 0) to_assign = sorted([ 'delete_contenttype', 'change_contenttype', ]) for perm in to_assign: GroupObjectPermission.objects.assign(perm, group, self.ctype) perms = GroupObjectPermission.objects.get_for_object(group, self.ctype) codenames = sorted(chain(*perms.values_list('permission__codename'))) self.assertEqual(to_assign, codenames) def test_assign_validation(self): self.assertRaises(Permission.DoesNotExist, GroupObjectPermission.objects.assign, 'change_user', self.group, self.group) user = User.objects.create(username='test_user_assign_validation') ctype = ContentType.objects.get_for_model(user) perm = Permission.objects.get(codename='change_group') create_info = dict( permission = perm, group = self.group, content_type = ctype, object_pk = user.pk ) self.assertRaises(ValidationError, GroupObjectPermission.objects.create, **create_info) def test_unicode(self): obj_perm = GroupObjectPermission.objects.assign("change_group", self.group, self.group) self.assertTrue(isinstance(obj_perm.__unicode__(), unicode)) def test_errors(self): not_saved_group = Group(name='not_saved_group') self.assertRaises(ObjectNotPersisted, GroupObjectPermission.objects.assign, "change_group", self.group, not_saved_group) self.assertRaises(ObjectNotPersisted, GroupObjectPermission.objects.remove_perm, "change_group", self.group, not_saved_group) self.assertRaises(ObjectNotPersisted, GroupObjectPermission.objects.get_for_object, "change_group", not_saved_group) class ObjectPermissionBackendTests(TestCase): def setUp(self): self.user = User.objects.create(username='jack') self.backend = ObjectPermissionBackend() def test_attrs(self): self.assertTrue(self.backend.supports_anonymous_user) self.assertTrue(self.backend.supports_object_permissions) self.assertTrue(self.backend.supports_inactive_user) def test_authenticate(self): self.assertEqual(self.backend.authenticate( self.user.username, self.user.password), None) def test_has_perm_noobj(self): result = self.backend.has_perm(self.user, "change_contenttype") self.assertFalse(result) def test_has_perm_notauthed(self): user = AnonymousUser() self.assertFalse(self.backend.has_perm(user, "change_user", self.user)) def test_has_perm_wrong_app(self): self.assertRaises(WrongAppError, self.backend.has_perm, self.user, "no_app.change_user", self.user) def test_obj_is_not_model(self): for obj in (Group, 666, "String", [2, 1, 5, 7], {}): self.assertFalse(self.backend.has_perm(self.user, "any perm", obj)) def test_not_active_user(self): user = User.objects.create(username='non active user') ctype = ContentType.objects.create(name='foo', model='bar', app_label='fake-for-guardian-tests') perm = 'change_contenttype' UserObjectPermission.objects.assign(perm, user, ctype) self.assertTrue(self.backend.has_perm(user, perm, ctype)) user.is_active = False user.save() self.assertFalse(self.backend.has_perm(user, perm, ctype)) class GuardianBaseTests(TestCase): def has_attrs(self): self.assertTrue(hasattr(guardian, '__version__')) def test_version(self): for x in guardian.VERSION: self.assertTrue(isinstance(x, (int, str))) def test_get_version(self): self.assertTrue(isinstance(guardian.get_version(), str)) class TestExceptions(TestCase): def _test_error_class(self, exc_cls): self.assertTrue(isinstance(exc_cls, GuardianError)) def test_error_classes(self): self.assertTrue(isinstance(GuardianError(), Exception)) guardian_errors = [NotUserNorGroup] for err in guardian_errors: self._test_error_class(err())
# -*- coding: utf-8 -*- """ flask.module ~~~~~~~~~~~~ Implements a class that represents module blueprints. :copyright: (c) 2011 by Armin Ronacher. :license: BSD, see LICENSE for more details. """ import os from .blueprints import Blueprint def blueprint_is_module(bp): """Used to figure out if something is actually a module""" return isinstance(bp, Module) class Module(Blueprint): """Deprecated module support. Until Flask 0.6 modules were a different name of the concept now available as blueprints in Flask. They are essentially doing the same but have some bad semantics for templates and static files that were fixed with blueprints. .. versionchanged:: 0.7 Modules were deprecated in favor for blueprints. """ def __init__(self, import_name, name=None, url_prefix=None, static_path=None, subdomain=None): if name is None: assert '.' in import_name, 'name required if package name ' \ 'does not point to a submodule' name = import_name.rsplit('.', 1)[1] Blueprint.__init__(self, name, import_name, url_prefix=url_prefix, subdomain=subdomain, template_folder='templates') if os.path.isdir(os.path.join(self.root_path, 'static')): self._static_folder = 'static'
""" Copyright (c) 2019 Red Hat, Inc All rights reserved. This software may be modified and distributed under the terms of the BSD license. See the LICENSE file for details. """ import os import subprocess import tempfile from flexmock import flexmock import pytest import json import tarfile import re from atomic_reactor.constants import PLUGIN_FETCH_SOURCES_KEY from atomic_reactor.inner import DockerBuildWorkflow from atomic_reactor.constants import EXPORTED_SQUASHED_IMAGE_NAME from atomic_reactor.core import DockerTasker from atomic_reactor.plugin import BuildStepPluginsRunner, PluginFailedException from atomic_reactor.plugins.build_source_container import SourceContainerPlugin from atomic_reactor.plugins.pre_reactor_config import ( ReactorConfigPlugin, ) from tests.docker_mock import mock_docker from tests.constants import MOCK_SOURCE class MockSource(object): def __init__(self, tmpdir): tmpdir = str(tmpdir) self.dockerfile_path = os.path.join(tmpdir, 'Dockerfile') self.path = tmpdir self.config = flexmock(image_build_method=None) def get_build_file_path(self): return self.dockerfile_path, self.path class MockInsideBuilder(object): def __init__(self): mock_docker() self.tasker = DockerTasker() self.base_image = None self.image_id = None self.image = None self.df_path = None self.df_dir = None self.parent_images_digests = {} def ensure_not_built(self): pass def mock_workflow(tmpdir, sources_dir='', remote_dir=''): workflow = DockerBuildWorkflow(source=MOCK_SOURCE) builder = MockInsideBuilder() source = MockSource(tmpdir) setattr(builder, 'source', source) setattr(workflow, 'source', source) setattr(workflow, 'builder', builder) workflow.plugin_workspace[ReactorConfigPlugin.key] = {} workflow.prebuild_results[PLUGIN_FETCH_SOURCES_KEY] = { 'image_sources_dir': os.path.join(tmpdir.strpath, sources_dir), 'remote_sources_dir': os.path.join(tmpdir.strpath, remote_dir), } return workflow @pytest.mark.parametrize('sources_dir, sources_dir_exists, sources_dir_empty', [ ('sources_dir', False, True), ('sources_dir', True, True), ('sources_dir', True, False)]) @pytest.mark.parametrize('remote_dir, remote_dir_exists, remote_dir_empty', [ ('remote_sources_dir', False, True), ('remote_sources_dir', True, True), ('remote_sources_dir', True, False)]) @pytest.mark.parametrize('export_failed', (True, False)) def test_running_build(tmpdir, caplog, user_params, sources_dir, sources_dir_exists, sources_dir_empty, remote_dir, remote_dir_exists, remote_dir_empty, export_failed): """ Test if proper result is returned and if plugin works """ sources_dir_path = os.path.join(tmpdir.strpath, sources_dir) if sources_dir_exists: os.mkdir(sources_dir_path) if not sources_dir_empty: os.mknod(os.path.join(sources_dir_path, 'stub.srpm')) remote_dir_path = os.path.join(tmpdir.strpath, remote_dir) if remote_dir_exists: os.mkdir(remote_dir_path) if not remote_dir_empty: os.mknod(os.path.join(remote_dir_path, 'remote-sources.tar.gz')) workflow = mock_workflow(tmpdir, sources_dir, remote_dir) mocked_tasker = flexmock(workflow.builder.tasker) mocked_tasker.should_receive('wait').and_return(0) runner = BuildStepPluginsRunner( mocked_tasker, workflow, [{ 'name': SourceContainerPlugin.key, 'args': {}, }] ) temp_image_output_dir = os.path.join(str(tmpdir), 'image_output_dir') temp_image_export_dir = os.path.join(str(tmpdir), 'image_export_dir') tempfile_chain = flexmock(tempfile).should_receive("mkdtemp").and_return(temp_image_output_dir) tempfile_chain.and_return(temp_image_export_dir) os.mkdir(temp_image_export_dir) os.makedirs(os.path.join(temp_image_output_dir, 'blobs', 'sha256')) def check_check_output(args, **kwargs): if args[0] == 'skopeo': assert args[0] == 'skopeo' assert args[1] == 'copy' assert args[2] == 'oci:%s' % temp_image_output_dir assert args[3] == 'docker-archive:%s' % os.path.join(temp_image_export_dir, EXPORTED_SQUASHED_IMAGE_NAME) if export_failed: raise subprocess.CalledProcessError(returncode=1, cmd=args, output="Failed") return '' else: args_expect = ['bsi', '-d'] drivers = [] if sources_dir and sources_dir_exists: drivers.append('sourcedriver_rpm_dir') if remote_dir and remote_dir_exists: drivers.append('sourcedriver_extra_src_dir') args_expect.append(','.join(drivers)) if sources_dir and sources_dir_exists: args_expect.append('-s') args_expect.append(sources_dir_path) if remote_dir and remote_dir_exists: args_expect.append('-e') args_expect.append(remote_dir_path) args_expect.append('-o') args_expect.append(temp_image_output_dir) assert args == args_expect return 'stub stdout' check_output_times = 2 if not sources_dir_exists and not remote_dir_exists: check_output_times = 0 (flexmock(subprocess) .should_receive("check_output") .times(check_output_times) .replace_with(check_check_output)) blob_sha = "f568c411849e21aa3917973f1c5b120f6b52fe69b1944dfb977bc11bed6fbb6d" index_json = {"schemaVersion": 2, "manifests": [{"mediaType": "application/vnd.oci.image.manifest.v1+json", "digest": "sha256:%s" % blob_sha, "size": 645, "annotations": {"org.opencontainers.image.ref.name": "latest-source"}, "platform": {"architecture": "amd64", "os": "linux"}}]} blob_json = {"schemaVersion": 2, "layers": []} with open(os.path.join(temp_image_output_dir, 'index.json'), 'w') as fp: fp.write(json.dumps(index_json)) with open(os.path.join(temp_image_output_dir, 'blobs', 'sha256', blob_sha), 'w') as fp: fp.write(json.dumps(blob_json)) if not export_failed: export_tar = os.path.join(temp_image_export_dir, EXPORTED_SQUASHED_IMAGE_NAME) with open(export_tar, "wb") as f: with tarfile.TarFile(mode="w", fileobj=f) as tf: for f in os.listdir(temp_image_output_dir): tf.add(os.path.join(temp_image_output_dir, f), f) if not sources_dir_exists and not remote_dir_exists: build_result = runner.run() err_msg = "No SRPMs directory '{}' available".format(sources_dir_path) err_msg += "\nNo Remote source directory '{}' available".format(remote_dir_path) # Since Python 3.7 logger adds additional whitespaces by default -> checking without them assert re.sub(r'\s+', " ", err_msg) in re.sub(r'\s+', " ", caplog.text) assert build_result.is_failed() elif export_failed: with pytest.raises(PluginFailedException): runner.run() else: build_result = runner.run() assert not build_result.is_failed() assert build_result.oci_image_path assert 'stub stdout' in caplog.text empty_srpm_msg = "SRPMs directory '{}' is empty".format(sources_dir_path) empty_remote_msg = "Remote source directory '{}' is empty".format(remote_dir_path) if sources_dir_exists and sources_dir_empty: assert empty_srpm_msg in caplog.text else: assert empty_srpm_msg not in caplog.text if remote_dir_exists and remote_dir_empty: assert empty_remote_msg in caplog.text else: assert empty_remote_msg not in caplog.text def test_failed_build(tmpdir, caplog, user_params): """ Test if proper error state is returned when build inside build container failed """ (flexmock(subprocess).should_receive('check_output') .and_raise(subprocess.CalledProcessError(1, 'cmd', output='stub stdout'))) workflow = mock_workflow(tmpdir) mocked_tasker = flexmock(workflow.builder.tasker) mocked_tasker.should_receive('wait').and_return(1) runner = BuildStepPluginsRunner( mocked_tasker, workflow, [{ 'name': SourceContainerPlugin.key, 'args': {}, }] ) build_result = runner.run() assert build_result.is_failed() assert 'BSI failed with output:' in caplog.text assert 'stub stdout' in caplog.text
""" Support for Tellstick lights. """ import logging # pylint: disable=no-name-in-module, import-error from homeassistant.components.light import Light, ATTR_BRIGHTNESS from homeassistant.const import ATTR_FRIENDLY_NAME import tellcore.constants as tellcore_constants def setup_platform(hass, config, add_devices_callback, discovery_info=None): """ Find and return tellstick lights. """ try: import tellcore.telldus as telldus except ImportError: logging.getLogger(__name__).exception( "Failed to import tellcore") return [] core = telldus.TelldusCore() switches_and_lights = core.devices() lights = [] for switch in switches_and_lights: if switch.methods(tellcore_constants.TELLSTICK_DIM): lights.append(TellstickLight(switch)) add_devices_callback(lights) class TellstickLight(Light): """ Represents a tellstick light """ last_sent_command_mask = (tellcore_constants.TELLSTICK_TURNON | tellcore_constants.TELLSTICK_TURNOFF | tellcore_constants.TELLSTICK_DIM | tellcore_constants.TELLSTICK_UP | tellcore_constants.TELLSTICK_DOWN) def __init__(self, tellstick): self.tellstick = tellstick self.state_attr = {ATTR_FRIENDLY_NAME: tellstick.name} self._brightness = 0 @property def name(self): """ Returns the name of the switch if any. """ return self.tellstick.name @property def is_on(self): """ True if switch is on. """ return self._brightness > 0 @property def brightness(self): """ Brightness of this light between 0..255. """ return self._brightness def turn_off(self, **kwargs): """ Turns the switch off. """ self.tellstick.turn_off() self._brightness = 0 def turn_on(self, **kwargs): """ Turns the switch on. """ brightness = kwargs.get(ATTR_BRIGHTNESS) if brightness is None: self._brightness = 255 else: self._brightness = brightness self.tellstick.dim(self._brightness) def update(self): """ Update state of the light. """ last_command = self.tellstick.last_sent_command( self.last_sent_command_mask) if last_command == tellcore_constants.TELLSTICK_TURNON: self._brightness = 255 elif last_command == tellcore_constants.TELLSTICK_TURNOFF: self._brightness = 0 elif (last_command == tellcore_constants.TELLSTICK_DIM or last_command == tellcore_constants.TELLSTICK_UP or last_command == tellcore_constants.TELLSTICK_DOWN): last_sent_value = self.tellstick.last_sent_value() if last_sent_value is not None: self._brightness = last_sent_value
# import modules import mcpi.minecraft as minecraft from time import sleep # connect python to minecraft mc = minecraft.Minecraft.create() # create CONSTANTS for block and light colours AIR = 0 STONE = 1 WOOL = 35 BLACK = 15 RED = 14 AMBER = 4 GREEN = 5 # clear area in middle of map and move player there mc.setBlocks(-60, 0, -60, 60, 50, 60, AIR) mc.setBlocks(-60, -1, -60, 60, -1, 60, STONE) mc.player.setPos(5, 0, 0) # create initial light stack for i in range(1, 7): mc.setBlock(10, 0 +i, 0, WOOL, 8) mc.setBlock(9, 6, 0, WOOL, BLACK) mc.setBlock(9, 5, 0, WOOL, BLACK) mc.setBlock(9, 4, 0, WOOL, BLACK) # wait three seconds before starting sequence sleep(3) # traffic light sequence while True: # turn on red mc.setBlock(9, 6, 0, WOOL, RED) # wait three seconds sleep(3) # turn on amber mc.setBlock(9, 5, 0, WOOL, AMBER) # wait one second sleep(1) # turn off red & amber, turn on green mc.setBlock(9, 6, 0, WOOL, BLACK) mc.setBlock(9, 5, 0, WOOL, BLACK) mc.setBlock(9, 4, 0, WOOL, GREEN) # wait three seconds sleep(3) # turn off green mc.setBlock(9, 4, 0, WOOL, BLACK) # turn on amber mc.setBlock(9, 5, 0, WOOL, AMBER) # wait one second sleep(1) # turn off amber mc.setBlock(9, 5, 0, WOOL, BLACK)
# -*- coding: utf-8 -*- ##################################################################################### # # Copyright (c) Crossbar.io Technologies GmbH # # Unless a separate license agreement exists between you and Crossbar.io GmbH (e.g. # you have purchased a commercial license), the license terms below apply. # # Should you enter into a separate license agreement after having received a copy of # this software, then the terms of such license agreement replace the terms below at # the time at which such license agreement becomes effective. # # In case a separate license agreement ends, and such agreement ends without being # replaced by another separate license agreement, the license terms below apply # from the time at which said agreement ends. # # LICENSE TERMS # # This program is free software: you can redistribute it and/or modify it under the # terms of the GNU General Public License, version 3, as published by the # Free Software Foundation. This program is distributed in the hope that it will be # useful, but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. # # See the GNU General Public License for more details. # # You should have received a copy of the GNU General Public License along # with this program. If not, see <https://www.gnu.org/licenses/gpl-3.0.en.html>. # ##################################################################################### #from __future__ import absolute_import import sys import os import shlex import time from cbsh import __version__ try: import sphinx_rtd_theme except ImportError: sphinx_rtd_theme = None try: from sphinxcontrib import spelling except ImportError: spelling = None # -- Path setup -------------------------------------------------------------- # If extensions (or modules to document with autodoc) are in another directory, # add these directories to sys.path here. If the directory is relative to the # documentation root, use os.path.abspath to make it absolute, like shown here. #sys.path.insert(0, os.path.abspath('./_extensions')) #sys.path.insert(0, os.path.abspath('..')) # Check if we are building on readthedocs RTD_BUILD = os.environ.get('READTHEDOCS', None) == 'True' # -- Project information ----------------------------------------------------- project = 'Crossbar.io Shell' copyright = '2018, Crossbar.io Technologies GmbH' author = 'Crossbar.io Technologies GmbH' # The short X.Y version version = __version__ # The full version, including alpha/beta/rc tags release = __version__ # -- General configuration --------------------------------------------------- # If your documentation needs a minimal Sphinx version, state it here. # # needs_sphinx = '1.0' # Add any Sphinx extension module names here, as strings. They can be # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom # ones. extensions = [ # 'sphinxcontrib.xbr', 'sphinxcontrib.spelling', ] # extensions not available on RTD if spelling is not None: extensions.append('sphinxcontrib.spelling') spelling_lang = 'en_US' spelling_show_suggestions = False spelling_word_list_filename = 'spelling_wordlist.txt' # Add any paths that contain templates here, relative to this directory. templates_path = ['_templates'] # The suffix(es) of source filenames. # You can specify multiple suffix as a list of string: # # source_suffix = ['.rst', '.md'] source_suffix = '.rst' # The master toctree document. master_doc = 'contents' # The language for content autogenerated by Sphinx. Refer to documentation # for a list of supported languages. # # This is also used if you do content translation via gettext catalogs. # Usually you set "language" from the command line for these cases. language = None # List of patterns, relative to source directory, that match files and # directories to ignore when looking for source files. # This pattern also affects html_static_path and html_extra_path . exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store'] # The name of the Pygments (syntax highlighting) style to use. pygments_style = 'sphinx' #pygments_style = 'monokai' #pygments_style = 'native' #pygments_style = 'pastie' #pygments_style = 'friendly' # -- Options for HTML output ------------------------------------------------- # The theme to use for HTML and HTML Help pages. See the documentation for # a list of builtin themes. # if sphinx_rtd_theme: html_theme = "sphinx_rtd_theme" html_theme_path = [sphinx_rtd_theme.get_html_theme_path()] else: html_theme = "default" # Theme options are theme-specific and customize the look and feel of a theme # further. For a list of options available for each theme, see the # documentation. # # html_theme_options = {} # Add any paths that contain custom static files (such as style sheets) here, # relative to this directory. They are copied after the builtin static files, # so a file named "default.css" will overwrite the builtin "default.css". html_static_path = ['_static'] # Custom sidebar templates, must be a dictionary that maps document names # to template names. # # The default sidebars (for documents that don't match any pattern) are # defined by theme itself. Builtin themes are using these templates by # default: ``['localtoc.html', 'relations.html', 'sourcelink.html', # 'searchbox.html']``. # # html_sidebars = {}
import sys import os #Put pydevconsole in the path. sys.argv[0] = os.path.dirname(sys.argv[0]) sys.path.insert(1, os.path.join(os.path.dirname(sys.argv[0]))) print('Running tests with:', sys.executable) print('PYTHONPATH:') print('\n'.join(sorted(sys.path))) import threading import unittest import pydevconsole from pydev_imports import xmlrpclib, SimpleXMLRPCServer try: raw_input raw_input_name = 'raw_input' except NameError: raw_input_name = 'input' #======================================================================================================================= # Test #======================================================================================================================= class Test(unittest.TestCase): def startClientThread(self, client_port): class ClientThread(threading.Thread): def __init__(self, client_port): threading.Thread.__init__(self) self.client_port = client_port def run(self): class HandleRequestInput: def RequestInput(self): return 'RequestInput: OK' handle_request_input = HandleRequestInput() import pydev_localhost print('Starting client with:', pydev_localhost.get_localhost(), self.client_port) client_server = SimpleXMLRPCServer((pydev_localhost.get_localhost(), self.client_port), logRequests=False) client_server.register_function(handle_request_input.RequestInput) client_server.serve_forever() client_thread = ClientThread(client_port) client_thread.setDaemon(True) client_thread.start() return client_thread def getFreeAddresses(self): import socket s = socket.socket() s.bind(('', 0)) port0 = s.getsockname()[1] s1 = socket.socket() s1.bind(('', 0)) port1 = s1.getsockname()[1] s.close() s1.close() return port0, port1 def testServer(self): client_port, server_port = self.getFreeAddresses() class ServerThread(threading.Thread): def __init__(self, client_port, server_port): threading.Thread.__init__(self) self.client_port = client_port self.server_port = server_port def run(self): import pydev_localhost print('Starting server with:', pydev_localhost.get_localhost(), self.server_port, self.client_port) pydevconsole.StartServer(pydev_localhost.get_localhost(), self.server_port, self.client_port) server_thread = ServerThread(client_port, server_port) server_thread.setDaemon(True) server_thread.start() client_thread = self.startClientThread(client_port) #@UnusedVariable import time time.sleep(.3) #let's give it some time to start the threads import pydev_localhost server = xmlrpclib.Server('http://%s:%s' % (pydev_localhost.get_localhost(), server_port)) server.addExec("import sys; print('Running with: %s %s' % (sys.executable or sys.platform, sys.version))") server.addExec('class Foo:') server.addExec(' pass') server.addExec('') server.addExec('foo = Foo()') server.addExec('a = %s()' % raw_input_name) server.addExec('print (a)') #======================================================================================================================= # main #======================================================================================================================= if __name__ == '__main__': unittest.main()
from __future__ import absolute_import from django.conf import settings import logging import traceback import platform from django.core import mail from django.http import HttpRequest from django.utils.log import AdminEmailHandler from django.views.debug import ExceptionReporter, get_exception_reporter_filter from zerver.lib.queue import queue_json_publish class AdminZulipHandler(logging.Handler): """An exception log handler that sends the exception to the queue to be sent to the Zulip feedback server. """ # adapted in part from django/utils/log.py def __init__(self): # type: () -> None logging.Handler.__init__(self) def emit(self, record): # type: (ExceptionReporter) -> None try: request = record.request # type: HttpRequest filter = get_exception_reporter_filter(request) if record.exc_info: stack_trace = ''.join(traceback.format_exception(*record.exc_info)) else: stack_trace = None try: user_profile = request.user user_full_name = user_profile.full_name user_email = user_profile.email except Exception: traceback.print_exc() # Error was triggered by an anonymous user. user_full_name = None user_email = None report = dict( node = platform.node(), method = request.method, path = request.path, data = request.GET if request.method == 'GET' else filter.get_post_parameters(request), remote_addr = request.META.get('REMOTE_ADDR', None), query_string = request.META.get('QUERY_STRING', None), server_name = request.META.get('SERVER_NAME', None), message = record.getMessage(), stack_trace = stack_trace, user_full_name = user_full_name, user_email = user_email, ) except: traceback.print_exc() report = dict( node = platform.node(), message = record.getMessage(), ) try: if settings.STAGING_ERROR_NOTIFICATIONS: # On staging, process the report directly so it can happen inside this # try/except to prevent looping from zilencer.error_notify import notify_server_error notify_server_error(report) else: queue_json_publish('error_reports', dict( type = "server", report = report, ), lambda x: None) except: # If this breaks, complain loudly but don't pass the traceback up the stream # However, we *don't* want to use logging.exception since that could trigger a loop. logging.warning("Reporting an exception triggered an exception!", exc_info=True)
#github data scrapper """ variables of interest: indp. variables - language, given as a binary variable. Need 4 positions for 5 langagues - #number of days created ago, 1 position - has wiki? Boolean, 1 position - followers, 1 position - following, 1 position - constant dep. variables -stars/watchers -forks """ from json import loads import datetime import numpy as np from requests import get MAX = 8000000 today = datetime.datetime.today() randint = np.random.randint N = 120 #sample size. auth = ("username", "password" ) language_mappings = {"Python": 0, "JavaScript": 1, "Ruby": 2, "Java":3, "Shell":4, "PHP":5} #define data matrix: X = np.zeros( (N , 12), dtype = int ) for i in xrange(N): is_fork = True is_valid_language = False while is_fork == True or is_valid_language == False: is_fork = True is_valid_language = False params = {"since":randint(0, MAX ) } r = get("https://api.github.com/repositories", params = params, auth=auth ) results = loads( r.text )[0] #im only interested in the first one, and if it is not a fork. is_fork = results["fork"] r = get( results["url"], auth = auth) #check the language repo_results = loads( r.text ) try: language_mappings[ repo_results["language" ] ] is_valid_language = True except: pass #languages X[ i, language_mappings[ repo_results["language" ] ] ] = 1 #delta time X[ i, 6] = ( today - datetime.datetime.strptime( repo_results["created_at"][:10], "%Y-%m-%d" ) ).days #haswiki X[i, 7] = repo_results["has_wiki"] #get user information r = get( results["owner"]["url"] , auth = auth) user_results = loads( r.text ) X[i, 8] = user_results["following"] X[i, 9] = user_results["followers"] #get dep. data X[i, 10] = repo_results["watchers_count"] X[i, 11] = repo_results["forks_count"] print print " -------------- " print i, ": ", results["full_name"], repo_results["language" ], repo_results["watchers_count"], repo_results["forks_count"] print " -------------- " print np.savetxt("data/github_data.csv", X, delimiter=",", fmt="%d" )
# Copyright 2008-2015 Nokia Solutions and Networks # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. from robot import utils from robot.errors import DataError from robot.model import Message as BaseMessage LEVELS = { 'NONE' : 6, 'ERROR' : 5, 'FAIL' : 4, 'WARN' : 3, 'INFO' : 2, 'DEBUG' : 1, 'TRACE' : 0, } class AbstractLogger: def __init__(self, level='TRACE'): self._is_logged = IsLogged(level) def set_level(self, level): return self._is_logged.set_level(level) def trace(self, msg): self.write(msg, 'TRACE') def debug(self, msg): self.write(msg, 'DEBUG') def info(self, msg): self.write(msg, 'INFO') def warn(self, msg): self.write(msg, 'WARN') def fail(self, msg): html = False if msg.startswith("*HTML*"): html = True msg = msg[6:].lstrip() self.write(msg, 'FAIL', html) def error(self, msg): self.write(msg, 'ERROR') def write(self, message, level, html=False): self.message(Message(message, level, html)) def message(self, msg): raise NotImplementedError(self.__class__) class Message(BaseMessage): __slots__ = ['_message'] def __init__(self, message, level='INFO', html=False, timestamp=None): message = self._normalize_message(message) level, html = self._get_level_and_html(level, html) timestamp = timestamp or utils.get_timestamp() BaseMessage.__init__(self, message, level, html, timestamp) def _normalize_message(self, msg): if callable(msg): return msg if not isinstance(msg, unicode): msg = utils.unic(msg) if '\r\n' in msg: msg = msg.replace('\r\n', '\n') return msg def _get_level_and_html(self, level, html): level = level.upper() if level == 'HTML': return 'INFO', True if level not in LEVELS: raise DataError("Invalid log level '%s'" % level) return level, html def _get_message(self): if callable(self._message): self._message = self._message() return self._message def _set_message(self, message): self._message = message message = property(_get_message, _set_message) class IsLogged: def __init__(self, level): self._str_level = level self._int_level = self._level_to_int(level) def __call__(self, level): return self._level_to_int(level) >= self._int_level def set_level(self, level): old = self._str_level.upper() self.__init__(level) return old def _level_to_int(self, level): try: return LEVELS[level.upper()] except KeyError: raise DataError("Invalid log level '%s'" % level) class AbstractLoggerProxy: _methods = NotImplemented _no_method = lambda *args: None def __init__(self, logger): self.logger = logger for name in self._methods: setattr(self, name, self._get_method(logger, name)) def _get_method(self, logger, name): for method_name in self._get_method_names(name): if hasattr(logger, method_name): return getattr(logger, method_name) return self._no_method def _get_method_names(self, name): return [name, self._toCamelCase(name)] def _toCamelCase(self, name): parts = name.split('_') return ''.join([parts[0]] + [part.capitalize() for part in parts[1:]])
#! /usr/bin/env python # -*- coding: utf-8 -*- # # Python motu client # # Motu, a high efficient, robust and Standard compliant Web Server for # Geographic Data Dissemination. # # http://cls-motu.sourceforge.net/ # # (C) Copyright 2009-2010, by CLS (Collecte Localisation Satellites) - # http://www.cls.fr - and Contributors # # # This library is free software; you can redistribute it and/or modify it # under the terms of the GNU Lesser General Public License as published by # the Free Software Foundation; either version 2.1 of the License, or # (at your option) any later version. # # This library is distributed in the hope that it will be useful, but # WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY # or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public # License for more details. # # You should have received a copy of the GNU Lesser General Public License # along with this library; if not, write to the Free Software Foundation, # Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA. from pkg_resources import resource_filename _messages = None MESSAGES_FILE = 'data/messages.properties' def get_external_messages(): """Return a table of externalized messages. The table is lazzy instancied (loaded once when called the first time).""" global _messages if _messages is None: propFile = file(resource_filename(__name__, MESSAGES_FILE), "rU") propDict = dict() for propLine in propFile: propDef = propLine.strip() if len(propDef) == 0: continue if propDef[0] in ('!', '#'): continue punctuation = [propDef.find(c) for c in ':= '] + [len(propDef)] found = min([pos for pos in punctuation if pos != -1]) name = propDef[:found].rstrip() value = propDef[found:].lstrip(":= ").rstrip() propDict[name] = value propFile.close() _messages = propDict return _messages
# Author: Nic Wolfe <nic@wolfeden.ca> # URL: http://code.google.com/p/sickbeard/ # # This file is part of SickGear. # # SickGear is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # SickGear is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with SickGear. If not, see <http://www.gnu.org/licenses/>. import urllib import urllib2 import base64 import re import sickbeard from sickbeard import logger from sickbeard import common from sickbeard.exceptions import ex from sickbeard.encodingKludge import fixStupidEncodings try: import xml.etree.cElementTree as etree except ImportError: import elementtree.ElementTree as etree class PLEXNotifier: def _send_to_plex(self, command, host, username=None, password=None): """Handles communication to Plex hosts via HTTP API Args: command: Dictionary of field/data pairs, encoded via urllib and passed to the legacy xbmcCmds HTTP API host: Plex host:port username: Plex API username password: Plex API password Returns: Returns 'OK' for successful commands or False if there was an error """ # fill in omitted parameters if not username: username = sickbeard.PLEX_USERNAME if not password: password = sickbeard.PLEX_PASSWORD if not host: logger.log(u'PLEX: No host specified, check your settings', logger.ERROR) return False for key in command: if type(command[key]) == unicode: command[key] = command[key].encode('utf-8') enc_command = urllib.urlencode(command) logger.log(u'PLEX: Encoded API command: ' + enc_command, logger.DEBUG) url = 'http://%s/xbmcCmds/xbmcHttp/?%s' % (host, enc_command) try: req = urllib2.Request(url) # if we have a password, use authentication if password: base64string = base64.encodestring('%s:%s' % (username, password))[:-1] authheader = 'Basic %s' % base64string req.add_header('Authorization', authheader) logger.log(u'PLEX: Contacting (with auth header) via url: ' + url, logger.DEBUG) else: logger.log(u'PLEX: Contacting via url: ' + url, logger.DEBUG) response = urllib2.urlopen(req) result = response.read().decode(sickbeard.SYS_ENCODING) response.close() logger.log(u'PLEX: HTTP response: ' + result.replace('\n', ''), logger.DEBUG) # could return result response = re.compile('<html><li>(.+\w)</html>').findall(result) return 'OK' except (urllib2.URLError, IOError) as e: logger.log(u'PLEX: Warning: Couldn\'t contact Plex at ' + fixStupidEncodings(url) + ' ' + ex(e), logger.WARNING) return False def _notify_pmc(self, message, title='SickGear', host=None, username=None, password=None, force=False): """Internal wrapper for the notify_snatch and notify_download functions Args: message: Message body of the notice to send title: Title of the notice to send host: Plex Media Client(s) host:port username: Plex username password: Plex password force: Used for the Test method to override config safety checks Returns: Returns a list results in the format of host:ip:result The result will either be 'OK' or False, this is used to be parsed by the calling function. """ # suppress notifications if the notifier is disabled but the notify options are checked if not sickbeard.USE_PLEX and not force: return False # fill in omitted parameters if not host: host = sickbeard.PLEX_HOST if not username: username = sickbeard.PLEX_USERNAME if not password: password = sickbeard.PLEX_PASSWORD result = '' for curHost in [x.strip() for x in host.split(',')]: logger.log(u'PLEX: Sending notification to \'%s\' - %s' % (curHost, message), logger.MESSAGE) command = {'command': 'ExecBuiltIn', 'parameter': 'Notification(%s,%s)' % (title.encode('utf-8'), message.encode('utf-8'))} notify_result = self._send_to_plex(command, curHost, username, password) if notify_result: result += '%s:%s' % (curHost, str(notify_result)) return result ############################################################################## # Public functions ############################################################################## def notify_snatch(self, ep_name): if sickbeard.PLEX_NOTIFY_ONSNATCH: self._notify_pmc(ep_name, common.notifyStrings[common.NOTIFY_SNATCH]) def notify_download(self, ep_name): if sickbeard.PLEX_NOTIFY_ONDOWNLOAD: self._notify_pmc(ep_name, common.notifyStrings[common.NOTIFY_DOWNLOAD]) def notify_subtitle_download(self, ep_name, lang): if sickbeard.PLEX_NOTIFY_ONSUBTITLEDOWNLOAD: self._notify_pmc(ep_name + ': ' + lang, common.notifyStrings[common.NOTIFY_SUBTITLE_DOWNLOAD]) def notify_git_update(self, new_version='??'): if sickbeard.USE_PLEX: update_text = common.notifyStrings[common.NOTIFY_GIT_UPDATE_TEXT] title = common.notifyStrings[common.NOTIFY_GIT_UPDATE] self._notify_pmc(update_text + new_version, title) def test_notify_pmc(self, host, username, password): return self._notify_pmc('This is a test notification from SickGear', 'Test', host, username, password, force=True) def test_notify_pms(self, host, username, password): return self.update_library(host=host, username=username, password=password, force=False) def update_library(self, ep_obj=None, host=None, username=None, password=None, force=True): """Handles updating the Plex Media Server host via HTTP API Plex Media Server currently only supports updating the whole video library and not a specific path. Returns: Returns None for no issue, else a string of host with connection issues """ if sickbeard.USE_PLEX and sickbeard.PLEX_UPDATE_LIBRARY: if not sickbeard.PLEX_SERVER_HOST: logger.log(u'PLEX: No Plex Media Server host specified, check your settings', logger.DEBUG) return False if not host: host = sickbeard.PLEX_SERVER_HOST if not username: username = sickbeard.PLEX_USERNAME if not password: password = sickbeard.PLEX_PASSWORD # if username and password were provided, fetch the auth token from plex.tv token_arg = '' if username and password: logger.log(u'PLEX: fetching plex.tv credentials for user: ' + username, logger.DEBUG) req = urllib2.Request('https://plex.tv/users/sign_in.xml', data='') authheader = 'Basic %s' % base64.encodestring('%s:%s' % (username, password))[:-1] req.add_header('Authorization', authheader) req.add_header('X-Plex-Device-Name', 'SickGear') req.add_header('X-Plex-Product', 'SickGear Notifier') req.add_header('X-Plex-Client-Identifier', '5f48c063eaf379a565ff56c9bb2b401e') req.add_header('X-Plex-Version', '1.0') try: response = urllib2.urlopen(req) auth_tree = etree.parse(response) token = auth_tree.findall('.//authentication-token')[0].text token_arg = '?X-Plex-Token=' + token except urllib2.URLError as e: logger.log(u'PLEX: Error fetching credentials from from plex.tv for user %s: %s' % (username, ex(e)), logger.MESSAGE) except (ValueError, IndexError) as e: logger.log(u'PLEX: Error parsing plex.tv response: ' + ex(e), logger.MESSAGE) file_location = '' if None is ep_obj else ep_obj.location host_list = [x.strip() for x in host.split(',')] hosts_all = {} hosts_match = {} hosts_failed = [] for cur_host in host_list: url = 'http://%s/library/sections%s' % (cur_host, token_arg) try: xml_tree = etree.parse(urllib.urlopen(url)) media_container = xml_tree.getroot() except IOError as e: logger.log(u'PLEX: Error while trying to contact Plex Media Server: ' + ex(e), logger.ERROR) hosts_failed.append(cur_host) continue sections = media_container.findall('.//Directory') if not sections: logger.log(u'PLEX: Plex Media Server not running on: ' + cur_host, logger.MESSAGE) hosts_failed.append(cur_host) continue for section in sections: if 'show' == section.attrib['type']: keyed_host = [(str(section.attrib['key']), cur_host)] hosts_all.update(keyed_host) if not file_location: continue for section_location in section.findall('.//Location'): section_path = re.sub(r'[/\\]+', '/', section_location.attrib['path'].lower()) section_path = re.sub(r'^(.{,2})[/\\]', '', section_path) location_path = re.sub(r'[/\\]+', '/', file_location.lower()) location_path = re.sub(r'^(.{,2})[/\\]', '', location_path) if section_path in location_path: hosts_match.update(keyed_host) hosts_try = (hosts_all.copy(), hosts_match.copy())[len(hosts_match)] host_list = [] for section_key, cur_host in hosts_try.items(): url = 'http://%s/library/sections/%s/refresh%s' % (cur_host, section_key, token_arg) try: force and urllib.urlopen(url) host_list.append(cur_host) except Exception as e: logger.log(u'PLEX: Error updating library section for Plex Media Server: ' + ex(e), logger.ERROR) hosts_failed.append(cur_host) if len(hosts_match): logger.log(u'PLEX: Updating hosts where TV section paths match the downloaded show: ' + ', '.join(set(host_list)), logger.MESSAGE) else: logger.log(u'PLEX: Updating all hosts with TV sections: ' + ', '.join(set(host_list)), logger.MESSAGE) return (', '.join(set(hosts_failed)), None)[not len(hosts_failed)] notifier = PLEXNotifier
#!/usr/bin/env python # -*- coding: utf-8 -*- # $Id$ # Last modified Wed Jul 7 20:53:01 2010 on stalker # update count: 604 # # subdms - A document management system based on subversion. # Copyright (C) 2009 Albert Thuswaldner # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. import sys # from . import lowlevel # Python 3.X # from . import frontend # Python 3.X import lowlevel import frontend import string class repository: def __init__(self): self.conf = lowlevel.config() self.cmd = lowlevel.command() self.link = lowlevel.linkname() self.proj = frontend.project() self.svncmd = lowlevel.svncmd() def checkrepotools(self): """Check if the needed repo tools exist.""" if not self.cmd.exists(self.conf.svnadmin): sys.exit("Error: Can not find svnadmin command.") if not self.cmd.exists(self.conf.svnlook): sys.exit("Error: Can not find svnlook command.") def createrepo(self): """ create repsitory and layout """ self.cmd.svncreaterepo(self.conf.repopath) # Create category dirs in repo for cat in self.conf.categories: self.proj.createcategory(cat) print("Create repository: "+self.conf.repopath) def installhooks(self): """ Install hooks in repository """ repohooklist=['pre-commit', 'post-commit'] for repohook in repohooklist: repohookpath = self.link.const_repohookpath(repohook) # Copy hooks to dir in repository and set to executable self.cmd.copyfile(self.link.const_hookfilepath(repohook), \ repohookpath) self.cmd.setexecutable(repohookpath) print("Install hook: "+repohook+" -> "+self.conf.hookspath) def installtemplates(self): """ Install templates in repository """ doc = frontend.document() # Create url for template types in repo category = self.conf.categories[1] project = "TMPL" description = 'Subdms Template' defaulttype = "GEN" doctypes = [defaulttype] doctypes.extend(self.conf.doctypes.split(",")) issue = '1' # Create template project self.proj.createproject(category, project, description, doctypes) # Add default templates to repo for tmpl in self.conf.tmpltypes: tmplnamelist = self.link.const_docnamelist(category, project, \ defaulttype, issue, tmpl) tmplfname = self.conf.gettemplate(tmpl) tmplpath = self.link.const_defaulttmplpath(tmplfname) keywords = "general, example, template" doc.adddocument(tmplpath, tmplnamelist, "default", keywords) doc.release(tmplnamelist) print("Install template: "+tmplfname+" -> "+self.conf.repourl) def walkrepo(self, path): """ Walk the repo and list the content. """ repolist= [] for p in self.svncmd.recursivels(path): repolist.append(p["name"]) return repolist def walkrepoleafs(self, path): """ Walk the repo and list all files. """ filenamelist= [] for p in self.svncmd.recursivels(path): if p["kind"] == self.svncmd.filekind: filenamelist.append(p["name"]) return filenamelist def walkreponodes(self, path): """ Walk the repo and list all paths. """ pathnamelist = [] for p in self.svncmd.recursivels(path): if p["kind"] != self.svncmd.filekind: pathnamelist.append(p["name"]) return pathnamelist def upgraderepo(self): """ Upgrade layout in repo. """ projpath = self.conf.repourl+"/P" trunkpath = self.conf.repourl+"/trunk" splitpath = trunkpath.rsplit("///")[1] # Create category dirs in repo for cat in self.conf.categories: self.proj.createcategory(cat) for old_path in self.walkreponodes(trunkpath): new_path = projpath + old_path.rsplit(splitpath)[1].upper() print(new_path) self.svncmd.mkdir(new_path, "Upgrade document path") def upgradefilename(self): """ Upgrade document file names. """ projpath = self.conf.repourl+"/P" trunkpath = self.conf.repourl+"/trunk" splitpath = trunkpath.rsplit("///")[1] for old_name in self.walkrepoleafs(trunkpath): docext = old_name.rsplit(".")[1] new_base = old_name.rsplit(splitpath)[1].rsplit(".")[0].upper() new_baselist = new_base.split("/") new_basename = "P-" + new_baselist[-1] new_path = string.join(new_baselist[:-1], "/") new_name = projpath + new_path + "/" + new_basename + \ "." + docext print(new_name) self.svncmd.server_side_copy(old_name, new_name, \ "Upgrade document name")
""" linguistics module (imdb package). This module provides functions and data to handle in a smart way languages and articles (in various languages) at the beginning of movie titles. Copyright 2009-2012 Davide Alberani <da@erlug.linux.it> 2012 Alberto Malagoli <albemala AT gmail.com> 2009 H. Turgut Uyar <uyar@tekir.org> This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA """ # List of generic articles used when the language of the title is unknown (or # we don't have information about articles in that language). # XXX: Managing titles in a lot of different languages, a function to recognize # an initial article can't be perfect; sometimes we'll stumble upon a short # word that is an article in some language, but it's not in another; in these # situations we have to choose if we want to interpret this little word # as an article or not (remember that we don't know what the original language # of the title was). # Example: 'en' is (I suppose) an article in Some Language. Unfortunately it # seems also to be a preposition in other languages (French?). # Running a script over the whole list of titles (and aliases), I've found # that 'en' is used as an article only 376 times, and as another thing 594 # times, so I've decided to _always_ consider 'en' as a non article. # # Here is a list of words that are _never_ considered as articles, complete # with the cound of times they are used in a way or another: # 'en' (376 vs 594), 'to' (399 vs 727), 'as' (198 vs 276), 'et' (79 vs 99), # 'des' (75 vs 150), 'al' (78 vs 304), 'ye' (14 vs 70), # 'da' (23 vs 298), "'n" (8 vs 12) # # I've left in the list 'i' (1939 vs 2151) and 'uno' (52 vs 56) # I'm not sure what '-al' is, and so I've left it out... # # Generic list of articles in utf-8 encoding: GENERIC_ARTICLES = ('the', 'la', 'a', 'die', 'der', 'le', 'el', "l'", 'il', 'das', 'les', 'i', 'o', 'ein', 'un', 'de', 'los', 'an', 'una', 'las', 'eine', 'den', 'het', 'gli', 'lo', 'os', 'ang', 'oi', 'az', 'een', 'ha-', 'det', 'ta', 'al-', 'mga', "un'", 'uno', 'ett', 'dem', 'egy', 'els', 'eines', '\xc3\x8f', '\xc3\x87', '\xc3\x94\xc3\xaf', '\xc3\x8f\xc3\xa9') # Lists of articles separated by language. If possible, the list should # be sorted by frequency (not very important, but...) # If you want to add a list of articles for another language, mail it # it at imdbpy-devel@lists.sourceforge.net; non-ascii articles must be utf-8 # encoded. LANG_ARTICLES = { 'English': ('the', 'a', 'an'), 'Italian': ('la', 'le', "l'", 'il', 'i', 'un', 'una', 'gli', 'lo', "un'", 'uno'), 'Spanish': ('la', 'le', 'el', 'les', 'un', 'los', 'una', 'uno', 'unos', 'unas'), 'Portuguese': ('a', 'as', 'o', 'os', 'um', 'uns', 'uma', 'umas'), 'Turkish': (), # Some languages doesn't have articles. } LANG_ARTICLESget = LANG_ARTICLES.get # Maps a language to countries where it is the main language. # If you want to add an entry for another language or country, mail it at # imdbpy-devel@lists.sourceforge.net . LANG_COUNTRIES = { 'English': ('Canada', 'Swaziland', 'Ghana', 'St. Lucia', 'Liberia', 'Jamaica', 'Bahamas', 'New Zealand', 'Lesotho', 'Kenya', 'Solomon Islands', 'United States', 'South Africa', 'St. Vincent and the Grenadines', 'Fiji', 'UK', 'Nigeria', 'Australia', 'USA', 'St. Kitts and Nevis', 'Belize', 'Sierra Leone', 'Gambia', 'Namibia', 'Micronesia', 'Kiribati', 'Grenada', 'Antigua and Barbuda', 'Barbados', 'Malta', 'Zimbabwe', 'Ireland', 'Uganda', 'Trinidad and Tobago', 'South Sudan', 'Guyana', 'Botswana', 'United Kingdom', 'Zambia'), 'Italian': ('Italy', 'San Marino', 'Vatican City'), 'Spanish': ('Spain', 'Mexico', 'Argentina', 'Bolivia', 'Guatemala', 'Uruguay', 'Peru', 'Cuba', 'Dominican Republic', 'Panama', 'Costa Rica', 'Ecuador', 'El Salvador', 'Chile', 'Equatorial Guinea', 'Spain', 'Colombia', 'Nicaragua', 'Venezuela', 'Honduras', 'Paraguay'), 'French': ('Cameroon', 'Burkina Faso', 'Dominica', 'Gabon', 'Monaco', 'France', "Cote d'Ivoire", 'Benin', 'Togo', 'Central African Republic', 'Mali', 'Niger', 'Congo, Republic of', 'Guinea', 'Congo, Democratic Republic of the', 'Luxembourg', 'Haiti', 'Chad', 'Burundi', 'Madagascar', 'Comoros', 'Senegal'), 'Portuguese': ('Portugal', 'Brazil', 'Sao Tome and Principe', 'Cape Verde', 'Angola', 'Mozambique', 'Guinea-Bissau'), 'German': ('Liechtenstein', 'Austria', 'West Germany', 'Switzerland', 'East Germany', 'Germany'), 'Arabic': ('Saudi Arabia', 'Kuwait', 'Jordan', 'Oman', 'Yemen', 'United Arab Emirates', 'Mauritania', 'Lebanon', 'Bahrain', 'Libya', 'Palestinian State (proposed)', 'Qatar', 'Algeria', 'Morocco', 'Iraq', 'Egypt', 'Djibouti', 'Sudan', 'Syria', 'Tunisia'), 'Turkish': ('Turkey', 'Azerbaijan'), 'Swahili': ('Tanzania',), 'Swedish': ('Sweden',), 'Icelandic': ('Iceland',), 'Estonian': ('Estonia',), 'Romanian': ('Romania',), 'Samoan': ('Samoa',), 'Slovenian': ('Slovenia',), 'Tok Pisin': ('Papua New Guinea',), 'Palauan': ('Palau',), 'Macedonian': ('Macedonia',), 'Hindi': ('India',), 'Dutch': ('Netherlands', 'Belgium', 'Suriname'), 'Marshallese': ('Marshall Islands',), 'Korean': ('Korea, North', 'Korea, South', 'North Korea', 'South Korea'), 'Vietnamese': ('Vietnam',), 'Danish': ('Denmark',), 'Khmer': ('Cambodia',), 'Lao': ('Laos',), 'Somali': ('Somalia',), 'Filipino': ('Philippines',), 'Hungarian': ('Hungary',), 'Ukrainian': ('Ukraine',), 'Bosnian': ('Bosnia and Herzegovina',), 'Georgian': ('Georgia',), 'Lithuanian': ('Lithuania',), 'Malay': ('Brunei',), 'Tetum': ('East Timor',), 'Norwegian': ('Norway',), 'Armenian': ('Armenia',), 'Russian': ('Russia',), 'Slovak': ('Slovakia',), 'Thai': ('Thailand',), 'Croatian': ('Croatia',), 'Turkmen': ('Turkmenistan',), 'Nepali': ('Nepal',), 'Finnish': ('Finland',), 'Uzbek': ('Uzbekistan',), 'Albanian': ('Albania', 'Kosovo'), 'Hebrew': ('Israel',), 'Bulgarian': ('Bulgaria',), 'Greek': ('Cyprus', 'Greece'), 'Burmese': ('Myanmar',), 'Latvian': ('Latvia',), 'Serbian': ('Serbia',), 'Afar': ('Eritrea',), 'Catalan': ('Andorra',), 'Chinese': ('China', 'Taiwan'), 'Czech': ('Czech Republic', 'Czechoslovakia'), 'Bislama': ('Vanuatu',), 'Japanese': ('Japan',), 'Kinyarwanda': ('Rwanda',), 'Amharic': ('Ethiopia',), 'Persian': ('Afghanistan', 'Iran'), 'Tajik': ('Tajikistan',), 'Mongolian': ('Mongolia',), 'Dzongkha': ('Bhutan',), 'Urdu': ('Pakistan',), 'Polish': ('Poland',), 'Sinhala': ('Sri Lanka',), } # Maps countries to their main language. COUNTRY_LANG = {} for lang in LANG_COUNTRIES: for country in LANG_COUNTRIES[lang]: COUNTRY_LANG[country] = lang def toUnicode(articles): """Convert a list of articles utf-8 encoded to unicode strings.""" return tuple([art.decode('utf_8') for art in articles]) def toDicts(articles): """Given a list of utf-8 encoded articles, build two dictionary (one utf-8 encoded and another one with unicode keys) for faster matches.""" uArticles = toUnicode(articles) return dict([(x, x) for x in articles]), dict([(x, x) for x in uArticles]) def addTrailingSpace(articles): """From the given list of utf-8 encoded articles, return two lists (one utf-8 encoded and another one in unicode) where a space is added at the end - if the last char is not ' or -.""" _spArticles = [] _spUnicodeArticles = [] for article in articles: if article[-1] not in ("'", '-'): article += ' ' _spArticles.append(article) _spUnicodeArticles.append(article.decode('utf_8')) return _spArticles, _spUnicodeArticles # Caches. _ART_CACHE = {} _SP_ART_CACHE = {} def articlesDictsForLang(lang): """Return dictionaries of articles specific for the given language, or the default one if the language is not known.""" if lang in _ART_CACHE: return _ART_CACHE[lang] artDicts = toDicts(LANG_ARTICLESget(lang, GENERIC_ARTICLES)) _ART_CACHE[lang] = artDicts return artDicts def spArticlesForLang(lang): """Return lists of articles (plus optional spaces) specific for the given language, or the default one if the language is not known.""" if lang in _SP_ART_CACHE: return _SP_ART_CACHE[lang] spArticles = addTrailingSpace(LANG_ARTICLESget(lang, GENERIC_ARTICLES)) _SP_ART_CACHE[lang] = spArticles return spArticles
#!/usr/bin/env python3 # Copyright (c) 2014-2017 The Bitcoin Core developers # Distributed under the MIT software license, see the accompanying # file COPYING or http://www.opensource.org/licenses/mit-license.php. """ ZMQ example using python3's asyncio Bitcoin should be started with the command line arguments: bitcoind -testnet -daemon \ -zmqpubrawtx=tcp://127.0.0.1:28332 \ -zmqpubrawblock=tcp://127.0.0.1:28332 \ -zmqpubhashtx=tcp://127.0.0.1:28332 \ -zmqpubhashblock=tcp://127.0.0.1:28332 We use the asyncio library here. `self.handle()` installs itself as a future at the end of the function. Since it never returns with the event loop having an empty stack of futures, this creates an infinite loop. An alternative is to wrap the contents of `handle` inside `while True`. A blocking example using python 2.7 can be obtained from the git history: https://github.com/bitcoin/bitcoin/blob/37a7fe9e440b83e2364d5498931253937abe9294/contrib/zmq/zmq_sub.py """ import binascii import asyncio import zmq import zmq.asyncio import signal import struct import sys if not (sys.version_info.major >= 3 and sys.version_info.minor >= 5): print("This example only works with Python 3.5 and greater") sys.exit(1) port = 28332 class ZMQHandler(): def __init__(self): self.loop = zmq.asyncio.install() self.zmqContext = zmq.asyncio.Context() self.zmqSubSocket = self.zmqContext.socket(zmq.SUB) self.zmqSubSocket.setsockopt_string(zmq.SUBSCRIBE, "hashblock") self.zmqSubSocket.setsockopt_string(zmq.SUBSCRIBE, "hashtx") self.zmqSubSocket.setsockopt_string(zmq.SUBSCRIBE, "rawblock") self.zmqSubSocket.setsockopt_string(zmq.SUBSCRIBE, "rawtx") self.zmqSubSocket.connect("tcp://127.0.0.1:%i" % port) async def handle(self) : msg = await self.zmqSubSocket.recv_multipart() topic = msg[0] body = msg[1] sequence = "Unknown" if len(msg[-1]) == 4: msgSequence = struct.unpack('<I', msg[-1])[-1] sequence = str(msgSequence) if topic == b"hashblock": print('- HASH BLOCK ('+sequence+') -') print(binascii.hexlify(body)) elif topic == b"hashtx": print('- HASH TX ('+sequence+') -') print(binascii.hexlify(body)) elif topic == b"rawblock": print('- RAW BLOCK HEADER ('+sequence+') -') print(binascii.hexlify(body[:80])) elif topic == b"rawtx": print('- RAW TX ('+sequence+') -') print(binascii.hexlify(body)) # schedule ourselves to receive the next message asyncio.ensure_future(self.handle()) def start(self): self.loop.add_signal_handler(signal.SIGINT, self.stop) self.loop.create_task(self.handle()) self.loop.run_forever() def stop(self): self.loop.stop() self.zmqContext.destroy() daemon = ZMQHandler() daemon.start()
# -*- coding: utf-8 -*- ############################################################################## # # Copyright (C) 2014 Agile Business Group (http://www.agilebg.com) # @author Lorenzo Battistini <lorenzo.battistini@agilebg.com> # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## from openerp import models, api class better_zip_geonames_import(models.TransientModel): _inherit = 'better.zip.geonames.import' @api.model def select_or_create_state( self, row, country_id, code_row_index=4, name_row_index=3 ): return super(better_zip_geonames_import, self).select_or_create_state( row, country_id, code_row_index=6, name_row_index=5)
# Copyright (c) 2012 The Chromium Authors. All rights reserved. # Use of this source code is governed by a BSD-style license that can be # found in the LICENSE file. """Layout tests module that is necessary for the layout analyzer. Layout tests are stored in an SVN repository and LayoutTestCaseManager collects these layout test cases (including description). """ import copy import csv import locale import re import sys import urllib2 import pysvn # LayoutTests SVN root location. DEFAULT_LAYOUTTEST_LOCATION = ( 'http://src.chromium.org/blink/trunk/LayoutTests/') # LayoutTests SVN view link DEFAULT_LAYOUTTEST_SVN_VIEW_LOCATION = ( 'http://src.chromium.org/viewvc/blink/trunk/LayoutTests/') # When parsing the test HTML file and finding the test description, # this script tries to find the test description using sentences # starting with these keywords. This is adhoc but it is the only way # since there is no standard for writing test description. KEYWORDS_FOR_TEST_DESCRIPTION = ['This test', 'Tests that', 'Test '] # If cannot find the keywords, this script tries to find test case # description by the following tags. TAGS_FOR_TEST_DESCRIPTION = ['title', 'p', 'div'] # If cannot find the tags, this script tries to find the test case # description in the sentence containing following words. KEYWORD_FOR_TEST_DESCRIPTION_FAIL_SAFE = ['PASSED ', 'PASS:'] class LayoutTests(object): """A class to store test names in layout tests. The test names (including regular expression patterns) are read from a CSV file and used for getting layout test names from repository. """ def __init__(self, layouttest_root_path=DEFAULT_LAYOUTTEST_LOCATION, parent_location_list=None, filter_names=None, recursion=False): """Initialize LayoutTests using root and CSV file. Args: layouttest_root_path: A location string where layout tests are stored. parent_location_list: A list of parent directories that are needed for getting layout tests. filter_names: A list of test name patterns that are used for filtering test names (e.g., media/*.html). recursion: a boolean indicating whether the test names are sought recursively. """ if layouttest_root_path.startswith('http://'): name_map = self.GetLayoutTestNamesFromSVN(parent_location_list, layouttest_root_path, recursion) else: # TODO(imasaki): support other forms such as CSV for reading test names. pass self.name_map = copy.copy(name_map) if filter_names: # Filter names. for lt_name in name_map.iterkeys(): match = False for filter_name in filter_names: if re.search(filter_name, lt_name): match = True break if not match: del self.name_map[lt_name] # We get description only for the filtered names. for lt_name in self.name_map.iterkeys(): self.name_map[lt_name] = 'No description available' @staticmethod def ExtractTestDescription(txt): """Extract the description description from test code in HTML. Currently, we have 4 rules described in the code below. (This example falls into rule 1): <p> This tests the intrinsic size of a video element is the default 300,150 before metadata is loaded, and 0,0 after metadata is loaded for an audio-only file. </p> The strategy is very adhoc since the original test case files (in HTML format) do not have standard way to store test description. Args: txt: A HTML text which may or may not contain test description. Returns: A string that contains test description. Returns 'UNKNOWN' if the test description is not found. """ # (1) Try to find test description that contains keywords such as # 'test that' and surrounded by p tag. # This is the most common case. for keyword in KEYWORDS_FOR_TEST_DESCRIPTION: # Try to find <p> and </p>. pattern = r'<p>(.*' + keyword + '.*)</p>' matches = re.search(pattern, txt) if matches is not None: return matches.group(1).strip() # (2) Try to find it by using more generic keywords such as 'PASS' etc. for keyword in KEYWORD_FOR_TEST_DESCRIPTION_FAIL_SAFE: # Try to find new lines. pattern = r'\n(.*' + keyword + '.*)\n' matches = re.search(pattern, txt) if matches is not None: # Remove 'p' tag. text = matches.group(1).strip() return text.replace('<p>', '').replace('</p>', '') # (3) Try to find it by using HTML tag such as title. for tag in TAGS_FOR_TEST_DESCRIPTION: pattern = r'<' + tag + '>(.*)</' + tag + '>' matches = re.search(pattern, txt) if matches is not None: return matches.group(1).strip() # (4) Try to find it by using test description and remove 'p' tag. for keyword in KEYWORDS_FOR_TEST_DESCRIPTION: # Try to find <p> and </p>. pattern = r'\n(.*' + keyword + '.*)\n' matches = re.search(pattern, txt) if matches is not None: # Remove 'p' tag. text = matches.group(1).strip() return text.replace('<p>', '').replace('</p>', '') # (5) cannot find test description using existing rules. return 'UNKNOWN' @staticmethod def GetLayoutTestNamesFromSVN(parent_location_list, layouttest_root_path, recursion): """Get LayoutTest names from SVN. Args: parent_location_list: a list of locations of parent directories. This is used when getting layout tests using PySVN.list(). layouttest_root_path: the root path of layout tests directory. recursion: a boolean indicating whether the test names are sought recursively. Returns: a map containing test names as keys for de-dupe. """ client = pysvn.Client() # Get directory structure in the repository SVN. name_map = {} for parent_location in parent_location_list: if parent_location.endswith('/'): full_path = layouttest_root_path + parent_location try: file_list = client.list(full_path, recurse=recursion) for file_name in file_list: if sys.stdout.isatty(): default_encoding = sys.stdout.encoding else: default_encoding = locale.getpreferredencoding() file_name = file_name[0].repos_path.encode(default_encoding) # Remove the word '/truck/LayoutTests'. file_name = file_name.replace('/trunk/LayoutTests/', '') if file_name.endswith('.html'): name_map[file_name] = True except: print 'Unable to list tests in %s.' % full_path return name_map @staticmethod def GetLayoutTestNamesFromCSV(csv_file_path): """Get layout test names from CSV file. Args: csv_file_path: the path for the CSV file containing test names (including regular expression patterns). The CSV file content has one column and each row contains a test name. Returns: a list of test names in string. """ file_object = file(csv_file_path, 'r') reader = csv.reader(file_object) names = [row[0] for row in reader] file_object.close() return names @staticmethod def GetParentDirectoryList(names): """Get parent directory list from test names. Args: names: a list of test names. The test names also have path information as well (e.g., media/video-zoom.html). Returns: a list of parent directories for the given test names. """ pd_map = {} for name in names: p_dir = name[0:name.rfind('/') + 1] pd_map[p_dir] = True return list(pd_map.iterkeys()) def JoinWithTestExpectation(self, test_expectations): """Join layout tests with the test expectation file using test name as key. Args: test_expectations: a test expectations object. Returns: test_info_map contains test name as key and another map as value. The other map contains test description and the test expectation information which contains keyword (e.g., 'GPU') as key (we do not care about values). The map data structure is used since we have to look up these keywords several times. """ test_info_map = {} for (lt_name, desc) in self.name_map.items(): test_info_map[lt_name] = {} test_info_map[lt_name]['desc'] = desc for (te_name, te_info) in ( test_expectations.all_test_expectation_info.items()): if te_name == lt_name or ( te_name in lt_name and te_name.endswith('/')): # Only keep the first match when found. test_info_map[lt_name]['te_info'] = te_info break return test_info_map
# (c) 2012, Michael DeHaan <michael.dehaan@gmail.com> # # This file is part of Ansible # # Ansible is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Ansible is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Ansible. If not, see <http://www.gnu.org/licenses/>. from __future__ import (absolute_import, division, print_function) __metaclass__ = type ######################################################## import datetime import os import platform import random import shutil import socket import sys import time from ansible.errors import AnsibleOptionsError from ansible.cli import CLI from ansible.module_utils._text import to_native from ansible.plugins import module_loader from ansible.utils.cmd_functions import run_cmd try: from __main__ import display except ImportError: from ansible.utils.display import Display display = Display() ######################################################## class PullCLI(CLI): ''' is used to up a remote copy of ansible on each managed node, each set to run via cron and update playbook source via a source repository. This inverts the default *push* architecture of ansible into a *pull* architecture, which has near-limitless scaling potential. The setup playbook can be tuned to change the cron frequency, logging locations, and parameters to ansible-pull. This is useful both for extreme scale-out as well as periodic remediation. Usage of the 'fetch' module to retrieve logs from ansible-pull runs would be an excellent way to gather and analyze remote logs from ansible-pull. ''' DEFAULT_REPO_TYPE = 'git' DEFAULT_PLAYBOOK = 'local.yml' PLAYBOOK_ERRORS = { 1: 'File does not exist', 2: 'File is not readable' } SUPPORTED_REPO_MODULES = ['git'] ARGUMENTS = { 'playbook.yml': 'The name of one the YAML format files to run as an Ansible playbook.' 'This can be a relative path within the checkout. By default, Ansible will' "look for a playbook based on the host's fully-qualified domain name," 'on the host hostname and finally a playbook named *local.yml*.', } def parse(self): ''' create an options parser for bin/ansible ''' self.parser = CLI.base_parser( usage='%prog -U <repository> [options] [<playbook.yml>]', connect_opts=True, vault_opts=True, runtask_opts=True, subset_opts=True, inventory_opts=True, module_opts=True, runas_prompt_opts=True, desc="pulls playbooks from a VCS repo and executes them for the local host", ) # options unique to pull self.parser.add_option('--purge', default=False, action='store_true', help='purge checkout after playbook run') self.parser.add_option('-o', '--only-if-changed', dest='ifchanged', default=False, action='store_true', help='only run the playbook if the repository has been updated') self.parser.add_option('-s', '--sleep', dest='sleep', default=None, help='sleep for random interval (between 0 and n number of seconds) before starting. This is a useful way to disperse git requests') self.parser.add_option('-f', '--force', dest='force', default=False, action='store_true', help='run the playbook even if the repository could not be updated') self.parser.add_option('-d', '--directory', dest='dest', default=None, help='directory to checkout repository to') self.parser.add_option('-U', '--url', dest='url', default=None, help='URL of the playbook repository') self.parser.add_option('--full', dest='fullclone', action='store_true', help='Do a full clone, instead of a shallow one.') self.parser.add_option('-C', '--checkout', dest='checkout', help='branch/tag/commit to checkout. Defaults to behavior of repository module.') self.parser.add_option('--accept-host-key', default=False, dest='accept_host_key', action='store_true', help='adds the hostkey for the repo url if not already added') self.parser.add_option('-m', '--module-name', dest='module_name', default=self.DEFAULT_REPO_TYPE, help='Repository module name, which ansible will use to check out the repo. Default is %s.' % self.DEFAULT_REPO_TYPE) self.parser.add_option('--verify-commit', dest='verify', default=False, action='store_true', help='verify GPG signature of checked out commit, if it fails abort running the playbook.' ' This needs the corresponding VCS module to support such an operation') self.parser.add_option('--clean', dest='clean', default=False, action='store_true', help='modified files in the working repository will be discarded') self.parser.add_option('--track-subs', dest='tracksubs', default=False, action='store_true', help='submodules will track the latest changes. This is equivalent to specifying the --remote flag to git submodule update') # for pull we don't wan't a default self.parser.set_defaults(inventory=None) super(PullCLI, self).parse() if not self.options.dest: hostname = socket.getfqdn() # use a hostname dependent directory, in case of $HOME on nfs self.options.dest = os.path.join('~/.ansible/pull', hostname) self.options.dest = os.path.expandvars(os.path.expanduser(self.options.dest)) if self.options.sleep: try: secs = random.randint(0,int(self.options.sleep)) self.options.sleep = secs except ValueError: raise AnsibleOptionsError("%s is not a number." % self.options.sleep) if not self.options.url: raise AnsibleOptionsError("URL for repository not specified, use -h for help") if self.options.module_name not in self.SUPPORTED_REPO_MODULES: raise AnsibleOptionsError("Unsuported repo module %s, choices are %s" % (self.options.module_name, ','.join(self.SUPPORTED_REPO_MODULES))) display.verbosity = self.options.verbosity self.validate_conflicts(vault_opts=True) def run(self): ''' use Runner lib to do SSH things ''' super(PullCLI, self).run() # log command line now = datetime.datetime.now() display.display(now.strftime("Starting Ansible Pull at %F %T")) display.display(' '.join(sys.argv)) # Build Checkout command # Now construct the ansible command node = platform.node() host = socket.getfqdn() limit_opts = 'localhost,%s,127.0.0.1' % ','.join(set([host, node, host.split('.')[0], node.split('.')[0]])) base_opts = '-c local ' if self.options.verbosity > 0: base_opts += ' -%s' % ''.join([ "v" for x in range(0, self.options.verbosity) ]) # Attempt to use the inventory passed in as an argument # It might not yet have been downloaded so use localhost as default if not self.options.inventory or ( ',' not in self.options.inventory and not os.path.exists(self.options.inventory)): inv_opts = 'localhost,' else: inv_opts = self.options.inventory #FIXME: enable more repo modules hg/svn? if self.options.module_name == 'git': repo_opts = "name=%s dest=%s" % (self.options.url, self.options.dest) if self.options.checkout: repo_opts += ' version=%s' % self.options.checkout if self.options.accept_host_key: repo_opts += ' accept_hostkey=yes' if self.options.private_key_file: repo_opts += ' key_file=%s' % self.options.private_key_file if self.options.verify: repo_opts += ' verify_commit=yes' if self.options.clean: repo_opts += ' force=yes' if self.options.tracksubs: repo_opts += ' track_submodules=yes' if not self.options.fullclone: repo_opts += ' depth=1' path = module_loader.find_plugin(self.options.module_name) if path is None: raise AnsibleOptionsError(("module '%s' not found.\n" % self.options.module_name)) bin_path = os.path.dirname(os.path.abspath(sys.argv[0])) # hardcode local and inventory/host as this is just meant to fetch the repo cmd = '%s/ansible -i "%s" %s -m %s -a "%s" all -l "%s"' % (bin_path, inv_opts, base_opts, self.options.module_name, repo_opts, limit_opts) for ev in self.options.extra_vars: cmd += ' -e "%s"' % ev # Nap? if self.options.sleep: display.display("Sleeping for %d seconds..." % self.options.sleep) time.sleep(self.options.sleep) # RUN the Checkout command display.debug("running ansible with VCS module to checkout repo") display.vvvv('EXEC: %s' % cmd) rc, out, err = run_cmd(cmd, live=True) if rc != 0: if self.options.force: display.warning("Unable to update repository. Continuing with (forced) run of playbook.") else: return rc elif self.options.ifchanged and '"changed": true' not in out: display.display("Repository has not changed, quitting.") return 0 playbook = self.select_playbook(self.options.dest) if playbook is None: raise AnsibleOptionsError("Could not find a playbook to run.") # Build playbook command cmd = '%s/ansible-playbook %s %s' % (bin_path, base_opts, playbook) if self.options.vault_password_file: cmd += " --vault-password-file=%s" % self.options.vault_password_file if self.options.inventory: cmd += ' -i "%s"' % self.options.inventory for ev in self.options.extra_vars: cmd += ' -e "%s"' % ev if self.options.ask_sudo_pass or self.options.ask_su_pass or self.options.become_ask_pass: cmd += ' --ask-become-pass' if self.options.skip_tags: cmd += ' --skip-tags "%s"' % to_native(u','.join(self.options.skip_tags)) if self.options.tags: cmd += ' -t "%s"' % to_native(u','.join(self.options.tags)) if self.options.subset: cmd += ' -l "%s"' % self.options.subset else: cmd += ' -l "%s"' % limit_opts os.chdir(self.options.dest) # RUN THE PLAYBOOK COMMAND display.debug("running ansible-playbook to do actual work") display.debug('EXEC: %s' % cmd) rc, out, err = run_cmd(cmd, live=True) if self.options.purge: os.chdir('/') try: shutil.rmtree(self.options.dest) except Exception as e: display.error("Failed to remove %s: %s" % (self.options.dest, str(e))) return rc def try_playbook(self, path): if not os.path.exists(path): return 1 if not os.access(path, os.R_OK): return 2 return 0 def select_playbook(self, path): playbook = None if len(self.args) > 0 and self.args[0] is not None: playbook = os.path.join(path, self.args[0]) rc = self.try_playbook(playbook) if rc != 0: display.warning("%s: %s" % (playbook, self.PLAYBOOK_ERRORS[rc])) return None return playbook else: fqdn = socket.getfqdn() hostpb = os.path.join(path, fqdn + '.yml') shorthostpb = os.path.join(path, fqdn.split('.')[0] + '.yml') localpb = os.path.join(path, self.DEFAULT_PLAYBOOK) errors = [] for pb in [hostpb, shorthostpb, localpb]: rc = self.try_playbook(pb) if rc == 0: playbook = pb break else: errors.append("%s: %s" % (pb, self.PLAYBOOK_ERRORS[rc])) if playbook is None: display.warning("\n".join(errors)) return playbook
# Copyright (c) 2009-2012 Mitch Garnaat http://garnaat.org/ # Copyright (c) 2012 Amazon.com, Inc. or its affiliates. All Rights Reserved # # Permission is hereby granted, free of charge, to any person obtaining a # copy of this software and associated documentation files (the # "Software"), to deal in the Software without restriction, including # without limitation the rights to use, copy, modify, merge, publish, dis- # tribute, sublicense, and/or sell copies of the Software, and to permit # persons to whom the Software is furnished to do so, subject to the fol- # lowing conditions: # # The above copyright notice and this permission notice shall be included # in all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS # OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL- # ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT # SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, # WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS # IN THE SOFTWARE. # class BlockDeviceType(object): """ Represents parameters for a block device. """ def __init__(self, connection=None, ephemeral_name=None, no_device=False, volume_id=None, snapshot_id=None, status=None, attach_time=None, delete_on_termination=False, size=None, volume_type=None, iops=None, encrypted=None): self.connection = connection self.ephemeral_name = ephemeral_name self.no_device = no_device self.volume_id = volume_id self.snapshot_id = snapshot_id self.status = status self.attach_time = attach_time self.delete_on_termination = delete_on_termination self.size = size self.volume_type = volume_type self.iops = iops self.encrypted = encrypted def startElement(self, name, attrs, connection): pass def endElement(self, name, value, connection): lname = name.lower() if name == 'volumeId': self.volume_id = value elif lname == 'virtualname': self.ephemeral_name = value elif lname == 'nodevice': self.no_device = (value == 'true') elif lname == 'snapshotid': self.snapshot_id = value elif lname == 'volumesize': self.size = int(value) elif lname == 'status': self.status = value elif lname == 'attachtime': self.attach_time = value elif lname == 'deleteontermination': self.delete_on_termination = (value == 'true') elif lname == 'volumetype': self.volume_type = value elif lname == 'iops': self.iops = int(value) elif lname == 'encrypted': self.encrypted = (value == 'true') else: setattr(self, name, value) # for backwards compatibility EBSBlockDeviceType = BlockDeviceType class BlockDeviceMapping(dict): """ Represents a collection of BlockDeviceTypes when creating ec2 instances. Example: dev_sda1 = BlockDeviceType() dev_sda1.size = 100 # change root volume to 100GB instead of default bdm = BlockDeviceMapping() bdm['/dev/sda1'] = dev_sda1 reservation = image.run(..., block_device_map=bdm, ...) """ def __init__(self, connection=None): """ :type connection: :class:`boto.ec2.EC2Connection` :param connection: Optional connection. """ dict.__init__(self) self.connection = connection self.current_name = None self.current_value = None def startElement(self, name, attrs, connection): lname = name.lower() if lname in ['ebs', 'virtualname']: self.current_value = BlockDeviceType(self) return self.current_value def endElement(self, name, value, connection): lname = name.lower() if lname in ['device', 'devicename']: self.current_name = value elif lname in ['item', 'member']: self[self.current_name] = self.current_value def ec2_build_list_params(self, params, prefix=''): pre = '%sBlockDeviceMapping' % prefix return self._build_list_params(params, prefix=pre) def autoscale_build_list_params(self, params, prefix=''): pre = '%sBlockDeviceMappings.member' % prefix return self._build_list_params(params, prefix=pre) def _build_list_params(self, params, prefix=''): i = 1 for dev_name in self: pre = '%s.%d' % (prefix, i) params['%s.DeviceName' % pre] = dev_name block_dev = self[dev_name] if block_dev.ephemeral_name: params['%s.VirtualName' % pre] = block_dev.ephemeral_name else: if block_dev.no_device: params['%s.NoDevice' % pre] = '' else: if block_dev.snapshot_id: params['%s.Ebs.SnapshotId' % pre] = block_dev.snapshot_id if block_dev.size: params['%s.Ebs.VolumeSize' % pre] = block_dev.size if block_dev.delete_on_termination: params['%s.Ebs.DeleteOnTermination' % pre] = 'true' else: params['%s.Ebs.DeleteOnTermination' % pre] = 'false' if block_dev.volume_type: params['%s.Ebs.VolumeType' % pre] = block_dev.volume_type if block_dev.iops is not None: params['%s.Ebs.Iops' % pre] = block_dev.iops # The encrypted flag (even if False) cannot be specified for the root EBS # volume. if block_dev.encrypted is not None: if block_dev.encrypted: params['%s.Ebs.Encrypted' % pre] = 'true' else: params['%s.Ebs.Encrypted' % pre] = 'false' i += 1
# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com> # # This file is part of Ansible # # Ansible is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Ansible is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Ansible. If not, see <http://www.gnu.org/licenses/>. # Make coding more python3-ish from __future__ import (absolute_import, division, print_function) __metaclass__ = type from yaml.constructor import SafeConstructor, ConstructorError from yaml.nodes import MappingNode from ansible.module_utils._text import to_bytes from ansible.parsing.yaml.objects import AnsibleMapping, AnsibleSequence, AnsibleUnicode from ansible.parsing.yaml.objects import AnsibleVaultEncryptedUnicode from ansible.utils.unsafe_proxy import wrap_var from ansible.parsing.vault import VaultLib from ansible.utils.display import Display display = Display() class AnsibleConstructor(SafeConstructor): def __init__(self, file_name=None, vault_secrets=None): self._ansible_file_name = file_name super(AnsibleConstructor, self).__init__() self._vaults = {} self.vault_secrets = vault_secrets or [] self._vaults['default'] = VaultLib(secrets=self.vault_secrets) def construct_yaml_map(self, node): data = AnsibleMapping() yield data value = self.construct_mapping(node) data.update(value) data.ansible_pos = self._node_position_info(node) def construct_mapping(self, node, deep=False): # Most of this is from yaml.constructor.SafeConstructor. We replicate # it here so that we can warn users when they have duplicate dict keys # (pyyaml silently allows overwriting keys) if not isinstance(node, MappingNode): raise ConstructorError(None, None, "expected a mapping node, but found %s" % node.id, node.start_mark) self.flatten_mapping(node) mapping = AnsibleMapping() # Add our extra information to the returned value mapping.ansible_pos = self._node_position_info(node) for key_node, value_node in node.value: key = self.construct_object(key_node, deep=deep) try: hash(key) except TypeError as exc: raise ConstructorError("while constructing a mapping", node.start_mark, "found unacceptable key (%s)" % exc, key_node.start_mark) if key in mapping: display.warning(u'While constructing a mapping from {1}, line {2}, column {3}, found a duplicate dict key ({0}).' u' Using last defined value only.'.format(key, *mapping.ansible_pos)) value = self.construct_object(value_node, deep=deep) mapping[key] = value return mapping def construct_yaml_str(self, node): # Override the default string handling function # to always return unicode objects value = self.construct_scalar(node) ret = AnsibleUnicode(value) ret.ansible_pos = self._node_position_info(node) return ret def construct_vault_encrypted_unicode(self, node): value = self.construct_scalar(node) b_ciphertext_data = to_bytes(value) # could pass in a key id here to choose the vault to associate with # TODO/FIXME: plugin vault selector vault = self._vaults['default'] if vault.secrets is None: raise ConstructorError(context=None, context_mark=None, problem="found !vault but no vault password provided", problem_mark=node.start_mark, note=None) ret = AnsibleVaultEncryptedUnicode(b_ciphertext_data) ret.vault = vault return ret def construct_yaml_seq(self, node): data = AnsibleSequence() yield data data.extend(self.construct_sequence(node)) data.ansible_pos = self._node_position_info(node) def construct_yaml_unsafe(self, node): return wrap_var(self.construct_yaml_str(node)) def _node_position_info(self, node): # the line number where the previous token has ended (plus empty lines) # Add one so that the first line is line 1 rather than line 0 column = node.start_mark.column + 1 line = node.start_mark.line + 1 # in some cases, we may have pre-read the data and then # passed it to the load() call for YAML, in which case we # want to override the default datasource (which would be # '<string>') to the actual filename we read in datasource = self._ansible_file_name or node.start_mark.name return (datasource, line, column) AnsibleConstructor.add_constructor( u'tag:yaml.org,2002:map', AnsibleConstructor.construct_yaml_map) AnsibleConstructor.add_constructor( u'tag:yaml.org,2002:python/dict', AnsibleConstructor.construct_yaml_map) AnsibleConstructor.add_constructor( u'tag:yaml.org,2002:str', AnsibleConstructor.construct_yaml_str) AnsibleConstructor.add_constructor( u'tag:yaml.org,2002:python/unicode', AnsibleConstructor.construct_yaml_str) AnsibleConstructor.add_constructor( u'tag:yaml.org,2002:seq', AnsibleConstructor.construct_yaml_seq) AnsibleConstructor.add_constructor( u'!unsafe', AnsibleConstructor.construct_yaml_unsafe) AnsibleConstructor.add_constructor( u'!vault', AnsibleConstructor.construct_vault_encrypted_unicode) AnsibleConstructor.add_constructor(u'!vault-encrypted', AnsibleConstructor.construct_vault_encrypted_unicode)
import WebIDL def WebIDLTest(parser, harness): parser.parse(""" interface TestIncompleteTypes { attribute FooInterface attr1; FooInterface method1(FooInterface arg); }; interface FooInterface { }; """) results = parser.finish() harness.ok(True, "TestIncompleteTypes interface parsed without error.") harness.check(len(results), 2, "Should be two productions.") iface = results[0] harness.ok(isinstance(iface, WebIDL.IDLInterface), "Should be an IDLInterface") harness.check(iface.identifier.QName(), "::TestIncompleteTypes", "Interface has the right QName") harness.check(iface.identifier.name, "TestIncompleteTypes", "Interface has the right name") harness.check(len(iface.members), 2, "Expect 2 members") attr = iface.members[0] harness.ok(isinstance(attr, WebIDL.IDLAttribute), "Should be an IDLAttribute") method = iface.members[1] harness.ok(isinstance(method, WebIDL.IDLMethod), "Should be an IDLMethod") harness.check(attr.identifier.QName(), "::TestIncompleteTypes::attr1", "Attribute has the right QName") harness.check(attr.type.name, "FooInterface", "Previously unresolved type has the right name") harness.check(method.identifier.QName(), "::TestIncompleteTypes::method1", "Attribute has the right QName") (returnType, args) = method.signatures()[0] harness.check(returnType.name, "FooInterface", "Previously unresolved type has the right name") harness.check(args[0].type.name, "FooInterface", "Previously unresolved type has the right name")
from matplotlib import pyplot from os import listdir def is_numeric(x): try: float(x) except ValueError: return False return True avg_lap_time = {} avg_pos = {} avg_speed = {} avg_top = {} total_rescued = {} tests = len(listdir('../../batch'))-1 for file in listdir('../../batch'): if (file == '.DS_Store'): continue f = open('../../batch/'+file,'r') ''' name_index = file.find('.') kart_name = str(file[:name_index]) first = file.find('.',name_index+1) track_name = file[name_index+1:first] second = file.find('.',first+1) run = int(file[first+1:second]) ''' track_name = "snowmountain" kart_names = ["gnu", "sara", "tux", "elephpant"] if track_name == "snowmountain": contents = f.readlines() ''' contents = contents[2:contents.index("[debug ] profile: \n")-1] content = [s for s in contents if kart_name in s] data = [float(x) for x in content[0].split() if is_numeric(x)] if kart_name not in avg_lap_time: avg_lap_time[kart_name] = [] avg_pos[kart_name] = [] avg_speed[kart_name] = [] avg_top[kart_name] = [] total_rescued[kart_name] = [] avg_lap_time[kart_name].append(data[2]/4) avg_pos[kart_name].append(data[1]) avg_speed[kart_name].append(data[3]) avg_top[kart_name].append(data[4]) total_rescued[kart_name].append(data[7]) ''' contents = contents[2:6] #TODO check if all is in here for kart in kart_names: content = [s for s in contents if kart in s] data = [float(x) for x in content[0].split() if is_numeric(x)] if kart not in avg_lap_time: avg_lap_time[kart] = [] avg_pos[kart] = [] avg_speed[kart] = [] avg_top[kart] = [] total_rescued[kart] = [] avg_lap_time[kart].append(data[2]/4) avg_pos[kart].append(data[1]) avg_speed[kart].append(data[3]) avg_top[kart].append(data[4]) total_rescued[kart].append(data[7]) tests = len(avg_lap_time["gnu"]) print total_rescued for kart in kart_names: print "rescues for ", kart , ": ", sum(total_rescued[kart])/tests print "avg_lap_time for " , kart , ": " , sum(avg_lap_time[kart])/tests print "avg_pos for " , kart , ": " , sum(avg_pos[kart])/tests print "avg_speed for " , kart , ": " , sum(avg_speed[kart])/tests print "avg_top for " , kart , ": " , sum(avg_top[kart])/tests pyplot.subplot(2,2,1) pyplot.plot(list(xrange(tests)),avg_pos["gnu"], "b-") pyplot.xlabel("tests") pyplot.ylabel("gnu") pyplot.subplot(2,2,2) pyplot.plot(list(xrange(tests)),avg_pos["sara"], "r-") pyplot.xlabel("tests") pyplot.ylabel("sara") pyplot.subplot(2,2,3) pyplot.plot(list(xrange(tests)),avg_pos["elephpant"], "y-") pyplot.xlabel("tests") pyplot.ylabel("elephpant") pyplot.subplot(2,2,4) pyplot.plot(list(xrange(tests)),avg_pos["tux"], "g-") pyplot.xlabel("tests") pyplot.ylabel("tux") pyplot.show()
from django.contrib.contenttypes.fields import ( GenericForeignKey, GenericRelation, ) from django.contrib.contenttypes.models import ContentType from django.core.checks import Error from django.core.exceptions import FieldDoesNotExist, FieldError from django.db import models from django.test import TestCase from django.test.utils import isolate_apps @isolate_apps('model_inheritance') class AbstractInheritanceTests(TestCase): def test_single_parent(self): class AbstractBase(models.Model): name = models.CharField(max_length=30) class Meta: abstract = True class AbstractDescendant(AbstractBase): name = models.CharField(max_length=50) class Meta: abstract = True class DerivedChild(AbstractBase): name = models.CharField(max_length=50) class DerivedGrandChild(AbstractDescendant): pass self.assertEqual(AbstractDescendant._meta.get_field('name').max_length, 50) self.assertEqual(DerivedChild._meta.get_field('name').max_length, 50) self.assertEqual(DerivedGrandChild._meta.get_field('name').max_length, 50) def test_multiple_parents_mro(self): class AbstractBaseOne(models.Model): class Meta: abstract = True class AbstractBaseTwo(models.Model): name = models.CharField(max_length=30) class Meta: abstract = True class DescendantOne(AbstractBaseOne, AbstractBaseTwo): class Meta: abstract = True class DescendantTwo(AbstractBaseOne, AbstractBaseTwo): name = models.CharField(max_length=50) class Meta: abstract = True class Derived(DescendantOne, DescendantTwo): pass self.assertEqual(DescendantOne._meta.get_field('name').max_length, 30) self.assertEqual(DescendantTwo._meta.get_field('name').max_length, 50) self.assertEqual(Derived._meta.get_field('name').max_length, 50) def test_multiple_inheritance_cannot_shadow_concrete_inherited_field(self): class ConcreteParent(models.Model): name = models.CharField(max_length=255) class AbstractParent(models.Model): name = models.IntegerField() class Meta: abstract = True class FirstChild(ConcreteParent, AbstractParent): pass class AnotherChild(AbstractParent, ConcreteParent): pass self.assertIsInstance(FirstChild._meta.get_field('name'), models.CharField) self.assertEqual( AnotherChild.check(), [Error( "The field 'name' clashes with the field 'name' " "from model 'model_inheritance.concreteparent'.", obj=AnotherChild._meta.get_field('name'), id="models.E006", )] ) def test_virtual_field(self): class RelationModel(models.Model): content_type = models.ForeignKey(ContentType, models.CASCADE) object_id = models.PositiveIntegerField() content_object = GenericForeignKey('content_type', 'object_id') class RelatedModelAbstract(models.Model): field = GenericRelation(RelationModel) class Meta: abstract = True class ModelAbstract(models.Model): field = models.CharField(max_length=100) class Meta: abstract = True class OverrideRelatedModelAbstract(RelatedModelAbstract): field = models.CharField(max_length=100) class ExtendModelAbstract(ModelAbstract): field = GenericRelation(RelationModel) self.assertIsInstance(OverrideRelatedModelAbstract._meta.get_field('field'), models.CharField) self.assertIsInstance(ExtendModelAbstract._meta.get_field('field'), GenericRelation) def test_cannot_override_indirect_abstract_field(self): class AbstractBase(models.Model): name = models.CharField(max_length=30) class Meta: abstract = True class ConcreteDescendant(AbstractBase): pass msg = ( "Local field 'name' in class 'Descendant' clashes with field of " "the same name from base class 'ConcreteDescendant'." ) with self.assertRaisesMessage(FieldError, msg): class Descendant(ConcreteDescendant): name = models.IntegerField() def test_override_field_with_attr(self): class AbstractBase(models.Model): first_name = models.CharField(max_length=50) last_name = models.CharField(max_length=50) middle_name = models.CharField(max_length=30) full_name = models.CharField(max_length=150) class Meta: abstract = True class Descendant(AbstractBase): middle_name = None def full_name(self): return self.first_name + self.last_name with self.assertRaises(FieldDoesNotExist): Descendant._meta.get_field('middle_name') with self.assertRaises(FieldDoesNotExist): Descendant._meta.get_field('full_name') def test_overriding_field_removed_by_concrete_model(self): class AbstractModel(models.Model): foo = models.CharField(max_length=30) class Meta: abstract = True class RemovedAbstractModelField(AbstractModel): foo = None class OverrideRemovedFieldByConcreteModel(RemovedAbstractModelField): foo = models.CharField(max_length=50) self.assertEqual(OverrideRemovedFieldByConcreteModel._meta.get_field('foo').max_length, 50) def test_shadowed_fkey_id(self): class Foo(models.Model): pass class AbstractBase(models.Model): foo = models.ForeignKey(Foo, models.CASCADE) class Meta: abstract = True class Descendant(AbstractBase): foo_id = models.IntegerField() self.assertEqual( Descendant.check(), [Error( "The field 'foo_id' clashes with the field 'foo' " "from model 'model_inheritance.descendant'.", obj=Descendant._meta.get_field('foo_id'), id='models.E006', )] ) def test_shadow_related_name_when_set_to_none(self): class AbstractBase(models.Model): bar = models.IntegerField() class Meta: abstract = True class Foo(AbstractBase): bar = None foo = models.IntegerField() class Bar(models.Model): bar = models.ForeignKey(Foo, models.CASCADE, related_name='bar') self.assertEqual(Bar.check(), []) def test_reverse_foreign_key(self): class AbstractBase(models.Model): foo = models.CharField(max_length=100) class Meta: abstract = True class Descendant(AbstractBase): pass class Foo(models.Model): foo = models.ForeignKey(Descendant, models.CASCADE, related_name='foo') self.assertEqual( Foo._meta.get_field('foo').check(), [ Error( "Reverse accessor for 'Foo.foo' clashes with field name 'Descendant.foo'.", hint=( "Rename field 'Descendant.foo', or add/change a related_name " "argument to the definition for field 'Foo.foo'." ), obj=Foo._meta.get_field('foo'), id='fields.E302', ), Error( "Reverse query name for 'Foo.foo' clashes with field name 'Descendant.foo'.", hint=( "Rename field 'Descendant.foo', or add/change a related_name " "argument to the definition for field 'Foo.foo'." ), obj=Foo._meta.get_field('foo'), id='fields.E303', ), ] ) def test_multi_inheritance_field_clashes(self): class AbstractBase(models.Model): name = models.CharField(max_length=30) class Meta: abstract = True class ConcreteBase(AbstractBase): pass class AbstractDescendant(ConcreteBase): class Meta: abstract = True class ConcreteDescendant(AbstractDescendant): name = models.CharField(max_length=100) self.assertEqual( ConcreteDescendant.check(), [Error( "The field 'name' clashes with the field 'name' from " "model 'model_inheritance.concretebase'.", obj=ConcreteDescendant._meta.get_field('name'), id="models.E006", )] ) def test_override_one2one_relation_auto_field_clashes(self): class ConcreteParent(models.Model): name = models.CharField(max_length=255) class AbstractParent(models.Model): name = models.IntegerField() class Meta: abstract = True msg = ( "Auto-generated field 'concreteparent_ptr' in class 'Descendant' " "for parent_link to base class 'ConcreteParent' clashes with " "declared field of the same name." ) with self.assertRaisesMessage(FieldError, msg): class Descendant(ConcreteParent, AbstractParent): concreteparent_ptr = models.CharField(max_length=30) def test_abstract_model_with_regular_python_mixin_mro(self): class AbstractModel(models.Model): name = models.CharField(max_length=255) age = models.IntegerField() class Meta: abstract = True class Mixin: age = None class Mixin2: age = 2 class DescendantMixin(Mixin): pass class ConcreteModel(models.Model): foo = models.IntegerField() class ConcreteModel2(ConcreteModel): age = models.SmallIntegerField() def fields(model): if not hasattr(model, '_meta'): return list() return list((f.name, f.__class__) for f in model._meta.get_fields()) model_dict = {'__module__': 'model_inheritance'} model1 = type('Model1', (AbstractModel, Mixin), model_dict.copy()) model2 = type('Model2', (Mixin2, AbstractModel), model_dict.copy()) model3 = type('Model3', (DescendantMixin, AbstractModel), model_dict.copy()) model4 = type('Model4', (Mixin2, Mixin, AbstractModel), model_dict.copy()) model5 = type('Model5', (Mixin2, ConcreteModel2, Mixin, AbstractModel), model_dict.copy()) self.assertEqual( fields(model1), [('id', models.AutoField), ('name', models.CharField), ('age', models.IntegerField)] ) self.assertEqual(fields(model2), [('id', models.AutoField), ('name', models.CharField)]) self.assertEqual(getattr(model2, 'age'), 2) self.assertEqual(fields(model3), [('id', models.AutoField), ('name', models.CharField)]) self.assertEqual(fields(model4), [('id', models.AutoField), ('name', models.CharField)]) self.assertEqual(getattr(model4, 'age'), 2) self.assertEqual( fields(model5), [ ('id', models.AutoField), ('foo', models.IntegerField), ('concretemodel_ptr', models.OneToOneField), ('age', models.SmallIntegerField), ('concretemodel2_ptr', models.OneToOneField), ('name', models.CharField), ] )
# Copyright (C) 2010 Simon Wessing # TU Dortmund University # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. import numpy try: # try importing the C version from ._hypervolume import hv as hv except ImportError: # fallback on python version from ._hypervolume import pyhv as hv def hypervolume(front, **kargs): """Returns the index of the individual with the least the hypervolume contribution. The provided *front* should be a set of non-dominated individuals having each a :attr:`fitness` attribute. """ # Must use wvalues * -1 since hypervolume use implicit minimization # And minimization in deap use max on -obj wobj = numpy.array([ind.fitness.wvalues for ind in front]) * -1 ref = kargs.get("ref", None) if ref is None: ref = numpy.max(wobj, axis=0) + 1 def contribution(i): # The contribution of point p_i in point set P # is the hypervolume of P without p_i return hv.hypervolume(numpy.concatenate((wobj[:i], wobj[i+1:])), ref) # Parallelization note: Cannot pickle local function contrib_values = map(contribution, range(len(front))) # Select the maximum hypervolume value (correspond to the minimum difference) return numpy.argmax(contrib_values) def additive_epsilon(front, **kargs): """Returns the index of the individual with the least the additive epsilon contribution. The provided *front* should be a set of non-dominated individuals having each a :attr:`fitness` attribute. .. warning:: This function has not been tested. """ wobj = numpy.array([ind.fitness.wvalues for ind in front]) * -1 def contribution(i): mwobj = numpy.ma.array(wobj) mwobj[i] = numpy.ma.masked return numpy.min(numpy.max(wobj[i] - mwobj, axis=1)) contrib_values = map(contribution, range(len(front))) # Select the minimum contribution value return numpy.argmin(contrib_values) def multiplicative_epsilon(front, **kargs): """Returns the index of the individual with the least the multiplicative epsilon contribution. The provided *front* should be a set of non-dominated individuals having each a :attr:`fitness` attribute. .. warning:: This function has not been tested. """ wobj = numpy.array([ind.fitness.wvalues for ind in front]) * -1 def contribution(i): mwobj = numpy.ma.array(wobj) mwobj[i] = numpy.ma.masked return numpy.min(numpy.max(wobj[i] / mwobj, axis=1)) contrib_values = map(contribution, range(len(front))) # Select the minimum contribution value return numpy.argmin(contrib_values) __all__ = ["hypervolume", "additive_epsilon", "multiplicative_epsilon"]
''' >>> story.run() True >>> print output.getvalue() Story: Faked Story #1 In order to write specifications As a python developer I want to write them in Python language <BLANKLINE> Scenario 1: Fake scenario Given I run it ... OK When I type X ... OK Then it shows me X ... OK <BLANKLINE> Ran 1 scenario with 0 failures, 0 errors and 0 pending steps <BLANKLINE> ''' from pyhistorian import Story from cStringIO import StringIO output = StringIO() class FakeScenario(object): _givens = _whens = _thens = [] title = 'Fake scenario' def __init__(self, story): """default interface (should do nothing)""" def run(self): output.write(' Given I run it ... OK\n') output.write(' When I type X ... OK\n') output.write(' Then it shows me X ... OK\n') return [], [], [] class FakedStory(Story): """In order to write specifications As a python developer I want to write them in Python language""" output = output scenarios = [FakeScenario] title = 'Faked Story #1' story = FakedStory()
"""Principal Component Analysis Base Classes""" # Author: Alexandre Gramfort <alexandre.gramfort@inria.fr> # Olivier Grisel <olivier.grisel@ensta.org> # Mathieu Blondel <mathieu@mblondel.org> # Denis A. Engemann <d.engemann@fz-juelich.de> # Kyle Kastner <kastnerkyle@gmail.com> # # License: BSD 3 clause import numpy as np from scipy import linalg from ..base import BaseEstimator, TransformerMixin from ..utils import check_array from ..utils.extmath import fast_dot from ..utils.validation import check_is_fitted from ..externals import six from abc import ABCMeta, abstractmethod class _BasePCA(six.with_metaclass(ABCMeta, BaseEstimator, TransformerMixin)): """Base class for PCA methods. Warning: This class should not be used directly. Use derived classes instead. """ def get_covariance(self): """Compute data covariance with the generative model. ``cov = components_.T * S**2 * components_ + sigma2 * eye(n_features)`` where S**2 contains the explained variances, and sigma2 contains the noise variances. Returns ------- cov : array, shape=(n_features, n_features) Estimated covariance of data. """ components_ = self.components_ exp_var = self.explained_variance_ if self.whiten: components_ = components_ * np.sqrt(exp_var[:, np.newaxis]) exp_var_diff = np.maximum(exp_var - self.noise_variance_, 0.) cov = np.dot(components_.T * exp_var_diff, components_) cov.flat[::len(cov) + 1] += self.noise_variance_ # modify diag inplace return cov def get_precision(self): """Compute data precision matrix with the generative model. Equals the inverse of the covariance but computed with the matrix inversion lemma for efficiency. Returns ------- precision : array, shape=(n_features, n_features) Estimated precision of data. """ n_features = self.components_.shape[1] # handle corner cases first if self.n_components_ == 0: return np.eye(n_features) / self.noise_variance_ if self.n_components_ == n_features: return linalg.inv(self.get_covariance()) # Get precision using matrix inversion lemma components_ = self.components_ exp_var = self.explained_variance_ if self.whiten: components_ = components_ * np.sqrt(exp_var[:, np.newaxis]) exp_var_diff = np.maximum(exp_var - self.noise_variance_, 0.) precision = np.dot(components_, components_.T) / self.noise_variance_ precision.flat[::len(precision) + 1] += 1. / exp_var_diff precision = np.dot(components_.T, np.dot(linalg.inv(precision), components_)) precision /= -(self.noise_variance_ ** 2) precision.flat[::len(precision) + 1] += 1. / self.noise_variance_ return precision @abstractmethod def fit(X, y=None): """Placeholder for fit. Subclasses should implement this method! Fit the model with X. Parameters ---------- X : array-like, shape (n_samples, n_features) Training data, where n_samples is the number of samples and n_features is the number of features. Returns ------- self : object Returns the instance itself. """ def transform(self, X, y=None): """Apply dimensionality reduction to X. X is projected on the first principal components previously extracted from a training set. Parameters ---------- X : array-like, shape (n_samples, n_features) New data, where n_samples is the number of samples and n_features is the number of features. Returns ------- X_new : array-like, shape (n_samples, n_components) Examples -------- >>> import numpy as np >>> from sklearn.decomposition import IncrementalPCA >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) >>> ipca = IncrementalPCA(n_components=2, batch_size=3) >>> ipca.fit(X) IncrementalPCA(batch_size=3, copy=True, n_components=2, whiten=False) >>> ipca.transform(X) # doctest: +SKIP """ check_is_fitted(self, ['mean_', 'components_'], all_or_any=all) X = check_array(X) if self.mean_ is not None: X = X - self.mean_ X_transformed = fast_dot(X, self.components_.T) if self.whiten: X_transformed /= np.sqrt(self.explained_variance_) return X_transformed def inverse_transform(self, X, y=None): """Transform data back to its original space. In other words, return an input X_original whose transform would be X. Parameters ---------- X : array-like, shape (n_samples, n_components) New data, where n_samples is the number of samples and n_components is the number of components. Returns ------- X_original array-like, shape (n_samples, n_features) Notes ----- If whitening is enabled, inverse_transform will compute the exact inverse operation, which includes reversing whitening. """ if self.whiten: return fast_dot(X, np.sqrt(self.explained_variance_[:, np.newaxis]) * self.components_) + self.mean_ else: return fast_dot(X, self.components_) + self.mean_
from sqlalchemy.testing import fixtures from sqlalchemy.sql.ddl import SchemaGenerator, SchemaDropper from sqlalchemy import MetaData, Table, Column, Integer, Sequence, ForeignKey from sqlalchemy import schema from sqlalchemy.testing.mock import Mock class EmitDDLTest(fixtures.TestBase): def _mock_connection(self, item_exists): def has_item(connection, name, schema): return item_exists(name) return Mock(dialect=Mock( supports_sequences=True, has_table=Mock(side_effect=has_item), has_sequence=Mock(side_effect=has_item) ) ) def _mock_create_fixture(self, checkfirst, tables, item_exists=lambda item: False): connection = self._mock_connection(item_exists) return SchemaGenerator(connection.dialect, connection, checkfirst=checkfirst, tables=tables) def _mock_drop_fixture(self, checkfirst, tables, item_exists=lambda item: True): connection = self._mock_connection(item_exists) return SchemaDropper(connection.dialect, connection, checkfirst=checkfirst, tables=tables) def _table_fixture(self): m = MetaData() return (m, ) + tuple( Table('t%d' % i, m, Column('x', Integer)) for i in range(1, 6) ) def _use_alter_fixture_one(self): m = MetaData() t1 = Table( 't1', m, Column('id', Integer, primary_key=True), Column('t2id', Integer, ForeignKey('t2.id')) ) t2 = Table( 't2', m, Column('id', Integer, primary_key=True), Column('t1id', Integer, ForeignKey('t1.id')) ) return m, t1, t2 def _fk_fixture_one(self): m = MetaData() t1 = Table( 't1', m, Column('id', Integer, primary_key=True), Column('t2id', Integer, ForeignKey('t2.id')) ) t2 = Table( 't2', m, Column('id', Integer, primary_key=True), ) return m, t1, t2 def _table_seq_fixture(self): m = MetaData() s1 = Sequence('s1') s2 = Sequence('s2') t1 = Table('t1', m, Column("x", Integer, s1, primary_key=True)) t2 = Table('t2', m, Column("x", Integer, s2, primary_key=True)) return m, t1, t2, s1, s2 def test_create_seq_checkfirst(self): m, t1, t2, s1, s2 = self._table_seq_fixture() generator = self._mock_create_fixture( True, [ t1, t2], item_exists=lambda t: t not in ( "t1", "s1")) self._assert_create([t1, s1], generator, m) def test_drop_seq_checkfirst(self): m, t1, t2, s1, s2 = self._table_seq_fixture() generator = self._mock_drop_fixture( True, [ t1, t2], item_exists=lambda t: t in ( "t1", "s1")) self._assert_drop([t1, s1], generator, m) def test_create_collection_checkfirst(self): m, t1, t2, t3, t4, t5 = self._table_fixture() generator = self._mock_create_fixture( True, [ t2, t3, t4], item_exists=lambda t: t not in ( "t2", "t4")) self._assert_create_tables([t2, t4], generator, m) def test_drop_collection_checkfirst(self): m, t1, t2, t3, t4, t5 = self._table_fixture() generator = self._mock_drop_fixture( True, [ t2, t3, t4], item_exists=lambda t: t in ( "t2", "t4")) self._assert_drop_tables([t2, t4], generator, m) def test_create_collection_nocheck(self): m, t1, t2, t3, t4, t5 = self._table_fixture() generator = self._mock_create_fixture( False, [ t2, t3, t4], item_exists=lambda t: t not in ( "t2", "t4")) self._assert_create_tables([t2, t3, t4], generator, m) def test_create_empty_collection(self): m, t1, t2, t3, t4, t5 = self._table_fixture() generator = self._mock_create_fixture( True, [], item_exists=lambda t: t not in ( "t2", "t4")) self._assert_create_tables([], generator, m) def test_drop_empty_collection(self): m, t1, t2, t3, t4, t5 = self._table_fixture() generator = self._mock_drop_fixture( True, [], item_exists=lambda t: t in ( "t2", "t4")) self._assert_drop_tables([], generator, m) def test_drop_collection_nocheck(self): m, t1, t2, t3, t4, t5 = self._table_fixture() generator = self._mock_drop_fixture( False, [ t2, t3, t4], item_exists=lambda t: t in ( "t2", "t4")) self._assert_drop_tables([t2, t3, t4], generator, m) def test_create_metadata_checkfirst(self): m, t1, t2, t3, t4, t5 = self._table_fixture() generator = self._mock_create_fixture( True, None, item_exists=lambda t: t not in ( "t2", "t4")) self._assert_create_tables([t2, t4], generator, m) def test_drop_metadata_checkfirst(self): m, t1, t2, t3, t4, t5 = self._table_fixture() generator = self._mock_drop_fixture( True, None, item_exists=lambda t: t in ( "t2", "t4")) self._assert_drop_tables([t2, t4], generator, m) def test_create_metadata_nocheck(self): m, t1, t2, t3, t4, t5 = self._table_fixture() generator = self._mock_create_fixture( False, None, item_exists=lambda t: t not in ( "t2", "t4")) self._assert_create_tables([t1, t2, t3, t4, t5], generator, m) def test_drop_metadata_nocheck(self): m, t1, t2, t3, t4, t5 = self._table_fixture() generator = self._mock_drop_fixture( False, None, item_exists=lambda t: t in ( "t2", "t4")) self._assert_drop_tables([t1, t2, t3, t4, t5], generator, m) def test_create_metadata_auto_alter_fk(self): m, t1, t2 = self._use_alter_fixture_one() generator = self._mock_create_fixture( False, [t1, t2] ) self._assert_create_w_alter( [t1, t2] + list(t1.foreign_key_constraints) + list(t2.foreign_key_constraints), generator, m ) def test_create_metadata_inline_fk(self): m, t1, t2 = self._fk_fixture_one() generator = self._mock_create_fixture( False, [t1, t2] ) self._assert_create_w_alter( [t1, t2] + list(t1.foreign_key_constraints) + list(t2.foreign_key_constraints), generator, m ) def _assert_create_tables(self, elements, generator, argument): self._assert_ddl(schema.CreateTable, elements, generator, argument) def _assert_drop_tables(self, elements, generator, argument): self._assert_ddl(schema.DropTable, elements, generator, argument) def _assert_create(self, elements, generator, argument): self._assert_ddl( (schema.CreateTable, schema.CreateSequence), elements, generator, argument) def _assert_drop(self, elements, generator, argument): self._assert_ddl( (schema.DropTable, schema.DropSequence), elements, generator, argument) def _assert_create_w_alter(self, elements, generator, argument): self._assert_ddl( (schema.CreateTable, schema.CreateSequence, schema.AddConstraint), elements, generator, argument) def _assert_drop_w_alter(self, elements, generator, argument): self._assert_ddl( (schema.DropTable, schema.DropSequence, schema.DropConstraint), elements, generator, argument) def _assert_ddl(self, ddl_cls, elements, generator, argument): generator.traverse_single(argument) for call_ in generator.connection.execute.mock_calls: c = call_[1][0] assert isinstance(c, ddl_cls) assert c.element in elements, "element %r was not expected"\ % c.element elements.remove(c.element) if getattr(c, 'include_foreign_key_constraints', None) is not None: elements[:] = [ e for e in elements if e not in set(c.include_foreign_key_constraints)] assert not elements, "elements remain in list: %r" % elements
import os import sys if os.name == 'posix': def become_daemon(our_home_dir='.', out_log='/dev/null', err_log='/dev/null', umask=022): "Robustly turn into a UNIX daemon, running in our_home_dir." # First fork try: if os.fork() > 0: sys.exit(0) # kill off parent except OSError, e: sys.stderr.write("fork #1 failed: (%d) %s\n" % (e.errno, e.strerror)) sys.exit(1) os.setsid() os.chdir(our_home_dir) os.umask(umask) # Second fork try: if os.fork() > 0: os._exit(0) except OSError, e: sys.stderr.write("fork #2 failed: (%d) %s\n" % (e.errno, e.strerror)) os._exit(1) si = open('/dev/null', 'r') so = open(out_log, 'a+', 0) se = open(err_log, 'a+', 0) os.dup2(si.fileno(), sys.stdin.fileno()) os.dup2(so.fileno(), sys.stdout.fileno()) os.dup2(se.fileno(), sys.stderr.fileno()) # Set custom file descriptors so that they get proper buffering. sys.stdout, sys.stderr = so, se else: def become_daemon(our_home_dir='.', out_log=None, err_log=None, umask=022): """ If we're not running under a POSIX system, just simulate the daemon mode by doing redirections and directory changing. """ os.chdir(our_home_dir) os.umask(umask) sys.stdin.close() sys.stdout.close() sys.stderr.close() if err_log: sys.stderr = open(err_log, 'a', 0) else: sys.stderr = NullDevice() if out_log: sys.stdout = open(out_log, 'a', 0) else: sys.stdout = NullDevice() class NullDevice: "A writeable object that writes to nowhere -- like /dev/null." def write(self, s): pass
#!/usr/bin/env python import re import json # http://mathiasbynens.be/notes/javascript-encoding#surrogate-formulae # http://stackoverflow.com/a/13436167/96656 def unisymbol(codePoint): if codePoint >= 0x0000 and codePoint <= 0xFFFF: return unichr(codePoint) elif codePoint >= 0x010000 and codePoint <= 0x10FFFF: highSurrogate = int((codePoint - 0x10000) / 0x400) + 0xD800 lowSurrogate = int((codePoint - 0x10000) % 0x400) + 0xDC00 return unichr(highSurrogate) + unichr(lowSurrogate) else: return 'Error' def hexify(codePoint): return 'U+' + hex(codePoint)[2:].upper().zfill(6) def writeFile(filename, contents): print filename with open(filename, 'w') as f: f.write(contents.strip() + '\n') data = [] for codePoint in range(0x000000, 0x10FFFF + 1): symbol = unisymbol(codePoint) # http://stackoverflow.com/a/17199950/96656 bytes = symbol.encode('utf8').decode('latin1') data.append({ 'codePoint': codePoint, 'decoded': symbol, 'encoded': bytes }); jsonData = json.dumps(data, sort_keys=False, indent=2, separators=(',', ': ')) # Use tabs instead of double spaces for indentation jsonData = jsonData.replace(' ', '\t') # Escape hexadecimal digits in escape sequences jsonData = re.sub( r'\\u([a-fA-F0-9]{4})', lambda match: r'\u{}'.format(match.group(1).upper()), jsonData ) writeFile('data.json', jsonData)
import json from subprocess import Popen, PIPE import os import shutil import sys from ask_amy.cli.code_gen.code_generator import CodeGenerator from time import sleep class DeployCLI(object): def create_template(self, skill_name, aws_role='', intent_schema_nm=None): with open(intent_schema_nm) as json_data: intent_schema = json.load(json_data) code_generator = CodeGenerator(skill_name, aws_role, intent_schema) code_generator.create_cli_config() code_generator.create_skill_config() code_generator.create_skill_py() def create_role(self, role_name): base_dir = self.module_path() role_json = 'file://' + base_dir + '/code_gen/templates/alexa_lambda_role.json' iam_create_role = self.run(self.iam_create_role, role_name, role_json) iam_attach_policy_cloud_watch = self.run(self.iam_attach_role_policy, role_name, 'arn:aws:iam::aws:policy/CloudWatchLogsFullAccess') iam_attach_policy_dynamo = self.run(self.iam_attach_role_policy, role_name, 'arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess') response_dict = {'iam_create_role': iam_create_role, 'iam_attach_policy_dynamo': iam_attach_policy_dynamo, 'iam_attach_policy_cloud_watch': iam_attach_policy_cloud_watch} return json.dumps(response_dict, indent=4) def deploy_lambda(self, config_file_name): deploy_dict = self.stage_to_dist(config_file_name) aws_region = deploy_dict['aws_region'] skill_name = deploy_dict['skill_name'] lambda_zip = 'fileb://' + deploy_dict['skill_home_dir'] + '/' + deploy_dict['lambda_zip'] aws_profile = deploy_dict['aws_profile'] deploy_response = self.run(self.lamabda_update_function, aws_region, skill_name, lambda_zip, aws_profile) return json.dumps(deploy_response, indent=4) def create_lambda(self, config_file_name): deploy_dict = self.stage_to_dist(config_file_name) skill_name = deploy_dict['skill_name'] lambda_runtime = deploy_dict['lambda_runtime'] aws_role = deploy_dict['aws_role'] lambda_handler = deploy_dict['lambda_handler'] lambda_timeout = deploy_dict['lambda_timeout'] lambda_memory = deploy_dict['lambda_memory'] lambda_zip = 'fileb://' + deploy_dict['skill_home_dir'] + '/' + deploy_dict['lambda_zip'] create_function_out = self.run(self.lambda_create_function, skill_name, lambda_runtime, aws_role, lambda_handler, skill_name, lambda_timeout, lambda_memory, lambda_zip) add_trigger_out = self.run(self.lambda_add_trigger, skill_name) response_dict = {'create_function': create_function_out, 'add_trigger': add_trigger_out} return json.dumps(response_dict, indent=4) def stage_to_dist(self, config_file_name): deploy_dict = self.load_json_file(config_file_name) skill_home_dir = deploy_dict['skill_home_dir'] distribution_dir = skill_home_dir + '/dist' ask_amy_impl = None if 'ask_amy_dev' in deploy_dict: if deploy_dict['ask_amy_dev']: # set to true ask_amy_home_dir = deploy_dict['ask_amy_home_dir'] ask_amy_impl = ask_amy_home_dir + '/ask_amy' self.install_ask_amy(distribution_dir, ask_amy_impl) self.copy_skill_to_dist(skill_home_dir, distribution_dir) self.make_zipfile(deploy_dict['lambda_zip'], distribution_dir) return deploy_dict def log(self, log_group_name, log_stream_name=None, next_forward_token=None): if log_stream_name is None: log_stream_name = self.latest_log_stream_for_log_group(log_group_name) if next_forward_token is None: log_events_dict = self.run(self.cloudwatch_get_log_events, log_group_name, log_stream_name) else: log_events_dict = self.run(self.cloudwatch_get_log_events, log_group_name, log_stream_name, next_forward_token) next_forward_token = log_events_dict['nextForwardToken'] log_events_lst = log_events_dict['events'] for event_dict in log_events_lst: message = event_dict['message'] sys.stdout.write(message) return next_forward_token, log_stream_name def latest_log_stream_for_log_group(self, log_group_name): log_streams_dict = self.run(self.cloudwatch_latest_log_stream, log_group_name) log_streams = log_streams_dict['logStreams'] latest_stream = log_streams[-1] log_stream_name = latest_stream['logStreamName'] return log_stream_name def log_tail(self, log_group_name): next_forward_token, log_stream_name = self.log(log_group_name) not_done = True try: while not_done: sleep(1) next_forward_token, log_stream_name = self.log(log_group_name, log_stream_name, next_forward_token) except KeyboardInterrupt: pass return def install_ask_amy(self, destination_dir, source_dir=None): ask_amy_dist = destination_dir + '/ask_amy' try: shutil.rmtree(ask_amy_dist, ignore_errors=True) if source_dir is not None: shutil.copytree(source_dir, ask_amy_dist) else: #pip.main(['install', '--upgrade', 'ask_amy', '-t', destination_dir]) self.run(self.install_ask_amy_for_upload, destination_dir) except FileNotFoundError: sys.stderr.write("ERROR: path not found {}\n".format(source_dir)) sys.exit(-1) def copy_skill_to_dist(self, source_dir, destination_dir): files = os.listdir(source_dir) try: for file in files: full_path = source_dir + os.sep + file if file.endswith(".py"): shutil.copy(full_path, destination_dir) if file.endswith(".json"): shutil.copy(full_path, destination_dir) except FileNotFoundError: sys.stderr.write("ERROR: filename not found\n") sys.exit(-1) def make_zipfile(self, output_filename, source_dir): output_filename = output_filename[:-4] shutil.make_archive(output_filename, 'zip', source_dir) install_ask_amy_for_upload = ('pip', 'install', '--upgrade', 'ask_amy', '-t', 0 ) lamabda_update_function = ('aws', '--output', 'json', 'lambda', 'update-function-code', '--region', 0, '--function-name', 1, '--zip-file', 2, '--profile', 3 ) lambda_create_function = ('aws', '--output', 'json', 'lambda', 'create-function', '--function-name', 0, '--runtime', 1, '--role', 2, '--handler', 3, '--description', 4, '--timeout', 5, '--memory-size', 6, '--zip-file', 7 ) lambda_add_trigger = ('aws', '--output', 'json', 'lambda', 'add-permission', '--function-name', 0, '--statement-id', 'alexa_trigger', '--action', 'lambda:InvokeFunction', '--principal', 'alexa-appkit.amazon.com' ) cloudwatch_latest_log_stream = ('aws', '--output', 'json', 'logs', 'describe-log-streams', '--log-group-name', 0, '--order-by', 'LastEventTime' ) iam_create_role = ('aws', '--output', 'json', 'iam', 'create-role', '--role-name', 0, '--assume-role-policy-document', 1 ) iam_attach_role_policy = ('aws', '--output', 'json', 'iam', 'attach-role-policy', '--role-name', 0, '--policy-arn', 1 ) cloudwatch_get_log_events = ('aws', '--output', 'json', 'logs', 'get-log-events', '--log-group-name', 0, '--log-stream-name', 1, '--next-token', 2 ) def load_json_file(self, config_file_name): try: file_ptr_r = open(config_file_name, 'r') deploy_dict = json.load(file_ptr_r) file_ptr_r.close() except FileNotFoundError: sys.stderr.write("ERROR: filename not found {}\n".format(config_file_name)) sys.exit(-1) return deploy_dict def module_path(self): try: modpath = os.path.dirname(os.path.abspath(__file__)) except AttributeError: sys.stderr.write("ERROR: could not find the path to module") sys.exit(-1) # Turn pyc files into py files if we can if modpath.endswith('.pyc') and os.path.exists(modpath[:-1]): modpath = modpath[:-1] # Sort out symlinks modpath = os.path.realpath(modpath) return modpath def run(self, arg_list, *args): try: processed_args = self.process_args(arg_list, *args) process = Popen(processed_args, stdout=PIPE) out, err = process.communicate() out = str(out, 'utf-8') if not out: out = '{}' try: json_out = json.loads(out) except Exception: json_out = {} return json_out except Exception as e: sys.stderr.write("ERROR: command line error %s\n" % args) sys.stderr.write("ERROR: %s\n" % e) sys.exit(-1) def process_args(self, arg_tuple, *args): # process the arg arg_list = list(arg_tuple) for index in range(0, len(arg_list)): if type(arg_list[index]) == int: # substitue for args passed in if arg_list[index] < len(args): arg_list[index] = args[arg_list[index]] # if we have more substitutions than args passed delete the extras else: del arg_list[index - 1:] break return arg_list
# Partname: AT90USB647 # generated automatically, do not edit MCUREGS = { 'WDTCSR': '&96', 'WDTCSR_WDIF': '$80', 'WDTCSR_WDIE': '$40', 'WDTCSR_WDP': '$27', 'WDTCSR_WDCE': '$10', 'WDTCSR_WDE': '$08', 'PORTA': '&34', 'DDRA': '&33', 'PINA': '&32', 'PORTB': '&37', 'DDRB': '&36', 'PINB': '&35', 'PORTC': '&40', 'DDRC': '&39', 'PINC': '&38', 'PORTD': '&43', 'DDRD': '&42', 'PIND': '&41', 'PORTE': '&46', 'DDRE': '&45', 'PINE': '&44', 'PORTF': '&49', 'DDRF': '&48', 'PINF': '&47', 'SREG': '&95', 'SREG_I': '$80', 'SREG_T': '$40', 'SREG_H': '$20', 'SREG_S': '$10', 'SREG_V': '$08', 'SREG_N': '$04', 'SREG_Z': '$02', 'SREG_C': '$01', 'SP': '&93', 'MCUCR': '&85', 'MCUCR_JTD': '$80', 'MCUCR_PUD': '$10', 'MCUCR_IVSEL': '$02', 'MCUCR_IVCE': '$01', 'MCUSR': '&84', 'MCUSR_JTRF': '$10', 'MCUSR_WDRF': '$08', 'MCUSR_BORF': '$04', 'MCUSR_EXTRF': '$02', 'MCUSR_PORF': '$01', 'XMCRA': '&116', 'XMCRA_SRE': '$80', 'XMCRA_SRL': '$70', 'XMCRA_SRW1': '$0C', 'XMCRA_SRW0': '$03', 'XMCRB': '&117', 'XMCRB_XMBK': '$80', 'XMCRB_XMM': '$07', 'OSCCAL': '&102', 'CLKPR': '&97', 'CLKPR_CLKPCE': '$80', 'CLKPR_CLKPS': '$0F', 'SMCR': '&83', 'SMCR_SM': '$0E', 'SMCR_SE': '$01', 'EIND': '&92', 'RAMPZ': '&91', 'GPIOR2': '&75', 'GPIOR2_GPIOR': '$FF', 'GPIOR1': '&74', 'GPIOR1_GPIOR': '$FF', 'GPIOR0': '&62', 'GPIOR0_GPIOR07': '$80', 'GPIOR0_GPIOR06': '$40', 'GPIOR0_GPIOR05': '$20', 'GPIOR0_GPIOR04': '$10', 'GPIOR0_GPIOR03': '$08', 'GPIOR0_GPIOR02': '$04', 'GPIOR0_GPIOR01': '$02', 'GPIOR0_GPIOR00': '$01', 'PRR1': '&101', 'PRR1_PRUSB': '$80', 'PRR1_PRTIM3': '$08', 'PRR1_PRUSART1': '$01', 'PRR0': '&100', 'PRR0_PRTWI': '$80', 'PRR0_PRTIM2': '$40', 'PRR0_PRTIM0': '$20', 'PRR0_PRTIM1': '$08', 'PRR0_PRSPI': '$04', 'PRR0_PRADC': '$01', 'TWAMR': '&189', 'TWAMR_TWAM': '$FE', 'TWBR': '&184', 'TWCR': '&188', 'TWCR_TWINT': '$80', 'TWCR_TWEA': '$40', 'TWCR_TWSTA': '$20', 'TWCR_TWSTO': '$10', 'TWCR_TWWC': '$08', 'TWCR_TWEN': '$04', 'TWCR_TWIE': '$01', 'TWSR': '&185', 'TWSR_TWS': '$F8', 'TWSR_TWPS': '$03', 'TWDR': '&187', 'TWAR': '&186', 'TWAR_TWA': '$FE', 'TWAR_TWGCE': '$01', 'SPCR': '&76', 'SPCR_SPIE': '$80', 'SPCR_SPE': '$40', 'SPCR_DORD': '$20', 'SPCR_MSTR': '$10', 'SPCR_CPOL': '$08', 'SPCR_CPHA': '$04', 'SPCR_SPR': '$03', 'SPSR': '&77', 'SPSR_SPIF': '$80', 'SPSR_WCOL': '$40', 'SPSR_SPI2X': '$01', 'SPDR': '&78', 'UDR1': '&206', 'UCSR1A': '&200', 'UCSR1A_RXC1': '$80', 'UCSR1A_TXC1': '$40', 'UCSR1A_UDRE1': '$20', 'UCSR1A_FE1': '$10', 'UCSR1A_DOR1': '$08', 'UCSR1A_UPE1': '$04', 'UCSR1A_U2X1': '$02', 'UCSR1A_MPCM1': '$01', 'UCSR1B': '&201', 'UCSR1B_RXCIE1': '$80', 'UCSR1B_TXCIE1': '$40', 'UCSR1B_UDRIE1': '$20', 'UCSR1B_RXEN1': '$10', 'UCSR1B_TXEN1': '$08', 'UCSR1B_UCSZ12': '$04', 'UCSR1B_RXB81': '$02', 'UCSR1B_TXB81': '$01', 'UCSR1C': '&202', 'UCSR1C_UMSEL1': '$C0', 'UCSR1C_UPM1': '$30', 'UCSR1C_USBS1': '$08', 'UCSR1C_UCSZ1': '$06', 'UCSR1C_UCPOL1': '$01', 'UBRR1': '&204', 'UEINT': '&244', 'UEBCHX': '&243', 'UEBCLX': '&242', 'UEDATX': '&241', 'UEIENX': '&240', 'UEIENX_FLERRE': '$80', 'UEIENX_NAKINE': '$40', 'UEIENX_NAKOUTE': '$10', 'UEIENX_RXSTPE': '$08', 'UEIENX_RXOUTE': '$04', 'UEIENX_STALLEDE': '$02', 'UEIENX_TXINE': '$01', 'UESTA1X': '&239', 'UESTA1X_CTRLDIR': '$04', 'UESTA1X_CURRBK': '$03', 'UESTA0X': '&238', 'UESTA0X_CFGOK': '$80', 'UESTA0X_OVERFI': '$40', 'UESTA0X_UNDERFI': '$20', 'UESTA0X_DTSEQ': '$0C', 'UESTA0X_NBUSYBK': '$03', 'UECFG1X': '&237', 'UECFG1X_EPSIZE': '$70', 'UECFG1X_EPBK': '$0C', 'UECFG1X_ALLOC': '$02', 'UECFG0X': '&236', 'UECFG0X_EPTYPE': '$C0', 'UECFG0X_EPDIR': '$01', 'UECONX': '&235', 'UECONX_STALLRQ': '$20', 'UECONX_STALLRQC': '$10', 'UECONX_RSTDT': '$08', 'UECONX_EPEN': '$01', 'UERST': '&234', 'UERST_EPRST': '$7F', 'UENUM': '&233', 'UEINTX': '&232', 'UEINTX_FIFOCON': '$80', 'UEINTX_NAKINI': '$40', 'UEINTX_RWAL': '$20', 'UEINTX_NAKOUTI': '$10', 'UEINTX_RXSTPI': '$08', 'UEINTX_RXOUTI': '$04', 'UEINTX_STALLEDI': '$02', 'UEINTX_TXINI': '$01', 'UDMFN': '&230', 'UDMFN_FNCERR': '$10', 'UDFNUM': '&228', 'UDADDR': '&227', 'UDADDR_ADDEN': '$80', 'UDADDR_UADD': '$7F', 'UDIEN': '&226', 'UDIEN_UPRSME': '$40', 'UDIEN_EORSME': '$20', 'UDIEN_WAKEUPE': '$10', 'UDIEN_EORSTE': '$08', 'UDIEN_SOFE': '$04', 'UDIEN_SUSPE': '$01', 'UDINT': '&225', 'UDINT_UPRSMI': '$40', 'UDINT_EORSMI': '$20', 'UDINT_WAKEUPI': '$10', 'UDINT_EORSTI': '$08', 'UDINT_SOFI': '$04', 'UDINT_SUSPI': '$01', 'UDCON': '&224', 'UDCON_LSM': '$04', 'UDCON_RMWKUP': '$02', 'UDCON_DETACH': '$01', 'OTGINT': '&223', 'OTGINT_STOI': '$20', 'OTGINT_HNPERRI': '$10', 'OTGINT_ROLEEXI': '$08', 'OTGINT_BCERRI': '$04', 'OTGINT_VBERRI': '$02', 'OTGINT_SRPI': '$01', 'OTGIEN': '&222', 'OTGIEN_STOE': '$20', 'OTGIEN_HNPERRE': '$10', 'OTGIEN_ROLEEXE': '$08', 'OTGIEN_BCERRE': '$04', 'OTGIEN_VBERRE': '$02', 'OTGIEN_SRPE': '$01', 'OTGCON': '&221', 'OTGCON_HNPREQ': '$20', 'OTGCON_SRPREQ': '$10', 'OTGCON_SRPSEL': '$08', 'OTGCON_VBUSHWC': '$04', 'OTGCON_VBUSREQ': '$02', 'OTGCON_VBUSRQC': '$01', 'OTGTCON': '&249', 'OTGTCON_OTGTCON_7': '$80', 'OTGTCON_PAGE': '$60', 'OTGTCON_VALUE_2': '$07', 'USBINT': '&218', 'USBINT_IDTI': '$02', 'USBINT_VBUSTI': '$01', 'USBSTA': '&217', 'USBSTA_SPEED': '$08', 'USBSTA_ID': '$02', 'USBSTA_VBUS': '$01', 'USBCON': '&216', 'USBCON_USBE': '$80', 'USBCON_HOST': '$40', 'USBCON_FRZCLK': '$20', 'USBCON_OTGPADE': '$10', 'USBCON_IDTE': '$02', 'USBCON_VBUSTE': '$01', 'UHWCON': '&215', 'UHWCON_UIMOD': '$80', 'UHWCON_UIDE': '$40', 'UHWCON_UVCONE': '$10', 'UHWCON_UVREGE': '$01', 'UPERRX': '&245', 'UPERRX_COUNTER': '$60', 'UPERRX_CRC16': '$10', 'UPERRX_TIMEOUT': '$08', 'UPERRX_PID': '$04', 'UPERRX_DATAPID': '$02', 'UPERRX_DATATGL': '$01', 'UPINT': '&248', 'UPBCHX': '&247', 'UPBCLX': '&246', 'UPDATX': '&175', 'UPIENX': '&174', 'UPIENX_FLERRE': '$80', 'UPIENX_NAKEDE': '$40', 'UPIENX_PERRE': '$10', 'UPIENX_TXSTPE': '$08', 'UPIENX_TXOUTE': '$04', 'UPIENX_RXSTALLE': '$02', 'UPIENX_RXINE': '$01', 'UPCFG2X': '&173', 'UPSTAX': '&172', 'UPSTAX_CFGOK': '$80', 'UPSTAX_OVERFI': '$40', 'UPSTAX_UNDERFI': '$20', 'UPSTAX_DTSEQ': '$0C', 'UPSTAX_NBUSYK': '$03', 'UPCFG1X': '&171', 'UPCFG1X_PSIZE': '$70', 'UPCFG1X_PBK': '$0C', 'UPCFG1X_ALLOC': '$02', 'UPCFG0X': '&170', 'UPCFG0X_PTYPE': '$C0', 'UPCFG0X_PTOKEN': '$30', 'UPCFG0X_PEPNUM': '$0F', 'UPCONX': '&169', 'UPCONX_PFREEZE': '$40', 'UPCONX_INMODE': '$20', 'UPCONX_RSTDT': '$08', 'UPCONX_PEN': '$01', 'UPRST': '&168', 'UPRST_PRST': '$7F', 'UPNUM': '&167', 'UPINTX': '&166', 'UPINTX_FIFOCON': '$80', 'UPINTX_NAKEDI': '$40', 'UPINTX_RWAL': '$20', 'UPINTX_PERRI': '$10', 'UPINTX_TXSTPI': '$08', 'UPINTX_TXOUTI': '$04', 'UPINTX_RXSTALLI': '$02', 'UPINTX_RXINI': '$01', 'UPINRQX': '&165', 'UHFLEN': '&164', 'UHFNUM': '&162', 'UHADDR': '&161', 'UHIEN': '&160', 'UHIEN_HWUPE': '$40', 'UHIEN_HSOFE': '$20', 'UHIEN_RXRSME': '$10', 'UHIEN_RSMEDE': '$08', 'UHIEN_RSTE': '$04', 'UHIEN_DDISCE': '$02', 'UHIEN_DCONNE': '$01', 'UHINT': '&159', 'UHINT_UHUPI': '$40', 'UHINT_HSOFI': '$20', 'UHINT_RXRSMI': '$10', 'UHINT_RSMEDI': '$08', 'UHINT_RSTI': '$04', 'UHINT_DDISCI': '$02', 'UHINT_DCONNI': '$01', 'UHCON': '&158', 'UHCON_RESUME': '$04', 'UHCON_RESET': '$02', 'UHCON_SOFEN': '$01', 'SPMCSR': '&87', 'SPMCSR_SPMIE': '$80', 'SPMCSR_RWWSB': '$40', 'SPMCSR_SIGRD': '$20', 'SPMCSR_RWWSRE': '$10', 'SPMCSR_BLBSET': '$08', 'SPMCSR_PGWRT': '$04', 'SPMCSR_PGERS': '$02', 'SPMCSR_SPMEN': '$01', 'EEAR': '&65', 'EEDR': '&64', 'EECR': '&63', 'EECR_EEPM': '$30', 'EECR_EERIE': '$08', 'EECR_EEMPE': '$04', 'EECR_EEPE': '$02', 'EECR_EERE': '$01', 'OCR0B': '&72', 'OCR0A': '&71', 'TCNT0': '&70', 'TCCR0B': '&69', 'TCCR0B_FOC0A': '$80', 'TCCR0B_FOC0B': '$40', 'TCCR0B_WGM02': '$08', 'TCCR0B_CS0': '$07', 'TCCR0A': '&68', 'TCCR0A_COM0A': '$C0', 'TCCR0A_COM0B': '$30', 'TCCR0A_WGM0': '$03', 'TIMSK0': '&110', 'TIMSK0_OCIE0B': '$04', 'TIMSK0_OCIE0A': '$02', 'TIMSK0_TOIE0': '$01', 'TIFR0': '&53', 'TIFR0_OCF0B': '$04', 'TIFR0_OCF0A': '$02', 'TIFR0_TOV0': '$01', 'GTCCR': '&67', 'GTCCR_TSM': '$80', 'GTCCR_PSRSYNC': '$01', 'TIMSK2': '&112', 'TIMSK2_OCIE2B': '$04', 'TIMSK2_OCIE2A': '$02', 'TIMSK2_TOIE2': '$01', 'TIFR2': '&55', 'TIFR2_OCF2B': '$04', 'TIFR2_OCF2A': '$02', 'TIFR2_TOV2': '$01', 'TCCR2A': '&176', 'TCCR2A_COM2A': '$C0', 'TCCR2A_COM2B': '$30', 'TCCR2A_WGM2': '$03', 'TCCR2B': '&177', 'TCCR2B_FOC2A': '$80', 'TCCR2B_FOC2B': '$40', 'TCCR2B_WGM22': '$08', 'TCCR2B_CS2': '$07', 'TCNT2': '&178', 'OCR2B': '&180', 'OCR2A': '&179', 'ASSR': '&182', 'ASSR_EXCLK': '$40', 'ASSR_AS2': '$20', 'ASSR_TCN2UB': '$10', 'ASSR_OCR2AUB': '$08', 'ASSR_OCR2BUB': '$04', 'ASSR_TCR2AUB': '$02', 'ASSR_TCR2BUB': '$01', 'TCCR3A': '&144', 'TCCR3A_COM3A': '$C0', 'TCCR3A_COM3B': '$30', 'TCCR3A_COM3C': '$0C', 'TCCR3A_WGM3': '$03', 'TCCR3B': '&145', 'TCCR3B_ICNC3': '$80', 'TCCR3B_ICES3': '$40', 'TCCR3B_WGM3': '$18', 'TCCR3B_CS3': '$07', 'TCCR3C': '&146', 'TCCR3C_FOC3A': '$80', 'TCCR3C_FOC3B': '$40', 'TCCR3C_FOC3C': '$20', 'TCNT3': '&148', 'OCR3A': '&152', 'OCR3B': '&154', 'OCR3C': '&156', 'ICR3': '&150', 'TIMSK3': '&113', 'TIMSK3_ICIE3': '$20', 'TIMSK3_OCIE3C': '$08', 'TIMSK3_OCIE3B': '$04', 'TIMSK3_OCIE3A': '$02', 'TIMSK3_TOIE3': '$01', 'TIFR3': '&56', 'TIFR3_ICF3': '$20', 'TIFR3_OCF3C': '$08', 'TIFR3_OCF3B': '$04', 'TIFR3_OCF3A': '$02', 'TIFR3_TOV3': '$01', 'TCCR1A': '&128', 'TCCR1A_COM1A': '$C0', 'TCCR1A_COM1B': '$30', 'TCCR1A_COM1C': '$0C', 'TCCR1A_WGM1': '$03', 'TCCR1B': '&129', 'TCCR1B_ICNC1': '$80', 'TCCR1B_ICES1': '$40', 'TCCR1B_WGM1': '$18', 'TCCR1B_CS1': '$07', 'TCCR1C': '&130', 'TCCR1C_FOC1A': '$80', 'TCCR1C_FOC1B': '$40', 'TCCR1C_FOC1C': '$20', 'TCNT1': '&132', 'OCR1A': '&136', 'OCR1B': '&138', 'OCR1C': '&140', 'ICR1': '&134', 'TIMSK1': '&111', 'TIMSK1_ICIE1': '$20', 'TIMSK1_OCIE1C': '$08', 'TIMSK1_OCIE1B': '$04', 'TIMSK1_OCIE1A': '$02', 'TIMSK1_TOIE1': '$01', 'TIFR1': '&54', 'TIFR1_ICF1': '$20', 'TIFR1_OCF1C': '$08', 'TIFR1_OCF1B': '$04', 'TIFR1_OCF1A': '$02', 'TIFR1_TOV1': '$01', 'OCDR': '&81', 'EICRA': '&105', 'EICRA_ISC3': '$C0', 'EICRA_ISC2': '$30', 'EICRA_ISC1': '$0C', 'EICRA_ISC0': '$03', 'EICRB': '&106', 'EICRB_ISC7': '$C0', 'EICRB_ISC6': '$30', 'EICRB_ISC5': '$0C', 'EICRB_ISC4': '$03', 'EIMSK': '&61', 'EIMSK_INT': '$FF', 'EIFR': '&60', 'EIFR_INTF': '$FF', 'PCMSK0': '&107', 'PCIFR': '&59', 'PCIFR_PCIF0': '$01', 'PCICR': '&104', 'PCICR_PCIE0': '$01', 'ADMUX': '&124', 'ADMUX_REFS': '$C0', 'ADMUX_ADLAR': '$20', 'ADMUX_MUX': '$1F', 'ADCSRA': '&122', 'ADCSRA_ADEN': '$80', 'ADCSRA_ADSC': '$40', 'ADCSRA_ADATE': '$20', 'ADCSRA_ADIF': '$10', 'ADCSRA_ADIE': '$08', 'ADCSRA_ADPS': '$07', 'ADC': '&120', 'ADCSRB': '&123', 'ADCSRB_ADHSM': '$80', 'ADCSRB_ADTS': '$07', 'DIDR0': '&126', 'DIDR0_ADC7D': '$80', 'DIDR0_ADC6D': '$40', 'DIDR0_ADC5D': '$20', 'DIDR0_ADC4D': '$10', 'DIDR0_ADC3D': '$08', 'DIDR0_ADC2D': '$04', 'DIDR0_ADC1D': '$02', 'DIDR0_ADC0D': '$01', 'ACSR': '&80', 'ACSR_ACD': '$80', 'ACSR_ACBG': '$40', 'ACSR_ACO': '$20', 'ACSR_ACI': '$10', 'ACSR_ACIE': '$08', 'ACSR_ACIC': '$04', 'ACSR_ACIS': '$03', 'DIDR1': '&127', 'DIDR1_AIN1D': '$02', 'DIDR1_AIN0D': '$01', 'PLLCSR': '&73', 'PLLCSR_PLLP': '$1C', 'PLLCSR_PLLE': '$02', 'PLLCSR_PLOCK': '$01', 'INT0Addr': '2', 'INT1Addr': '4', 'INT2Addr': '6', 'INT3Addr': '8', 'INT4Addr': '10', 'INT5Addr': '12', 'INT6Addr': '14', 'INT7Addr': '16', 'PCINT0Addr': '18', 'USB_GENAddr': '20', 'USB_COMAddr': '22', 'WDTAddr': '24', 'TIMER2_COMPAAddr': '26', 'TIMER2_COMPBAddr': '28', 'TIMER2_OVFAddr': '30', 'TIMER1_CAPTAddr': '32', 'TIMER1_COMPAAddr': '34', 'TIMER1_COMPBAddr': '36', 'TIMER1_COMPCAddr': '38', 'TIMER1_OVFAddr': '40', 'TIMER0_COMPAAddr': '42', 'TIMER0_COMPBAddr': '44', 'TIMER0_OVFAddr': '46', 'SPI__STCAddr': '48', 'USART1__RXAddr': '50', 'USART1__UDREAddr': '52', 'USART1__TXAddr': '54', 'ANALOG_COMPAddr': '56', 'ADCAddr': '58', 'EE_READYAddr': '60', 'TIMER3_CAPTAddr': '62', 'TIMER3_COMPAAddr': '64', 'TIMER3_COMPBAddr': '66', 'TIMER3_COMPCAddr': '68', 'TIMER3_OVFAddr': '70', 'TWIAddr': '72', 'SPM_READYAddr': '74' }
# -*- coding: utf-8 -*- __author__ = 'Jace Xu' __version__ = "0.3" from .autoit import options, properties, commands from .autoit import AutoItError from .autoit import error from .autoit import auto_it_set_option from .autoit import clip_get from .autoit import clip_put from .autoit import is_admin from .autoit import drive_map_add from .autoit import drive_map_del from .autoit import drive_map_get from .autoit import mouse_click from .autoit import mouse_click_drag from .autoit import mouse_down from .autoit import mouse_get_cursor from .autoit import mouse_get_pos from .autoit import mouse_move from .autoit import mouse_up from .autoit import mouse_wheel from .autoit import opt from .autoit import pixel_checksum from .autoit import pixel_get_color from .autoit import pixel_search from .autoit import send from .autoit import tooltip from .process import run from .process import run_wait from .process import process_close from .process import process_exists from .process import process_set_priority from .process import process_wait from .process import process_wait_close from .process import run_as from .process import run_as_wait from .process import shutdown from .win import win_activate from .win import win_activate_by_handle from .win import win_active from .win import win_active_by_handle from .win import win_close from .win import win_close_by_handle from .win import win_exists from .win import win_exists_by_handle from .win import win_get_caret_pos from .win import win_get_class_list from .win import win_get_class_list_by_handle from .win import win_get_client_size from .win import win_get_client_size_by_handle from .win import win_get_handle from .win import win_get_handle_as_text from .win import win_get_pos from .win import win_get_pos_by_handle from .win import win_get_process from .win import win_get_process_by_handle from .win import win_get_state from .win import win_get_state_by_handle from .win import win_get_text from .win import win_get_text_by_handle from .win import win_get_title from .win import win_get_title_by_handle from .win import win_kill from .win import win_kill_by_handle from .win import win_menu_select_item from .win import win_menu_select_item_by_handle from .win import win_minimize_all from .win import win_minimize_all_undo from .win import win_move from .win import win_move_by_handle from .win import win_set_on_top from .win import win_set_on_top_by_handle from .win import win_set_state from .win import win_set_state_by_handle from .win import win_set_title from .win import win_set_title_by_handle from .win import win_set_trans from .win import win_set_trans_by_handle from .win import win_wait from .win import win_wait_by_handle from .win import win_wait_active from .win import win_wait_active_by_handle from .win import win_wait_close from .win import win_wait_close_by_handle from .win import win_wait_not_active from .win import win_wait_not_active_by_handle from .control import control_click from .control import control_click_by_handle from .control import control_command from .control import control_command_by_handle from .control import control_list_view from .control import control_list_view_by_handle from .control import control_disable from .control import control_disable_by_handle from .control import control_enable from .control import control_enable_by_handle from .control import control_focus from .control import control_focus_by_handle from .control import control_get_focus from .control import control_get_focus_by_handle from .control import control_get_handle from .control import control_get_handle_as_text from .control import control_get_pos from .control import control_get_pos_by_handle from .control import control_get_text from .control import control_get_text_by_handle from .control import control_hide from .control import control_hide_by_handle from .control import control_move from .control import control_move_by_handle from .control import control_send from .control import control_send_by_handle from .control import control_set_text from .control import control_set_text_by_handle from .control import control_show from .control import control_show_by_handle from .control import control_tree_view from .control import control_tree_view_by_handle from .control import statusbar_get_text from .control import statusbar_get_text_by_handle
#! /usr/bin/env python # encoding: utf-8 # WARNING! Do not edit! http://waf.googlecode.com/git/docs/wafbook/single.html#_obtaining_the_waf_file import os,tempfile,optparse,sys,re from waflib import Logs,Utils,Context cmds='distclean configure build install clean uninstall check dist distcheck'.split() options={} commands=[] lockfile=os.environ.get('WAFLOCK','.lock-waf_%s_build'%sys.platform) try:cache_global=os.path.abspath(os.environ['WAFCACHE']) except KeyError:cache_global='' platform=Utils.unversioned_sys_platform() class opt_parser(optparse.OptionParser): def __init__(self,ctx): optparse.OptionParser.__init__(self,conflict_handler="resolve",version='waf %s (%s)'%(Context.WAFVERSION,Context.WAFREVISION)) self.formatter.width=Logs.get_term_cols() p=self.add_option self.ctx=ctx jobs=ctx.jobs() p('-j','--jobs',dest='jobs',default=jobs,type='int',help='amount of parallel jobs (%r)'%jobs) p('-k','--keep',dest='keep',default=0,action='count',help='keep running happily even if errors are found') p('-v','--verbose',dest='verbose',default=0,action='count',help='verbosity level -v -vv or -vvv [default: 0]') p('--nocache',dest='nocache',default=False,action='store_true',help='ignore the WAFCACHE (if set)') p('--zones',dest='zones',default='',action='store',help='debugging zones (task_gen, deps, tasks, etc)') gr=optparse.OptionGroup(self,'configure options') self.add_option_group(gr) gr.add_option('-o','--out',action='store',default='',help='build dir for the project',dest='out') gr.add_option('-t','--top',action='store',default='',help='src dir for the project',dest='top') default_prefix=os.environ.get('PREFIX') if not default_prefix: if platform=='win32': d=tempfile.gettempdir() default_prefix=d[0].upper()+d[1:] else: default_prefix='/usr/local/' gr.add_option('--prefix',dest='prefix',default=default_prefix,help='installation prefix [default: %r]'%default_prefix) gr.add_option('--download',dest='download',default=False,action='store_true',help='try to download the tools if missing') gr=optparse.OptionGroup(self,'build and install options') self.add_option_group(gr) gr.add_option('-p','--progress',dest='progress_bar',default=0,action='count',help='-p: progress bar; -pp: ide output') gr.add_option('--targets',dest='targets',default='',action='store',help='task generators, e.g. "target1,target2"') gr=optparse.OptionGroup(self,'step options') self.add_option_group(gr) gr.add_option('--files',dest='files',default='',action='store',help='files to process, by regexp, e.g. "*/main.c,*/test/main.o"') default_destdir=os.environ.get('DESTDIR','') gr=optparse.OptionGroup(self,'install/uninstall options') self.add_option_group(gr) gr.add_option('--destdir',help='installation root [default: %r]'%default_destdir,default=default_destdir,dest='destdir') gr.add_option('-f','--force',dest='force',default=False,action='store_true',help='force file installation') gr.add_option('--distcheck-args',help='arguments to pass to distcheck',default=None,action='store') def get_usage(self): cmds_str={} for cls in Context.classes: if not cls.cmd or cls.cmd=='options': continue s=cls.__doc__ or'' cmds_str[cls.cmd]=s if Context.g_module: for(k,v)in Context.g_module.__dict__.items(): if k in['options','init','shutdown']: continue if type(v)is type(Context.create_context): if v.__doc__ and not k.startswith('_'): cmds_str[k]=v.__doc__ just=0 for k in cmds_str: just=max(just,len(k)) lst=[' %s: %s'%(k.ljust(just),v)for(k,v)in cmds_str.items()] lst.sort() ret='\n'.join(lst) return'''waf [commands] [options] Main commands (example: ./waf build -j4) %s '''%ret class OptionsContext(Context.Context): cmd='options' fun='options' def __init__(self,**kw): super(OptionsContext,self).__init__(**kw) self.parser=opt_parser(self) self.option_groups={} def jobs(self): count=int(os.environ.get('JOBS',0)) if count<1: if'NUMBER_OF_PROCESSORS'in os.environ: count=int(os.environ.get('NUMBER_OF_PROCESSORS',1)) else: if hasattr(os,'sysconf_names'): if'SC_NPROCESSORS_ONLN'in os.sysconf_names: count=int(os.sysconf('SC_NPROCESSORS_ONLN')) elif'SC_NPROCESSORS_CONF'in os.sysconf_names: count=int(os.sysconf('SC_NPROCESSORS_CONF')) if not count and os.name not in('nt','java'): try: tmp=self.cmd_and_log(['sysctl','-n','hw.ncpu'],quiet=0) except Exception: pass else: if re.match('^[0-9]+$',tmp): count=int(tmp) if count<1: count=1 elif count>1024: count=1024 return count def add_option(self,*k,**kw): return self.parser.add_option(*k,**kw) def add_option_group(self,*k,**kw): try: gr=self.option_groups[k[0]] except KeyError: gr=self.parser.add_option_group(*k,**kw) self.option_groups[k[0]]=gr return gr def get_option_group(self,opt_str): try: return self.option_groups[opt_str] except KeyError: for group in self.parser.option_groups: if group.title==opt_str: return group return None def parse_args(self,_args=None): global options,commands (options,leftover_args)=self.parser.parse_args(args=_args) commands=leftover_args if options.destdir: options.destdir=os.path.abspath(os.path.expanduser(options.destdir)) if options.verbose>=1: self.load('errcheck') def execute(self): super(OptionsContext,self).execute() self.parse_args()
from .base import TestBase from ..object import ObjectFile from ..object import Relocation from ..object import Section from ..object import Symbol class TestObjectFile(TestBase): def get_object_file(self): source = self.get_test_binary() return ObjectFile(filename=source) def test_create_from_file(self): self.get_object_file() def test_get_sections(self): o = self.get_object_file() count = 0 for section in o.get_sections(): count += 1 assert isinstance(section, Section) assert isinstance(section.name, str) assert isinstance(section.size, long) assert isinstance(section.contents, str) assert isinstance(section.address, long) assert len(section.contents) == section.size self.assertGreater(count, 0) for section in o.get_sections(): section.cache() def test_get_symbols(self): o = self.get_object_file() count = 0 for symbol in o.get_symbols(): count += 1 assert isinstance(symbol, Symbol) assert isinstance(symbol.name, str) assert isinstance(symbol.address, long) assert isinstance(symbol.size, long) self.assertGreater(count, 0) for symbol in o.get_symbols(): symbol.cache() def test_symbol_section_accessor(self): o = self.get_object_file() for symbol in o.get_symbols(): section = symbol.section assert isinstance(section, Section) break def test_get_relocations(self): o = self.get_object_file() for section in o.get_sections(): for relocation in section.get_relocations(): assert isinstance(relocation, Relocation) assert isinstance(relocation.address, long) assert isinstance(relocation.offset, long) assert isinstance(relocation.type_number, long) assert isinstance(relocation.type_name, str) assert isinstance(relocation.value_string, str)
# -*- coding: utf-8 -*- """ Use nose `$ pip install nose` `$ nosetests` """ from hyde.fs import File, Folder from hyde.generator import Generator from hyde.site import Site from pyquery import PyQuery from nose.tools import nottest TEST_SITE = File(__file__).parent.parent.child_folder('_test') class TestAutoExtend(object): def setUp(self): TEST_SITE.make() TEST_SITE.parent.child_folder( 'sites/test_jinja').copy_contents_to(TEST_SITE) def tearDown(self): TEST_SITE.delete() @nottest def assert_extended(self, s, txt, templ): content = (templ.strip() % txt).strip() bd = File(TEST_SITE.child('content/auto_extend.html')) bd.write(content) gen = Generator(s) gen.generate_resource_at_path(bd.path) res = s.content.resource_from_path(bd.path) target = File(s.config.deploy_root_path.child(res.relative_deploy_path)) assert target.exists text = target.read_all() q = PyQuery(text) assert q('title').text().strip() == txt.strip() def test_can_auto_extend(self): s = Site(TEST_SITE) s.config.plugins = ['hyde.ext.plugins.meta.MetaPlugin', 'hyde.ext.plugins.auto_extend.AutoExtendPlugin', 'hyde.ext.plugins.blockdown.BlockdownPlugin'] txt ="This template tests to make sure blocks can be replaced with markdownish syntax." templ = """ --- extends: base.html --- =====title======== %s ====/title========""" self.assert_extended(s, txt, templ) def test_can_auto_extend_with_default_blocks(self): s = Site(TEST_SITE) s.config.plugins = ['hyde.ext.plugins.meta.MetaPlugin', 'hyde.ext.plugins.auto_extend.AutoExtendPlugin', 'hyde.ext.plugins.blockdown.BlockdownPlugin'] txt ="This template tests to make sure blocks can be replaced with markdownish syntax." templ = """ --- extends: base.html default_block: title --- %s """ self.assert_extended(s, txt, templ)
# -*- coding: utf-8 -*- from __future__ import with_statement from contextlib import contextmanager from django.test import TestCase from cms.api import create_page from cms.signals import urls_need_reloading class SignalTester(object): def __init__(self): self.call_count = 0 self.calls = [] def __call__(self, *args, **kwargs): self.call_count += 1 self.calls.append((args, kwargs)) @contextmanager def signal_tester(signal): env = SignalTester() signal.connect(env, weak=True) try: yield env finally: signal.disconnect(env, weak=True) class SignalTests(TestCase): def test_urls_need_reloading_signal_create(self): with signal_tester(urls_need_reloading) as env: self.client.get('/') self.assertEqual(env.call_count, 0) create_page( "apphooked-page", "nav_playground.html", "en", published=True, apphook="SampleApp", apphook_namespace="test" ) self.client.get('/') self.assertEqual(env.call_count, 1) def test_urls_need_reloading_signal_delete(self): with signal_tester(urls_need_reloading) as env: self.client.get('/') self.assertEqual(env.call_count, 0) page = create_page( "apphooked-page", "nav_playground.html", "en", published=True, apphook="SampleApp", apphook_namespace="test" ) page.delete() self.client.get('/') self.assertEqual(env.call_count, 1) def test_urls_need_reloading_signal_change_slug(self): with signal_tester(urls_need_reloading) as env: self.assertEqual(env.call_count, 0) page = create_page( "apphooked-page", "nav_playground.html", "en", published=True, apphook="SampleApp", apphook_namespace="test" ) self.client.get('/') self.assertEqual(env.call_count, 1) title = page.title_set.get(language="en") title.slug += 'test' title.save() page.publish('en') self.client.get('/') self.assertEqual(env.call_count, 2)
#!/usr/bin/env python # Copyright (c) 2012 The Chromium Authors. All rights reserved. # Use of this source code is governed by a BSD-style license that can be # found in the LICENSE file. """Script to check that the Windows 8 SDK has been appropriately patched so that it can be used with VS 2010. In practice, this checks for the presence of 'enum class' in asyncinfo.h. Changing that to 'enum' is the only thing needed to build with the WinRT headers in VS 2010. """ import os import sys def main(argv): if len(argv) < 2: print "Usage: check_sdk_patch.py path_to_windows_8_sdk [dummy_output_file]" return 1 # Look for asyncinfo.h async_info_path = os.path.join(argv[1], 'Include/winrt/asyncinfo.h') if not os.path.exists(async_info_path): print ("Could not find %s in provided SDK path. Please check input." % async_info_path) print "CWD: %s" % os.getcwd() return 2 else: if 'enum class' in open(async_info_path).read(): print ("\nERROR: You are using an unpatched Windows 8 SDK located at %s." "\nPlease see instructions at" "\nhttp://www.chromium.org/developers/how-tos/" "build-instructions-windows\nfor how to apply the patch to build " "with VS2010.\n" % argv[1]) return 3 else: if len(argv) > 2: with open(argv[2], 'w') as dummy_file: dummy_file.write('Windows 8 SDK has been patched!') # Patched Windows 8 SDK found. return 0 if '__main__' == __name__: sys.exit(main(sys.argv))
"""SCons.Tool.ifl Tool-specific initialization for the Intel Fortran compiler. There normally shouldn't be any need to import this module directly. It will usually be imported through the generic SCons.Tool.Tool() selection method. """ # # Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010 The SCons Foundation # # Permission is hereby granted, free of charge, to any person obtaining # a copy of this software and associated documentation files (the # "Software"), to deal in the Software without restriction, including # without limitation the rights to use, copy, modify, merge, publish, # distribute, sublicense, and/or sell copies of the Software, and to # permit persons to whom the Software is furnished to do so, subject to # the following conditions: # # The above copyright notice and this permission notice shall be included # in all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY # KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE # WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND # NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE # LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION # OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION # WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. # __revision__ = "src/engine/SCons/Tool/ifl.py 5134 2010/08/16 23:02:40 bdeegan" import SCons.Defaults from SCons.Scanner.Fortran import FortranScan from FortranCommon import add_all_to_env def generate(env): """Add Builders and construction variables for ifl to an Environment.""" fscan = FortranScan("FORTRANPATH") SCons.Tool.SourceFileScanner.add_scanner('.i', fscan) SCons.Tool.SourceFileScanner.add_scanner('.i90', fscan) if 'FORTRANFILESUFFIXES' not in env: env['FORTRANFILESUFFIXES'] = ['.i'] else: env['FORTRANFILESUFFIXES'].append('.i') if 'F90FILESUFFIXES' not in env: env['F90FILESUFFIXES'] = ['.i90'] else: env['F90FILESUFFIXES'].append('.i90') add_all_to_env(env) env['FORTRAN'] = 'ifl' env['SHFORTRAN'] = '$FORTRAN' env['FORTRANCOM'] = '$FORTRAN $FORTRANFLAGS $_FORTRANINCFLAGS /c $SOURCES /Fo$TARGET' env['FORTRANPPCOM'] = '$FORTRAN $FORTRANFLAGS $CPPFLAGS $_CPPDEFFLAGS $_FORTRANINCFLAGS /c $SOURCES /Fo$TARGET' env['SHFORTRANCOM'] = '$SHFORTRAN $SHFORTRANFLAGS $_FORTRANINCFLAGS /c $SOURCES /Fo$TARGET' env['SHFORTRANPPCOM'] = '$SHFORTRAN $SHFORTRANFLAGS $CPPFLAGS $_CPPDEFFLAGS $_FORTRANINCFLAGS /c $SOURCES /Fo$TARGET' def exists(env): return env.Detect('ifl') # Local Variables: # tab-width:4 # indent-tabs-mode:nil # End: # vim: set expandtab tabstop=4 shiftwidth=4:
# -*- encoding: utf-8 -*- # Part of Odoo. See LICENSE file for full copyright and licensing details. # Copyright (C) 2014 Tech Receptives (<http://techreceptives.com>) { 'name': 'Singapore - Accounting', 'version': '1.0', 'author': 'Tech Receptives', 'website': 'http://www.techreceptives.com', 'category': 'Localization/Account Charts', 'description': """ Singapore accounting chart and localization. ======================================================= After installing this module, the Configuration wizard for accounting is launched. * The Chart of Accounts consists of the list of all the general ledger accounts required to maintain the transactions of Singapore. * On that particular wizard, you will be asked to pass the name of the company, the chart template to follow, the no. of digits to generate, the code for your account and bank account, currency to create journals. * The Chart of Taxes would display the different types/groups of taxes such as Standard Rates, Zeroed, Exempted, MES and Out of Scope. * The tax codes are specified considering the Tax Group and for easy accessibility of submission of GST Tax Report. """, 'depends': ['base', 'account'], 'demo': [ ], 'data': [ 'l10n_sg_chart.xml', 'l10n_sg_chart_tax.xml', 'account_chart_template.yml', ], 'installable': True, }
""" Extension for building Qt-like documentation. - Method lists preceding the actual method documentation - Inherited members documented separately - Members inherited from Qt have links to qt-project documentation - Signal documentation """ def setup(app): # probably we will be making a wrapper around autodoc app.setup_extension('sphinx.ext.autodoc') # would it be useful to define a new domain? #app.add_domain(QtDomain) ## Add new configuration options app.add_config_value('todo_include_todos', False, False) ## Nodes are the basic objects representing documentation directives ## and roles app.add_node(Todolist) app.add_node(Todo, html=(visit_todo_node, depart_todo_node), latex=(visit_todo_node, depart_todo_node), text=(visit_todo_node, depart_todo_node)) ## New directives like ".. todo:" app.add_directive('todo', TodoDirective) app.add_directive('todolist', TodolistDirective) ## Connect callbacks to specific hooks in the build process app.connect('doctree-resolved', process_todo_nodes) app.connect('env-purge-doc', purge_todos) from docutils import nodes from sphinx.util.compat import Directive from sphinx.util.compat import make_admonition # Just a general node class Todolist(nodes.General, nodes.Element): pass # .. and its directive class TodolistDirective(Directive): # all directives have 'run' method that returns a list of nodes def run(self): return [Todolist('')] # Admonition classes are like notes or warnings class Todo(nodes.Admonition, nodes.Element): pass def visit_todo_node(self, node): self.visit_admonition(node) def depart_todo_node(self, node): self.depart_admonition(node) class TodoDirective(Directive): # this enables content in the directive has_content = True def run(self): env = self.state.document.settings.env # create a new target node for linking to targetid = "todo-%d" % env.new_serialno('todo') targetnode = nodes.target('', '', ids=[targetid]) # make the admonition node ad = make_admonition(Todo, self.name, [('Todo')], self.options, self.content, self.lineno, self.content_offset, self.block_text, self.state, self.state_machine) # store a handle in a global list of all todos if not hasattr(env, 'todo_all_todos'): env.todo_all_todos = [] env.todo_all_todos.append({ 'docname': env.docname, 'lineno': self.lineno, 'todo': ad[0].deepcopy(), 'target': targetnode, }) # return both the linking target and the node itself return [targetnode] + ad # env data is persistent across source files so we purge whenever the source file has changed. def purge_todos(app, env, docname): if not hasattr(env, 'todo_all_todos'): return env.todo_all_todos = [todo for todo in env.todo_all_todos if todo['docname'] != docname] # called at the end of resolving phase; we will convert temporary nodes # into finalized nodes def process_todo_nodes(app, doctree, fromdocname): if not app.config.todo_include_todos: for node in doctree.traverse(Todo): node.parent.remove(node) # Replace all todolist nodes with a list of the collected todos. # Augment each todo with a backlink to the original location. env = app.builder.env for node in doctree.traverse(Todolist): if not app.config.todo_include_todos: node.replace_self([]) continue content = [] for todo_info in env.todo_all_todos: para = nodes.paragraph() filename = env.doc2path(todo_info['docname'], base=None) description = ( ('(The original entry is located in %s, line %d and can be found ') % (filename, todo_info['lineno'])) para += nodes.Text(description, description) # Create a reference newnode = nodes.reference('', '') innernode = nodes.emphasis(('here'), ('here')) newnode['refdocname'] = todo_info['docname'] newnode['refuri'] = app.builder.get_relative_uri( fromdocname, todo_info['docname']) newnode['refuri'] += '#' + todo_info['target']['refid'] newnode.append(innernode) para += newnode para += nodes.Text('.)', '.)') # Insert into the todolist content.append(todo_info['todo']) content.append(para) node.replace_self(content)
#!/usr/bin/env python from validictory.validator import (SchemaValidator, FieldValidationError, ValidationError, SchemaError) __all__ = ['validate', 'SchemaValidator', 'FieldValidationError', 'ValidationError', 'SchemaError'] __version__ = '0.9.3' def validate(data, schema, validator_cls=SchemaValidator, format_validators=None, required_by_default=True, blank_by_default=False, disallow_unknown_properties=False, apply_default_to_data=False): ''' Validates a parsed json document against the provided schema. If an error is found a :class:`ValidationError` is raised. If there is an issue in the schema a :class:`SchemaError` will be raised. :param data: python data to validate :param schema: python dictionary representing the schema (see `schema format`_) :param validator_cls: optional validator class (default is :class:`SchemaValidator`) :param format_validators: optional dictionary of custom format validators :param required_by_default: defaults to True, set to False to make ``required`` schema attribute False by default. :param disallow_unknown_properties: defaults to False, set to True to disallow properties not listed in the schema definition :param apply_default_to_data: defaults to False, set to True to modify the data in case the schema definition includes a "default" property ''' v = validator_cls(format_validators, required_by_default, blank_by_default, disallow_unknown_properties, apply_default_to_data) return v.validate(data, schema) if __name__ == '__main__': import sys import json if len(sys.argv) == 2: if sys.argv[1] == "--help": raise SystemExit("%s SCHEMAFILE [INFILE]" % (sys.argv[0],)) schemafile = open(sys.argv[1], 'rb') infile = sys.stdin elif len(sys.argv) == 3: schemafile = open(sys.argv[1], 'rb') infile = open(sys.argv[2], 'rb') else: raise SystemExit("%s SCHEMAFILE [INFILE]" % (sys.argv[0],)) try: obj = json.load(infile) schema = json.load(schemafile) validate(obj, schema) except ValueError as e: raise SystemExit(e)
"""This class stores all of the samples for training. It is able to construct randomly selected batches of phi's from the stored history. It allocates more memory than necessary, then shifts all of the data back to 0 when the samples reach the end of the allocated memory. """ import pyximport import numpy as np pyximport.install(setup_args={'include_dirs': np.get_include()}) import shift import time import theano floatX = theano.config.floatX class DataSet(object): """ Class represents a data set that stores a fixed-length history. """ def __init__(self, width, height, rng, max_steps=1000, phi_length=4, capacity=None): """ Construct a DataSet. Arguments: width,height - image size max_steps - the length of history to store. phi_length - number of images to concatenate into a state. rng - initialized numpy random number generator. capacity - amount of memory to allocate (just for debugging.) """ self.rng = rng self.count = 0 self.max_steps = max_steps self.phi_length = phi_length if capacity == None: self.capacity = max_steps + int(np.ceil(max_steps * .1)) else: self.capacity = capacity self.states = np.zeros((self.capacity, height, width), dtype='uint8') self.actions = np.zeros(self.capacity, dtype='int32') self.rewards = np.zeros(self.capacity, dtype=floatX) self.terminal = np.zeros(self.capacity, dtype='bool') def _min_index(self): return max(0, self.count - self.max_steps) def _max_index(self): return self.count - (self.phi_length + 1) def __len__(self): """ Return the total number of avaible data items. """ return max(0, (self._max_index() - self._min_index()) + 1) def add_sample(self, state, action, reward, terminal): self.states[self.count, ...] = state self.actions[self.count] = action self.rewards[self.count] = reward self.terminal[self.count] = terminal self.count += 1 # Shift the final max_steps back to the beginning. if self.count == self.capacity: roll_amount = self.capacity - self.max_steps shift.shift3d_uint8(self.states, roll_amount) self.actions = np.roll(self.actions, -roll_amount) self.rewards = np.roll(self.rewards, -roll_amount) self.terminal = np.roll(self.terminal, -roll_amount) self.count = self.max_steps def single_episode(self, start, end): """ Make sure that a possible phi does not cross a trial boundary. """ return np.alltrue(np.logical_not(self.terminal[start:end])) def last_phi(self): """ Return the most recent phi. """ phi = self._make_phi(self.count - self.phi_length) return np.array(phi, dtype=floatX) def phi(self, state): """ Return a phi based on the latest image, by grabbing enough history from the data set to fill it out. """ phi = np.empty((self.phi_length, self.states.shape[1], self.states.shape[2]), dtype=floatX) phi[0:(self.phi_length-1), ...] = self.last_phi()[1::] phi[self.phi_length-1, ...] = state return phi def _make_phi(self, index): end_index = index + self.phi_length - 1 #assert self.single_episode(index, end_index) return self.states[index:end_index + 1, ...] def _empty_batch(self, batch_size): # Set aside memory for the batch states = np.empty((batch_size, self.phi_length, self.states.shape[1], self.states.shape[2]), dtype=floatX) actions = np.empty((batch_size, 1), dtype='int32') rewards = np.empty((batch_size, 1), dtype=floatX) terminals = np.empty((batch_size, 1), dtype=bool) next_states = np.empty((batch_size, self.phi_length, self.states.shape[1], self.states.shape[2]), dtype=floatX) return states, actions, rewards, terminals, next_states def batch_iterator(self, batch_size): """ Generator for iterating over all valid batches. """ index = self._min_index() batch_count = 0 states, actions, rewards, terminals, next_states = \ self._empty_batch(batch_size) while index <= self._max_index(): end_index = index + self.phi_length - 1 if self.single_episode(index, end_index): states[batch_count, ...] = self._make_phi(index) actions[batch_count, 0] = self.actions[end_index] rewards[batch_count, 0] = self.rewards[end_index] terminals[batch_count, 0] = self.terminal[end_index] next_states[batch_count, ...] = self._make_phi(index + 1) batch_count += 1 index += 1 if batch_count == batch_size: yield states, actions, rewards, terminals, next_states batch_count = 0 states, actions, rewards, terminals, next_states = \ self._empty_batch(batch_size) def random_batch(self, batch_size): count = 0 states, actions, rewards, terminals, next_states = \ self._empty_batch(batch_size) # Grab random samples until we have enough while count < batch_size: index = self.rng.randint(self._min_index(), self._max_index()+1) end_index = index + self.phi_length - 1 if self.single_episode(index, end_index): states[count, ...] = self._make_phi(index) actions[count, 0] = self.actions[end_index] rewards[count, 0] = self.rewards[end_index] terminals[count, 0] = self.terminal[end_index] next_states[count, ...] = self._make_phi(index + 1) count += 1 return states, actions, rewards, next_states, terminals # TESTING CODE BELOW THIS POINT... def simple_tests(): np.random.seed(222) dataset = DataSet(width=2, height=3, max_steps=6, phi_length=4, capacity=7) for i in range(10): img = np.random.randint(0, 256, size=(3, 2)) action = np.random.randint(16) reward = np.random.random() terminal = False if np.random.random() < .05: terminal = True print 'img', img dataset.add_sample(img, action, reward, terminal) print "S", dataset.states print "A", dataset.actions print "R", dataset.rewards print "T", dataset.terminal print "COUNT", "CAPACITY", dataset.count, dataset.capacity print print "LAST PHI", dataset.last_phi() print print 'BATCH', dataset.random_batch(2) def speed_tests(): dataset = DataSet(width=80, height=80, max_steps=20000, phi_length=4) img = np.random.randint(0, 256, size=(80, 80)) action = np.random.randint(16) reward = np.random.random() start = time.time() for i in range(100000): terminal = False if np.random.random() < .05: terminal = True dataset.add_sample(img, action, reward, terminal) print "samples per second: ", 100000 / (time.time() - start) start = time.time() for i in range(200): a = dataset.random_batch(32) print "batches per second: ", 200 / (time.time() - start) print dataset.last_phi() def trivial_tests(): dataset = DataSet(width=2, height=1, max_steps=3, phi_length=2) img1 = np.array([[1, 1]], dtype='uint8') img2 = np.array([[2, 2]], dtype='uint8') img3 = np.array([[3, 3]], dtype='uint8') dataset.add_sample(img1, 1, 1, False) dataset.add_sample(img2, 2, 2, False) dataset.add_sample(img3, 2, 2, True) print "last", dataset.last_phi() print "random", dataset.random_batch(1) def max_size_tests(): dataset1 = DataSet(width=3, height=4, max_steps=10, phi_length=4) dataset2 = DataSet(width=3, height=4, max_steps=1000, phi_length=4) for i in range(100): img = np.random.randint(0, 256, size=(4, 3)) action = np.random.randint(16) reward = np.random.random() terminal = False if np.random.random() < .05: terminal = True dataset1.add_sample(img, action, reward, terminal) dataset2.add_sample(img, action, reward, terminal) np.testing.assert_array_almost_equal(dataset1.last_phi(), dataset2.last_phi()) print "passed" def test_iterator(): dataset = DataSet(width=2, height=1, max_steps=10, phi_length=2) img1 = np.array([[1, 1]], dtype='uint8') img2 = np.array([[2, 2]], dtype='uint8') img3 = np.array([[3, 3]], dtype='uint8') img4 = np.array([[3, 3]], dtype='uint8') dataset.add_sample(img1, 1, 1, False) dataset.add_sample(img2, 2, 2, False) dataset.add_sample(img3, 3, 3, False) dataset.add_sample(img4, 4, 4, True) for s, a, r, t, ns in dataset.batch_iterator(2): print "s ", s, "a ",a, "r ",r,"t ", t,"ns ", ns def test_random_batch(): dataset1 = DataSet(width=3, height=4, max_steps=50, phi_length=4) dataset2 = DataSet(width=3, height=4, max_steps=50, phi_length=4, capacity=2000) np.random.seed(hash(time.time())) for i in range(100): img = np.random.randint(0, 256, size=(4, 3)) action = np.random.randint(16) reward = np.random.random() terminal = False if np.random.random() < .05: terminal = True dataset1.add_sample(img, action, reward, terminal) dataset2.add_sample(img, action, reward, terminal) if i > 10: np.random.seed(i*11 * i) states1, actions1, rewards1, next_states1, terminals1 = \ dataset1.random_batch(10) np.random.seed(i*11 * i) states2, actions2, rewards2, next_states2, terminals2 = \ dataset2.random_batch(10) np.testing.assert_array_almost_equal(states1, states2) np.testing.assert_array_almost_equal(actions1, actions2) np.testing.assert_array_almost_equal(rewards1, rewards2) np.testing.assert_array_almost_equal(next_states1, next_states2) np.testing.assert_array_almost_equal(terminals1, terminals2) # if not np.array_equal(states1, states2): # print states1,"\n", states2 # if not np.array_equal(actions1, actions2): # print actions1, "\n",actions2 # if not np.array_equal(rewards1, rewards2): # print rewards1, "\n",rewards2 # if not np.array_equal(next_states1, next_states2): # print next_states1, "\n",next_states2 # if not np.array_equal(terminals1, terminals2): # print terminals1, "\n",terminals2 np.random.seed(hash(time.time())) def test_memory_usage_ok(): import memory_profiler dataset = DataSet(width=80, height=80, max_steps=100000, phi_length=4) last = time.time() for i in xrange(1000000000): if (i % 100000) == 0: print i dataset.add_sample(np.random.random((80, 80)), 1, 1, False) if i > 200000: states, actions, rewards, next_states, terminals = \ dataset.random_batch(32) if (i % 10007) == 0: print time.time() - last mem_usage = memory_profiler.memory_usage(-1) print len(dataset), mem_usage last = time.time() def main(): #speed_tests() test_memory_usage_ok() #test_random_batch() #max_size_tests() #simple_tests() #test_iterator() if __name__ == "__main__": main()
""" Tests for barbante.maintenance.tasks.py. """ import datetime as dt import nose.tools import barbante.maintenance.tasks as tasks from barbante.maintenance.tests.fixtures.MaintenanceFixture import MaintenanceFixture import barbante.tests as tests class TestTasks(MaintenanceFixture): """ Test class for maintenance tasks. """ def __init__(self): super().__init__() def test_increment_product_popularity(self): product_1 = "p_mus_1" product_2 = "p_empty" product_ids = [product_1, product_2] popularity_map = self.session_context.data_proxy.fetch_product_popularity(product_ids=product_ids) # sanity check nose.tools.eq_(popularity_map[product_1], 3, "Wrong initial popularity") nose.tools.eq_(popularity_map.get(product_2), None, "Popularity should be None since no one consumed it") activity = {"external_user_id": "u_eco_1", "external_product_id": product_1, "activity": "buy", "created_at": self.session_context.get_present_date() - dt.timedelta(2)} tasks.update_summaries(self.session_context, activity) popularity_map = self.session_context.data_proxy.fetch_product_popularity(product_ids=product_ids) nose.tools.ok_(abs(popularity_map[product_1] - 2) < tests.FLOAT_DELTA, "Wrong popularity") # another activity by the same user, without extending the date range activity = {"external_user_id": "u_eco_1", "external_product_id": product_1, "activity": "buy", "created_at": self.session_context.get_present_date() - dt.timedelta(2)} tasks.update_summaries(self.session_context, activity) popularity_map = self.session_context.data_proxy.fetch_product_popularity(product_ids=product_ids) nose.tools.ok_(abs(popularity_map[product_1] - 2) < tests.FLOAT_DELTA, "Wrong popularity") # another activity by the same user, now extending the date range activity = {"external_user_id": "u_eco_1", "external_product_id": product_1, "activity": "buy", "created_at": self.session_context.get_present_date() - dt.timedelta(3)} tasks.update_summaries(self.session_context, activity) popularity_map = self.session_context.data_proxy.fetch_product_popularity(product_ids=product_ids) nose.tools.ok_(abs(popularity_map[product_1] - 4/3) < tests.FLOAT_DELTA, "Wrong popularity") def test_delete_product(self): product_id = "p_mus_1" tasks.delete_product(self.session_context, product_id) product_models = self.db_proxy.fetch_product_models() tf_map = self.db_proxy.fetch_tf_map("full_content", [product_id]) tfidf_map = self.db_proxy.fetch_tfidf_map("full_content", [product_id]) nose.tools.ok_(product_id not in product_models, "Product model should have been physically deleted") nose.tools.ok_(product_id not in tf_map, "TF's should have been physically deleted") nose.tools.ok_(product_id not in tfidf_map, "TFIDF's should have been physically deleted")
# Copyright (c) 2012 ARM Limited # All rights reserved. # # The license below extends only to copyright in the software and shall # not be construed as granting a license to any other intellectual # property including but not limited to intellectual property relating # to a hardware implementation of the functionality of the software # licensed hereunder. You may use the software subject to the license # terms below provided that you ensure that this notice is replicated # unmodified and in its entirety in all distributions of the software, # modified or unmodified, in source code or in binary form. # # Redistribution and use in source and binary forms, with or without # modification, are permitted provided that the following conditions are # met: redistributions of source code must retain the above copyright # notice, this list of conditions and the following disclaimer; # redistributions in binary form must reproduce the above copyright # notice, this list of conditions and the following disclaimer in the # documentation and/or other materials provided with the distribution; # neither the name of the copyright holders nor the names of its # contributors may be used to endorse or promote products derived from # this software without specific prior written permission. # # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS # "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT # LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR # A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT # OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, # SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT # LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, # DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY # THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT # (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE # OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. # # Authors: Andreas Sandberg from m5.params import * from m5.proxy import * from BaseCPU import BaseCPU from KvmVM import KvmVM class BaseKvmCPU(BaseCPU): type = 'BaseKvmCPU' cxx_header = "cpu/kvm/base.hh" abstract = True @classmethod def export_method_cxx_predecls(cls, code): code('#include "cpu/kvm/base.hh"') @classmethod def export_methods(cls, code): code(''' void dump(); ''') @classmethod def memory_mode(cls): return 'atomic_noncaching' @classmethod def require_caches(cls): return False @classmethod def support_take_over(cls): return True kvmVM = Param.KvmVM(Parent.any, 'KVM VM (i.e., shared memory domain)') useCoalescedMMIO = Param.Bool(False, "Use coalesced MMIO (EXPERIMENTAL)") usePerfOverflow = Param.Bool(False, "Use perf event overflow counters (EXPERIMENTAL)") hostFreq = Param.Clock("2GHz", "Host clock frequency") hostFactor = Param.Float(1.0, "Cycle scale factor")
# -*- coding: utf-8 -*- # # OpenERP Technical Documentation configuration file, created by # sphinx-quickstart on Fri Feb 17 16:14:06 2012. # # This file is execfile()d with the current directory set to its containing dir. # # Note that not all possible configuration values are present in this # autogenerated file. # # All configuration values have a default; values that are commented out # serve to show the default. import sys, os # If extensions (or modules to document with autodoc) are in another directory, # add these directories to sys.path here. If the directory is relative to the # documentation root, use os.path.abspath to make it absolute, like shown here. sys.path.append(os.path.abspath('_themes')) sys.path.insert(0, os.path.abspath('../addons')) sys.path.insert(0, os.path.abspath('..')) # -- General configuration ----------------------------------------------------- # If your documentation needs a minimal Sphinx version, state it here. #needs_sphinx = '1.0' # Add any Sphinx extension module names here, as strings. They can be extensions # coming with Sphinx (named 'sphinx.ext.*') or your custom ones. extensions = [ 'sphinx.ext.autodoc', 'sphinx.ext.intersphinx', 'sphinx.ext.todo', 'sphinx.ext.viewcode', 'patchqueue' ] # Add any paths that contain templates here, relative to this directory. templates_path = ['_templates'] # The suffix of source filenames. source_suffix = '.rst' # The encoding of source files. #source_encoding = 'utf-8-sig' # The master toctree document. master_doc = 'index' # General information about the project. project = u'OpenERP Web Developers Documentation' copyright = u'2012, OpenERP s.a.' # The version info for the project you're documenting, acts as replacement for # |version| and |release|, also used in various other places throughout the # built documents. # # The short X.Y version. version = '7.0' # The full version, including alpha/beta/rc tags. release = '7.0' # The language for content autogenerated by Sphinx. Refer to documentation # for a list of supported languages. #language = None # There are two options for replacing |today|: either, you set today to some # non-false value, then it is used: #today = '' # Else, today_fmt is used as the format for a strftime call. #today_fmt = '%B %d, %Y' # List of patterns, relative to source directory, that match files and # directories to ignore when looking for source files. exclude_patterns = ['_build'] # The reST default role (used for this markup: `text`) to use for all documents. #default_role = None # If true, '()' will be appended to :func: etc. cross-reference text. #add_function_parentheses = True # If true, the current module name will be prepended to all description # unit titles (such as .. function::). #add_module_names = True # If true, sectionauthor and moduleauthor directives will be shown in the # output. They are ignored by default. #show_authors = False # The name of the Pygments (syntax highlighting) style to use. pygments_style = 'sphinx' # A list of ignored prefixes for module index sorting. #modindex_common_prefix = [] # -- Options for HTML output --------------------------------------------------- # The theme to use for HTML and HTML Help pages. See the documentation for # a list of builtin themes. html_theme = 'flask' # Theme options are theme-specific and customize the look and feel of a theme # further. For a list of options available for each theme, see the # documentation. #html_theme_options = {} # Add any paths that contain custom themes here, relative to this directory. html_theme_path = ['_themes'] # The name for this set of Sphinx documents. If None, it defaults to # "<project> v<release> documentation". #html_title = None # A shorter title for the navigation bar. Default is the same as html_title. #html_short_title = None # The name of an image file (relative to this directory) to place at the top # of the sidebar. #html_logo = None # The name of an image file (within the static path) to use as favicon of the # docs. This file should be a Windows icon file (.ico) being 16x16 or 32x32 # pixels large. #html_favicon = None # Add any paths that contain custom static files (such as style sheets) here, # relative to this directory. They are copied after the builtin static files, # so a file named "default.css" will overwrite the builtin "default.css". html_static_path = ['_static'] # If not '', a 'Last updated on:' timestamp is inserted at every page bottom, # using the given strftime format. #html_last_updated_fmt = '%b %d, %Y' # If true, SmartyPants will be used to convert quotes and dashes to # typographically correct entities. #html_use_smartypants = True # Custom sidebar templates, maps document names to template names. html_sidebars = { 'index': ['sidebarintro.html', 'sourcelink.html', 'searchbox.html'], '**': ['sidebarlogo.html', 'localtoc.html', 'relations.html', 'sourcelink.html', 'searchbox.html'] } # Additional templates that should be rendered to pages, maps page names to # template names. #html_additional_pages = {} # If false, no module index is generated. #html_domain_indices = True # If false, no index is generated. #html_use_index = True # If true, the index is split into individual pages for each letter. #html_split_index = False # If true, links to the reST sources are added to the pages. #html_show_sourcelink = True # If true, "Created using Sphinx" is shown in the HTML footer. Default is True. #html_show_sphinx = True # If true, "(C) Copyright ..." is shown in the HTML footer. Default is True. #html_show_copyright = True # If true, an OpenSearch description file will be output, and all pages will # contain a <link> tag referring to it. The value of this option must be the # base URL from which the finished HTML is served. #html_use_opensearch = '' # This is the file name suffix for HTML files (e.g. ".xhtml"). #html_file_suffix = None # Output file base name for HTML help builder. htmlhelp_basename = 'openerp-web-doc' # -- Options for LaTeX output -------------------------------------------------- latex_elements = { # The paper size ('letterpaper' or 'a4paper'). #'papersize': 'letterpaper', # The font size ('10pt', '11pt' or '12pt'). #'pointsize': '10pt', # Additional stuff for the LaTeX preamble. #'preamble': '', } # Grouping the document tree into LaTeX files. List of tuples # (source start file, target name, title, author, documentclass [howto/manual]). latex_documents = [ ('index', 'openerp-web-doc.tex', u'OpenERP Web Developers Documentation', u'OpenERP s.a.', 'manual'), ] # The name of an image file (relative to this directory) to place at the top of # the title page. #latex_logo = None # For "manual" documents, if this is true, then toplevel headings are parts, # not chapters. #latex_use_parts = False # If true, show page references after internal links. #latex_show_pagerefs = False # If true, show URL addresses after external links. #latex_show_urls = False # Documents to append as an appendix to all manuals. #latex_appendices = [] # If false, no module index is generated. #latex_domain_indices = True # -- Options for manual page output -------------------------------------------- # One entry per manual page. List of tuples # (source start file, name, description, authors, manual section). man_pages = [ ('index', 'openerp-web-doc', u'OpenERP Web Developers Documentation', [u'OpenERP s.a.'], 1) ] # If true, show URL addresses after external links. #man_show_urls = False # -- Options for Texinfo output ------------------------------------------------ # Grouping the document tree into Texinfo files. List of tuples # (source start file, target name, title, author, # dir menu entry, description, category) texinfo_documents = [ ('index', 'OpenERPWebDocumentation', u'OpenERP Web Developers Documentation', u'OpenERP s.a.', 'OpenERPWebDocumentation', 'Developers documentation for the openerp-web project.', 'Miscellaneous'), ] # Documents to append as an appendix to all manuals. #texinfo_appendices = [] # If false, no module index is generated. #texinfo_domain_indices = True # How to display URL addresses: 'footnote', 'no', or 'inline'. #texinfo_show_urls = 'footnote' todo_include_todos = True # Example configuration for intersphinx: refer to the Python standard library. intersphinx_mapping = { 'python': ('http://docs.python.org/', None), 'openerpserver': ('http://doc.openerp.com/trunk/developers/server', None), }
#!/usr/bin/env python """Simple source code checks, e.g. trailing whitespace.""" from __future__ import with_statement import bisect import re import sys import violations RULE_TRAILING_WHITESPACE = 'whitespace.trailing' RULE_TEXT_TRAILING_WHITESPACE = 'Trailing whitespace is not allowed.' RULE_TODO_ONE_SPACE = 'whitespace.todo' RULE_TEXT_TODO_ONE_SPACE= 'There should be exactly one space before TODO.' RULE_TODO_USERNAME = 'readability.todo' RULE_TEXT_TODO_USERNAME = 'TODO comments should look like this: "// TODO(username): Text".' RULE_TODO_SPACE_AFTER = 'whitespace.todo' RULE_TEXT_TODO_SPACE_AFTER = '"TODO (username):" should be followed by a space.' RE_TODO = r'^//(\s*)TODO(\(.+?\))?:?(\s|$)?' class WhitespaceChecker(object): """Performs simple white space checks.""" # TODO(holtgrew): Do not allow tabs. def check(self, filename): vs = [] with open(filename, 'rb') as f: line_no = 0 for line in f: line_no += 1 if line.rstrip() == line.rstrip('\r\n'): continue v = violations.SimpleRuleViolation( RULE_TRAILING_WHITESPACE, filename, line_no, len(line.rstrip()) + 1, RULE_TEXT_TRAILING_WHITESPACE) vs.append(v) return dict([(v.key(), v) for v in vs]) class SourceFile(object): def __init__(self, name): self.name = name class SourceLocation(object): def __init__(self, filename, line, column, offset): self.file = SourceFile(filename) self.line = line self.column = column self.offset = offset def __str__(self): return '%s:%d/%d (@%d)' % (self.file.name, self.line, self.column, self.offset) def __repr__(self): return str(self) def enumerateComments(filename): # Read file. with open (filename, 'rb') as f: lines = f.readlines() fcontents = ''.join(lines) # Build line break index. line_starts = [0] for line in lines: line_starts.append(line_starts[-1] + len(line)) #print line_starts # Search for all comments. pattern = re.compile( r'//.*?$|/\*.*?\*/|\'(?:\\.|[^\\\'])*\'|"(?:\\.|[^\\"])*"', re.DOTALL | re.MULTILINE) for match in re.finditer(pattern, fcontents): line_start = bisect.bisect(line_starts, match.start(0)) line_end = bisect.bisect(line_starts, match.end(0) - 1) column_start = match.start(0) - line_starts[line_start - 1] column_end = match.end(0) - line_starts[line_end - 1] yield (SourceLocation(filename, line_start, column_start + 1, match.start(0)), SourceLocation(filename, line_end, column_end + 1, match.end(0)), match.group(0)) class CommentChecker(object): """Performs the checks on comments.""" def check(self, filename): vs = [] for cstart, cend, comment in enumerateComments(filename): if comment.startswith('//'): # Check TODO comments. match = re.match(RE_TODO, comment) if match: if len(match.group(1)) > 1: print comment v = violations.SimpleRuleViolation( RULE_TODO_ONE_SPACE, filename, cstart.line, cstart.column, RULE_TEXT_TODO_ONE_SPACE) vs.append(v) if not match.group(2): v = violations.SimpleRuleViolation( RULE_TODO_USERNAME, filename, cstart.line, cstart.column, RULE_TEXT_TODO_USERNAME) vs.append(v) if match.group(3) != ' ' and match.group(3) != '': v = violations.SimpleRuleViolation( RULE_TODO_SPACE_AFTER, filename, cstart.line, cstart.column, RULE_TEXT_TODO_SPACE_AFTER) vs.append(v) return dict([(v.key(), v) for v in vs])
# Copyright (c) 2009-2011 Reza Lotun http://reza.lotun.name/ # # Permission is hereby granted, free of charge, to any person obtaining a # copy of this software and associated documentation files (the # "Software"), to deal in the Software without restriction, including # without limitation the rights to use, copy, modify, merge, publish, dis- # tribute, sublicense, and/or sell copies of the Software, and to permit # persons to whom the Software is furnished to do so, subject to the fol- # lowing conditions: # # The above copyright notice and this permission notice shall be included # in all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS # OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL- # ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT # SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, # WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS # IN THE SOFTWARE. from datetime import datetime class Activity(object): def __init__(self, connection=None): self.connection = connection self.start_time = None self.end_time = None self.activity_id = None self.progress = None self.status_code = None self.cause = None self.description = None self.status_message = None self.group_name = None def __repr__(self): return 'Activity<%s>: For group:%s, progress:%s, cause:%s' % (self.activity_id, self.group_name, self.status_message, self.cause) def startElement(self, name, attrs, connection): return None def endElement(self, name, value, connection): if name == 'ActivityId': self.activity_id = value elif name == 'AutoScalingGroupName': self.group_name = value elif name == 'StartTime': try: self.start_time = datetime.strptime(value, '%Y-%m-%dT%H:%M:%S.%fZ') except ValueError: self.start_time = datetime.strptime(value, '%Y-%m-%dT%H:%M:%SZ') elif name == 'EndTime': try: self.end_time = datetime.strptime(value, '%Y-%m-%dT%H:%M:%S.%fZ') except ValueError: self.end_time = datetime.strptime(value, '%Y-%m-%dT%H:%M:%SZ') elif name == 'Progress': self.progress = value elif name == 'Cause': self.cause = value elif name == 'Description': self.description = value elif name == 'StatusMessage': self.status_message = value elif name == 'StatusCode': self.status_code = value else: setattr(self, name, value)
#!/usr/bin/python # # @author: Gaurav Rastogi (grastogi@avinetworks.com) # Eric Anderson (eanderson@avinetworks.com) # module_check: supported # # Copyright: (c) 2017 Gaurav Rastogi, <grastogi@avinetworks.com> # GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt) # ANSIBLE_METADATA = {'metadata_version': '1.1', 'status': ['preview'], 'supported_by': 'community'} DOCUMENTATION = ''' --- module: avi_wafprofile author: Gaurav Rastogi (@grastogi23) <grastogi@avinetworks.com> short_description: Module for setup of WafProfile Avi RESTful Object description: - This module is used to configure WafProfile object - more examples at U(https://github.com/avinetworks/devops) requirements: [ avisdk ] version_added: "2.5" options: state: description: - The state that should be applied on the entity. default: present choices: ["absent", "present"] avi_api_update_method: description: - Default method for object update is HTTP PUT. - Setting to patch will override that behavior to use HTTP PATCH. version_added: "2.5" default: put choices: ["put", "patch"] avi_api_patch_op: description: - Patch operation to use when using avi_api_update_method as patch. version_added: "2.5" choices: ["add", "replace", "delete"] config: description: - Config params for waf. - Field introduced in 17.2.1. required: true description: description: - Field introduced in 17.2.1. files: description: - List of data files used for waf rules. - Field introduced in 17.2.1. name: description: - Field introduced in 17.2.1. required: true tenant_ref: description: - It is a reference to an object of type tenant. - Field introduced in 17.2.1. url: description: - Avi controller URL of the object. uuid: description: - Field introduced in 17.2.1. extends_documentation_fragment: - avi ''' EXAMPLES = """ - name: Example to create WafProfile object avi_wafprofile: controller: 10.10.25.42 username: admin password: something state: present name: sample_wafprofile """ RETURN = ''' obj: description: WafProfile (api/wafprofile) object returned: success, changed type: dict ''' from ansible.module_utils.basic import AnsibleModule try: from ansible.module_utils.network.avi.avi import ( avi_common_argument_spec, HAS_AVI, avi_ansible_api) except ImportError: HAS_AVI = False def main(): argument_specs = dict( state=dict(default='present', choices=['absent', 'present']), avi_api_update_method=dict(default='put', choices=['put', 'patch']), avi_api_patch_op=dict(choices=['add', 'replace', 'delete']), config=dict(type='dict', required=True), description=dict(type='str',), files=dict(type='list',), name=dict(type='str', required=True), tenant_ref=dict(type='str',), url=dict(type='str',), uuid=dict(type='str',), ) argument_specs.update(avi_common_argument_spec()) module = AnsibleModule( argument_spec=argument_specs, supports_check_mode=True) if not HAS_AVI: return module.fail_json(msg=( 'Avi python API SDK (avisdk>=17.1) is not installed. ' 'For more details visit https://github.com/avinetworks/sdk.')) return avi_ansible_api(module, 'wafprofile', set([])) if __name__ == '__main__': main()
"""Simple example to show how to use weave.inline on SWIG2 wrapped objects. SWIG2 refers to SWIG versions >= 1.3. To run this example you must build the trivial SWIG2 extension called swig2_ext. To do this you need to do something like this:: $ swig -c++ -python -I. -o swig2_ext_wrap.cxx swig2_ext.i $ g++ -Wall -O2 -I/usr/include/python2.3 -fPIC -I. -c \ -o swig2_ext_wrap.os swig2_ext_wrap.cxx $ g++ -shared -o _swig2_ext.so swig2_ext_wrap.os \ -L/usr/lib/python2.3/config The files swig2_ext.i and swig2_ext.h are included in the same directory that contains this file. Note that weave's SWIG2 support works fine whether SWIG_COBJECT_TYPES are used or not. Author: Prabhu Ramachandran Copyright (c) 2004, Prabhu Ramachandran License: BSD Style. """ from __future__ import absolute_import, print_function # Import our SWIG2 wrapped library import swig2_ext import scipy.weave as weave from scipy.weave import swig2_spec, converters # SWIG2 support is not enabled by default. We do this by adding the # swig2 converter to the default list of converters. converters.default.insert(0, swig2_spec.swig2_converter()) def test(): """Instantiate the SWIG wrapped object and then call its method from C++ using weave.inline """ a = swig2_ext.A() b = swig2_ext.foo() # This will be an APtr instance. b.thisown = 1 # Prevent memory leaks. code = """a->f(); b->f(); """ weave.inline(code, ['a', 'b'], include_dirs=['.'], headers=['"swig2_ext.h"'], verbose=1) if __name__ == "__main__": test()
# coding=utf-8 # Copyright 2014 Pants project contributors (see CONTRIBUTORS.md). # Licensed under the Apache License, Version 2.0 (see LICENSE). from __future__ import (nested_scopes, generators, division, absolute_import, with_statement, print_function, unicode_literals) import errno import os import shutil import tarfile from pants.util.contextutil import open_tar from pants.util.dirutil import safe_mkdir, safe_mkdir_for, safe_walk class ArtifactError(Exception): pass class Artifact(object): """Represents a set of files in an artifact.""" def __init__(self, artifact_root): # All files must be under this root. self._artifact_root = artifact_root # The files known to be in this artifact, relative to artifact_root. self._relpaths = set() def get_paths(self): for relpath in self._relpaths: yield os.path.join(self._artifact_root, relpath) def override_paths(self, paths): # Use with care. self._relpaths = set([os.path.relpath(path, self._artifact_root) for path in paths]) def collect(self, paths): """Collect the paths (which must be under artifact root) into this artifact.""" raise NotImplementedError() def extract(self): """Extract the files in this artifact to their locations under artifact root.""" raise NotImplementedError() class DirectoryArtifact(Artifact): """An artifact stored as loose files under a directory.""" def __init__(self, artifact_root, directory): Artifact.__init__(self, artifact_root) self._directory = directory def collect(self, paths): for path in paths or (): relpath = os.path.relpath(path, self._artifact_root) dst = os.path.join(self._directory, relpath) safe_mkdir(os.path.dirname(dst)) if os.path.isdir(path): shutil.copytree(path, dst) else: shutil.copy(path, dst) self._relpaths.add(relpath) def extract(self): for dir_name, _, filenames in safe_walk(self._directory): for filename in filenames: filename = os.path.join(dir_name, filename) relpath = os.path.relpath(filename, self._directory) dst = os.path.join(self._artifact_root, relpath) safe_mkdir_for(dst) shutil.copy(filename, dst) self._relpaths.add(relpath) class TarballArtifact(Artifact): """An artifact stored in a tarball.""" def __init__(self, artifact_root, tarfile, compression=9): Artifact.__init__(self, artifact_root) self._tarfile = tarfile self._compression = compression def collect(self, paths): # In our tests, gzip is slightly less compressive than bzip2 on .class files, # but decompression times are much faster. mode = 'w:gz' if self._compression else 'w' tar_kwargs = {'dereference': True, 'errorlevel': 2} if self._compression: tar_kwargs['compresslevel'] = self._compression with open_tar(self._tarfile, mode, **tar_kwargs) as tarout: for path in paths or (): # Adds dirs recursively. relpath = os.path.relpath(path, self._artifact_root) tarout.add(path, relpath) self._relpaths.add(relpath) def extract(self): try: with open_tar(self._tarfile, 'r', errorlevel=2) as tarin: # Note: We create all needed paths proactively, even though extractall() can do this for us. # This is because we may be called concurrently on multiple artifacts that share directories, # and there will be a race condition inside extractall(): task T1 A) sees that a directory # doesn't exist and B) tries to create it. But in the gap between A) and B) task T2 creates # the same directory, so T1 throws "File exists" in B). # This actually happened, and was very hard to debug. # Creating the paths here up front allows us to squelch that "File exists" error. paths = [] dirs = set() for tarinfo in tarin.getmembers(): paths.append(tarinfo.name) if tarinfo.isdir(): dirs.add(tarinfo.name) else: dirs.add(os.path.dirname(tarinfo.name)) for d in dirs: try: os.makedirs(os.path.join(self._artifact_root, d)) except OSError as e: if e.errno != errno.EEXIST: raise tarin.extractall(self._artifact_root) self._relpaths.update(paths) except tarfile.ReadError as e: raise ArtifactError(e.message)
''' tableCodeEval.py - Solution to Problem Multiplication Tables (Category - Easy) Copyright (C) 2013, Shubham Verma This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see <http://www.gnu.org/licenses/>. ''' ''' Description: Print out the grade school multiplication table upto 12 X 12. Input sample: None Output sample: Print out the table in a matrix like fashion, each number formatted to a width of 4 (The numbers are right-aligned and strip out leading/trailing spaces on each line). The first 3 line will look like: e.g. 1 2 3 4 5 6 7 8 9 10 11 12 2 4 6 8 10 12 14 16 18 20 22 24 3 6 9 12 15 18 21 24 27 30 33 36 ''' if __name__ == '__main__': for i in xrange(1, 13): tmp = [] for j in xrange(1, 13): tmp.append( '%4d' % (i * j)) print (''.join(tmp)).lstrip()
from __future__ import print_function, division from pyglet.gl import * from pyglet.window import Window from pyglet.clock import Clock from threading import Thread, Lock gl_lock = Lock() class ManagedWindow(Window): """ A pyglet window with an event loop which executes automatically in a separate thread. Behavior is added by creating a subclass which overrides setup, update, and/or draw. """ fps_limit = 30 default_win_args = dict(width=600, height=500, vsync=False, resizable=True) def __init__(self, **win_args): """ It is best not to override this function in the child class, unless you need to take additional arguments. Do any OpenGL initialization calls in setup(). """ # check if this is run from the doctester if win_args.get('runfromdoctester', False): return self.win_args = dict(self.default_win_args, **win_args) self.Thread = Thread(target=self.__event_loop__) self.Thread.start() def __event_loop__(self, **win_args): """ The event loop thread function. Do not override or call directly (it is called by __init__). """ gl_lock.acquire() try: try: super(ManagedWindow, self).__init__(**self.win_args) self.switch_to() self.setup() except Exception as e: print("Window initialization failed: %s" % (str(e))) self.has_exit = True finally: gl_lock.release() clock = Clock() clock.set_fps_limit(self.fps_limit) while not self.has_exit: dt = clock.tick() gl_lock.acquire() try: try: self.switch_to() self.dispatch_events() self.clear() self.update(dt) self.draw() self.flip() except Exception as e: print("Uncaught exception in event loop: %s" % str(e)) self.has_exit = True finally: gl_lock.release() super(ManagedWindow, self).close() def close(self): """ Closes the window. """ self.has_exit = True def setup(self): """ Called once before the event loop begins. Override this method in a child class. This is the best place to put things like OpenGL initialization calls. """ pass def update(self, dt): """ Called before draw during each iteration of the event loop. dt is the elapsed time in seconds since the last update. OpenGL rendering calls are best put in draw() rather than here. """ pass def draw(self): """ Called after update during each iteration of the event loop. Put OpenGL rendering calls here. """ pass if __name__ == '__main__': ManagedWindow()
from django.db import connections from django.db.models.query import sql from django.contrib.gis.db.models.fields import GeometryField from django.contrib.gis.db.models.sql import aggregates as gis_aggregates from django.contrib.gis.db.models.sql.conversion import AreaField, DistanceField, GeomField from django.contrib.gis.db.models.sql.where import GeoWhereNode from django.contrib.gis.geometry.backend import Geometry from django.contrib.gis.measure import Area, Distance ALL_TERMS = set([ 'bbcontains', 'bboverlaps', 'contained', 'contains', 'contains_properly', 'coveredby', 'covers', 'crosses', 'disjoint', 'distance_gt', 'distance_gte', 'distance_lt', 'distance_lte', 'dwithin', 'equals', 'exact', 'intersects', 'overlaps', 'relate', 'same_as', 'touches', 'within', 'left', 'right', 'overlaps_left', 'overlaps_right', 'overlaps_above', 'overlaps_below', 'strictly_above', 'strictly_below' ]) ALL_TERMS.update(sql.constants.QUERY_TERMS) class GeoQuery(sql.Query): """ A single spatial SQL query. """ # Overridding the valid query terms. query_terms = ALL_TERMS aggregates_module = gis_aggregates compiler = 'GeoSQLCompiler' #### Methods overridden from the base Query class #### def __init__(self, model, where=GeoWhereNode): super(GeoQuery, self).__init__(model, where) # The following attributes are customized for the GeoQuerySet. # The GeoWhereNode and SpatialBackend classes contain backend-specific # routines and functions. self.custom_select = {} self.transformed_srid = None self.extra_select_fields = {} def clone(self, *args, **kwargs): obj = super(GeoQuery, self).clone(*args, **kwargs) # Customized selection dictionary and transformed srid flag have # to also be added to obj. obj.custom_select = self.custom_select.copy() obj.transformed_srid = self.transformed_srid obj.extra_select_fields = self.extra_select_fields.copy() return obj def convert_values(self, value, field, connection): """ Using the same routines that Oracle does we can convert our extra selection objects into Geometry and Distance objects. TODO: Make converted objects 'lazy' for less overhead. """ if connection.ops.oracle: # Running through Oracle's first. value = super(GeoQuery, self).convert_values(value, field or GeomField(), connection) if value is None: # Output from spatial function is NULL (e.g., called # function on a geometry field with NULL value). pass elif isinstance(field, DistanceField): # Using the field's distance attribute, can instantiate # `Distance` with the right context. value = Distance(**{field.distance_att : value}) elif isinstance(field, AreaField): value = Area(**{field.area_att : value}) elif isinstance(field, (GeomField, GeometryField)) and value: value = Geometry(value) elif field is not None: return super(GeoQuery, self).convert_values(value, field, connection) return value def get_aggregation(self, using): # Remove any aggregates marked for reduction from the subquery # and move them to the outer AggregateQuery. connection = connections[using] for alias, aggregate in self.aggregate_select.items(): if isinstance(aggregate, gis_aggregates.GeoAggregate): if not getattr(aggregate, 'is_extent', False) or connection.ops.oracle: self.extra_select_fields[alias] = GeomField() return super(GeoQuery, self).get_aggregation(using) def resolve_aggregate(self, value, aggregate, connection): """ Overridden from GeoQuery's normalize to handle the conversion of GeoAggregate objects. """ if isinstance(aggregate, self.aggregates_module.GeoAggregate): if aggregate.is_extent: if aggregate.is_extent == '3D': return connection.ops.convert_extent3d(value) else: return connection.ops.convert_extent(value) else: return connection.ops.convert_geom(value, aggregate.source) else: return super(GeoQuery, self).resolve_aggregate(value, aggregate, connection) # Private API utilities, subject to change. def _geo_field(self, field_name=None): """ Returns the first Geometry field encountered; or specified via the `field_name` keyword. The `field_name` may be a string specifying the geometry field on this GeoQuery's model, or a lookup string to a geometry field via a ForeignKey relation. """ if field_name is None: # Incrementing until the first geographic field is found. for fld in self.model._meta.fields: if isinstance(fld, GeometryField): return fld return False else: # Otherwise, check by the given field name -- which may be # a lookup to a _related_ geographic field. return GeoWhereNode._check_geo_field(self.model._meta, field_name)
#!/usr/bin/env python3 # -*- coding: utf-8 -*- """ Created on Tue Aug 27 12:21:17 2019 @author: zijiazhang """ import itertools import xml.etree.ElementTree as ET from ast import literal_eval as make_tuple from string import Template def findcontain(elem, tag): for child in elem: if tag in child.tag: return child return None def findallcontain(elem, tag): temp = [] for child in elem: if tag in child.tag: temp.append(child.text.replace(' ', '_')) return temp def str2bool(v): return v.lower() in ("yes", "true", "t", "1") def xml_to_python(path): tree = ET.parse(path) root = tree.getroot() CSP = root.find('CSP') domains = {} positions = {} constraints = [] name = CSP.find('NAME').text.replace(' ', '_') for i in CSP: if i.tag == 'VARIABLE': mytype = i.get('TYPE') if(mytype == 'Integer'): pass elif(mytype == 'String'): pass elif(mytype == 'Boolean'): pass varDomain = [] for e in i: if e.tag == 'NAME': varName = e.text if e.tag == 'VALUE': if(mytype == 'Integer'): varDomain.append(int(e.text)) elif(mytype == 'String'): varDomain.append(e.text) elif(mytype == 'Boolean'): varDomain.append(str2bool(e.text)) if e.tag == 'PROPERTY': positions[varName] = make_tuple(e.text.split('=')[1][1:]) domains[varName] = varDomain elif i.tag == 'CONSTRAINT': if i.get('TYPE') == 'Custom': relatedvar = [] table = [] for e in i: if e.tag == 'GIVEN': relatedvar.append(e.text) elif e.tag == 'TABLE': table = e.text.split(" ") table = ['T' in x for x in table] trueTuples = [] targetlist = [domains[x] for x in relatedvar] allTuple = list(itertools.product(*targetlist)) for i in range(0, len(allTuple)): if table[i]: trueTuples.append(str(allTuple[i])) relatedvarName = ["'" + x + "'" for x in relatedvar] constraints.append(Template('Constraint(($var),lambda $vNames: ($vNames) in [$true])').substitute( var=','.join(relatedvarName), vNames=','.join(relatedvar), true=','.join(trueTuples) )) else: relatedvar = [] complement = False for e in i: if e.tag == 'GIVEN': relatedvar.append(e.text) elif e.tag == 'ARGS': if e.text != None and 'complement' in e.text: complement = True relatedvarName = ["'" + x + "'" for x in relatedvar] if complement: constraints.append(Template('Constraint(($var),NOT($function))').substitute( var=','.join(relatedvarName), function=i.get('TYPE') )) else: constraints.append(Template('Constraint(($var),$function)').substitute( var=','.join(relatedvarName), function=i.get('TYPE') )) template = """$name = CSP( domains=$domains, constraints=[$constraints], positions=$positions)""" print(Template(template).substitute( name=name, domains=domains, constraints=', '.join(constraints), positions=positions))
""" Support to communicate with a Buderus KM200 unit. """ import logging import base64 import json import binascii import urllib.request, urllib.error, urllib.parse import voluptuous as vol from io import StringIO from Crypto.Cipher import AES from homeassistant.helpers import config_validation as cv from homeassistant.const import (CONF_HOST, CONF_PASSWORD, CONF_NAME) DOMAIN = 'buderus' DEFAULT_NAME = 'Buderus' CONFIG_SCHEMA = vol.Schema({ DOMAIN: vol.Schema({ vol.Required(CONF_HOST): cv.string, vol.Required(CONF_PASSWORD): cv.string, vol.Optional(CONF_NAME, default=DEFAULT_NAME): cv.string, }), }, extra=vol.ALLOW_EXTRA) def setup(hass, config): conf = config[DOMAIN] host = conf.get(CONF_HOST) name = conf.get(CONF_NAME) password = conf.get(CONF_PASSWORD) bridge = BuderusBridge(name, host, password) hass.data[DOMAIN] = bridge return True class BuderusBridge(object): BS = AES.block_size INTERRUPT = '\u0001' PAD = '\u0000' def __init__(self, name, host, password): self.logger = logging.getLogger(__name__) self.logger.info("Init Buderus") self.__ua = "TeleHeater/2.2.3" self.__content_type = "application/json" self._host = host self._key = binascii.unhexlify(password) self._ids = {} self.opener = urllib.request.build_opener() self.opener.addheaders = [('User-agent', self.__ua), ('Accept', self.__content_type)] self.name = name def _decrypt(self, enc): decobj = AES.new(self._key, AES.MODE_ECB) data = decobj.decrypt(base64.b64decode(enc)) data = data.rstrip(self.PAD.encode()).rstrip(self.INTERRUPT.encode()) return data def _encrypt(self, plain): plain = plain + (AES.block_size - len(plain) % self.BS) * self.PAD encobj = AES.new(self._key, AES.MODE_ECB) data = encobj.encrypt(plain.encode()) return base64.b64encode(data) def _get_data(self, path): try: url = 'http://' + self._host + path self.logger.debug("Buderus fetching data from {}".format(path)) resp = self.opener.open(url) plain = self._decrypt(resp.read()) self.logger.debug("Buderus data received from {}: {}".format(url, plain)) return plain except Exception as e: self.logger.error("Buderus error happened at {}: {}".format(url, e)) return None def _get_json(self, data): try: j = json.load(StringIO(data.decode())) return j except Exception as e: self.logger.error("Buderus error happened while reading JSON data {}: {}".format(data, e)) return False def _get_value(self, j): return j['value'] def _json_encode(self, value): d = {"value": value} return json.dumps(d) def _set_data(self, path, data): try: url = 'http://' + self._host + path self.logger.info("Buderus setting value for {}".format(path)) headers = {"User-Agent": self.__ua, "Content-Type": self.__content_type} request = urllib.request.Request(url, data=data, headers=headers, method='PUT') req = urllib.request.urlopen(request) self.logger.info("Buderus returned {}: {}".format(req.status, req.reason)) if not req.status == 204: self.logger.debug(req.read()) except Exception as e: self.logger.error("Buderus error happened at {}: {}".format(url, e)) return None def _submit_data(self, path, data): self.logger.info("Buderus SETTING {} to {}".format(path, data)) payload = self._json_encode(data) self.logger.debug(payload) req = self._set_data(path, self._encrypt(str(payload))) """ def _get_type(self, j): return j['type'] def _get_writeable(self, j): if j['writeable'] == 1: return True else: return False def _get_allowed_values(self, j, value_type): if value_type == "stringValue": try: return j['allowedValues'] except: return None elif value_type == "floatValue": return {"minValue": j['minValue'], "maxValue": j['maxValue']} def update_item(self, item, caller=None, source=None, dest=None): if caller != "Buderus": id = item.conf['km_id'] plain = self._get_data(id) data = self._get_json(plain) if self._get_writeable(data): value_type = self._get_type(data) allowed_values = self._get_allowed_values(data, value_type) if value_type == "stringValue" and item() in allowed_values or not allowed_values: self._submit_data(item, id) return elif value_type == "floatValue" and item() >= allowed_values['minValue'] and item() <= allowed_values[ 'maxValue']: self._submit_data(item, id) return else: self.logger.error("Buderus value {} not allowed [{}]".format(item(), allowed_values)) item(item.prev_value(), "Buderus") else: self.logger.error("Buderus item {} not writeable!".format(item)) item(item.prev_value(), "Buderus") """
"""Test suite for the profile module.""" import sys import pstats import unittest from difflib import unified_diff from io import StringIO from test.support import run_unittest import profile from test.profilee import testfunc, timer class ProfileTest(unittest.TestCase): profilerclass = profile.Profile methodnames = ['print_stats', 'print_callers', 'print_callees'] expected_max_output = ':0(max)' def get_expected_output(self): return _ProfileOutput @classmethod def do_profiling(cls): results = [] prof = cls.profilerclass(timer, 0.001) start_timer = timer() prof.runctx("testfunc()", globals(), locals()) results.append(timer() - start_timer) for methodname in cls.methodnames: s = StringIO() stats = pstats.Stats(prof, stream=s) stats.strip_dirs().sort_stats("stdname") getattr(stats, methodname)() output = s.getvalue().splitlines() mod_name = testfunc.__module__.rsplit('.', 1)[1] # Only compare against stats originating from the test file. # Prevents outside code (e.g., the io module) from causing # unexpected output. output = [line.rstrip() for line in output if mod_name in line] results.append('\n'.join(output)) return results def test_cprofile(self): results = self.do_profiling() expected = self.get_expected_output() self.assertEqual(results[0], 1000) for i, method in enumerate(self.methodnames): if results[i+1] != expected[method]: print("Stats.%s output for %s doesn't fit expectation!" % (method, self.profilerclass.__name__)) print('\n'.join(unified_diff( results[i+1].split('\n'), expected[method].split('\n')))) def test_calling_conventions(self): # Issue #5330: profile and cProfile wouldn't report C functions called # with keyword arguments. We test all calling conventions. stmts = [ "max([0])", "max([0], key=int)", "max([0], **dict(key=int))", "max(*([0],))", "max(*([0],), key=int)", "max(*([0],), **dict(key=int))", ] for stmt in stmts: s = StringIO() prof = self.profilerclass(timer, 0.001) prof.runctx(stmt, globals(), locals()) stats = pstats.Stats(prof, stream=s) stats.print_stats() res = s.getvalue() self.assertIn(self.expected_max_output, res, "Profiling {0!r} didn't report max:\n{1}".format(stmt, res)) def regenerate_expected_output(filename, cls): filename = filename.rstrip('co') print('Regenerating %s...' % filename) results = cls.do_profiling() newfile = [] with open(filename, 'r') as f: for line in f: newfile.append(line) if line.startswith('#--cut'): break with open(filename, 'w') as f: f.writelines(newfile) f.write("_ProfileOutput = {}\n") for i, method in enumerate(cls.methodnames): f.write('_ProfileOutput[%r] = """\\\n%s"""\n' % ( method, results[i+1])) f.write('\nif __name__ == "__main__":\n main()\n') def test_main(): run_unittest(ProfileTest) def main(): if '-r' not in sys.argv: test_main() else: regenerate_expected_output(__file__, ProfileTest) # Don't remove this comment. Everything below it is auto-generated. #--cut-------------------------------------------------------------------------- _ProfileOutput = {} _ProfileOutput['print_stats'] = """\ 28 27.972 0.999 27.972 0.999 profilee.py:110(__getattr__) 1 269.996 269.996 999.769 999.769 profilee.py:25(testfunc) 23/3 149.937 6.519 169.917 56.639 profilee.py:35(factorial) 20 19.980 0.999 19.980 0.999 profilee.py:48(mul) 2 39.986 19.993 599.830 299.915 profilee.py:55(helper) 4 115.984 28.996 119.964 29.991 profilee.py:73(helper1) 2 -0.006 -0.003 139.946 69.973 profilee.py:84(helper2_indirect) 8 311.976 38.997 399.912 49.989 profilee.py:88(helper2) 8 63.976 7.997 79.960 9.995 profilee.py:98(subhelper)""" _ProfileOutput['print_callers'] = """\ :0(append) <- profilee.py:73(helper1)(4) 119.964 :0(exc_info) <- profilee.py:73(helper1)(4) 119.964 :0(hasattr) <- profilee.py:73(helper1)(4) 119.964 profilee.py:88(helper2)(8) 399.912 profilee.py:110(__getattr__) <- :0(hasattr)(12) 11.964 profilee.py:98(subhelper)(16) 79.960 profilee.py:25(testfunc) <- <string>:1(<module>)(1) 999.767 profilee.py:35(factorial) <- profilee.py:25(testfunc)(1) 999.769 profilee.py:35(factorial)(20) 169.917 profilee.py:84(helper2_indirect)(2) 139.946 profilee.py:48(mul) <- profilee.py:35(factorial)(20) 169.917 profilee.py:55(helper) <- profilee.py:25(testfunc)(2) 999.769 profilee.py:73(helper1) <- profilee.py:55(helper)(4) 599.830 profilee.py:84(helper2_indirect) <- profilee.py:55(helper)(2) 599.830 profilee.py:88(helper2) <- profilee.py:55(helper)(6) 599.830 profilee.py:84(helper2_indirect)(2) 139.946 profilee.py:98(subhelper) <- profilee.py:88(helper2)(8) 399.912""" _ProfileOutput['print_callees'] = """\ :0(hasattr) -> profilee.py:110(__getattr__)(12) 27.972 <string>:1(<module>) -> profilee.py:25(testfunc)(1) 999.769 profilee.py:110(__getattr__) -> profilee.py:25(testfunc) -> profilee.py:35(factorial)(1) 169.917 profilee.py:55(helper)(2) 599.830 profilee.py:35(factorial) -> profilee.py:35(factorial)(20) 169.917 profilee.py:48(mul)(20) 19.980 profilee.py:48(mul) -> profilee.py:55(helper) -> profilee.py:73(helper1)(4) 119.964 profilee.py:84(helper2_indirect)(2) 139.946 profilee.py:88(helper2)(6) 399.912 profilee.py:73(helper1) -> :0(append)(4) -0.004 profilee.py:84(helper2_indirect) -> profilee.py:35(factorial)(2) 169.917 profilee.py:88(helper2)(2) 399.912 profilee.py:88(helper2) -> :0(hasattr)(8) 11.964 profilee.py:98(subhelper)(8) 79.960 profilee.py:98(subhelper) -> profilee.py:110(__getattr__)(16) 27.972""" if __name__ == "__main__": main()
""" Testing for the partial dependence module. """ import numpy as np from numpy.testing import assert_array_equal from sklearn.utils.testing import assert_raises from sklearn.utils.testing import if_matplotlib from sklearn.ensemble.partial_dependence import partial_dependence from sklearn.ensemble.partial_dependence import plot_partial_dependence from sklearn.ensemble import GradientBoostingClassifier from sklearn.ensemble import GradientBoostingRegressor from sklearn import datasets # toy sample X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]] y = [-1, -1, -1, 1, 1, 1] T = [[-1, -1], [2, 2], [3, 2]] true_result = [-1, 1, 1] # also load the boston dataset boston = datasets.load_boston() # also load the iris dataset iris = datasets.load_iris() def test_partial_dependence_classifier(): # Test partial dependence for classifier clf = GradientBoostingClassifier(n_estimators=10, random_state=1) clf.fit(X, y) pdp, axes = partial_dependence(clf, [0], X=X, grid_resolution=5) # only 4 grid points instead of 5 because only 4 unique X[:,0] vals assert pdp.shape == (1, 4) assert axes[0].shape[0] == 4 # now with our own grid X_ = np.asarray(X) grid = np.unique(X_[:, 0]) pdp_2, axes = partial_dependence(clf, [0], grid=grid) assert axes is None assert_array_equal(pdp, pdp_2) def test_partial_dependence_multiclass(): # Test partial dependence for multi-class classifier clf = GradientBoostingClassifier(n_estimators=10, random_state=1) clf.fit(iris.data, iris.target) grid_resolution = 25 n_classes = clf.n_classes_ pdp, axes = partial_dependence( clf, [0], X=iris.data, grid_resolution=grid_resolution) assert pdp.shape == (n_classes, grid_resolution) assert len(axes) == 1 assert axes[0].shape[0] == grid_resolution def test_partial_dependence_regressor(): # Test partial dependence for regressor clf = GradientBoostingRegressor(n_estimators=10, random_state=1) clf.fit(boston.data, boston.target) grid_resolution = 25 pdp, axes = partial_dependence( clf, [0], X=boston.data, grid_resolution=grid_resolution) assert pdp.shape == (1, grid_resolution) assert axes[0].shape[0] == grid_resolution def test_partial_dependecy_input(): # Test input validation of partial dependence. clf = GradientBoostingClassifier(n_estimators=10, random_state=1) clf.fit(X, y) assert_raises(ValueError, partial_dependence, clf, [0], grid=None, X=None) assert_raises(ValueError, partial_dependence, clf, [0], grid=[0, 1], X=X) # first argument must be an instance of BaseGradientBoosting assert_raises(ValueError, partial_dependence, {}, [0], X=X) # Gradient boosting estimator must be fit assert_raises(ValueError, partial_dependence, GradientBoostingClassifier(), [0], X=X) assert_raises(ValueError, partial_dependence, clf, [-1], X=X) assert_raises(ValueError, partial_dependence, clf, [100], X=X) # wrong ndim for grid grid = np.random.rand(10, 2, 1) assert_raises(ValueError, partial_dependence, clf, [0], grid=grid) @if_matplotlib def test_plot_partial_dependence(): # Test partial dependence plot function. clf = GradientBoostingRegressor(n_estimators=10, random_state=1) clf.fit(boston.data, boston.target) grid_resolution = 25 fig, axs = plot_partial_dependence(clf, boston.data, [0, 1, (0, 1)], grid_resolution=grid_resolution, feature_names=boston.feature_names) assert len(axs) == 3 assert all(ax.has_data for ax in axs) # check with str features and array feature names fig, axs = plot_partial_dependence(clf, boston.data, ['CRIM', 'ZN', ('CRIM', 'ZN')], grid_resolution=grid_resolution, feature_names=boston.feature_names) assert len(axs) == 3 assert all(ax.has_data for ax in axs) # check with list feature_names feature_names = boston.feature_names.tolist() fig, axs = plot_partial_dependence(clf, boston.data, ['CRIM', 'ZN', ('CRIM', 'ZN')], grid_resolution=grid_resolution, feature_names=feature_names) assert len(axs) == 3 assert all(ax.has_data for ax in axs) @if_matplotlib def test_plot_partial_dependence_input(): # Test partial dependence plot function input checks. clf = GradientBoostingClassifier(n_estimators=10, random_state=1) # not fitted yet assert_raises(ValueError, plot_partial_dependence, clf, X, [0]) clf.fit(X, y) assert_raises(ValueError, plot_partial_dependence, clf, np.array(X)[:, :0], [0]) # first argument must be an instance of BaseGradientBoosting assert_raises(ValueError, plot_partial_dependence, {}, X, [0]) # must be larger than -1 assert_raises(ValueError, plot_partial_dependence, clf, X, [-1]) # too large feature value assert_raises(ValueError, plot_partial_dependence, clf, X, [100]) # str feature but no feature_names assert_raises(ValueError, plot_partial_dependence, clf, X, ['foobar']) # not valid features value assert_raises(ValueError, plot_partial_dependence, clf, X, [{'foo': 'bar'}]) @if_matplotlib def test_plot_partial_dependence_multiclass(): # Test partial dependence plot function on multi-class input. clf = GradientBoostingClassifier(n_estimators=10, random_state=1) clf.fit(iris.data, iris.target) grid_resolution = 25 fig, axs = plot_partial_dependence(clf, iris.data, [0, 1], label=0, grid_resolution=grid_resolution) assert len(axs) == 2 assert all(ax.has_data for ax in axs) # now with symbol labels target = iris.target_names[iris.target] clf = GradientBoostingClassifier(n_estimators=10, random_state=1) clf.fit(iris.data, target) grid_resolution = 25 fig, axs = plot_partial_dependence(clf, iris.data, [0, 1], label='setosa', grid_resolution=grid_resolution) assert len(axs) == 2 assert all(ax.has_data for ax in axs) # label not in gbrt.classes_ assert_raises(ValueError, plot_partial_dependence, clf, iris.data, [0, 1], label='foobar', grid_resolution=grid_resolution) # label not provided assert_raises(ValueError, plot_partial_dependence, clf, iris.data, [0, 1], grid_resolution=grid_resolution)
# Copyright 2014 IBM Corp. # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from oslo_config import cfg from nova.tests.functional.api_sample_tests import api_sample_base CONF = cfg.CONF CONF.import_opt('osapi_compute_extension', 'nova.api.openstack.compute.legacy_v2.extensions') class SecurityGroupDefaultRulesSampleJsonTest( api_sample_base.ApiSampleTestBaseV21): ADMIN_API = True extension_name = 'os-security-group-default-rules' def _get_flags(self): f = super(SecurityGroupDefaultRulesSampleJsonTest, self)._get_flags() f['osapi_compute_extension'] = CONF.osapi_compute_extension[:] f['osapi_compute_extension'].append('nova.api.openstack.compute.' 'contrib.security_group_default_rules.' 'Security_group_default_rules') return f def test_security_group_default_rules_create(self): response = self._do_post('os-security-group-default-rules', 'security-group-default-rules-create-req', {}) self._verify_response('security-group-default-rules-create-resp', {}, response, 200) def test_security_group_default_rules_list(self): self.test_security_group_default_rules_create() response = self._do_get('os-security-group-default-rules') self._verify_response('security-group-default-rules-list-resp', {}, response, 200) def test_security_group_default_rules_show(self): self.test_security_group_default_rules_create() rule_id = '1' response = self._do_get('os-security-group-default-rules/%s' % rule_id) self._verify_response('security-group-default-rules-show-resp', {}, response, 200)
""" Matplotlib Exporter =================== This submodule contains tools for crawling a matplotlib figure and exporting relevant pieces to a renderer. """ import warnings import io from . import utils import matplotlib from matplotlib import transforms from matplotlib.backends.backend_agg import FigureCanvasAgg class Exporter(object): """Matplotlib Exporter Parameters ---------- renderer : Renderer object The renderer object called by the exporter to create a figure visualization. See mplexporter.Renderer for information on the methods which should be defined within the renderer. close_mpl : bool If True (default), close the matplotlib figure as it is rendered. This is useful for when the exporter is used within the notebook, or with an interactive matplotlib backend. """ def __init__(self, renderer, close_mpl=True): self.close_mpl = close_mpl self.renderer = renderer def run(self, fig): """ Run the exporter on the given figure Parmeters --------- fig : matplotlib.Figure instance The figure to export """ # Calling savefig executes the draw() command, putting elements # in the correct place. if fig.canvas is None: fig.canvas = FigureCanvasAgg(fig) fig.savefig(io.BytesIO(), format='png', dpi=fig.dpi) if self.close_mpl: import matplotlib.pyplot as plt plt.close(fig) self.crawl_fig(fig) @staticmethod def process_transform(transform, ax=None, data=None, return_trans=False, force_trans=None): """Process the transform and convert data to figure or data coordinates Parameters ---------- transform : matplotlib Transform object The transform applied to the data ax : matplotlib Axes object (optional) The axes the data is associated with data : ndarray (optional) The array of data to be transformed. return_trans : bool (optional) If true, return the final transform of the data force_trans : matplotlib.transform instance (optional) If supplied, first force the data to this transform Returns ------- code : string Code is either "data", "axes", "figure", or "display", indicating the type of coordinates output. transform : matplotlib transform the transform used to map input data to output data. Returned only if return_trans is True new_data : ndarray Data transformed to match the given coordinate code. Returned only if data is specified """ if isinstance(transform, transforms.BlendedGenericTransform): warnings.warn("Blended transforms not yet supported. " "Zoom behavior may not work as expected.") if force_trans is not None: if data is not None: data = (transform - force_trans).transform(data) transform = force_trans code = "display" if ax is not None: for (c, trans) in [("data", ax.transData), ("axes", ax.transAxes), ("figure", ax.figure.transFigure), ("display", transforms.IdentityTransform())]: if transform.contains_branch(trans): code, transform = (c, transform - trans) break if data is not None: if return_trans: return code, transform.transform(data), transform else: return code, transform.transform(data) else: if return_trans: return code, transform else: return code def crawl_fig(self, fig): """Crawl the figure and process all axes""" with self.renderer.draw_figure(fig=fig, props=utils.get_figure_properties(fig)): for ax in fig.axes: self.crawl_ax(ax) def crawl_ax(self, ax): """Crawl the axes and process all elements within""" with self.renderer.draw_axes(ax=ax, props=utils.get_axes_properties(ax)): for line in ax.lines: self.draw_line(ax, line) for text in ax.texts: self.draw_text(ax, text) for (text, ttp) in zip([ax.xaxis.label, ax.yaxis.label, ax.title], ["xlabel", "ylabel", "title"]): if(hasattr(text, 'get_text') and text.get_text()): self.draw_text(ax, text, force_trans=ax.transAxes, text_type=ttp) for artist in ax.artists: # TODO: process other artists if isinstance(artist, matplotlib.text.Text): self.draw_text(ax, artist) for patch in ax.patches: self.draw_patch(ax, patch) for collection in ax.collections: self.draw_collection(ax, collection) for image in ax.images: self.draw_image(ax, image) legend = ax.get_legend() if legend is not None: props = utils.get_legend_properties(ax, legend) with self.renderer.draw_legend(legend=legend, props=props): if props['visible']: self.crawl_legend(ax, legend) def crawl_legend(self, ax, legend): """ Recursively look through objects in legend children """ legendElements = list(utils.iter_all_children(legend._legend_box, skipContainers=True)) legendElements.append(legend.legendPatch) for child in legendElements: # force a large zorder so it appears on top child.set_zorder(1E6 + child.get_zorder()) try: # What kind of object... if isinstance(child, matplotlib.patches.Patch): self.draw_patch(ax, child, force_trans=ax.transAxes) elif isinstance(child, matplotlib.text.Text): if not (child is legend.get_children()[-1] and child.get_text() == 'None'): self.draw_text(ax, child, force_trans=ax.transAxes) elif isinstance(child, matplotlib.lines.Line2D): self.draw_line(ax, child, force_trans=ax.transAxes) elif isinstance(child, matplotlib.collections.Collection): self.draw_collection(ax, child, force_pathtrans=ax.transAxes) else: warnings.warn("Legend element %s not impemented" % child) except NotImplementedError: warnings.warn("Legend element %s not impemented" % child) def draw_line(self, ax, line, force_trans=None): """Process a matplotlib line and call renderer.draw_line""" coordinates, data = self.process_transform(line.get_transform(), ax, line.get_xydata(), force_trans=force_trans) linestyle = utils.get_line_style(line) if linestyle['dasharray'] is None: linestyle = None markerstyle = utils.get_marker_style(line) if (markerstyle['marker'] in ['None', 'none', None] or markerstyle['markerpath'][0].size == 0): markerstyle = None label = line.get_label() if markerstyle or linestyle: self.renderer.draw_marked_line(data=data, coordinates=coordinates, linestyle=linestyle, markerstyle=markerstyle, label=label, mplobj=line) def draw_text(self, ax, text, force_trans=None, text_type=None): """Process a matplotlib text object and call renderer.draw_text""" content = text.get_text() if content: transform = text.get_transform() position = text.get_position() coords, position = self.process_transform(transform, ax, position, force_trans=force_trans) style = utils.get_text_style(text) self.renderer.draw_text(text=content, position=position, coordinates=coords, text_type=text_type, style=style, mplobj=text) def draw_patch(self, ax, patch, force_trans=None): """Process a matplotlib patch object and call renderer.draw_path""" vertices, pathcodes = utils.SVG_path(patch.get_path()) transform = patch.get_transform() coordinates, vertices = self.process_transform(transform, ax, vertices, force_trans=force_trans) linestyle = utils.get_path_style(patch, fill=patch.get_fill()) self.renderer.draw_path(data=vertices, coordinates=coordinates, pathcodes=pathcodes, style=linestyle, mplobj=patch) def draw_collection(self, ax, collection, force_pathtrans=None, force_offsettrans=None): """Process a matplotlib collection and call renderer.draw_collection""" (transform, transOffset, offsets, paths) = collection._prepare_points() offset_coords, offsets = self.process_transform( transOffset, ax, offsets, force_trans=force_offsettrans) path_coords = self.process_transform( transform, ax, force_trans=force_pathtrans) processed_paths = [utils.SVG_path(path) for path in paths] processed_paths = [(self.process_transform( transform, ax, path[0], force_trans=force_pathtrans)[1], path[1]) for path in processed_paths] path_transforms = collection.get_transforms() try: # matplotlib 1.3: path_transforms are transform objects. # Convert them to numpy arrays. path_transforms = [t.get_matrix() for t in path_transforms] except AttributeError: # matplotlib 1.4: path transforms are already numpy arrays. pass styles = {'linewidth': collection.get_linewidths(), 'facecolor': collection.get_facecolors(), 'edgecolor': collection.get_edgecolors(), 'alpha': collection._alpha, 'zorder': collection.get_zorder()} offset_dict = {"data": "before", "screen": "after"} offset_order = offset_dict[collection.get_offset_position()] self.renderer.draw_path_collection(paths=processed_paths, path_coordinates=path_coords, path_transforms=path_transforms, offsets=offsets, offset_coordinates=offset_coords, offset_order=offset_order, styles=styles, mplobj=collection) def draw_image(self, ax, image): """Process a matplotlib image object and call renderer.draw_image""" self.renderer.draw_image(imdata=utils.image_to_base64(image), extent=image.get_extent(), coordinates="data", style={"alpha": image.get_alpha(), "zorder": image.get_zorder()}, mplobj=image)
""" robotparser.py Copyright (C) 2000 Bastian Kleineidam You can choose between two licenses when using this package: 1) GNU GPLv2 2) PSF license for Python 2.2 The robots.txt Exclusion Protocol is implemented as specified in http://www.robotstxt.org/norobots-rfc.txt """ import urlparse import urllib __all__ = ["RobotFileParser"] class RobotFileParser: """ This class provides a set of methods to read, parse and answer questions about a single robots.txt file. """ def __init__(self, url=''): self.entries = [] self.default_entry = None self.disallow_all = False self.allow_all = False self.set_url(url) self.last_checked = 0 def mtime(self): """Returns the time the robots.txt file was last fetched. This is useful for long-running web spiders that need to check for new robots.txt files periodically. """ return self.last_checked def modified(self): """Sets the time the robots.txt file was last fetched to the current time. """ import time self.last_checked = time.time() def set_url(self, url): """Sets the URL referring to a robots.txt file.""" self.url = url self.host, self.path = urlparse.urlparse(url)[1:3] def read(self): """Reads the robots.txt URL and feeds it to the parser.""" opener = URLopener() f = opener.open(self.url) lines = [line.strip() for line in f] f.close() self.errcode = opener.errcode if self.errcode in (401, 403): self.disallow_all = True elif self.errcode >= 400 and self.errcode < 500: self.allow_all = True elif self.errcode == 200 and lines: self.parse(lines) def _add_entry(self, entry): if "*" in entry.useragents: # the default entry is considered last if self.default_entry is None: # the first default entry wins self.default_entry = entry else: self.entries.append(entry) def parse(self, lines): """parse the input lines from a robots.txt file. We allow that a user-agent: line is not preceded by one or more blank lines.""" # states: # 0: start state # 1: saw user-agent line # 2: saw an allow or disallow line state = 0 linenumber = 0 entry = Entry() self.modified() for line in lines: linenumber += 1 if not line: if state == 1: entry = Entry() state = 0 elif state == 2: self._add_entry(entry) entry = Entry() state = 0 # remove optional comment and strip line i = line.find('#') if i >= 0: line = line[:i] line = line.strip() if not line: continue line = line.split(':', 1) if len(line) == 2: line[0] = line[0].strip().lower() line[1] = urllib.unquote(line[1].strip()) if line[0] == "user-agent": if state == 2: self._add_entry(entry) entry = Entry() entry.useragents.append(line[1]) state = 1 elif line[0] == "disallow": if state != 0: entry.rulelines.append(RuleLine(line[1], False)) state = 2 elif line[0] == "allow": if state != 0: entry.rulelines.append(RuleLine(line[1], True)) state = 2 if state == 2: self._add_entry(entry) def can_fetch(self, useragent, url): """using the parsed robots.txt decide if useragent can fetch url""" if self.disallow_all: return False if self.allow_all: return True # Until the robots.txt file has been read or found not # to exist, we must assume that no url is allowable. # This prevents false positives when a user erronenously # calls can_fetch() before calling read(). if not self.last_checked: return False # search for given user agent matches # the first match counts parsed_url = urlparse.urlparse(urllib.unquote(url)) url = urlparse.urlunparse(('', '', parsed_url.path, parsed_url.params, parsed_url.query, parsed_url.fragment)) url = urllib.quote(url) if not url: url = "/" for entry in self.entries: if entry.applies_to(useragent): return entry.allowance(url) # try the default entry last if self.default_entry: return self.default_entry.allowance(url) # agent not found ==> access granted return True def __str__(self): return ''.join([str(entry) + "\n" for entry in self.entries]) class RuleLine: """A rule line is a single "Allow:" (allowance==True) or "Disallow:" (allowance==False) followed by a path.""" def __init__(self, path, allowance): if path == '' and not allowance: # an empty value means allow all allowance = True path = urlparse.urlunparse(urlparse.urlparse(path)) self.path = urllib.quote(path) self.allowance = allowance def applies_to(self, filename): return self.path == "*" or filename.startswith(self.path) def __str__(self): return (self.allowance and "Allow" or "Disallow") + ": " + self.path class Entry: """An entry has one or more user-agents and zero or more rulelines""" def __init__(self): self.useragents = [] self.rulelines = [] def __str__(self): ret = [] for agent in self.useragents: ret.extend(["User-agent: ", agent, "\n"]) for line in self.rulelines: ret.extend([str(line), "\n"]) return ''.join(ret) def applies_to(self, useragent): """check if this entry applies to the specified agent""" # split the name token and make it lower case useragent = useragent.split("/")[0].lower() for agent in self.useragents: if agent == '*': # we have the catch-all agent return True agent = agent.lower() if agent in useragent: return True return False def allowance(self, filename): """Preconditions: - our agent applies to this entry - filename is URL decoded""" for line in self.rulelines: if line.applies_to(filename): return line.allowance return True class URLopener(urllib.FancyURLopener): def __init__(self, *args): urllib.FancyURLopener.__init__(self, *args) self.errcode = 200 def prompt_user_passwd(self, host, realm): ## If robots.txt file is accessible only with a password, ## we act as if the file wasn't there. return None, None def http_error_default(self, url, fp, errcode, errmsg, headers): self.errcode = errcode return urllib.FancyURLopener.http_error_default(self, url, fp, errcode, errmsg, headers)
# -*- Mode: Python -*- # vi:si:et:sw=4:sts=4:ts=4 # # gst-python - Python bindings for GStreamer # Copyright (C) 2002 David I. Lehn # Copyright (C) 2004 Johan Dahlin # Copyright (C) 2005 Edward Hervey # # This library is free software; you can redistribute it and/or # modify it under the terms of the GNU Lesser General Public # License as published by the Free Software Foundation; either # version 2.1 of the License, or (at your option) any later version. # # This library is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU # Lesser General Public License for more details. # # You should have received a copy of the GNU Lesser General Public # License along with this library; if not, write to the Free Software # Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA try: from dl import RTLD_LAZY, RTLD_GLOBAL except ImportError: # dl doesn't seem to be available on 64bit systems try: from DLFCN import RTLD_LAZY, RTLD_GLOBAL except ImportError: pass import os import sys import gc import unittest import pygtk pygtk.require('2.0') import gobject try: gobject.threads_init() except: print "WARNING: gobject doesn't have threads_init, no threadsafety" # Detect the version of pygobject # In pygobject >= 2.13.0 the refcounting of objects has changed. pgmaj,pgmin,pgmac = gobject.pygobject_version if pgmaj >= 2 and pgmin >= 13: pygobject_2_13 = True else: pygobject_2_13 = False # Don't insert before . # sys.path.insert(1, os.path.join('..')) # Load GST and make sure we load it from the current build sys.setdlopenflags(RTLD_LAZY | RTLD_GLOBAL) topbuilddir = os.path.abspath(os.path.join('..')) topsrcdir = os.path.abspath(os.path.join('..')) if topsrcdir.endswith('_build'): topsrcdir = os.path.dirname(topsrcdir) # gst's __init__.py is in topsrcdir/gst path = os.path.abspath(os.path.join(topsrcdir, 'gst')) import gst file = gst.__file__ assert file.startswith(path), 'bad gst path: %s' % file # gst's interfaces is in topbuilddir/gst path = os.path.abspath(os.path.join(topbuilddir, 'gst')) try: import gst.interfaces except ImportError: # hack: we import it from our builddir/gst/.libs instead; ugly import interfaces gst.interfaces = interfaces file = gst.interfaces.__file__ assert file.startswith(path), 'bad gst.interfaces path: %s' % file # gst's pbutils is in topbuilddir/gst path = os.path.abspath(os.path.join(topbuilddir, 'gst')) try: import gst.pbutils except ImportError: # hack: we import it from our builddir/gst/.libs instead; ugly import pbutils gst.pbutils = pbutils file = gst.pbutils.__file__ assert file.startswith(path), 'bad gst.pbutils path: %s' % file # testhelper needs gstltihooks import gstltihooks import testhelper gstltihooks.uninstall() _stderr = None def disable_stderr(): global _stderr _stderr = file('/tmp/stderr', 'w+') sys.stderr = os.fdopen(os.dup(2), 'w') os.close(2) os.dup(_stderr.fileno()) def enable_stderr(): global _stderr os.close(2) os.dup(sys.stderr.fileno()) _stderr.seek(0, 0) data = _stderr.read() _stderr.close() os.remove('/tmp/stderr') return data def run_silent(function, *args, **kwargs): disable_stderr() try: function(*args, **kwargs) except Exception, exc: enable_stderr() raise exc output = enable_stderr() return output class TestCase(unittest.TestCase): _types = [gst.Object, gst.MiniObject] def gccollect(self): # run the garbage collector ret = 0 gst.debug('garbage collecting') while True: c = gc.collect() ret += c if c == 0: break gst.debug('done garbage collecting, %d objects' % ret) return ret def gctrack(self): # store all gst objects in the gc in a tracking dict # call before doing any allocation in your test, from setUp gst.debug('tracking gc GstObjects for types %r' % self._types) self.gccollect() self._tracked = {} for c in self._types: self._tracked[c] = [o for o in gc.get_objects() if isinstance(o, c)] def gcverify(self): # verify no new gst objects got added to the gc # call after doing all cleanup in your test, from tearDown gst.debug('verifying gc GstObjects for types %r' % self._types) new = [] for c in self._types: objs = [o for o in gc.get_objects() if isinstance(o, c)] new.extend([o for o in objs if o not in self._tracked[c]]) self.failIf(new, new) #self.failIf(new, ["%r:%d" % (type(o), id(o)) for o in new]) del self._tracked def setUp(self): """ Override me by chaining up to me at the start of your setUp. """ # Using private variables is BAD ! this variable changed name in # python 2.5 try: methodName = self.__testMethodName except: methodName = self._testMethodName gst.debug('%s.%s' % (self.__class__.__name__, methodName)) self.gctrack() def tearDown(self): """ Override me by chaining up to me at the end of your tearDown. """ # Using private variables is BAD ! this variable changed name in # python 2.5 try: methodName = self.__testMethodName except: methodName = self._testMethodName gst.debug('%s.%s' % (self.__class__.__name__, methodName)) self.gccollect() self.gcverify()
import types class LazyFactory(object): def __init__(self, module_str, callable_str): self.module_str = module_str self.callable_str = callable_str self.factory = None def resolve(self): if self.factory == None: self.factory = reduce(lambda x, y: getattr(x, y), self.module_str.split('.')[1:] + self.callable_str.split('.'), __import__(self.module_str)) return self.factory def instantiate(self, *args, **kw): self.resolve() return self.factory(*args, **kw) def __call__(self, *args, **kw): self.instantiate(*args, **kw) class Registry(object): def __init__(self): self._registry = dict() def register(self, keys, factory): try: iter(keys) except TypeError, e: keys = (keys) for k in keys: self._registry[k] = factory def get(self, key): return self._registry[key] source = Registry() sink = Registry() __all__ = ['LazyFactory', 'source', 'sink', 'Registry']
from __future__ import unicode_literals from django.core.urlresolvers import reverse_lazy from django.views import generic from django.shortcuts import redirect from django.contrib.auth import get_user_model from django.contrib import auth from django.contrib import messages from authtools import views as authviews from braces import views as bracesviews from django.conf import settings from . import forms User = get_user_model() class LoginView(bracesviews.AnonymousRequiredMixin, authviews.LoginView): template_name = "accounts/login.html" form_class = forms.LoginForm def form_valid(self, form): redirect = super(LoginView, self).form_valid(form) remember_me = form.cleaned_data.get('remember_me') if remember_me is True: ONE_MONTH = 30*24*60*60 expiry = getattr(settings, "KEEP_LOGGED_DURATION", ONE_MONTH) self.request.session.set_expiry(expiry) return redirect class LogoutView(authviews.LogoutView): url = reverse_lazy('home') class PasswordChangeView(authviews.PasswordChangeView): form_class = forms.PasswordChangeForm template_name = 'accounts/password-change.html' success_url = reverse_lazy('home') def form_valid(self, form): form.save() messages.success(self.request, "Your password was changed, " "hence you have been logged out. Please relogin") return redirect("home") class PasswordResetView(authviews.PasswordResetView): form_class = forms.PasswordResetForm template_name = 'accounts/password-reset.html' success_url = reverse_lazy('accounts:password-reset-done') subject_template_name = 'accounts/emails/password-reset-subject.txt' email_template_name = 'accounts/emails/password-reset-email.html' class PasswordResetDoneView(authviews.PasswordResetDoneView): template_name = 'accounts/password-reset-done.html' class PasswordResetConfirmView(authviews.PasswordResetConfirmAndLoginView): template_name = 'accounts/password-reset-confirm.html' form_class = forms.SetPasswordForm
""" Providing iterator functions that are not in all version of Python we support. Where possible, we try to use the system-native version and only fall back to these implementations if necessary. """ import itertools # Fallback for Python 2.4, Python 2.5 def product(*args, **kwds): """ Taken from http://docs.python.org/library/itertools.html#itertools.product """ # product('ABCD', 'xy') --> Ax Ay Bx By Cx Cy Dx Dy # product(range(2), repeat=3) --> 000 001 010 011 100 101 110 111 pools = map(tuple, args) * kwds.get('repeat', 1) result = [[]] for pool in pools: result = [x+[y] for x in result for y in pool] for prod in result: yield tuple(prod) if hasattr(itertools, 'product'): product = itertools.product def is_iterable(x): "A implementation independent way of checking for iterables" try: iter(x) except TypeError: return False else: return True def all(iterable): for item in iterable: if not item: return False return True def any(iterable): for item in iterable: if item: return True return False
#!/usr/bin/env python class Image(object): """NOTE: This class is auto generated by the swagger code generator program. Do not edit the class manually.""" def __init__(self): """ Attributes: swaggerTypes (dict): The key is attribute name and the value is attribute type. attributeMap (dict): The key is attribute name and the value is json key in definition. """ self.swaggerTypes = { 'Width': 'int', 'Height': 'int', 'SelfUri': 'ResourceUri', 'AlternateLinks': 'list[ResourceUri]', 'Links': 'list[ResourceUri]' } self.attributeMap = { 'Width': 'Width','Height': 'Height','SelfUri': 'SelfUri','AlternateLinks': 'AlternateLinks','Links': 'Links'} self.Width = None # int self.Height = None # int self.SelfUri = None # ResourceUri self.AlternateLinks = None # list[ResourceUri] self.Links = None # list[ResourceUri]
def new_section(concept_title, concept_description): html_a = ''' <div class="concept"> <div class="concept-title"> <h3>''' + concept_title + '</h3>' html_b= ''' </div> <div class="concept-description"> <p> ''' + concept_description html_c=''' </p> </div> </div>''' full_html = html_a + html_b + html_c return full_html def create_HTML(concept): concept_title = concept[0] concept_description = concept[1] return new_section(concept_title, concept_description) EXAMPLE_LIST_OF_CONCEPTS = [ ['test title 1', 'test description 1'], ['test title 2', 'test description 2'], ['test title 3', 'test description 3']] def create_HTML_for_list_of_concepts(list_of_concepts): HTML = "" for concept in list_of_concepts: new_HTML = create_HTML(concept) HTML += new_HTML return HTML print create_HTML_for_list_of_concepts(EXAMPLE_LIST_OF_CONCEPTS)
""" A script for testing DraftRegistrationApprovals. Automatically approves all pending DraftRegistrationApprovals. """ import sys import logging from framework.celery_tasks.handlers import celery_teardown_request from website.app import init_app from website.project.model import DraftRegistration, Sanction logger = logging.getLogger(__name__) logging.basicConfig(level=logging.WARN) logging.disable(level=logging.INFO) def main(dry_run=True): if dry_run: logger.warn('DRY RUN mode') pending_approval_drafts = DraftRegistration.find() need_approval_drafts = [draft for draft in pending_approval_drafts if draft.approval and draft.requires_approval and draft.approval.state == Sanction.UNAPPROVED] for draft in need_approval_drafts: sanction = draft.approval try: if not dry_run: sanction.state = Sanction.APPROVED sanction._on_complete(None) sanction.save() logger.warn('Approved {0}'.format(draft._id)) except Exception as e: logger.error(e) if __name__ == '__main__': dry_run = 'dry' in sys.argv app = init_app(routes=False) main(dry_run=dry_run) celery_teardown_request()
""" Django settings for ccswm project. Generated by 'django-admin startproject' using Django 1.10. For more information on this file, see https://docs.djangoproject.com/en/1.10/topics/settings/ For the full list of settings and their values, see https://docs.djangoproject.com/en/1.10/ref/settings/ """ import os # Build paths inside the project like this: os.path.join(BASE_DIR, ...) BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) # Quick-start development settings - unsuitable for production # See https://docs.djangoproject.com/en/1.10/howto/deployment/checklist/ # SECURITY WARNING: keep the secret key used in production secret! SECRET_KEY = 'b)2xn=0)bhu89x#@*eiwvce6+5*=2+n4((er3^1phiu7@qjgo4' # SECURITY WARNING: don't run with debug turned on in production! DEBUG = True ALLOWED_HOSTS = [] # Application definition INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'rest_framework', 'ccswm.statApi', 'corsheaders' ] MIDDLEWARE = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'corsheaders.middleware.CorsMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware', ] CORS_ORIGIN_ALLOW_ALL = False CORS_ORIGIN_WHITELIST = ( 'localhost:8080', 'apiUrl', 'couplescomestatwithme.co.uk', '138.68.146.190', ) ROOT_URLCONF = 'ccswm.urls' TEMPLATES = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [], 'APP_DIRS': True, 'OPTIONS': { 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, }, ] WSGI_APPLICATION = 'ccswm.wsgi.application' # Database # https://docs.djangoproject.com/en/1.10/ref/settings/#databases DATABASES = { 'default': { 'ENGINE': 'django.db.backends.sqlite3', 'NAME': os.path.join(BASE_DIR, 'db.sqlite3'), } } # Password validation # https://docs.djangoproject.com/en/1.10/ref/settings/#auth-password-validators AUTH_PASSWORD_VALIDATORS = [ { 'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator', }, { 'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator', }, { 'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator', }, { 'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator', }, ] # Internationalization # https://docs.djangoproject.com/en/1.10/topics/i18n/ LANGUAGE_CODE = 'en-us' TIME_ZONE = 'UTC' USE_I18N = True USE_L10N = True USE_TZ = True # Static files (CSS, JavaScript, Images) # https://docs.djangoproject.com/en/1.10/howto/static-files/ STATIC_URL = '/static/'
################################################################################ # Licensed to the Apache Software Foundation (ASF) under one # or more contributor license agreements. See the NOTICE file # distributed with this work for additional information # regarding copyright ownership. The ASF licenses this file # to you under the Apache License, Version 2.0 (the # "License"); you may not use this file except in compliance # with the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. ################################################################################ from pyflink.dataset.execution_environment import ExecutionEnvironment from pyflink.datastream.stream_execution_environment import StreamExecutionEnvironment from pyflink.table.table_environment import BatchTableEnvironment, StreamTableEnvironment class MLEnvironment(object): """ The MLEnvironment stores the necessary context in Flink. Each MLEnvironment will be associated with a unique ID. The operations associated with the same MLEnvironment ID will share the same Flink job context. Both MLEnvironment ID and MLEnvironment can only be retrieved from MLEnvironmentFactory. .. versionadded:: 1.11.0 """ def __init__(self, exe_env=None, stream_exe_env=None, batch_tab_env=None, stream_tab_env=None): self._exe_env = exe_env self._stream_exe_env = stream_exe_env self._batch_tab_env = batch_tab_env self._stream_tab_env = stream_tab_env def get_execution_environment(self) -> ExecutionEnvironment: """ Get the ExecutionEnvironment. If the ExecutionEnvironment has not been set, it initial the ExecutionEnvironment with default Configuration. :return: the batch ExecutionEnvironment. .. versionadded:: 1.11.0 """ if self._exe_env is None: self._exe_env = ExecutionEnvironment.get_execution_environment() return self._exe_env def get_stream_execution_environment(self) -> StreamExecutionEnvironment: """ Get the StreamExecutionEnvironment. If the StreamExecutionEnvironment has not been set, it initial the StreamExecutionEnvironment with default Configuration. :return: the StreamExecutionEnvironment. .. versionadded:: 1.11.0 """ if self._stream_exe_env is None: self._stream_exe_env = StreamExecutionEnvironment.get_execution_environment() return self._stream_exe_env def get_batch_table_environment(self) -> BatchTableEnvironment: """ Get the BatchTableEnvironment. If the BatchTableEnvironment has not been set, it initial the BatchTableEnvironment with default Configuration. :return: the BatchTableEnvironment. .. versionadded:: 1.11.0 """ if self._batch_tab_env is None: self._batch_tab_env = BatchTableEnvironment.create( ExecutionEnvironment.get_execution_environment()) return self._batch_tab_env def get_stream_table_environment(self) -> StreamTableEnvironment: """ Get the StreamTableEnvironment. If the StreamTableEnvironment has not been set, it initial the StreamTableEnvironment with default Configuration. :return: the StreamTableEnvironment. .. versionadded:: 1.11.0 """ if self._stream_tab_env is None: self._stream_tab_env = StreamTableEnvironment.create( StreamExecutionEnvironment.get_execution_environment()) return self._stream_tab_env
# Copyright 2013 The Chromium Authors. All rights reserved. # Use of this source code is governed by a BSD-style license that can be # found in the LICENSE file. """Subclass of CloudBucket used for testing.""" from tests.rendering_test_manager import cloud_bucket class MockCloudBucket(cloud_bucket.CloudBucket): """Subclass of CloudBucket used for testing.""" def __init__(self): """Initializes the MockCloudBucket with its datastore. Returns: An instance of MockCloudBucket. """ self.datastore = {} def Reset(self): """Clears the MockCloudBucket's datastore.""" self.datastore = {} # override def UploadFile(self, path, contents, content_type): self.datastore[path] = contents # override def DownloadFile(self, path): if self.datastore.has_key(path): return self.datastore[path] else: raise cloud_bucket.FileNotFoundError # override def RemoveFile(self, path): if self.datastore.has_key(path): self.datastore.pop(path) # override def FileExists(self, path): return self.datastore.has_key(path) # override def GetURL(self, path): if self.datastore.has_key(path): return path else: raise cloud_bucket.FileNotFoundError # override def GetAllPaths(self, prefix): return (item[0] for item in self.datastore.items() if item[0].startswith(prefix))
""" PyCounters is a light weight library to monitor performance in production system. It is meant to be used in scenarios where using a profile is unrealistic due to the overhead it requires. Use PyCounters to get high level and concise overview of what's going on in your production code. See #### (read the docs) for more information """ import logging from pycounters.reporters.base import CollectingRole from shortcuts import _reporting_decorator_context_manager from . import reporters, base def report_start(name): """ reports an event's start. NOTE: you *must* fire off a corresponding event end with report_end """ base.THREAD_DISPATCHER.dispatch_event(name, "start", None) def report_end(name): """ reports an event's end. NOTE: you *must* have fired off a corresponding event start with report_start """ base.THREAD_DISPATCHER.dispatch_event(name, "end", None) def report_start_end(name=None): """ returns a function decorator and/or context manager which raises start and end events. If name is None events name is set to the name of the decorated function. In that case report_start_end can not be used as a context manager. """ return _reporting_decorator_context_manager(name) def report_value(name, value): """ reports a value event to the counters. """ base.THREAD_DISPATCHER.dispatch_event(name, "value", value) def register_counter(counter, throw_if_exists=True): """ Register a counter with PyCounters """ base.GLOBAL_REGISTRY.add_counter(counter, throw=throw_if_exists) def unregister_counter(counter=None, name=None): """ Removes a previously registered counter """ base.GLOBAL_REGISTRY.remove_counter(counter=counter, name=name) def output_report(): """ Manually cause the current values of all registered counters to be reported. """ reporters.base.GLOBAL_REPORTING_CONTROLLER.report() def start_auto_reporting(seconds=300): """ Start reporting in a background thread. Reporting frequency is set by seconds param. """ reporters.base.GLOBAL_REPORTING_CONTROLLER.start_auto_report(seconds=seconds) def stop_auto_reporting(): """ Stop auto reporting """ reporters.base.GLOBAL_REPORTING_CONTROLLER.stop_auto_report() def register_reporter(reporter=None): """ add a reporter to PyCounters. Registered reporters will output collected metrics """ reporters.base.GLOBAL_REPORTING_CONTROLLER.register_reporter(reporter) def unregister_reporter(reporter=None): """ remove a reporter from PyCounters. """ reporters.base.GLOBAL_REPORTING_CONTROLLER.unregister_reporter(reporter) def configure_multi_process_collection(collecting_address=[("", 60907), ("", 60906)], timeout_in_sec=120, role=CollectingRole.AUTO_ROLE): """ configures PyCounters to collect values from multiple processes :param collecting_address: a list of (address,port) tuples address of machines and ports data should be collected on. the extra tuples are used as backup in case the first address/port combination is (temporarily) unavailable. PyCounters would automatically start using the preferred address/port when it becomes available again. This behavior is handy when restarting the program and the old port is not yet freed by the OS. :param timeout_in_sec: timeout configuration for connections. Default should be good enough for pratically everyone. :param role: the role of this process. Leave at the default of AUTO_ROLE for pycounters to automatically choose a collecting leader. """ reporters.base.GLOBAL_REPORTING_CONTROLLER.configure_multi_process(collecting_address=collecting_address, timeout_in_sec=timeout_in_sec, debug_log=logging.getLogger(name="pycounters_multi_proc"), role=role)
""" Template tags and helper functions for displaying breadcrumbs in page titles based on the current micro site. """ from django import template from django.conf import settings from microsite_configuration import microsite from django.templatetags.static import static register = template.Library() def page_title_breadcrumbs(*crumbs, **kwargs): """ This function creates a suitable page title in the form: Specific | Less Specific | General | edX It will output the correct platform name for the request. Pass in a `separator` kwarg to override the default of " | " """ separator = kwargs.get("separator", " | ") if crumbs: return u'{}{}{}'.format(separator.join(crumbs), separator, platform_name()) else: return platform_name() @register.simple_tag(name="page_title_breadcrumbs", takes_context=True) def page_title_breadcrumbs_tag(context, *crumbs): """ Django template that creates breadcrumbs for page titles: {% page_title_breadcrumbs "Specific" "Less Specific" General %} """ return page_title_breadcrumbs(*crumbs) @register.simple_tag(name="platform_name") def platform_name(): """ Django template tag that outputs the current platform name: {% platform_name %} """ return microsite.get_value('platform_name', settings.PLATFORM_NAME) @register.simple_tag(name="favicon_path") def favicon_path(default=getattr(settings, 'FAVICON_PATH', 'images/favicon.ico')): """ Django template tag that outputs the configured favicon: {% favicon_path %} """ return static(microsite.get_value('favicon_path', default)) @register.simple_tag(name="microsite_css_overrides_file") def microsite_css_overrides_file(): """ Django template tag that outputs the css import for a: {% microsite_css_overrides_file %} """ file_path = microsite.get_value('css_overrides_file', None) if file_path is not None: return "<link href='{}' rel='stylesheet' type='text/css'>".format(static(file_path)) else: return ""
"""Test core types like Molecule and Atom.""" from chemlab.core import Molecule, Atom from chemlab.core import System, subsystem_from_molecules, subsystem_from_atoms from chemlab.core import merge_systems from chemlab.core import crystal, random_lattice_box import numpy as np from nose.tools import eq_, assert_equals from nose.plugins.attrib import attr from chemlab.graphics import display_system def assert_npequal(a, b): assert np.array_equal(a, b), '\n{} != {}'.format(a, b) def assert_eqbonds(a, b): # compare bonds by sorting a = np.sort(np.sort(a, axis=0)) b = np.sort(np.sort(b, axis=0)) assert_npequal(a, b) def assert_allclose(a, b): assert np.allclose(a, b), '\n{} != {}'.format(a, b) def _make_water(): mol = Molecule([Atom("O", [-4.99, 2.49, 0.0]), Atom("H", [-4.02, 2.49, 0.0]), Atom("H", [-5.32, 1.98, 1.0])], bonds=[[0, 1], [0, 2]], export={'hello': 1.0}) return mol class TestMolecule(object): def test_init(self): mol = _make_water() assert_npequal(mol.type_array, ['O', 'H', 'H']) class TestSystem(object): def _make_molecules(self): wat = _make_water() wat.r_array *= 0.1 # Initialization from empty s = System.empty(4, 4*3) mols = [] # Array to be compared for _ in range(s.n_mol): wat.r_array += 0.1 mols.append(wat.copy()) return mols def _assert_init(self, system): assert_npequal(system.type_array, ['O', 'H', 'H', 'O', 'H', 'H', 'O', 'H', 'H', 'O', 'H', 'H',]) # Test atom coordinates #print "Atom Coordinates" #print s.r_array # Test atom masses #print s.m_array # Test charges assert_allclose(system.charge_array, [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]) # Test mol indices assert_npequal(system.mol_indices, [0, 3, 6, 9]) # Test mol n_atoms assert_npequal(system.mol_n_atoms, [3, 3, 3, 3]) # Test get molecule entry assert_npequal(system.molecules[0].type_array, ['O', 'H', 'H']) # Test derived property -- center of mass assert_allclose(system.get_derived_molecule_array('center_of_mass'), [[-1.00621917, 0.05572538, 0.02237967], [-0.73978867, 0.07251013, 0.03916442], [-0.47335818, 0.08929488, 0.05594917], [-0.20692768, 0.10607963, 0.07273392]]) # Test bonds assert_eqbonds(system.bonds, [[0, 1], [0, 2], [3, 4], [3, 5], [6, 7], [6, 8], [9, 10], [9, 11]]) # Test bond orders #print 'Test Indexing of system.molecule' #print s.molecules[0] #print s.molecules[:], s.molecules[:-5] #print s.atoms[0] #print s.atoms[:] def test_init(self): mols = self._make_molecules() system = System(mols) self._assert_init(system) def test_from_empty(self): mols = self._make_molecules() system = System.empty(4, 4*3) [system.add(mol) for mol in mols] self._assert_init(system) def test_from_actual_empty(self): mols = self._make_molecules() system = System([]) [system.add(mol) for mol in mols] def test_from_arrays(self): mols = self._make_molecules() r_array = np.concatenate([m.r_array for m in mols]) type_array = np.concatenate([m.type_array for m in mols]) mol_indices = [0, 3, 6, 9] bonds = np.concatenate([m.bonds + 3*i for i, m in enumerate(mols)]) system = System.from_arrays(r_array=r_array, type_array=type_array, mol_indices=mol_indices, bonds=bonds) self._assert_init(system) def test_subsystem_from_molecules(self): mols = self._make_molecules() system = System(mols) subsystem = subsystem_from_molecules(system, np.array([0, 2])) assert_equals(subsystem.n_mol, 2) def test_subsystem_from_atoms(self): mols = self._make_molecules() system = System(mols) sub = subsystem_from_atoms(system, np.array([True, True, False, False, False, False, False, False, False])) assert_equals(sub.n_mol, 1) def test_remove_atoms(self): # This will remove the first and last molecules mols = self._make_molecules() system = System(mols) system.remove_atoms([0, 1, 11]) assert_eqbonds(system.bonds, [[0, 1], [0, 2], [3, 4], [3, 5]]) assert_npequal(system.type_array, np.array(['O', 'H', 'H', 'O', 'H', 'H'], dtype='object')) def test_reorder_molecules(self): mols = self._make_molecules() system = System(mols) system.bonds = np.array([[0, 1], [3, 5]]) # Reordering system.reorder_molecules([1, 0, 2, 3]) assert_eqbonds(system.bonds, [[0, 2], [3, 4]]) @attr('slow') def test_merge_system(): # take a protein from chemlab.io import datafile from chemlab.graphics import display_system from chemlab.db import ChemlabDB water = ChemlabDB().get("molecule", "example.water") prot = datafile("tests/data/3ZJE.pdb").read("system") # Take a box of water NWAT = 50000 bsize = 20.0 pos = np.random.random((NWAT, 3)) * bsize wat = water.copy() s = System.empty(NWAT, NWAT*3, box_vectors=np.eye(3)*bsize) for i in range(NWAT): wat.move_to(pos[i]) s.add(wat) prot.r_array += 10 s = merge_systems(s, prot, 0.5) display_system(s, 'ball-and-stick') def test_crystal(): '''Building a crystal by using spacegroup module''' na = Molecule([Atom('Na', [0.0, 0.0, 0.0])]) cl = Molecule([Atom('Cl', [0.0, 0.0, 0.0])]) # Fract position of Na and Cl, space group 255 tsys = crystal([[0.0, 0.0, 0.0],[0.5, 0.5, 0.5]], [na, cl], 225, repetitions=[13,13,13]) def test_sort(): na = Molecule([Atom('Na', [0.0, 0.0, 0.0])]) cl = Molecule([Atom('Cl', [0.0, 0.0, 0.0])]) # Fract position of Na and Cl, space group 255 tsys = crystal([[0.0, 0.0, 0.0],[0.5, 0.5, 0.5]], [na, cl], 225, repetitions=[3,3,3]) tsys.sort() assert np.all(tsys.type_array[:tsys.n_mol/2] == 'Cl') def test_bonds(): from chemlab.io import datafile bz = datafile("tests/data/benzene.mol").read('molecule') na = Molecule([Atom('Na', [0.0, 0.0, 0.0])]) # Adding bonds s = System.empty(2, 2*bz.n_atoms) s.add(bz) assert_npequal(s.bonds, bz.bonds) assert_npequal(bz.bond_orders, [1, 2, 2, 1, 1, 2]) assert_npequal(s.bond_orders, bz.bond_orders) s.add(bz) assert_npequal(s.bonds, np.concatenate((bz.bonds, bz.bonds + 6))) #assert_npequal(s.bond_orders) # Reordering orig = np.array([[0, 1], [6, 8]]) s.bonds = orig s.reorder_molecules([1, 0]) assert_npequal(s.bonds, np.array([[6, 7], [0, 2]])) # This doesn't change the bond_ordering # Selection ss = subsystem_from_molecules(s, [1]) assert_npequal(ss.bonds, np.array([[0, 1]])) import inspect ss2 = System.from_arrays(**dict(inspect.getmembers(ss))) ss2.r_array += 10.0 ms = merge_systems(ss, ss2) assert_npequal(ms.bonds, np.array([[0, 1], [6, 7]])) assert_npequal(ms.bond_orders, np.array([1, 1])) # From_arrays s = System.from_arrays(mol_indices=[0], bonds=bz.bonds, **bz.__dict__) assert_npequal(s.bonds, bz.bonds) assert_npequal(s.bond_orders, bz.bond_orders) # Get molecule entry # Test the bonds when they're 0 s.bonds = np.array([]) assert_equals(s.get_derived_molecule_array('formula'), 'C6') def test_bond_orders(): # Get a molecule with some bonds wat = _make_water() wat_o = wat.copy() # 0,1 0,2 assert_npequal(wat.bond_orders, np.array([1, 1])) # Remove a bond wat.bonds = np.array([[0, 1]]) assert_npequal(wat.bond_orders, np.array([1])) wat.bond_orders = np.array([2]) # Try with a system s = System.empty(2, 6) s.add(wat_o) s.add(wat) assert_npequal(s.bond_orders , np.array([1, 1, 2])) s.reorder_molecules([1, 0]) # We don't actually sort bonds again assert_npequal(s.bond_orders , np.array([1, 1, 2])) s.bonds = np.array([[0, 1], [0, 2], [3, 4], [3, 5]]) assert_npequal(s.bond_orders, np.array([1, 1, 2, 1])) def test_random(): '''Testing random made box''' from chemlab.db import ChemlabDB cdb = ChemlabDB() na = Molecule([Atom('Na', [0.0, 0.0, 0.0])]) cl = Molecule([Atom('Cl', [0.0, 0.0, 0.0])]) wat = cdb.get("molecule", 'gromacs.spce') s = random_lattice_box([na, cl, wat], [160, 160, 160], [4, 4, 4]) #display_system(s) def test_bond_guessing(): from chemlab.db import ChemlabDB, CirDB from chemlab.graphics import display_molecule from chemlab.io import datafile mol = datafile('tests/data/3ZJE.pdb').read('molecule') print(mol.r_array) mol.guess_bonds() assert mol.bonds.size > 0 # We should find the bond guessing also for systems # System Made of two benzenes bz = datafile("tests/data/benzene.mol").read('molecule') bzbonds = bz.bonds bz.bonds = np.array([]) # Separating the benzenes by large amount bz2 = bz.copy() bz2.r_array += 2.0 s = System([bz, bz2]) s.guess_bonds() assert_eqbonds(s.bonds, np.concatenate((bzbonds, bzbonds + 6))) # Separating benzenes by small amount bz2 = bz.copy() bz2.r_array += 0.15 s = System([bz, bz2]) s.guess_bonds() assert_eqbonds(s.bonds, np.concatenate((bzbonds, bzbonds + 6))) #display_molecule(mol) def test_extending(): from chemlab.core.attributes import NDArrayAttr, MArrayAttr from chemlab.core.fields import AtomicField class MySystem(System): attributes = System.attributes + [NDArrayAttr('v_array', 'v_array', np.float, 3)] class MyMolecule(Molecule): attributes = Molecule.attributes + [MArrayAttr('v_array', 'v', np.float)] class MyAtom(Atom): fields = Atom.fields + [AtomicField('v', default=lambda at: np.zeros(3, np.float))] na = MyMolecule([MyAtom.from_fields(type='Na', r=[0.0, 0.0, 0.0], v=[1.0, 0.0, 0.0])]) cl = MyMolecule([MyAtom.from_fields(type='Cl', r=[0.0, 0.0, 0.0])]) s = MySystem([na, cl]) na_atom = MyAtom.from_fields(type='Na', r=[0.0, 0.0, 0.0], v=[1.0, 0.0, 0.0]) print(na_atom.copy()) print(s.v_array) # Try to adapt orig_s = s.astype(System) s = orig_s.astype(MySystem) # We lost the v information by converting back and forth print(orig_s, s) print(s.v_array) # Adapt for molecule and atoms print(type(na.astype(Molecule))) na_atom = MyAtom.from_fields(type='Na', r=[0.0, 0.0, 0.0], v=[1.0, 0.0, 0.0]) print(type(na_atom.astype(Atom))) def test_serialization(): cl = Molecule([Atom.from_fields(type='Cl', r=[0.0, 0.0, 0.0])]) jsonstr = cl.tojson() assert Molecule.from_json(jsonstr).tojson() == jsonstr na = Molecule([Atom('Na', [0.0, 0.0, 0.0])]) cl = Molecule([Atom('Cl', [0.0, 0.0, 0.0])]) # Fract position of Na and Cl, space group 255 tsys = crystal([[0.0, 0.0, 0.0],[0.5, 0.5, 0.5]], [na, cl], 225, repetitions=[3,3,3]) jsonstr = tsys.tojson() assert System.from_json(jsonstr).tojson() == jsonstr
import inspect import os import pkgutil import warnings from importlib import import_module from threading import local from django.conf import settings from django.core.exceptions import ImproperlyConfigured from django.utils import six from django.utils._os import upath from django.utils.deprecation import RemovedInDjango20Warning from django.utils.functional import cached_property from django.utils.module_loading import import_string DEFAULT_DB_ALIAS = 'default' DJANGO_VERSION_PICKLE_KEY = '_django_version' class Error(Exception if six.PY3 else StandardError): pass class InterfaceError(Error): pass class DatabaseError(Error): pass class DataError(DatabaseError): pass class OperationalError(DatabaseError): pass class IntegrityError(DatabaseError): pass class InternalError(DatabaseError): pass class ProgrammingError(DatabaseError): pass class NotSupportedError(DatabaseError): pass class DatabaseErrorWrapper(object): """ Context manager and decorator that re-throws backend-specific database exceptions using Django's common wrappers. """ def __init__(self, wrapper): """ wrapper is a database wrapper. It must have a Database attribute defining PEP-249 exceptions. """ self.wrapper = wrapper def __enter__(self): pass def __exit__(self, exc_type, exc_value, traceback): if exc_type is None: return for dj_exc_type in ( DataError, OperationalError, IntegrityError, InternalError, ProgrammingError, NotSupportedError, DatabaseError, InterfaceError, Error, ): db_exc_type = getattr(self.wrapper.Database, dj_exc_type.__name__) if issubclass(exc_type, db_exc_type): dj_exc_value = dj_exc_type(*exc_value.args) dj_exc_value.__cause__ = exc_value # Only set the 'errors_occurred' flag for errors that may make # the connection unusable. if dj_exc_type not in (DataError, IntegrityError): self.wrapper.errors_occurred = True six.reraise(dj_exc_type, dj_exc_value, traceback) def __call__(self, func): # Note that we are intentionally not using @wraps here for performance # reasons. Refs #21109. def inner(*args, **kwargs): with self: return func(*args, **kwargs) return inner def load_backend(backend_name): # Look for a fully qualified database backend name try: return import_module('%s.base' % backend_name) except ImportError as e_user: # The database backend wasn't found. Display a helpful error message # listing all possible (built-in) database backends. backend_dir = os.path.join(os.path.dirname(upath(__file__)), 'backends') try: builtin_backends = [ name for _, name, ispkg in pkgutil.iter_modules([backend_dir]) if ispkg and name != 'dummy'] except EnvironmentError: builtin_backends = [] if backend_name not in ['django.db.backends.%s' % b for b in builtin_backends]: backend_reprs = map(repr, sorted(builtin_backends)) error_msg = ("%r isn't an available database backend.\n" "Try using 'django.db.backends.XXX', where XXX " "is one of:\n %s\nError was: %s" % (backend_name, ", ".join(backend_reprs), e_user)) raise ImproperlyConfigured(error_msg) else: # If there's some other error, this must be an error in Django raise class ConnectionDoesNotExist(Exception): pass class ConnectionHandler(object): def __init__(self, databases=None): """ databases is an optional dictionary of database definitions (structured like settings.DATABASES). """ self._databases = databases self._connections = local() @cached_property def databases(self): if self._databases is None: self._databases = settings.DATABASES if self._databases == {}: self._databases = { DEFAULT_DB_ALIAS: { 'ENGINE': 'django.db.backends.dummy', }, } if self._databases[DEFAULT_DB_ALIAS] == {}: self._databases[DEFAULT_DB_ALIAS]['ENGINE'] = 'django.db.backends.dummy' if DEFAULT_DB_ALIAS not in self._databases: raise ImproperlyConfigured("You must define a '%s' database" % DEFAULT_DB_ALIAS) return self._databases def ensure_defaults(self, alias): """ Puts the defaults into the settings dictionary for a given connection where no settings is provided. """ try: conn = self.databases[alias] except KeyError: raise ConnectionDoesNotExist("The connection %s doesn't exist" % alias) conn.setdefault('ATOMIC_REQUESTS', False) conn.setdefault('AUTOCOMMIT', True) conn.setdefault('ENGINE', 'django.db.backends.dummy') if conn['ENGINE'] == 'django.db.backends.' or not conn['ENGINE']: conn['ENGINE'] = 'django.db.backends.dummy' conn.setdefault('CONN_MAX_AGE', 0) conn.setdefault('OPTIONS', {}) conn.setdefault('TIME_ZONE', None) for setting in ['NAME', 'USER', 'PASSWORD', 'HOST', 'PORT']: conn.setdefault(setting, '') def prepare_test_settings(self, alias): """ Makes sure the test settings are available in the 'TEST' sub-dictionary. """ try: conn = self.databases[alias] except KeyError: raise ConnectionDoesNotExist("The connection %s doesn't exist" % alias) test_settings = conn.setdefault('TEST', {}) for key in ['CHARSET', 'COLLATION', 'NAME', 'MIRROR']: test_settings.setdefault(key, None) def __getitem__(self, alias): if hasattr(self._connections, alias): return getattr(self._connections, alias) self.ensure_defaults(alias) self.prepare_test_settings(alias) db = self.databases[alias] backend = load_backend(db['ENGINE']) conn = backend.DatabaseWrapper(db, alias) setattr(self._connections, alias, conn) return conn def __setitem__(self, key, value): setattr(self._connections, key, value) def __delitem__(self, key): delattr(self._connections, key) def __iter__(self): return iter(self.databases) def all(self): return [self[alias] for alias in self] def close_all(self): for alias in self: try: connection = getattr(self._connections, alias) except AttributeError: continue connection.close() class ConnectionRouter(object): def __init__(self, routers=None): """ If routers is not specified, will default to settings.DATABASE_ROUTERS. """ self._routers = routers @cached_property def routers(self): if self._routers is None: self._routers = settings.DATABASE_ROUTERS routers = [] for r in self._routers: if isinstance(r, six.string_types): router = import_string(r)() else: router = r routers.append(router) return routers def _router_func(action): def _route_db(self, model, **hints): chosen_db = None for router in self.routers: try: method = getattr(router, action) except AttributeError: # If the router doesn't have a method, skip to the next one. pass else: chosen_db = method(model, **hints) if chosen_db: return chosen_db instance = hints.get('instance') if instance is not None and instance._state.db: return instance._state.db return DEFAULT_DB_ALIAS return _route_db db_for_read = _router_func('db_for_read') db_for_write = _router_func('db_for_write') def allow_relation(self, obj1, obj2, **hints): for router in self.routers: try: method = router.allow_relation except AttributeError: # If the router doesn't have a method, skip to the next one. pass else: allow = method(obj1, obj2, **hints) if allow is not None: return allow return obj1._state.db == obj2._state.db def allow_migrate(self, db, app_label, **hints): for router in self.routers: try: method = router.allow_migrate except AttributeError: # If the router doesn't have a method, skip to the next one. continue argspec = inspect.getargspec(router.allow_migrate) if len(argspec.args) == 3 and not argspec.keywords: warnings.warn( "The signature of allow_migrate has changed from " "allow_migrate(self, db, model) to " "allow_migrate(self, db, app_label, model_name=None, **hints). " "Support for the old signature will be removed in Django 2.0.", RemovedInDjango20Warning) model = hints.get('model') allow = None if model is None else method(db, model) else: allow = method(db, app_label, **hints) if allow is not None: return allow return True def allow_migrate_model(self, db, model): return self.allow_migrate( db, model._meta.app_label, model_name=model._meta.model_name, model=model, ) def get_migratable_models(self, app_config, db, include_auto_created=False): """ Return app models allowed to be synchronized on provided db. """ models = app_config.get_models(include_auto_created=include_auto_created) return [model for model in models if self.allow_migrate_model(db, model)]
import unittest from django.contrib.gis.gdal import HAS_GDAL from django.db import connection from django.test import skipUnlessDBFeature from django.utils import six from .utils import SpatialRefSys, oracle, postgis, spatialite test_srs = ({ 'srid': 4326, 'auth_name': ('EPSG', True), 'auth_srid': 4326, # Only the beginning, because there are differences depending on installed libs 'srtext': 'GEOGCS["WGS 84",DATUM["WGS_1984",SPHEROID["WGS 84"', # +ellps=WGS84 has been removed in the 4326 proj string in proj-4.8 'proj4_re': r'\+proj=longlat (\+ellps=WGS84 )?(\+datum=WGS84 |\+towgs84=0,0,0,0,0,0,0 )\+no_defs ', 'spheroid': 'WGS 84', 'name': 'WGS 84', 'geographic': True, 'projected': False, 'spatialite': True, # From proj's "cs2cs -le" and Wikipedia (semi-minor only) 'ellipsoid': (6378137.0, 6356752.3, 298.257223563), 'eprec': (1, 1, 9), }, { 'srid': 32140, 'auth_name': ('EPSG', False), 'auth_srid': 32140, 'srtext': ( 'PROJCS["NAD83 / Texas South Central",GEOGCS["NAD83",' 'DATUM["North_American_Datum_1983",SPHEROID["GRS 1980"' ), 'proj4_re': r'\+proj=lcc \+lat_1=30.28333333333333 \+lat_2=28.38333333333333 \+lat_0=27.83333333333333 ' r'\+lon_0=-99 \+x_0=600000 \+y_0=4000000 (\+ellps=GRS80 )?' r'(\+datum=NAD83 |\+towgs84=0,0,0,0,0,0,0 )?\+units=m \+no_defs ', 'spheroid': 'GRS 1980', 'name': 'NAD83 / Texas South Central', 'geographic': False, 'projected': True, 'spatialite': False, # From proj's "cs2cs -le" and Wikipedia (semi-minor only) 'ellipsoid': (6378137.0, 6356752.31414, 298.257222101), 'eprec': (1, 5, 10), }) @unittest.skipUnless(HAS_GDAL, "SpatialRefSysTest needs gdal support") @skipUnlessDBFeature("has_spatialrefsys_table") class SpatialRefSysTest(unittest.TestCase): def test_retrieve(self): """ Test retrieval of SpatialRefSys model objects. """ for sd in test_srs: srs = SpatialRefSys.objects.get(srid=sd['srid']) self.assertEqual(sd['srid'], srs.srid) # Some of the authority names are borked on Oracle, e.g., SRID=32140. # also, Oracle Spatial seems to add extraneous info to fields, hence the # the testing with the 'startswith' flag. auth_name, oracle_flag = sd['auth_name'] if postgis or (oracle and oracle_flag): self.assertTrue(srs.auth_name.startswith(auth_name)) self.assertEqual(sd['auth_srid'], srs.auth_srid) # No proj.4 and different srtext on oracle backends :( if postgis: self.assertTrue(srs.wkt.startswith(sd['srtext'])) six.assertRegex(self, srs.proj4text, sd['proj4_re']) def test_osr(self): """ Test getting OSR objects from SpatialRefSys model objects. """ for sd in test_srs: sr = SpatialRefSys.objects.get(srid=sd['srid']) self.assertTrue(sr.spheroid.startswith(sd['spheroid'])) self.assertEqual(sd['geographic'], sr.geographic) self.assertEqual(sd['projected'], sr.projected) if not (spatialite and not sd['spatialite']): # Can't get 'NAD83 / Texas South Central' from PROJ.4 string # on SpatiaLite self.assertTrue(sr.name.startswith(sd['name'])) # Testing the SpatialReference object directly. if postgis or spatialite: srs = sr.srs six.assertRegex(self, srs.proj4, sd['proj4_re']) # No `srtext` field in the `spatial_ref_sys` table in SpatiaLite < 4 if not spatialite or connection.ops.spatial_version[0] >= 4: self.assertTrue(srs.wkt.startswith(sd['srtext'])) def test_ellipsoid(self): """ Test the ellipsoid property. """ for sd in test_srs: # Getting the ellipsoid and precision parameters. ellps1 = sd['ellipsoid'] prec = sd['eprec'] # Getting our spatial reference and its ellipsoid srs = SpatialRefSys.objects.get(srid=sd['srid']) ellps2 = srs.ellipsoid for i in range(3): self.assertAlmostEqual(ellps1[i], ellps2[i], prec[i]) @skipUnlessDBFeature('supports_add_srs_entry') def test_add_entry(self): """ Test adding a new entry in the SpatialRefSys model using the add_srs_entry utility. """ from django.contrib.gis.utils import add_srs_entry add_srs_entry(3857) self.assertTrue( SpatialRefSys.objects.filter(srid=3857).exists() ) srs = SpatialRefSys.objects.get(srid=3857) self.assertTrue( SpatialRefSys.get_spheroid(srs.wkt).startswith('SPHEROID[') )
# Copyright (C) 2017 Google Inc. # Licensed under http://www.apache.org/licenses/LICENSE-2.0 <see LICENSE file> """Add roles and permissions tables Revision ID: 3bf5430a8c6f Revises: None Create Date: 2013-06-27 03:25:26.571232 """ # revision identifiers, used by Alembic. revision = '3bf5430a8c6f' down_revision = None from alembic import op import sqlalchemy as sa def upgrade(): op.create_table('roles', sa.Column('id', sa.Integer(), nullable=False, primary_key=True), sa.Column('name', sa.String(length=128), nullable=False), sa.Column('permissions_json', sa.Text(), nullable=False), sa.Column('description', sa.Text(), nullable=True), sa.Column('modified_by_id', sa.Integer()), sa.Column( 'created_at', sa.DateTime(), default=sa.text('current_timestamp')), sa.Column( 'updated_at', sa.DateTime(), default=sa.text('current_timestamp'), onupdate=sa.text('current_timestamp')), sa.Column('context_id', sa.Integer()), ) op.create_table('users_roles', sa.Column('id', sa.Integer(), nullable=False, primary_key=True), sa.Column('role_id', sa.Integer(), nullable=False), sa.Column('user_email', sa.String(length=128), nullable=False), sa.Column('target_context_id', sa.Integer(), nullable=False), sa.Column('modified_by_id', sa.Integer()), sa.Column( 'created_at', sa.DateTime(), default=sa.text('current_timestamp')), sa.Column( 'updated_at', sa.DateTime(), default=sa.text('current_timestamp'), onupdate=sa.text('current_timestamp')), sa.Column('context_id', sa.Integer()), sa.ForeignKeyConstraint(['role_id',], ['roles.id',]), ) def downgrade(): op.drop_table('users_roles') op.drop_table('roles')
# -*- coding: utf-8 -*- """ Commerce app tests package. """ import datetime import json from django.conf import settings from django.test import TestCase from django.test.utils import override_settings from freezegun import freeze_time import httpretty import jwt import mock from ecommerce_api_client import auth from commerce import ecommerce_api_client from student.tests.factories import UserFactory JSON = 'application/json' TEST_PUBLIC_URL_ROOT = 'http://www.example.com' TEST_API_URL = 'http://www-internal.example.com/api' TEST_API_SIGNING_KEY = 'edx' TEST_BASKET_ID = 7 TEST_ORDER_NUMBER = '100004' TEST_PAYMENT_DATA = { 'payment_processor_name': 'test-processor', 'payment_form_data': {}, 'payment_page_url': 'http://example.com/pay', } @override_settings(ECOMMERCE_API_SIGNING_KEY=TEST_API_SIGNING_KEY, ECOMMERCE_API_URL=TEST_API_URL) class EcommerceApiClientTest(TestCase): """ Tests to ensure the client is initialized properly. """ TEST_USER_EMAIL = 'test@example.com' TEST_CLIENT_ID = 'test-client-id' def setUp(self): super(EcommerceApiClientTest, self).setUp() self.user = UserFactory() self.user.email = self.TEST_USER_EMAIL self.user.save() # pylint: disable=no-member @httpretty.activate @freeze_time('2015-7-2') @override_settings(JWT_ISSUER='http://example.com/oauth', JWT_EXPIRATION=30) def test_tracking_context(self): """ Ensure the tracking context is set up in the api client correctly and automatically. """ # fake an ecommerce api request. httpretty.register_uri( httpretty.POST, '{}/baskets/1/'.format(TEST_API_URL), status=200, body='{}', adding_headers={'Content-Type': JSON} ) mock_tracker = mock.Mock() mock_tracker.resolve_context = mock.Mock(return_value={'client_id': self.TEST_CLIENT_ID}) with mock.patch('commerce.tracker.get_tracker', return_value=mock_tracker): ecommerce_api_client(self.user).baskets(1).post() # make sure the request's JWT token payload included correct tracking context values. actual_header = httpretty.last_request().headers['Authorization'] expected_payload = { 'username': self.user.username, 'full_name': self.user.profile.name, 'email': self.user.email, 'iss': settings.JWT_ISSUER, 'exp': datetime.datetime.utcnow() + datetime.timedelta(seconds=settings.JWT_EXPIRATION), 'tracking_context': { 'lms_user_id': self.user.id, # pylint: disable=no-member 'lms_client_id': self.TEST_CLIENT_ID, }, } expected_header = 'JWT {}'.format(jwt.encode(expected_payload, TEST_API_SIGNING_KEY)) self.assertEqual(actual_header, expected_header) @httpretty.activate def test_client_unicode(self): """ The client should handle json responses properly when they contain unicode character data. Regression test for ECOM-1606. """ expected_content = '{"result": "Prparatoire"}' httpretty.register_uri( httpretty.GET, '{}/baskets/1/order/'.format(TEST_API_URL), status=200, body=expected_content, adding_headers={'Content-Type': JSON}, ) actual_object = ecommerce_api_client(self.user).baskets(1).order.get() self.assertEqual(actual_object, {u"result": u"Prparatoire"})
from django.template.defaultfilters import urlizetrunc from django.test import SimpleTestCase from django.utils.safestring import mark_safe from ..utils import setup class UrlizetruncTests(SimpleTestCase): @setup({'urlizetrunc01': '{% autoescape off %}{{ a|urlizetrunc:"8" }} {{ b|urlizetrunc:"8" }}{% endautoescape %}'}) def test_urlizetrunc01(self): output = self.engine.render_to_string( 'urlizetrunc01', { 'a': '"Unsafe" http://example.com/x=&y=', 'b': mark_safe('&quot;Safe&quot; http://example.com?x=&amp;y='), }, ) self.assertEqual( output, '"Unsafe" <a href="http://example.com/x=&amp;y=" rel="nofollow">http:...</a> ' '&quot;Safe&quot; <a href="http://example.com?x=&amp;y=" rel="nofollow">http:...</a>' ) @setup({'urlizetrunc02': '{{ a|urlizetrunc:"8" }} {{ b|urlizetrunc:"8" }}'}) def test_urlizetrunc02(self): output = self.engine.render_to_string( 'urlizetrunc02', { 'a': '"Unsafe" http://example.com/x=&y=', 'b': mark_safe('&quot;Safe&quot; http://example.com?x=&amp;y='), }, ) self.assertEqual( output, '&quot;Unsafe&quot; <a href="http://example.com/x=&amp;y=" rel="nofollow">http:...</a> ' '&quot;Safe&quot; <a href="http://example.com?x=&amp;y=" rel="nofollow">http:...</a>' ) class FunctionTests(SimpleTestCase): def test_truncate(self): uri = 'http://31characteruri.com/test/' self.assertEqual(len(uri), 31) self.assertEqual( urlizetrunc(uri, 31), '<a href="http://31characteruri.com/test/" rel="nofollow">' 'http://31characteruri.com/test/</a>', ) self.assertEqual( urlizetrunc(uri, 30), '<a href="http://31characteruri.com/test/" rel="nofollow">' 'http://31characteruri.com/t...</a>', ) self.assertEqual( urlizetrunc(uri, 2), '<a href="http://31characteruri.com/test/"' ' rel="nofollow">...</a>', ) def test_overtruncate(self): self.assertEqual( urlizetrunc('http://short.com/', 20), '<a href=' '"http://short.com/" rel="nofollow">http://short.com/</a>', ) def test_query_string(self): self.assertEqual( urlizetrunc('http://www.google.co.uk/search?hl=en&q=some+long+url&btnG=Search&meta=', 20), '<a href="http://www.google.co.uk/search?hl=en&amp;q=some+long+url&amp;btnG=Search&amp;' 'meta=" rel="nofollow">http://www.google...</a>', ) def test_non_string_input(self): self.assertEqual(urlizetrunc(123, 1), '123') def test_autoescape(self): self.assertEqual( urlizetrunc('foo<a href=" google.com ">bar</a>buz', 10), 'foo&lt;a href=&quot; <a href="http://google.com" rel="nofollow">google.com</a> &quot;&gt;bar&lt;/a&gt;buz', ) def test_autoescape_off(self): self.assertEqual( urlizetrunc('foo<a href=" google.com ">bar</a>buz', 9, autoescape=False), 'foo<a href=" <a href="http://google.com" rel="nofollow">google...</a> ">bar</a>buz', )
# # lasheight_classify.py # # (c) 2012, Martin Isenburg # LASSO - rapid tools to catch reality # # uses lasheight to compute the height of LiDAR points above the ground # and uses the height information to classify the points. # # The LiDAR input can be in LAS/LAZ/BIN/TXT/SHP/... format. # The LiDAR output can be in LAS/LAZ/BIN/TXT format. # # for licensing details see http://rapidlasso.com/download/LICENSE.txt # import sys, os, arcgisscripting, subprocess def return_classification(classification): if (classification == "created, never classified (0)"): return "0" if (classification == "unclassified (1)"): return "1" if (classification == "ground (2)"): return "2" if (classification == "low vegetation (3)"): return "3" if (classification == "medium vegetation (4)"): return "4" if (classification == "high vegetation (5)"): return "5" if (classification == "building (6)"): return "6" if (classification == "low point (7)"): return "7" if (classification == "keypoint (8)"): return "8" if (classification == "water (9)"): return "9" if (classification == "high point (10)"): return "10" if (classification == "(11)"): return "11" if (classification == "overlap point (12)"): return "12" if (classification == "(13)"): return "13" if (classification == "(14)"): return "14" if (classification == "(15)"): return "15" if (classification == "(16)"): return "16" if (classification == "(17)"): return "17" if (classification == "(18)"): return "18" return "unknown" def check_output(command,console): if console == True: process = subprocess.Popen(command) else: process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True) output,error = process.communicate() returncode = process.poll() return returncode,output ### create the geoprocessor object gp = arcgisscripting.create(9.3) ### report that something is happening gp.AddMessage("Starting lasheight ...") ### get number of arguments argc = len(sys.argv) ### report arguments (for debug) #gp.AddMessage("Arguments:") #for i in range(0, argc): # gp.AddMessage("[" + str(i) + "]" + sys.argv[i]) ### get the path to the LAStools binaries lastools_path = os.path.dirname(os.path.dirname(os.path.dirname(sys.argv[0])))+"\\bin" ### check if path exists if os.path.exists(lastools_path) == False: gp.AddMessage("Cannot find .\lastools\bin at " + lastools_path) sys.exit(1) else: gp.AddMessage("Found " + lastools_path + " ...") ### create the full path to the lasheight executable lasheight_path = lastools_path+"\\lasheight.exe" ### check if executable exists if os.path.exists(lastools_path) == False: gp.AddMessage("Cannot find lasheight.exe at " + lasheight_path) sys.exit(1) else: gp.AddMessage("Found " + lasheight_path + " ...") ### create the command string for lasheight.exe command = [lasheight_path] ### maybe use '-verbose' option if sys.argv[argc-1] == "true": command.append("-v") ### add input LiDAR command.append("-i") command.append(sys.argv[1]) ### maybe use ground points from external file if sys.argv[2] != "#": command.append("-ground_points") command.append(sys.argv[2]) ### else maybe use points with a different classification as ground elif sys.argv[3] != "#": command.append("-class") command.append(return_classification(sys.argv[3])) ### maybe we should ignore/preserve some existing classifications when classifying if sys.argv[4] != "#": command.append("-ignore_class") command.append(return_classification(sys.argv[4])) ### maybe we should ignore/preserve some more existing classifications when classifying if sys.argv[5] != "#": command.append("-ignore_class") command.append(return_classification(sys.argv[5])) ### maybe we classify points below if sys.argv[6] != "#": command.append("-classify_below") command.append(sys.argv[7]) command.append(return_classification(sys.argv[6])) ### maybe we classify points between [interval 1] if sys.argv[8] != "#": command.append("-classify_between") command.append(sys.argv[9]) command.append(sys.argv[10]) command.append(return_classification(sys.argv[8])) ### maybe we classify points between [interval 2] if sys.argv[11] != "#": command.append("-classify_between") command.append(sys.argv[12]) command.append(sys.argv[13]) command.append(return_classification(sys.argv[11])) ### maybe we classify points between [interval 3] if sys.argv[14] != "#": command.append("-classify_between") command.append(sys.argv[15]) command.append(sys.argv[16]) command.append(return_classification(sys.argv[14])) ### maybe we classify points below if sys.argv[17] != "#": command.append("-classify_above") command.append(sys.argv[18]) command.append(return_classification(sys.argv[17])) ### this is where the output arguments start out = 19 ### maybe an output format was selected if sys.argv[out] != "#": if sys.argv[out] == "las": command.append("-olas") elif sys.argv[out] == "laz": command.append("-olaz") elif sys.argv[out] == "bin": command.append("-obin") elif sys.argv[out] == "xyzc": command.append("-otxt") command.append("-oparse") command.append("xyzc") elif sys.argv[out] == "xyzci": command.append("-otxt") command.append("-oparse") command.append("xyzci") elif sys.argv[out] == "txyzc": command.append("-otxt") command.append("-oparse") command.append("txyzc") elif sys.argv[out] == "txyzci": command.append("-otxt") command.append("-oparse") command.append("txyzci") ### maybe an output file name was selected if sys.argv[out+1] != "#": command.append("-o") command.append(sys.argv[out+1]) ### maybe an output directory was selected if sys.argv[out+2] != "#": command.append("-odir") command.append(sys.argv[out+2]) ### maybe an output appendix was selected if sys.argv[out+3] != "#": command.append("-odix") command.append(sys.argv[out+3]) ### report command string gp.AddMessage("LAStools command line:") command_length = len(command) command_string = str(command[0]) for i in range(1, command_length): command_string = command_string + " " + str(command[i]) gp.AddMessage(command_string) ### run command returncode,output = check_output(command, False) ### report output of lasheight gp.AddMessage(str(output)) ### check return code if returncode != 0: gp.AddMessage("Error. lasheight failed.") sys.exit(1) ### report happy end gp.AddMessage("Success. lasheight done.")
#!/usr/bin/python -u import socket import time import os import sys from subprocess import call import random FILENAME_BREAK = "/tmp/break.tmp" FILENAME_OVERLOAD = "/tmp/load.tmp" IP_ADDRESS = socket.gethostbyname(socket.gethostname()) MEMORY_LIMIT = "50" FIXED_STRING = "event FIXED " + IP_ADDRESS LOG_FILE = "/tmp/logging/" + IP_ADDRESS + ".log" SIMULATION_DIRECTORY = "/tmp/simulation/" + IP_ADDRESS MEMORY_FILE = SIMULATION_DIRECTORY + "/memory.tmp" LAST_MEM = None while True: time.sleep(1) memory_use = None if os.path.isfile(FILENAME_BREAK): try: print "Restarting service and removing " + FILENAME_BREAK call(["serf", "event", "-coalesce=false", "FIXING", IP_ADDRESS]) time.sleep(random.randint(2, 4)) try: os.remove(FILENAME_BREAK) except: pass #don't care if it's already been deleted call(["serf", "event", "-coalesce=false", "FIXED", IP_ADDRESS]) except Exception as e: call(["serf", "event", "-coalesce=false", "EXCEPTION", "%s" % e]) if os.path.isfile(FILENAME_OVERLOAD): print "Restarting service and removing" + FILENAME_OVERLOAD try: os.remove(FILENAME_OVERLOAD) except: pass # don't care if it's already been deleted call(["serf", "event", "-coalesce=false", "OVERLOADED", IP_ADDRESS]) if os.path.isfile(MEMORY_FILE): print "Found memory file" with open(MEMORY_FILE, 'r') as f: memory_use = f.readline().rstrip() print "Memory use: " + memory_use if LAST_MEM != memory_use: print "Memory use changed, sending event" call(["serf", "event", "-coalesce=false", "MEMORY_LEVEL", "MEMORY_LEVEL=%s IP=%s" % (memory_use, IP_ADDRESS)]) global LAST_MEM LAST_MEM = memory_use
import numpy as np from ase.units import Bohr from ase.parallel import paropen from ase.utils import prnt from gpaw.spherical_harmonics import Y from gpaw.utilities.tools import coordinates class Multipole: """Expand a function on the grid in multipole moments relative to a given center. center: Vector [Angstrom] """ def __init__(self, center, calculator=None, lmax=6): self.center = center / Bohr self.lmax = lmax self.gd = None self.y_Lg = None self.l_L = None if calculator is not None: self.initialize(calculator.density.finegd) def initialize(self, gd): """Initialize Y_L arrays""" self.gd = gd r_cg, r2_g = coordinates(gd, self.center, tiny=1.e-78) r_g = np.sqrt(r2_g) rhat_cg = r_cg / r_g self.l_L = [] self.y_Lg = [] npY = np.vectorize(Y, (float,), 'spherical harmonic') L = 0 for l in range(self.lmax + 1): for m in range(2 * l + 1): self.y_Lg.append( np.sqrt(4 * np.pi / (2 * l + 1)) * r_g**l * npY(L, rhat_cg[0], rhat_cg[1], rhat_cg[2]) ) self.l_L.append(l) L += 1 def expand(self, f_g): """Expand a function f_g in multipole moments units [e * Angstrom**l]""" assert(f_g.shape == self.gd.empty().shape) q_L = [] for L, y_g in enumerate(self.y_Lg): q_L.append(self.gd.integrate(f_g * y_g)) q_L[L] *= Bohr**self.l_L[L] return np.array(q_L) def to_file(self, calculator, filename='multipole.dat', mode='a'): """Expand the charge distribution in multipoles and write the result to a file""" if self.gd is None: self.initialize(calculator.density.finegd) q_L = self.expand(-calculator.density.rhot_g) f = paropen(filename, mode) prnt('# Multipole expansion of the charge density', file=f) prnt('# center =', self.center * Bohr, 'Angstrom', file=f) prnt('# lmax =', self.lmax, file=f) prnt(('# see https://trac.fysik.dtu.dk/projects/gpaw/browser/' + 'trunk/c/bmgs/sharmonic.py'), file=f) prnt('# for the definition of spherical harmonics', file=f) prnt('# l m q_lm[|e| Angstrom**l]', file=f) L = 0 for l in range(self.lmax + 1): for m in range(-l, l + 1): prnt('{0:2d} {1:3d} {2:g}'.format(l, m, q_L[L]), file=f) L += 1 f.close()
#!/usr/bin/python # # Copyright 2014 Google Inc. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. """This code example runs a report equal to the "Whole network report" on the DFP website. """ __author__ = ('Nicholas Chen', 'Joseph DiLallo') import tempfile # Import appropriate modules from the client library. from googleads import dfp from googleads import errors def main(client): # Initialize appropriate service. network_service = client.GetService('NetworkService', version='v201408') # Initialize a DataDownloader. report_downloader = client.GetDataDownloader(version='v201408') # Get root ad unit id for network. root_ad_unit_id = ( network_service.getCurrentNetwork()['effectiveRootAdUnitId']) # Set filter statement and bind value for reportQuery. values = [{ 'key': 'parent_ad_unit_id', 'value': { 'xsi_type': 'NumberValue', 'value': root_ad_unit_id } }] filter_statement = {'query': 'WHERE PARENT_AD_UNIT_ID = :parent_ad_unit_id', 'values': values} # Create report job. report_job = { 'reportQuery': { 'dimensions': ['DATE', 'AD_UNIT_NAME'], 'adUnitView': 'HIERARCHICAL', 'columns': ['AD_SERVER_IMPRESSIONS', 'AD_SERVER_CLICKS', 'DYNAMIC_ALLOCATION_INVENTORY_LEVEL_IMPRESSIONS', 'DYNAMIC_ALLOCATION_INVENTORY_LEVEL_CLICKS', 'TOTAL_INVENTORY_LEVEL_IMPRESSIONS', 'TOTAL_INVENTORY_LEVEL_CPM_AND_CPC_REVENUE'], 'dateRangeType': 'LAST_WEEK', 'statement': filter_statement } } try: # Run the report and wait for it to finish. report_job_id = report_downloader.WaitForReport(report_job) except errors.DfpReportError, e: print 'Failed to generate report. Error was: %s' % e # Change to your preferred export format. export_format = 'CSV_DUMP' report_file = tempfile.NamedTemporaryFile(suffix='.csv.gz', delete=False) # Download report data. report_downloader.DownloadReportToFile( report_job_id, export_format, report_file) report_file.close() # Display results. print 'Report job with id \'%s\' downloaded to:\n%s' % ( report_job_id, report_file.name) if __name__ == '__main__': # Initialize client object. dfp_client = dfp.DfpClient.LoadFromStorage() main(dfp_client)
#!/usr/bin/env python # NOT FINISHED (Needs tidying, can only follow OpaqueRefs, whereas some references are by uuid) # This is supposed to be a general xapi database graphing tool. # I've got a bit distracted, so I'm checking it in so it doesn't get lost. # sample command line # ./graphxapi.py | dot -Tpng >doom.png ; eog doom.png & import XenAPI import sanitychecklib from pprint import pprint,pformat #I've written a lot of database graphing programs recently. #For instance there's a program called newmetrics graph which: #finds all host objects, and then looks up all their resident VMs, and then for all those, looks up all VIFs and VBDs, then for all these VIFs looks up the network, and then for all those networks finds all the PIFs. #I'd like to abstract this procedure of constructing a subgraph of the database graph by chasing keys from starting points. #so for instance, a description of newmetricsgraph might be: #considering objects (host, VM, VIF, VBD, network, PIF) #starting from all host objects #follow-keys (host, resident_VMs) (VM, VIFs) (VM, VBDs) (VIF, network) (network, PIFs) #Similarly Richard is interested in the trees of VBDs which are created, so we might wish to say: #considering objects (VBD) #starting from all VBD where true #follow-keys (VBD, parent) #More formally, here are some functions which return 3-tuples describing these algorithms #find ivory's pifs and associated networks def ivorys_pifs_and_networks(): return ( ['PIF','host','network'], [('host','PIFs'),('PIF', 'network')], [('host',lambda(x): x['name_label']=='ivory')] ) #find all the objects with associated metrics def all_metrics(): return( ['host', 'VM', 'VIF', 'VBD', 'network', 'PIF', 'host_metrics','VIF_metrics','PIF_metrics','VBD_metrics','VM_metrics','VM_guest_metrics'], [ ('host', 'resident_VMs'), ('host','PIFs'), ('host','metrics'), ('VM','VIFs'), ('VM','VBDs'), ('VM','metrics'), ('VM','guest_metrics'), ('VIF','network'),('VIF','metrics'), ('PIF', 'network'),('PIF','metrics')], [('host',lambda(x): True)] ) #this should have been Richard's vdi tree, but the parent field is apparently a uuid, not an OpaqueRef, so the program will need to be modified to deal with this. Grrr. #this command may come in handy when fixing it #xe vdi-param-set uuid=895ba851-6a04-c06c-49ac-1bbd3021668c other-config:foo=bar def vdi_tree(uuid): return( ['VDI'], [('VDI',('other_config','parent'))], [('VDI', lambda(x): x['uuid']==uuid) ] ) #Choose the subgraph we want to walk through (object_types, keys_to_chase, start_from)=all_metrics() ##(object_types, keys_to_chase, start_from)=vdi_tree('d295fe98-eea4-e5bd-a776-d1c335612256') ##(object_types, keys_to_chase, start_from)=ivorys_pifs_and_networks() #Generally, we wish to announce the name of this file. #When running in the interpreter, however, this doesn't exist, and we #probably shouldn't log out either try: this_test_name = __file__ logout_after_test = True except NameError: this_test_name = "unknown" logout_after_test = False print "/*------------", this_test_name, "*/" #log in to the master print "/*logging in to ",sanitychecklib.server, "*/" session=sanitychecklib.getsession() sx=session.xenapi class typed_record(): def __init__(self, type_string, record, marked=False): self.type_string=type_string self.record=record self.marked=marked #when chasing through the graph we need to avoid loops, so mark when visited def __repr__(self): return "typed_record("+self.type_string.__repr__()+","+self.record.__repr__()+","+self.marked.__repr__()+")" #first fill a database with the data of all the objects we might be interested in object_database={} for obj_type in object_types: objs=sx.__getattr__(obj_type).get_all_records() for k,v in objs.items(): object_database[k]=typed_record(obj_type, v) def mark_object_and_chase_keys(ref, keys_to_chase): if(ref=='OpaqueRef:NULL'): return obj=object_database[ref] if(obj.marked==True): return else: obj.marked=True type_string=obj.type_string record=obj.record for t,k in keys_to_chase: if type_string==t: try: if (type(k)==type("key")): value=record[k] elif(type(k)==type(("key","subkey"))): value=record[k[0]][k[1]] else: error("key "+k+" should be either string or tuple") except KeyError: return try: if(type(value)==type([])): for x in value: mark_object_and_chase_keys(x, keys_to_chase) else: mark_object_and_chase_keys(value, keys_to_chase) except: print "Error while chasing key ",k, "in type", t raise #mark all starting objects in the database, and for each one chase the key list through the database for ref,obj in object_database.items(): for t,f in start_from: if (obj.type_string == t ) and (f(obj.record)): mark_object_and_chase_keys(ref, keys_to_chase) def print_edge(x, y, label=None): print '"%s" -> "%s"' % (x,y), if label != None: print ' [label="%s"]' % label, print ';' def print_node(x, label=None): print '"%s"' % x, if label != None: print ' [label="%s"]' % label, print ';' #emit the relevant subgraph in dot format print "digraph graphxapi { " for ref,obj in object_database.items(): if (obj.marked): print_node(ref, obj.record.get('name_label', obj.type_string)) for t,k in keys_to_chase: if obj.type_string==t: if (type(k)==type("key")): value=obj.record[k] elif(type(k)==type(("key","subkey"))): try: value=obj.record[k[0]][k[1]] except KeyError: continue; else: error("key "+k+" should be either string or tuple") if(type(value)==type([])): for x in value: print_edge(ref, x, k) else: print_edge(ref, value, k) print "}" #log out if logout_after_test: print "/*logging out*/" session.logout() print "/*End of------", this_test_name, "*/"
#!/usr/bin/python # -*- coding: utf-8 -*- # # (c) 2015, Brian Coca <bcoca@ansible.com> # # This file is part of Ansible # # Ansible is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Ansible is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Ansible. If not, see <http://www.gnu.org/licenses/> DOCUMENTATION = ''' --- version_added: "1.2" module: jabber short_description: Send a message to jabber user or chat room description: - Send a message to jabber options: user: description: - User as which to connect required: true password: description: - password for user to connect required: true to: description: - user ID or name of the room, when using room use a slash to indicate your nick. required: true msg: description: - The message body. required: true default: null host: description: - host to connect, overrides user info required: false port: description: - port to connect to, overrides default required: false default: 5222 encoding: description: - message encoding required: false # informational: requirements for nodes requirements: - python xmpp (xmpppy) author: "Brian Coca (@bcoca)" ''' EXAMPLES = ''' # send a message to a user - jabber: user=mybot@example.net password=secret to=friend@example.net msg="Ansible task finished" # send a message to a room - jabber: user=mybot@example.net password=secret to=mychaps@conference.example.net/ansiblebot msg="Ansible task finished" # send a message, specifying the host and port - jabber user=mybot@example.net host=talk.example.net port=5223 password=secret to=mychaps@example.net msg="Ansible task finished" ''' import os import re import time HAS_XMPP = True try: import xmpp except ImportError: HAS_XMPP = False def main(): module = AnsibleModule( argument_spec=dict( user=dict(required=True), password=dict(required=True), to=dict(required=True), msg=dict(required=True), host=dict(required=False), port=dict(required=False,default=5222), encoding=dict(required=False), ), supports_check_mode=True ) if not HAS_XMPP: module.fail_json(msg="The required python xmpp library (xmpppy) is not installed") jid = xmpp.JID(module.params['user']) user = jid.getNode() server = jid.getDomain() port = module.params['port'] password = module.params['password'] try: to, nick = module.params['to'].split('/', 1) except ValueError: to, nick = module.params['to'], None if module.params['host']: host = module.params['host'] else: host = server if module.params['encoding']: xmpp.simplexml.ENCODING = params['encoding'] msg = xmpp.protocol.Message(body=module.params['msg']) try: conn=xmpp.Client(server) if not conn.connect(server=(host,port)): module.fail_json(rc=1, msg='Failed to connect to server: %s' % (server)) if not conn.auth(user,password,'Ansible'): module.fail_json(rc=1, msg='Failed to authorize %s on: %s' % (user,server)) # some old servers require this, also the sleep following send conn.sendInitPresence(requestRoster=0) if nick: # sending to room instead of user, need to join msg.setType('groupchat') msg.setTag('x', namespace='http://jabber.org/protocol/muc#user') conn.send(xmpp.Presence(to=module.params['to'])) time.sleep(1) else: msg.setType('chat') msg.setTo(to) if not module.check_mode: conn.send(msg) time.sleep(1) conn.disconnect() except Exception, e: module.fail_json(msg="unable to send msg: %s" % e) module.exit_json(changed=False, to=to, user=user, msg=msg.getBody()) # import module snippets from ansible.module_utils.basic import * main()
#!/usr/bin/env python """ An example using networkx.Graph(). miles_graph() returns an undirected graph over the 128 US cities from the datafile miles_dat.txt. The cities each have location and population data. The edges are labeled with the distance betwen the two cities. This example is described in Section 1.1 in Knuth's book [1,2]. References. ----------- [1] Donald E. Knuth, "The Stanford GraphBase: A Platform for Combinatorial Computing", ACM Press, New York, 1993. [2] http://www-cs-faculty.stanford.edu/~knuth/sgb.html """ __author__ = """Aric Hagberg (hagberg@lanl.gov)""" # Copyright (C) 2004-2015 by # Aric Hagberg <hagberg@lanl.gov> # Dan Schult <dschult@colgate.edu> # Pieter Swart <swart@lanl.gov> # All rights reserved. # BSD license. import networkx as nx def miles_graph(): """ Return the cites example graph in miles_dat.txt from the Stanford GraphBase. """ # open file miles_dat.txt.gz (or miles_dat.txt) import gzip fh = gzip.open('knuth_miles.txt.gz','r') G=nx.Graph() G.position={} G.population={} cities=[] for line in fh.readlines(): line = line.decode() if line.startswith("*"): # skip comments continue numfind=re.compile("^\d+") if numfind.match(line): # this line is distances dist=line.split() for d in dist: G.add_edge(city,cities[i],weight=int(d)) i=i+1 else: # this line is a city, position, population i=1 (city,coordpop)=line.split("[") cities.insert(0,city) (coord,pop)=coordpop.split("]") (y,x)=coord.split(",") G.add_node(city) # assign position - flip x axis for matplotlib, shift origin G.position[city]=(-int(x)+7500,int(y)-3000) G.population[city]=float(pop)/1000.0 return G if __name__ == '__main__': import networkx as nx import re import sys G=miles_graph() print("Loaded miles_dat.txt containing 128 cities.") print("digraph has %d nodes with %d edges"\ %(nx.number_of_nodes(G),nx.number_of_edges(G))) # make new graph of cites, edge if less then 300 miles between them H=nx.Graph() for v in G: H.add_node(v) for (u,v,d) in G.edges(data=True): if d['weight'] < 300: H.add_edge(u,v) # draw with matplotlib/pylab try: import matplotlib.pyplot as plt plt.figure(figsize=(8,8)) # with nodes colored by degree sized by population node_color=[float(H.degree(v)) for v in H] nx.draw(H,G.position, node_size=[G.population[v] for v in H], node_color=node_color, with_labels=False) # scale the axes equally plt.xlim(-5000,500) plt.ylim(-2000,3500) plt.savefig("knuth_miles.png") except: pass
## This file is part of Invenio. ## Copyright (C) 2006, 2007, 2008, 2009, 2010, 2011 CERN. ## ## Invenio is free software; you can redistribute it and/or ## modify it under the terms of the GNU General Public License as ## published by the Free Software Foundation; either version 2 of the ## License, or (at your option) any later version. ## ## Invenio is distributed in the hope that it will be useful, but ## WITHOUT ANY WARRANTY; without even the implied warranty of ## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU ## General Public License for more details. ## ## You should have received a copy of the GNU General Public License ## along with Invenio; if not, write to the Free Software Foundation, Inc., ## 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA. """BibFormat element - Prints reprinted editions """ __revision__ = "$Id$" def format_element(bfo, separator): """ Prints the reprinted editions of a record @param separator: a separator between reprinted editions @see: place.py, publisher.py, imprint.py, date.py, pagination.py """ reprints = bfo.field('260__g') if len(reprints) > 0: return separator.join(reprints)
# -*- encoding: utf-8 -*- ############################################################################## # # OpenERP, Open Source Management Solution # Copyright (c) 2011 Zikzakmedia S.L. (http://zikzakmedia.com) All Rights Reserved. # Raimon Esteve <resteve@zikzakmedia.com> # $Id$ # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## import delivery import mgn import mgn_referential import partner import product import product_attributes import product_images import sale import wizard
#!/usr/bin/python # # This file is part of Ansible # # Ansible is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Ansible is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Ansible. If not, see <http://www.gnu.org/licenses/>. # ANSIBLE_METADATA = {'metadata_version': '1.1', 'status': ['preview'], 'supported_by': 'network'} DOCUMENTATION = """ --- module: eos_system version_added: "2.3" author: "Peter Sprygada (@privateip)" short_description: Manage the system attributes on Arista EOS devices description: - This module provides declarative management of node system attributes on Arista EOS devices. It provides an option to configure host system parameters or remove those parameters from the device active configuration. extends_documentation_fragment: eos notes: - Tested against EOS 4.15 options: hostname: description: - Configure the device hostname parameter. This option takes an ASCII string value. domain_name: description: - Configure the IP domain name on the remote device to the provided value. Value should be in the dotted name form and will be appended to the C(hostname) to create a fully-qualified domain name. domain_search: description: - Provides the list of domain suffixes to append to the hostname for the purpose of doing name resolution. This argument accepts a list of names and will be reconciled with the current active configuration on the running node. lookup_source: description: - Provides one or more source interfaces to use for performing DNS lookups. The interface provided in C(lookup_source) can only exist in a single VRF. This argument accepts either a list of interface names or a list of hashes that configure the interface name and VRF name. See examples. name_servers: description: - List of DNS name servers by IP address to use to perform name resolution lookups. This argument accepts either a list of DNS servers or a list of hashes that configure the name server and VRF name. See examples. state: description: - State of the configuration values in the device's current active configuration. When set to I(present), the values should be configured in the device active configuration and when set to I(absent) the values should not be in the device active configuration default: present choices: ['present', 'absent'] """ EXAMPLES = """ - name: configure hostname and domain-name eos_system: hostname: eos01 domain_name: test.example.com - name: remove configuration eos_system: state: absent - name: configure DNS lookup sources eos_system: lookup_source: Management1 - name: configure DNS lookup sources with VRF support eos_system: lookup_source: - interface: Management1 vrf: mgmt - interface: Ethernet1 vrf: myvrf - name: configure name servers eos_system: name_servers: - 8.8.8.8 - 8.8.4.4 - name: configure name servers with VRF support eos_system: name_servers: - { server: 8.8.8.8, vrf: mgmt } - { server: 8.8.4.4, vrf: mgmt } """ RETURN = """ commands: description: The list of configuration mode commands to send to the device returned: always type: list sample: - hostname eos01 - ip domain-name test.example.com session_name: description: The EOS config session name used to load the configuration returned: changed type: str sample: ansible_1479315771 """ import re from ansible.module_utils.basic import AnsibleModule from ansible.module_utils.network_common import ComplexList from ansible.module_utils.eos import load_config, get_config from ansible.module_utils.eos import eos_argument_spec _CONFIGURED_VRFS = None def has_vrf(module, vrf): global _CONFIGURED_VRFS if _CONFIGURED_VRFS is not None: return vrf in _CONFIGURED_VRFS config = get_config(module) _CONFIGURED_VRFS = re.findall('vrf definition (\S+)', config) _CONFIGURED_VRFS.append('default') return vrf in _CONFIGURED_VRFS def map_obj_to_commands(want, have, module): commands = list() state = module.params['state'] needs_update = lambda x: want.get(x) and (want.get(x) != have.get(x)) if state == 'absent': if have['domain_name']: commands.append('no ip domain-name') if have['hostname'] != 'localhost': commands.append('no hostname') if state == 'present': if needs_update('hostname'): commands.append('hostname %s' % want['hostname']) if needs_update('domain_name'): commands.append('ip domain-name %s' % want['domain_name']) if want['domain_list']: # handle domain_list items to be removed for item in set(have['domain_list']).difference(want['domain_list']): commands.append('no ip domain-list %s' % item) # handle domain_list items to be added for item in set(want['domain_list']).difference(have['domain_list']): commands.append('ip domain-list %s' % item) if want['lookup_source']: # handle lookup_source items to be removed for item in have['lookup_source']: if item not in want['lookup_source']: if item['vrf']: if not has_vrf(module, item['vrf']): module.fail_json(msg='vrf %s is not configured' % item['vrf']) values = (item['vrf'], item['interface']) commands.append('no ip domain lookup vrf %s source-interface %s' % values) else: commands.append('no ip domain lookup source-interface %s' % item['interface']) # handle lookup_source items to be added for item in want['lookup_source']: if item not in have['lookup_source']: if item['vrf']: if not has_vrf(module, item['vrf']): module.fail_json(msg='vrf %s is not configured' % item['vrf']) values = (item['vrf'], item['interface']) commands.append('ip domain lookup vrf %s source-interface %s' % values) else: commands.append('ip domain lookup source-interface %s' % item['interface']) if want['name_servers']: # handle name_servers items to be removed. Order does matter here # since name servers can only be in one vrf at a time for item in have['name_servers']: if item not in want['name_servers']: if not has_vrf(module, item['vrf']): module.fail_json(msg='vrf %s is not configured' % item['vrf']) if item['vrf'] not in ('default', None): values = (item['vrf'], item['server']) commands.append('no ip name-server vrf %s %s' % values) else: commands.append('no ip name-server %s' % item['server']) # handle name_servers items to be added for item in want['name_servers']: if item not in have['name_servers']: if not has_vrf(module, item['vrf']): module.fail_json(msg='vrf %s is not configured' % item['vrf']) if item['vrf'] not in ('default', None): values = (item['vrf'], item['server']) commands.append('ip name-server vrf %s %s' % values) else: commands.append('ip name-server %s' % item['server']) return commands def parse_hostname(config): match = re.search('^hostname (\S+)', config, re.M) if match: return match.group(1) def parse_domain_name(config): match = re.search('^ip domain-name (\S+)', config, re.M) if match: return match.group(1) def parse_lookup_source(config): objects = list() regex = 'ip domain lookup (?:vrf (\S+) )*source-interface (\S+)' for vrf, intf in re.findall(regex, config, re.M): if len(vrf) == 0: vrf= None objects.append({'interface': intf, 'vrf': vrf}) return objects def parse_name_servers(config): objects = list() for vrf, addr in re.findall('ip name-server vrf (\S+) (\S+)', config, re.M): objects.append({'server': addr, 'vrf': vrf}) return objects def map_config_to_obj(module): config = get_config(module) return { 'hostname': parse_hostname(config), 'domain_name': parse_domain_name(config), 'domain_list': re.findall('^ip domain-list (\S+)', config, re.M), 'lookup_source': parse_lookup_source(config), 'name_servers': parse_name_servers(config) } def map_params_to_obj(module): obj = { 'hostname': module.params['hostname'], 'domain_name': module.params['domain_name'], 'domain_list': module.params['domain_list'] } lookup_source = ComplexList(dict( interface=dict(key=True), vrf=dict() ), module) name_servers = ComplexList(dict( server=dict(key=True), vrf=dict(default='default') ), module) for arg, cast in [('lookup_source', lookup_source), ('name_servers', name_servers)]: if module.params[arg] is not None: obj[arg] = cast(module.params[arg]) else: obj[arg] = None return obj def main(): """ main entry point for module execution """ argument_spec = dict( hostname=dict(), domain_name=dict(), domain_list=dict(type='list', aliases=['domain_search']), # { interface: <str>, vrf: <str> } lookup_source=dict(type='list'), # { server: <str>; vrf: <str> } name_servers=dict(type='list'), state=dict(default='present', choices=['present', 'absent']) ) argument_spec.update(eos_argument_spec) module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=True) result = {'changed': False} want = map_params_to_obj(module) have = map_config_to_obj(module) commands = map_obj_to_commands(want, have, module) result['commands'] = commands if commands: commit = not module.check_mode response = load_config(module, commands, commit=commit) if response.get('diff') and module._diff: result['diff'] = {'prepared': response.get('diff')} result['session_name'] = response.get('session') result['changed'] = True module.exit_json(**result) if __name__ == '__main__': main()
#!python """Bootstrap setuptools installation If you want to use setuptools in your package's setup.py, just include this file in the same directory with it, and add this to the top of your setup.py:: from ez_setup import use_setuptools use_setuptools() If you want to require a specific version of setuptools, set a download mirror, or use an alternate download directory, you can do so by supplying the appropriate options to ``use_setuptools()``. This file can also be run as a script to install or upgrade setuptools. """ import sys DEFAULT_VERSION = "0.6c9" DEFAULT_URL = "http://pypi.python.org/packages/%s/s/setuptools/" % sys.version[:3] md5_data = { 'setuptools-0.6b1-py2.3.egg': '8822caf901250d848b996b7f25c6e6ca', 'setuptools-0.6b1-py2.4.egg': 'b79a8a403e4502fbb85ee3f1941735cb', 'setuptools-0.6b2-py2.3.egg': '5657759d8a6d8fc44070a9d07272d99b', 'setuptools-0.6b2-py2.4.egg': '4996a8d169d2be661fa32a6e52e4f82a', 'setuptools-0.6b3-py2.3.egg': 'bb31c0fc7399a63579975cad9f5a0618', 'setuptools-0.6b3-py2.4.egg': '38a8c6b3d6ecd22247f179f7da669fac', 'setuptools-0.6b4-py2.3.egg': '62045a24ed4e1ebc77fe039aa4e6f7e5', 'setuptools-0.6b4-py2.4.egg': '4cb2a185d228dacffb2d17f103b3b1c4', 'setuptools-0.6c1-py2.3.egg': 'b3f2b5539d65cb7f74ad79127f1a908c', 'setuptools-0.6c1-py2.4.egg': 'b45adeda0667d2d2ffe14009364f2a4b', 'setuptools-0.6c2-py2.3.egg': 'f0064bf6aa2b7d0f3ba0b43f20817c27', 'setuptools-0.6c2-py2.4.egg': '616192eec35f47e8ea16cd6a122b7277', 'setuptools-0.6c3-py2.3.egg': 'f181fa125dfe85a259c9cd6f1d7b78fa', 'setuptools-0.6c3-py2.4.egg': 'e0ed74682c998bfb73bf803a50e7b71e', 'setuptools-0.6c3-py2.5.egg': 'abef16fdd61955514841c7c6bd98965e', 'setuptools-0.6c4-py2.3.egg': 'b0b9131acab32022bfac7f44c5d7971f', 'setuptools-0.6c4-py2.4.egg': '2a1f9656d4fbf3c97bf946c0a124e6e2', 'setuptools-0.6c4-py2.5.egg': '8f5a052e32cdb9c72bcf4b5526f28afc', 'setuptools-0.6c5-py2.3.egg': 'ee9fd80965da04f2f3e6b3576e9d8167', 'setuptools-0.6c5-py2.4.egg': 'afe2adf1c01701ee841761f5bcd8aa64', 'setuptools-0.6c5-py2.5.egg': 'a8d3f61494ccaa8714dfed37bccd3d5d', 'setuptools-0.6c6-py2.3.egg': '35686b78116a668847237b69d549ec20', 'setuptools-0.6c6-py2.4.egg': '3c56af57be3225019260a644430065ab', 'setuptools-0.6c6-py2.5.egg': 'b2f8a7520709a5b34f80946de5f02f53', 'setuptools-0.6c7-py2.3.egg': '209fdf9adc3a615e5115b725658e13e2', 'setuptools-0.6c7-py2.4.egg': '5a8f954807d46a0fb67cf1f26c55a82e', 'setuptools-0.6c7-py2.5.egg': '45d2ad28f9750e7434111fde831e8372', 'setuptools-0.6c8-py2.3.egg': '50759d29b349db8cfd807ba8303f1902', 'setuptools-0.6c8-py2.4.egg': 'cba38d74f7d483c06e9daa6070cce6de', 'setuptools-0.6c8-py2.5.egg': '1721747ee329dc150590a58b3e1ac95b', 'setuptools-0.6c9-py2.3.egg': 'a83c4020414807b496e4cfbe08507c03', 'setuptools-0.6c9-py2.4.egg': '260a2be2e5388d66bdaee06abec6342a', 'setuptools-0.6c9-py2.5.egg': 'fe67c3e5a17b12c0e7c541b7ea43a8e6', 'setuptools-0.6c9-py2.6.egg': 'ca37b1ff16fa2ede6e19383e7b59245a', } import sys, os try: from hashlib import md5 except ImportError: from md5 import md5 def _validate_md5(egg_name, data): if egg_name in md5_data: digest = md5(data).hexdigest() if digest != md5_data[egg_name]: print >>sys.stderr, ( "md5 validation of %s failed! (Possible download problem?)" % egg_name ) sys.exit(2) return data def use_setuptools( version=DEFAULT_VERSION, download_base=DEFAULT_URL, to_dir=os.curdir, download_delay=15 ): """Automatically find/download setuptools and make it available on sys.path `version` should be a valid setuptools version number that is available as an egg for download under the `download_base` URL (which should end with a '/'). `to_dir` is the directory where setuptools will be downloaded, if it is not already available. If `download_delay` is specified, it should be the number of seconds that will be paused before initiating a download, should one be required. If an older version of setuptools is installed, this routine will print a message to ``sys.stderr`` and raise SystemExit in an attempt to abort the calling script. """ was_imported = 'pkg_resources' in sys.modules or 'setuptools' in sys.modules def do_download(): egg = download_setuptools(version, download_base, to_dir, download_delay) sys.path.insert(0, egg) import setuptools; setuptools.bootstrap_install_from = egg try: import pkg_resources except ImportError: return do_download() try: pkg_resources.require("setuptools>="+version); return except pkg_resources.VersionConflict, e: if was_imported: print >>sys.stderr, ( "The required version of setuptools (>=%s) is not available, and\n" "can't be installed while this script is running. Please install\n" " a more recent version first, using 'easy_install -U setuptools'." "\n\n(Currently using %r)" ) % (version, e.args[0]) sys.exit(2) else: del pkg_resources, sys.modules['pkg_resources'] # reload ok return do_download() except pkg_resources.DistributionNotFound: return do_download() def download_setuptools( version=DEFAULT_VERSION, download_base=DEFAULT_URL, to_dir=os.curdir, delay = 15 ): """Download setuptools from a specified location and return its filename `version` should be a valid setuptools version number that is available as an egg for download under the `download_base` URL (which should end with a '/'). `to_dir` is the directory where the egg will be downloaded. `delay` is the number of seconds to pause before an actual download attempt. """ import urllib2, shutil egg_name = "setuptools-%s-py%s.egg" % (version,sys.version[:3]) url = download_base + egg_name saveto = os.path.join(to_dir, egg_name) src = dst = None if not os.path.exists(saveto): # Avoid repeated downloads try: from distutils import log if delay: log.warn(""" --------------------------------------------------------------------------- This script requires setuptools version %s to run (even to display help). I will attempt to download it for you (from %s), but you may need to enable firewall access for this script first. I will start the download in %d seconds. (Note: if this machine does not have network access, please obtain the file %s and place it in this directory before rerunning this script.) ---------------------------------------------------------------------------""", version, download_base, delay, url ); from time import sleep; sleep(delay) log.warn("Downloading %s", url) src = urllib2.urlopen(url) # Read/write all in one block, so we don't create a corrupt file # if the download is interrupted. data = _validate_md5(egg_name, src.read()) dst = open(saveto,"wb"); dst.write(data) finally: if src: src.close() if dst: dst.close() return os.path.realpath(saveto) def main(argv, version=DEFAULT_VERSION): """Install or upgrade setuptools and EasyInstall""" try: import setuptools except ImportError: egg = None try: egg = download_setuptools(version, delay=0) sys.path.insert(0,egg) from setuptools.command.easy_install import main return main(list(argv)+[egg]) # we're done here finally: if egg and os.path.exists(egg): os.unlink(egg) else: if setuptools.__version__ == '0.0.1': print >>sys.stderr, ( "You have an obsolete version of setuptools installed. Please\n" "remove it from your system entirely before rerunning this script." ) sys.exit(2) req = "setuptools>="+version import pkg_resources try: pkg_resources.require(req) except pkg_resources.VersionConflict: try: from setuptools.command.easy_install import main except ImportError: from easy_install import main main(list(argv)+[download_setuptools(delay=0)]) sys.exit(0) # try to force an exit else: if argv: from setuptools.command.easy_install import main main(argv) else: print "Setuptools version",version,"or greater has been installed." print '(Run "ez_setup.py -U setuptools" to reinstall or upgrade.)' def update_md5(filenames): """Update our built-in md5 registry""" import re for name in filenames: base = os.path.basename(name) f = open(name,'rb') md5_data[base] = md5(f.read()).hexdigest() f.close() data = [" %r: %r,\n" % it for it in md5_data.items()] data.sort() repl = "".join(data) import inspect srcfile = inspect.getsourcefile(sys.modules[__name__]) f = open(srcfile, 'rb'); src = f.read(); f.close() match = re.search("\nmd5_data = {\n([^}]+)}", src) if not match: print >>sys.stderr, "Internal error!" sys.exit(2) src = src[:match.start(1)] + repl + src[match.end(1):] f = open(srcfile,'w') f.write(src) f.close() if __name__=='__main__': if len(sys.argv)>2 and sys.argv[1]=='--md5update': update_md5(sys.argv[2:]) else: main(sys.argv[1:])
# (c) 2014 James Cammarata, <jcammarata@ansible.com> # # This file is part of Ansible # # Ansible is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Ansible is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Ansible. If not, see <http://www.gnu.org/licenses/>. # Make coding more python3-ish from __future__ import (absolute_import, division, print_function) __metaclass__ = type def is_quoted(data): return len(data) > 1 and data[0] == data[-1] and data[0] in ('"', "'") and data[-2] != '\\' def unquote(data): ''' removes first and last quotes from a string, if the string starts and ends with the same quotes ''' if is_quoted(data): return data[1:-1] return data
def __boot(): import sys, imp, os, os.path PYTHONPATH = os.environ.get('PYTHONPATH') if PYTHONPATH is None or (sys.platform=='win32' and not PYTHONPATH): PYTHONPATH = [] else: PYTHONPATH = PYTHONPATH.split(os.pathsep) pic = getattr(sys,'path_importer_cache',{}) stdpath = sys.path[len(PYTHONPATH):] mydir = os.path.dirname(__file__) #print "searching",stdpath,sys.path for item in stdpath: if item==mydir or not item: continue # skip if current dir. on Windows, or my own directory importer = pic.get(item) if importer is not None: loader = importer.find_module('site') if loader is not None: # This should actually reload the current module loader.load_module('site') break else: try: stream, path, descr = imp.find_module('site',[item]) except ImportError: continue if stream is None: continue try: # This should actually reload the current module imp.load_module('site',stream,path,descr) finally: stream.close() break else: raise ImportError("Couldn't find the real 'site' module") #print "loaded", __file__ known_paths = dict([(makepath(item)[1],1) for item in sys.path]) # 2.2 comp oldpos = getattr(sys,'__egginsert',0) # save old insertion position sys.__egginsert = 0 # and reset the current one for item in PYTHONPATH: addsitedir(item) sys.__egginsert += oldpos # restore effective old position d,nd = makepath(stdpath[0]) insert_at = None new_path = [] for item in sys.path: p,np = makepath(item) if np==nd and insert_at is None: # We've hit the first 'system' path entry, so added entries go here insert_at = len(new_path) if np in known_paths or insert_at is None: new_path.append(item) else: # new path after the insert point, back-insert it new_path.insert(insert_at, item) insert_at += 1 sys.path[:] = new_path if __name__=='site': __boot() del __boot
from typing import Any from django.db import ProgrammingError from confirmation.models import generate_realm_creation_url from zerver.lib.management import ZulipBaseCommand, CommandError from zerver.models import Realm class Command(ZulipBaseCommand): help = """ Outputs a randomly generated, 1-time-use link for Organization creation. Whoever visits the link can create a new organization on this server, regardless of whether settings.OPEN_REALM_CREATION is enabled. The link would expire automatically after settings.REALM_CREATION_LINK_VALIDITY_DAYS. Usage: ./manage.py generate_realm_creation_link """ def handle(self, *args: Any, **options: Any) -> None: try: # first check if the db has been initalized Realm.objects.first() except ProgrammingError: raise CommandError("The Zulip database does not appear to exist. " "Have you run initialize-database?") url = generate_realm_creation_url(by_admin=True) self.stdout.write(self.style.SUCCESS("Please visit the following " "secure single-use link to register your ")) self.stdout.write(self.style.SUCCESS("new Zulip organization:\033[0m")) self.stdout.write("") self.stdout.write(self.style.SUCCESS(" \033[1;92m%s\033[0m" % (url,))) self.stdout.write("")
# -*- coding: utf-8 *-* import pyglet from . import util, resources # update and collision helper functions def process_sprite_group(group, dt): """ calls update for the whole group and removes after who returns True """ for item in set(group): try: if item.update(dt): # remove expired items group.remove(item) item.delete() except AttributeError: try: group.remove(item) except KeyError: pass continue def group_collide(group, other_object): """ Check collision between a group and another object returns how many object collided removes the collided object in the group and calls method "destroy" in them """ collided = set() for item in set(group): try: if item.collide(other_object): collided.add(item.destroy()) group.remove(item) item.delete() # free batch except AttributeError: continue # remove collide objects from group return collided def group_group_collide(group1, group2): """ For each item in group1 calls group collide if a collision happened destroy the item """ collided = set() for item in set(group1): c = group_collide(group2, item) if len(c) > 0: # do not destroy collided.update(c) try: group1.remove(item) item.delete() # free batch except AttributeError: continue return collided class MovingSprite(pyglet.sprite.Sprite): """A sprite with physical properties such as velocity, and angle velocity""" def __init__(self, rotation=0, vel=(0,0), rotation_speed=0, screensize=(800, 600), *args, **kwargs): super().__init__(*args, **kwargs) self.screensize = screensize # Velocity self.vel = list(vel) # Angle (pyglet uses negative degrees) self.rotation = rotation self.rotation_speed = rotation_speed self.should_delete = False def update(self, dt): if self.should_delete: self.delete() return True # rotate object self.rotation += self.rotation_speed * dt # Update position according to velocity and time self.x = (self.x + self.vel[0] * dt) % (self.screensize[0] + self.width) self.y = (self.y + self.vel[1] * dt) % (self.screensize[1] + self.height) # update methods could be checked for expiring return False class ScaledMovingSprite(MovingSprite): """A Fullscreen Moving sprite""" def __init__(self, radius=None, lifespan=float("inf"), *args, **kwargs): """ Interesting super() params: rotation=0, vel=(0,0), rotation_speed=0, screensize=(800, 600) """ super().__init__(*args, **kwargs) self.scale = self.screensize[0] / self.image.width class PhysicalObject(MovingSprite): """A Moving sprite with collision and expiring""" def __init__(self, radius=None, lifespan="inf", *args, **kwargs): """ Interesting super() params: rotation=0, vel=(0,0), rotation_speed=0, screensize=(800, 600) """ super().__init__(*args, **kwargs) # collision radius if radius: self.radius = radius else: self.radius = (max(self.width, self.height) / 2) * self.scale # track how much it should last before disappearing self.lifespan = float(lifespan) self.age = float(0) def update(self, dt): self.age += dt # age the object return super().update(dt) or (self.age > self.lifespan) # update could be checked for expiring def collide(self, other_object): """Determine if this object collides with another""" # Calculate distance between object centers that would be a collision, # assuming circular images collision_distance = self.radius * self.scale + other_object.radius * other_object.scale # Get distance using position tuples actual_distance = util.distance(self.position, other_object.position) # update methods could be checked for expiring return (actual_distance <= collision_distance) def destroy(self): pos = list(self.position) vel = (self.vel[0]/2, self.vel[1] / 2) explosion = MovingSprite(img=resources.explosion_animation, vel=vel, screensize=self.screensize, x=pos[0], y=pos[1], batch=self.batch, group=self.group) # monkey patching done well @explosion.event def on_animation_end(): explosion.visible = False explosion.should_delete = True resources.explosion_sound.play() return explosion
from Foundation import * from PyObjCTools.TestSupport import * try: unicode except NameError: unicode = str class PythonListAsValue (TestCase): def testSettingPythonList(self): defaults = NSUserDefaults.standardUserDefaults() defaults.setObject_forKey_([b'a'.decode('ascii'), b'b'.decode('ascii'), b'c'.decode('ascii')], b'randomKey'.decode('ascii')) self.assertEqual(defaults.arrayForKey_(b'randomKey'.decode('ascii')), [b'a'.decode('ascii'), b'b'.decode('ascii'), b'c'.decode('ascii')]) def testMethods(self): self.assertResultIsBOOL(NSUserDefaults.boolForKey_) self.assertArgIsBOOL(NSUserDefaults.setBool_forKey_, 0) self.assertResultIsBOOL(NSUserDefaults.synchronize) self.assertResultIsBOOL(NSUserDefaults.objectIsForcedForKey_) self.assertResultIsBOOL(NSUserDefaults.objectIsForcedForKey_inDomain_) def testConstants(self): self.assertIsInstance(NSGlobalDomain, unicode) self.assertIsInstance(NSArgumentDomain, unicode) self.assertIsInstance(NSRegistrationDomain, unicode) self.assertIsInstance(NSUserDefaultsDidChangeNotification, unicode) self.assertIsInstance(NSWeekDayNameArray, unicode) self.assertIsInstance(NSShortWeekDayNameArray, unicode) self.assertIsInstance(NSMonthNameArray, unicode) self.assertIsInstance(NSShortMonthNameArray, unicode) self.assertIsInstance(NSTimeFormatString, unicode) self.assertIsInstance(NSDateFormatString, unicode) self.assertIsInstance(NSTimeDateFormatString, unicode) self.assertIsInstance(NSShortTimeDateFormatString, unicode) self.assertIsInstance(NSCurrencySymbol, unicode) self.assertIsInstance(NSDecimalSeparator, unicode) self.assertIsInstance(NSThousandsSeparator, unicode) self.assertIsInstance(NSDecimalDigits, unicode) self.assertIsInstance(NSAMPMDesignation, unicode) self.assertIsInstance(NSHourNameDesignations, unicode) self.assertIsInstance(NSYearMonthWeekDesignations, unicode) self.assertIsInstance(NSEarlierTimeDesignations, unicode) self.assertIsInstance(NSLaterTimeDesignations, unicode) self.assertIsInstance(NSThisDayDesignations, unicode) self.assertIsInstance(NSNextDayDesignations, unicode) self.assertIsInstance(NSNextNextDayDesignations, unicode) self.assertIsInstance(NSPriorDayDesignations, unicode) self.assertIsInstance(NSDateTimeOrdering, unicode) self.assertIsInstance(NSInternationalCurrencyString, unicode) self.assertIsInstance(NSShortDateFormatString, unicode) self.assertIsInstance(NSPositiveCurrencyFormatString, unicode) self.assertIsInstance(NSNegativeCurrencyFormatString, unicode) if __name__ == "__main__": main()
""" Discrete Fourier Transform (:mod:`numpy.fft`) ============================================= .. currentmodule:: numpy.fft Standard FFTs ------------- .. autosummary:: :toctree: generated/ fft Discrete Fourier transform. ifft Inverse discrete Fourier transform. fft2 Discrete Fourier transform in two dimensions. ifft2 Inverse discrete Fourier transform in two dimensions. fftn Discrete Fourier transform in N-dimensions. ifftn Inverse discrete Fourier transform in N dimensions. Real FFTs --------- .. autosummary:: :toctree: generated/ rfft Real discrete Fourier transform. irfft Inverse real discrete Fourier transform. rfft2 Real discrete Fourier transform in two dimensions. irfft2 Inverse real discrete Fourier transform in two dimensions. rfftn Real discrete Fourier transform in N dimensions. irfftn Inverse real discrete Fourier transform in N dimensions. Hermitian FFTs -------------- .. autosummary:: :toctree: generated/ hfft Hermitian discrete Fourier transform. ihfft Inverse Hermitian discrete Fourier transform. Helper routines --------------- .. autosummary:: :toctree: generated/ fftfreq Discrete Fourier Transform sample frequencies. rfftfreq DFT sample frequencies (for usage with rfft, irfft). fftshift Shift zero-frequency component to center of spectrum. ifftshift Inverse of fftshift. Background information ---------------------- Fourier analysis is fundamentally a method for expressing a function as a sum of periodic components, and for recovering the function from those components. When both the function and its Fourier transform are replaced with discretized counterparts, it is called the discrete Fourier transform (DFT). The DFT has become a mainstay of numerical computing in part because of a very fast algorithm for computing it, called the Fast Fourier Transform (FFT), which was known to Gauss (1805) and was brought to light in its current form by Cooley and Tukey [CT]_. Press et al. [NR]_ provide an accessible introduction to Fourier analysis and its applications. Because the discrete Fourier transform separates its input into components that contribute at discrete frequencies, it has a great number of applications in digital signal processing, e.g., for filtering, and in this context the discretized input to the transform is customarily referred to as a *signal*, which exists in the *time domain*. The output is called a *spectrum* or *transform* and exists in the *frequency domain*. Implementation details ---------------------- There are many ways to define the DFT, varying in the sign of the exponent, normalization, etc. In this implementation, the DFT is defined as .. math:: A_k = \\sum_{m=0}^{n-1} a_m \\exp\\left\\{-2\\pi i{mk \\over n}\\right\\} \\qquad k = 0,\\ldots,n-1. The DFT is in general defined for complex inputs and outputs, and a single-frequency component at linear frequency :math:`f` is represented by a complex exponential :math:`a_m = \\exp\\{2\\pi i\\,f m\\Delta t\\}`, where :math:`\\Delta t` is the sampling interval. The values in the result follow so-called "standard" order: If ``A = fft(a, n)``, then ``A[0]`` contains the zero-frequency term (the mean of the signal), which is always purely real for real inputs. Then ``A[1:n/2]`` contains the positive-frequency terms, and ``A[n/2+1:]`` contains the negative-frequency terms, in order of decreasingly negative frequency. For an even number of input points, ``A[n/2]`` represents both positive and negative Nyquist frequency, and is also purely real for real input. For an odd number of input points, ``A[(n-1)/2]`` contains the largest positive frequency, while ``A[(n+1)/2]`` contains the largest negative frequency. The routine ``np.fft.fftfreq(n)`` returns an array giving the frequencies of corresponding elements in the output. The routine ``np.fft.fftshift(A)`` shifts transforms and their frequencies to put the zero-frequency components in the middle, and ``np.fft.ifftshift(A)`` undoes that shift. When the input `a` is a time-domain signal and ``A = fft(a)``, ``np.abs(A)`` is its amplitude spectrum and ``np.abs(A)**2`` is its power spectrum. The phase spectrum is obtained by ``np.angle(A)``. The inverse DFT is defined as .. math:: a_m = \\frac{1}{n}\\sum_{k=0}^{n-1}A_k\\exp\\left\\{2\\pi i{mk\\over n}\\right\\} \\qquad m = 0,\\ldots,n-1. It differs from the forward transform by the sign of the exponential argument and the default normalization by :math:`1/n`. Normalization ------------- The default normalization has the direct transforms unscaled and the inverse transforms are scaled by :math:`1/n`. It is possible to obtain unitary transforms by setting the keyword argument ``norm`` to ``"ortho"`` (default is `None`) so that both direct and inverse transforms will be scaled by :math:`1/\\sqrt{n}`. Real and Hermitian transforms ----------------------------- When the input is purely real, its transform is Hermitian, i.e., the component at frequency :math:`f_k` is the complex conjugate of the component at frequency :math:`-f_k`, which means that for real inputs there is no information in the negative frequency components that is not already available from the positive frequency components. The family of `rfft` functions is designed to operate on real inputs, and exploits this symmetry by computing only the positive frequency components, up to and including the Nyquist frequency. Thus, ``n`` input points produce ``n/2+1`` complex output points. The inverses of this family assumes the same symmetry of its input, and for an output of ``n`` points uses ``n/2+1`` input points. Correspondingly, when the spectrum is purely real, the signal is Hermitian. The `hfft` family of functions exploits this symmetry by using ``n/2+1`` complex points in the input (time) domain for ``n`` real points in the frequency domain. In higher dimensions, FFTs are used, e.g., for image analysis and filtering. The computational efficiency of the FFT means that it can also be a faster way to compute large convolutions, using the property that a convolution in the time domain is equivalent to a point-by-point multiplication in the frequency domain. Higher dimensions ----------------- In two dimensions, the DFT is defined as .. math:: A_{kl} = \\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} a_{mn}\\exp\\left\\{-2\\pi i \\left({mk\\over M}+{nl\\over N}\\right)\\right\\} \\qquad k = 0, \\ldots, M-1;\\quad l = 0, \\ldots, N-1, which extends in the obvious way to higher dimensions, and the inverses in higher dimensions also extend in the same way. References ---------- .. [CT] Cooley, James W., and John W. Tukey, 1965, "An algorithm for the machine calculation of complex Fourier series," *Math. Comput.* 19: 297-301. .. [NR] Press, W., Teukolsky, S., Vetterline, W.T., and Flannery, B.P., 2007, *Numerical Recipes: The Art of Scientific Computing*, ch. 12-13. Cambridge Univ. Press, Cambridge, UK. Examples -------- For examples, see the various functions. """ from __future__ import division, absolute_import, print_function depends = ['core']
import inspect import logging from gloria.service.runnable import Task, Service on_task_wrapped = None on_service_wrapped = None class _WrapperHelper(object): def __call__(self, klass, base, _enabled, _autostart, _respawn): return type(klass.__name__, (base,), dict(klass.__dict__, enabled=_enabled, autostart=_autostart, respawn=_respawn)) def task(enabled=True, autostart=False, respawn=False): """Mark class as a task""" def _task(klass): class Wrapper(object): wrapped_task = _WrapperHelper()(klass, Task, enabled, autostart, respawn) # Notify whoever is interested in the task being wrapped if on_task_wrapped is not None: on_task_wrapped(Wrapper.wrapped_task) return Wrapper return _task class _ServiceWrapperHelper: def __call__(self, klass, service_dir=''): if klass == Service: return type(service_dir, (object,), dict(klass.__dict__)) return type(klass.__name__, (Service,), dict(klass.__dict__)) def service(tasks=[]): """Mark class as a service""" ##################################### # TODO: add autostart=False parameter ##################################### def _service(klass, service_dir=''): class Wrapper: wrapped_class = _ServiceWrapperHelper()(klass, service_dir) wrapped_tasks = tasks # Notify whoever is interested in the class being wrapped if on_service_wrapped is not None: on_service_wrapped(Wrapper) return Wrapper return _service class Property(object): """Expose task's property to the outside world""" def __init__(self, fget=None, fset=None, doc=None): self.fget = fget self.fset = fset if doc is None and fget is not None: doc = fget.__doc__ self.__doc__ = doc def __get__(self, obj, objtype=None): if obj is None: return self if self.fget is None: raise AttributeError('unreadable attribute') return self.fget(obj) def __set__(self, obj, value): if self.fset is None: raise AttributeError('cannot set attribute') self.fset(obj, value) def __str__(self): return '{0}:{1}'.format(self.fget.__name__, self.__doc__) def getter(self, fget): return type(self)(fget, self.fset, self.__doc__) def setter(self, fset): return type(self)(self.fget, fset, self.__doc__) class Command(object): """Expose task's command to the outside world""" def __init__(self, func=None): self.func = func # Number of arguments this function takes ('self' is counted as well) self.argc = len(inspect.getargspec(func)[0]) logging.debug('Command __init__: func={0} argc={1}'.format(func.__name__, self.argc)) def __call__(self, obj, args=''): if obj is None: return self if self.func is None: raise AttributeError('uncallable method') if self.argc == 1: return self.func(obj) return self.func(obj, args) def __str__(self): return '{0}:{1}'.format(self.func.__name__, self.func.__doc__)
#! /usr/bin/env python # -*- coding: utf-8 -*- # =========================================================================== # Copyright (c) 2007-2012 Barend Gehrels, Amsterdam, the Netherlands. # Copyright (c) 2008-2012 Bruno Lalande, Paris, France. # Copyright (c) 2009-2012 Mateusz Loskot (mateusz@loskot.net), London, UK # # Use, modification and distribution is subject to the Boost Software License, # Version 1.0. (See accompanying file LICENSE_1_0.txt or copy at # http://www.boost.org/LICENSE_1_0.txt) # ============================================================================ import os, sys script_dir = os.path.dirname(__file__) os.chdir(os.path.abspath(script_dir)) print("Boost.Geometry is making .qbk files in %s" % os.getcwd()) if 'DOXYGEN' in os.environ: doxygen_cmd = os.environ['DOXYGEN'] else: doxygen_cmd = 'doxygen' if 'DOXYGEN_XML2QBK' in os.environ: doxygen_xml2qbk_cmd = os.environ['DOXYGEN_XML2QBK'] elif '--doxygen-xml2qbk' in sys.argv: doxygen_xml2qbk_cmd = sys.argv[sys.argv.index('--doxygen-xml2qbk')+1] else: doxygen_xml2qbk_cmd = 'doxygen_xml2qbk' os.environ['PATH'] = os.environ['PATH']+os.pathsep+os.path.dirname(doxygen_xml2qbk_cmd) doxygen_xml2qbk_cmd = os.path.basename(doxygen_xml2qbk_cmd) cmd = doxygen_xml2qbk_cmd cmd = cmd + " --xml doxy/doxygen_output/xml/%s.xml" cmd = cmd + " --start_include boost/geometry/" cmd = cmd + " --convenience_header_path ../../../boost/geometry/" cmd = cmd + " --convenience_headers geometry.hpp,geometries/geometries.hpp" cmd = cmd + " --skip_namespace boost::geometry::" cmd = cmd + " --copyright src/copyright_block.qbk" cmd = cmd + " --output_member_variables false" cmd = cmd + " > generated/%s.qbk" def run_command(command): if os.system(command) != 0: raise Exception("Error running %s" % command) def remove_all_files(dir): if os.path.exists(dir): for f in os.listdir(dir): os.remove(dir+f) def call_doxygen(): os.chdir("doxy") remove_all_files("doxygen_output/xml/") run_command(doxygen_cmd) os.chdir("..") def group_to_quickbook(section): run_command(cmd % ("group__" + section.replace("_", "__"), section)) def model_to_quickbook(section): run_command(cmd % ("classboost_1_1geometry_1_1model_1_1" + section.replace("_", "__"), section)) def model_to_quickbook2(classname, section): run_command(cmd % ("classboost_1_1geometry_1_1model_1_1" + classname, section)) def struct_to_quickbook(section): run_command(cmd % ("structboost_1_1geometry_1_1" + section.replace("_", "__"), section)) def class_to_quickbook(section): run_command(cmd % ("classboost_1_1geometry_1_1" + section.replace("_", "__"), section)) def class_to_quickbook2(classname, section): run_command(cmd % ("classboost_1_1geometry_1_1" + classname, section)) def strategy_to_quickbook(section): p = section.find("::") ns = section[:p] strategy = section[p+2:] run_command(cmd % ("classboost_1_1geometry_1_1strategy_1_1" + ns.replace("_", "__") + "_1_1" + strategy.replace("_", "__"), ns + "_" + strategy)) def cs_to_quickbook(section): run_command(cmd % ("structboost_1_1geometry_1_1cs_1_1" + section.replace("_", "__"), section)) call_doxygen() algorithms = ["append", "assign", "make", "clear" , "area", "buffer", "centroid", "convert", "correct", "covered_by" , "convex_hull", "crosses", "difference", "disjoint", "distance" , "envelope", "equals", "expand", "for_each", "is_empty" , "is_simple", "is_valid", "intersection", "intersects", "length" , "num_geometries", "num_interior_rings", "num_points" , "num_segments", "overlaps", "perimeter", "relate", "relation" , "reverse", "simplify", "sym_difference", "touches" , "transform", "union", "unique", "within"] access_functions = ["get", "set", "exterior_ring", "interior_rings" , "num_points", "num_interior_rings", "num_geometries"] coordinate_systems = ["cartesian", "geographic", "polar", "spherical", "spherical_equatorial"] core = ["closure", "coordinate_system", "coordinate_type", "cs_tag" , "dimension", "exception", "interior_type" , "degree", "radian" , "is_radian", "point_order" , "point_type", "ring_type", "tag", "tag_cast" ] exceptions = ["exception", "centroid_exception"]; iterators = ["circular_iterator", "closing_iterator" , "ever_circling_iterator"] models = ["point", "linestring", "box" , "polygon", "segment", "ring" , "multi_linestring", "multi_point", "multi_polygon", "referring_segment"] strategies = ["distance::pythagoras", "distance::pythagoras_box_box" , "distance::pythagoras_point_box", "distance::haversine" , "distance::cross_track", "distance::cross_track_point_box" , "distance::projected_point" , "within::winding", "within::franklin", "within::crossings_multiply" , "area::surveyor", "area::huiller" , "buffer::point_circle", "buffer::point_square" , "buffer::join_round", "buffer::join_miter" , "buffer::end_round", "buffer::end_flat" , "buffer::distance_symmetric", "buffer::distance_asymmetric" , "buffer::side_straight" , "centroid::bashein_detmer", "centroid::average" , "convex_hull::graham_andrew" , "simplify::douglas_peucker" , "side::side_by_triangle", "side::side_by_cross_track", "side::spherical_side_formula" , "transform::inverse_transformer", "transform::map_transformer" , "transform::rotate_transformer", "transform::scale_transformer" , "transform::translate_transformer", "transform::ublas_transformer" ] views = ["box_view", "segment_view" , "closeable_view", "reversible_view", "identity_view"] for i in algorithms: group_to_quickbook(i) for i in access_functions: group_to_quickbook(i) for i in coordinate_systems: cs_to_quickbook(i) for i in core: struct_to_quickbook(i) for i in exceptions: class_to_quickbook(i) for i in iterators: struct_to_quickbook(i) for i in models: model_to_quickbook(i) for i in strategies: strategy_to_quickbook(i) for i in views: struct_to_quickbook(i) model_to_quickbook2("d2_1_1point__xy", "point_xy") group_to_quickbook("arithmetic") group_to_quickbook("enum") group_to_quickbook("register") group_to_quickbook("svg") class_to_quickbook("svg_mapper") group_to_quickbook("wkt") class_to_quickbook2("de9im_1_1matrix", "de9im_matrix") class_to_quickbook2("de9im_1_1mask", "de9im_mask") class_to_quickbook2("de9im_1_1static__mask", "de9im_static_mask") os.chdir("index") execfile("make_qbk.py") os.chdir("..") # Use either bjam or b2 or ../../../b2 (the last should be done on Release branch) if "--release-build" not in sys.argv: run_command("b2")
# -*- coding: utf-8 -*- ############################################################################## # # OpenERP, Open Source Management Solution # Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>). # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## from openerp.osv import fields,osv from openerp.tools.sql import drop_view_if_exists class report_timesheet_line(osv.osv): _name = "report.timesheet.line" _description = "Timesheet Line" _auto = False _columns = { 'name': fields.char('Year', required=False, readonly=True), 'user_id': fields.many2one('res.users', 'User', readonly=True), 'date': fields.date('Date', readonly=True), 'day': fields.char('Day', size=128, readonly=True), 'quantity': fields.float('Time', readonly=True), 'cost': fields.float('Cost', readonly=True), 'product_id': fields.many2one('product.product', 'Product',readonly=True), 'account_id': fields.many2one('account.analytic.account', 'Analytic Account', readonly=True), 'general_account_id': fields.many2one('account.account', 'Financial Account', readonly=True), 'invoice_id': fields.many2one('account.invoice', 'Invoiced', readonly=True), 'month': fields.selection([('01','January'), ('02','February'), ('03','March'), ('04','April'), ('05','May'), ('06','June'), ('07','July'), ('08','August'), ('09','September'), ('10','October'), ('11','November'), ('12','December')],'Month', readonly=True), } _order = 'name desc,user_id desc' def init(self, cr): drop_view_if_exists(cr, 'report_timesheet_line') cr.execute(""" create or replace view report_timesheet_line as ( select min(l.id) as id, l.date as date, to_char(l.date,'YYYY') as name, to_char(l.date,'MM') as month, l.user_id, to_char(l.date, 'YYYY-MM-DD') as day, l.invoice_id, l.product_id, l.account_id, l.general_account_id, sum(l.unit_amount) as quantity, sum(l.amount) as cost from account_analytic_line l where l.user_id is not null group by l.date, l.user_id, l.product_id, l.account_id, l.general_account_id, l.invoice_id ) """) class report_timesheet_user(osv.osv): _name = "report_timesheet.user" _description = "Timesheet per day" _auto = False _columns = { 'name': fields.char('Year', required=False, readonly=True), 'user_id':fields.many2one('res.users', 'User', readonly=True), 'quantity': fields.float('Time', readonly=True), 'cost': fields.float('Cost', readonly=True), 'month':fields.selection([('01','January'), ('02','February'), ('03','March'), ('04','April'), ('05','May'), ('06','June'), ('07','July'), ('08','August'), ('09','September'), ('10','October'), ('11','November'), ('12','December')],'Month', readonly=True), } _order = 'name desc,user_id desc' def init(self, cr): drop_view_if_exists(cr, 'report_timesheet_user') cr.execute(""" create or replace view report_timesheet_user as ( select min(l.id) as id, to_char(l.date,'YYYY') as name, to_char(l.date,'MM') as month, l.user_id, sum(l.unit_amount) as quantity, sum(l.amount) as cost from account_analytic_line l where user_id is not null group by l.date, to_char(l.date,'YYYY'),to_char(l.date,'MM'), l.user_id ) """) class report_timesheet_account(osv.osv): _name = "report_timesheet.account" _description = "Timesheet per account" _auto = False _columns = { 'name': fields.char('Year', required=False, readonly=True), 'user_id':fields.many2one('res.users', 'User', readonly=True), 'account_id':fields.many2one('account.analytic.account', 'Analytic Account', readonly=True), 'quantity': fields.float('Time', readonly=True), 'month':fields.selection([('01','January'), ('02','February'), ('03','March'), ('04','April'), ('05','May'), ('06','June'), ('07','July'), ('08','August'), ('09','September'), ('10','October'), ('11','November'), ('12','December')],'Month', readonly=True), } _order = 'name desc,account_id desc,user_id desc' def init(self, cr): drop_view_if_exists(cr, 'report_timesheet_account') cr.execute(""" create or replace view report_timesheet_account as ( select min(id) as id, to_char(create_date, 'YYYY') as name, to_char(create_date,'MM') as month, user_id, account_id, sum(unit_amount) as quantity from account_analytic_line group by to_char(create_date, 'YYYY'),to_char(create_date, 'MM'), user_id, account_id ) """) class report_timesheet_account_date(osv.osv): _name = "report_timesheet.account.date" _description = "Daily timesheet per account" _auto = False _columns = { 'name': fields.char('Year', required=False, readonly=True), 'user_id':fields.many2one('res.users', 'User', readonly=True), 'account_id':fields.many2one('account.analytic.account', 'Analytic Account', readonly=True), 'quantity': fields.float('Time', readonly=True), 'month':fields.selection([('01','January'), ('02','February'), ('03','March'), ('04','April'), ('05','May'), ('06','June'), ('07','July'), ('08','August'), ('09','September'), ('10','October'), ('11','November'), ('12','December')],'Month', readonly=True), } _order = 'name desc,account_id desc,user_id desc' def init(self, cr): drop_view_if_exists(cr, 'report_timesheet_account_date') cr.execute(""" create or replace view report_timesheet_account_date as ( select min(id) as id, to_char(date,'YYYY') as name, to_char(date,'MM') as month, user_id, account_id, sum(unit_amount) as quantity from account_analytic_line group by to_char(date,'YYYY'),to_char(date,'MM'), user_id, account_id ) """) class report_timesheet_invoice(osv.osv): _name = "report_timesheet.invoice" _description = "Costs to invoice" _auto = False _columns = { 'user_id':fields.many2one('res.users', 'User', readonly=True), 'account_id':fields.many2one('account.analytic.account', 'Project', readonly=True), 'manager_id':fields.many2one('res.users', 'Manager', readonly=True), 'quantity': fields.float('Time', readonly=True), 'amount_invoice': fields.float('To invoice', readonly=True) } _rec_name = 'user_id' _order = 'user_id desc' def init(self, cr): drop_view_if_exists(cr, 'report_timesheet_invoice') cr.execute(""" create or replace view report_timesheet_invoice as ( select min(l.id) as id, l.user_id as user_id, l.account_id as account_id, a.user_id as manager_id, sum(l.unit_amount) as quantity, sum(l.unit_amount * t.list_price) as amount_invoice from account_analytic_line l left join hr_timesheet_invoice_factor f on (l.to_invoice=f.id) left join account_analytic_account a on (l.account_id=a.id) left join product_product p on (l.to_invoice=f.id) left join product_template t on (l.to_invoice=f.id) where l.to_invoice is not null and l.invoice_id is null group by l.user_id, l.account_id, a.user_id ) """) # vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
# -*- test-case-name: twisted.mail.test.test_mailmail -*- # Copyright (c) Twisted Matrix Laboratories. # See LICENSE for details. """ Implementation module for the I{mailmail} command. """ import os import sys import rfc822 import getpass from ConfigParser import ConfigParser try: import cStringIO as StringIO except: import StringIO from twisted.copyright import version from twisted.internet import reactor from twisted.mail import smtp GLOBAL_CFG = "/etc/mailmail" LOCAL_CFG = os.path.expanduser("~/.twisted/mailmail") SMARTHOST = '127.0.0.1' ERROR_FMT = """\ Subject: Failed Message Delivery Message delivery failed. The following occurred: %s -- The Twisted sendmail application. """ def log(message, *args): sys.stderr.write(str(message) % args + '\n') class Options: """ @type to: C{list} of C{str} @ivar to: The addresses to which to deliver this message. @type sender: C{str} @ivar sender: The address from which this message is being sent. @type body: C{file} @ivar body: The object from which the message is to be read. """ def getlogin(): try: return os.getlogin() except: return getpass.getuser() _unsupportedOption = SystemExit("Unsupported option.") def parseOptions(argv): o = Options() o.to = [e for e in argv if not e.startswith('-')] o.sender = getlogin() # Just be very stupid # Skip -bm -- it is the default # Add a non-standard option for querying the version of this tool. if '--version' in argv: print 'mailmail version:', version raise SystemExit() # -bp lists queue information. Screw that. if '-bp' in argv: raise _unsupportedOption # -bs makes sendmail use stdin/stdout as its transport. Screw that. if '-bs' in argv: raise _unsupportedOption # -F sets who the mail is from, but is overridable by the From header if '-F' in argv: o.sender = argv[argv.index('-F') + 1] o.to.remove(o.sender) # -i and -oi makes us ignore lone "." if ('-i' in argv) or ('-oi' in argv): raise _unsupportedOption # -odb is background delivery if '-odb' in argv: o.background = True else: o.background = False # -odf is foreground delivery if '-odf' in argv: o.background = False else: o.background = True # -oem and -em cause errors to be mailed back to the sender. # It is also the default. # -oep and -ep cause errors to be printed to stderr if ('-oep' in argv) or ('-ep' in argv): o.printErrors = True else: o.printErrors = False # -om causes a copy of the message to be sent to the sender if the sender # appears in an alias expansion. We do not support aliases. if '-om' in argv: raise _unsupportedOption # -t causes us to pick the recipients of the message from the To, Cc, and Bcc # headers, and to remove the Bcc header if present. if '-t' in argv: o.recipientsFromHeaders = True o.excludeAddresses = o.to o.to = [] else: o.recipientsFromHeaders = False o.exludeAddresses = [] requiredHeaders = { 'from': [], 'to': [], 'cc': [], 'bcc': [], 'date': [], } headers = [] buffer = StringIO.StringIO() while 1: write = 1 line = sys.stdin.readline() if not line.strip(): break hdrs = line.split(': ', 1) hdr = hdrs[0].lower() if o.recipientsFromHeaders and hdr in ('to', 'cc', 'bcc'): o.to.extend([ a[1] for a in rfc822.AddressList(hdrs[1]).addresslist ]) if hdr == 'bcc': write = 0 elif hdr == 'from': o.sender = rfc822.parseaddr(hdrs[1])[1] if hdr in requiredHeaders: requiredHeaders[hdr].append(hdrs[1]) if write: buffer.write(line) if not requiredHeaders['from']: buffer.write('From: %s\r\n' % (o.sender,)) if not requiredHeaders['to']: if not o.to: raise SystemExit("No recipients specified.") buffer.write('To: %s\r\n' % (', '.join(o.to),)) if not requiredHeaders['date']: buffer.write('Date: %s\r\n' % (smtp.rfc822date(),)) buffer.write(line) if o.recipientsFromHeaders: for a in o.excludeAddresses: try: o.to.remove(a) except: pass buffer.seek(0, 0) o.body = StringIO.StringIO(buffer.getvalue() + sys.stdin.read()) return o class Configuration: """ @ivar allowUIDs: A list of UIDs which are allowed to send mail. @ivar allowGIDs: A list of GIDs which are allowed to send mail. @ivar denyUIDs: A list of UIDs which are not allowed to send mail. @ivar denyGIDs: A list of GIDs which are not allowed to send mail. @type defaultAccess: C{bool} @ivar defaultAccess: C{True} if access will be allowed when no other access control rule matches or C{False} if it will be denied in that case. @ivar useraccess: Either C{'allow'} to check C{allowUID} first or C{'deny'} to check C{denyUID} first. @ivar groupaccess: Either C{'allow'} to check C{allowGID} first or C{'deny'} to check C{denyGID} first. @ivar identities: A C{dict} mapping hostnames to credentials to use when sending mail to that host. @ivar smarthost: C{None} or a hostname through which all outgoing mail will be sent. @ivar domain: C{None} or the hostname with which to identify ourselves when connecting to an MTA. """ def __init__(self): self.allowUIDs = [] self.denyUIDs = [] self.allowGIDs = [] self.denyGIDs = [] self.useraccess = 'deny' self.groupaccess= 'deny' self.identities = {} self.smarthost = None self.domain = None self.defaultAccess = True def loadConfig(path): # [useraccess] # allow=uid1,uid2,... # deny=uid1,uid2,... # order=allow,deny # [groupaccess] # allow=gid1,gid2,... # deny=gid1,gid2,... # order=deny,allow # [identity] # host1=username:password # host2=username:password # [addresses] # smarthost=a.b.c.d # default_domain=x.y.z c = Configuration() if not os.access(path, os.R_OK): return c p = ConfigParser() p.read(path) au = c.allowUIDs du = c.denyUIDs ag = c.allowGIDs dg = c.denyGIDs for (section, a, d) in (('useraccess', au, du), ('groupaccess', ag, dg)): if p.has_section(section): for (mode, L) in (('allow', a), ('deny', d)): if p.has_option(section, mode) and p.get(section, mode): for id in p.get(section, mode).split(','): try: id = int(id) except ValueError: log("Illegal %sID in [%s] section: %s", section[0].upper(), section, id) else: L.append(id) order = p.get(section, 'order') order = map(str.split, map(str.lower, order.split(','))) if order[0] == 'allow': setattr(c, section, 'allow') else: setattr(c, section, 'deny') if p.has_section('identity'): for (host, up) in p.items('identity'): parts = up.split(':', 1) if len(parts) != 2: log("Illegal entry in [identity] section: %s", up) continue p.identities[host] = parts if p.has_section('addresses'): if p.has_option('addresses', 'smarthost'): c.smarthost = p.get('addresses', 'smarthost') if p.has_option('addresses', 'default_domain'): c.domain = p.get('addresses', 'default_domain') return c def success(result): reactor.stop() failed = None def failure(f): global failed reactor.stop() failed = f def sendmail(host, options, ident): d = smtp.sendmail(host, options.sender, options.to, options.body) d.addCallbacks(success, failure) reactor.run() def senderror(failure, options): recipient = [options.sender] sender = '"Internally Generated Message (%s)"<postmaster@%s>' % (sys.argv[0], smtp.DNSNAME) error = StringIO.StringIO() failure.printTraceback(file=error) body = StringIO.StringIO(ERROR_FMT % error.getvalue()) d = smtp.sendmail('localhost', sender, recipient, body) d.addBoth(lambda _: reactor.stop()) def deny(conf): uid = os.getuid() gid = os.getgid() if conf.useraccess == 'deny': if uid in conf.denyUIDs: return True if uid in conf.allowUIDs: return False else: if uid in conf.allowUIDs: return False if uid in conf.denyUIDs: return True if conf.groupaccess == 'deny': if gid in conf.denyGIDs: return True if gid in conf.allowGIDs: return False else: if gid in conf.allowGIDs: return False if gid in conf.denyGIDs: return True return not conf.defaultAccess def run(): o = parseOptions(sys.argv[1:]) gConf = loadConfig(GLOBAL_CFG) lConf = loadConfig(LOCAL_CFG) if deny(gConf) or deny(lConf): log("Permission denied") return host = lConf.smarthost or gConf.smarthost or SMARTHOST ident = gConf.identities.copy() ident.update(lConf.identities) if lConf.domain: smtp.DNSNAME = lConf.domain elif gConf.domain: smtp.DNSNAME = gConf.domain sendmail(host, o, ident) if failed: if o.printErrors: failed.printTraceback(file=sys.stderr) raise SystemExit(1) else: senderror(failed, o)
"""Restricted Boltzmann Machine """ # Authors: Yann N. Dauphin <dauphiya@iro.umontreal.ca> # Vlad Niculae # Gabriel Synnaeve # Lars Buitinck # License: BSD 3 clause import time import numpy as np import scipy.sparse as sp from ..base import BaseEstimator from ..base import TransformerMixin from ..externals.six.moves import xrange from ..utils import check_array from ..utils import check_random_state from ..utils import gen_even_slices from ..utils import issparse from ..utils.extmath import safe_sparse_dot from ..utils.extmath import log_logistic from ..utils.fixes import expit # logistic function from ..utils.validation import check_is_fitted class BernoulliRBM(BaseEstimator, TransformerMixin): """Bernoulli Restricted Boltzmann Machine (RBM). A Restricted Boltzmann Machine with binary visible units and binary hidden units. Parameters are estimated using Stochastic Maximum Likelihood (SML), also known as Persistent Contrastive Divergence (PCD) [2]. The time complexity of this implementation is ``O(d ** 2)`` assuming d ~ n_features ~ n_components. Read more in the :ref:`User Guide <rbm>`. Parameters ---------- n_components : int, optional Number of binary hidden units. learning_rate : float, optional The learning rate for weight updates. It is *highly* recommended to tune this hyper-parameter. Reasonable values are in the 10**[0., -3.] range. batch_size : int, optional Number of examples per minibatch. n_iter : int, optional Number of iterations/sweeps over the training dataset to perform during training. verbose : int, optional The verbosity level. The default, zero, means silent mode. random_state : integer or numpy.RandomState, optional A random number generator instance to define the state of the random permutations generator. If an integer is given, it fixes the seed. Defaults to the global numpy random number generator. Attributes ---------- intercept_hidden_ : array-like, shape (n_components,) Biases of the hidden units. intercept_visible_ : array-like, shape (n_features,) Biases of the visible units. components_ : array-like, shape (n_components, n_features) Weight matrix, where n_features in the number of visible units and n_components is the number of hidden units. Examples -------- >>> import numpy as np >>> from sklearn.neural_network import BernoulliRBM >>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]]) >>> model = BernoulliRBM(n_components=2) >>> model.fit(X) BernoulliRBM(batch_size=10, learning_rate=0.1, n_components=2, n_iter=10, random_state=None, verbose=0) References ---------- [1] Hinton, G. E., Osindero, S. and Teh, Y. A fast learning algorithm for deep belief nets. Neural Computation 18, pp 1527-1554. http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf [2] Tieleman, T. Training Restricted Boltzmann Machines using Approximations to the Likelihood Gradient. International Conference on Machine Learning (ICML) 2008 """ def __init__(self, n_components=256, learning_rate=0.1, batch_size=10, n_iter=10, verbose=0, random_state=None): self.n_components = n_components self.learning_rate = learning_rate self.batch_size = batch_size self.n_iter = n_iter self.verbose = verbose self.random_state = random_state def transform(self, X): """Compute the hidden layer activation probabilities, P(h=1|v=X). Parameters ---------- X : {array-like, sparse matrix} shape (n_samples, n_features) The data to be transformed. Returns ------- h : array, shape (n_samples, n_components) Latent representations of the data. """ check_is_fitted(self, "components_") X = check_array(X, accept_sparse='csr', dtype=np.float64) return self._mean_hiddens(X) def _mean_hiddens(self, v): """Computes the probabilities P(h=1|v). Parameters ---------- v : array-like, shape (n_samples, n_features) Values of the visible layer. Returns ------- h : array-like, shape (n_samples, n_components) Corresponding mean field values for the hidden layer. """ p = safe_sparse_dot(v, self.components_.T) p += self.intercept_hidden_ return expit(p, out=p) def _sample_hiddens(self, v, rng): """Sample from the distribution P(h|v). Parameters ---------- v : array-like, shape (n_samples, n_features) Values of the visible layer to sample from. rng : RandomState Random number generator to use. Returns ------- h : array-like, shape (n_samples, n_components) Values of the hidden layer. """ p = self._mean_hiddens(v) return (rng.random_sample(size=p.shape) < p) def _sample_visibles(self, h, rng): """Sample from the distribution P(v|h). Parameters ---------- h : array-like, shape (n_samples, n_components) Values of the hidden layer to sample from. rng : RandomState Random number generator to use. Returns ------- v : array-like, shape (n_samples, n_features) Values of the visible layer. """ p = np.dot(h, self.components_) p += self.intercept_visible_ expit(p, out=p) return (rng.random_sample(size=p.shape) < p) def _free_energy(self, v): """Computes the free energy F(v) = - log sum_h exp(-E(v,h)). Parameters ---------- v : array-like, shape (n_samples, n_features) Values of the visible layer. Returns ------- free_energy : array-like, shape (n_samples,) The value of the free energy. """ return (- safe_sparse_dot(v, self.intercept_visible_) - np.logaddexp(0, safe_sparse_dot(v, self.components_.T) + self.intercept_hidden_).sum(axis=1)) def gibbs(self, v): """Perform one Gibbs sampling step. Parameters ---------- v : array-like, shape (n_samples, n_features) Values of the visible layer to start from. Returns ------- v_new : array-like, shape (n_samples, n_features) Values of the visible layer after one Gibbs step. """ check_is_fitted(self, "components_") if not hasattr(self, "random_state_"): self.random_state_ = check_random_state(self.random_state) h_ = self._sample_hiddens(v, self.random_state_) v_ = self._sample_visibles(h_, self.random_state_) return v_ def partial_fit(self, X, y=None): """Fit the model to the data X which should contain a partial segment of the data. Parameters ---------- X : array-like, shape (n_samples, n_features) Training data. Returns ------- self : BernoulliRBM The fitted model. """ X = check_array(X, accept_sparse='csr', dtype=np.float64) if not hasattr(self, 'random_state_'): self.random_state_ = check_random_state(self.random_state) if not hasattr(self, 'components_'): self.components_ = np.asarray( self.random_state_.normal( 0, 0.01, (self.n_components, X.shape[1]) ), order='F') if not hasattr(self, 'intercept_hidden_'): self.intercept_hidden_ = np.zeros(self.n_components, ) if not hasattr(self, 'intercept_visible_'): self.intercept_visible_ = np.zeros(X.shape[1], ) if not hasattr(self, 'h_samples_'): self.h_samples_ = np.zeros((self.batch_size, self.n_components)) self._fit(X, self.random_state_) def _fit(self, v_pos, rng): """Inner fit for one mini-batch. Adjust the parameters to maximize the likelihood of v using Stochastic Maximum Likelihood (SML). Parameters ---------- v_pos : array-like, shape (n_samples, n_features) The data to use for training. rng : RandomState Random number generator to use for sampling. """ h_pos = self._mean_hiddens(v_pos) v_neg = self._sample_visibles(self.h_samples_, rng) h_neg = self._mean_hiddens(v_neg) lr = float(self.learning_rate) / v_pos.shape[0] update = safe_sparse_dot(v_pos.T, h_pos, dense_output=True).T update -= np.dot(h_neg.T, v_neg) self.components_ += lr * update self.intercept_hidden_ += lr * (h_pos.sum(axis=0) - h_neg.sum(axis=0)) self.intercept_visible_ += lr * (np.asarray( v_pos.sum(axis=0)).squeeze() - v_neg.sum(axis=0)) h_neg[rng.uniform(size=h_neg.shape) < h_neg] = 1.0 # sample binomial self.h_samples_ = np.floor(h_neg, h_neg) def score_samples(self, X): """Compute the pseudo-likelihood of X. Parameters ---------- X : {array-like, sparse matrix} shape (n_samples, n_features) Values of the visible layer. Must be all-boolean (not checked). Returns ------- pseudo_likelihood : array-like, shape (n_samples,) Value of the pseudo-likelihood (proxy for likelihood). Notes ----- This method is not deterministic: it computes a quantity called the free energy on X, then on a randomly corrupted version of X, and returns the log of the logistic function of the difference. """ check_is_fitted(self, "components_") v = check_array(X, accept_sparse='csr') rng = check_random_state(self.random_state) # Randomly corrupt one feature in each sample in v. ind = (np.arange(v.shape[0]), rng.randint(0, v.shape[1], v.shape[0])) if issparse(v): data = -2 * v[ind] + 1 v_ = v + sp.csr_matrix((data.A.ravel(), ind), shape=v.shape) else: v_ = v.copy() v_[ind] = 1 - v_[ind] fe = self._free_energy(v) fe_ = self._free_energy(v_) return v.shape[1] * log_logistic(fe_ - fe) def fit(self, X, y=None): """Fit the model to the data X. Parameters ---------- X : {array-like, sparse matrix} shape (n_samples, n_features) Training data. Returns ------- self : BernoulliRBM The fitted model. """ X = check_array(X, accept_sparse='csr', dtype=np.float64) n_samples = X.shape[0] rng = check_random_state(self.random_state) self.components_ = np.asarray( rng.normal(0, 0.01, (self.n_components, X.shape[1])), order='F') self.intercept_hidden_ = np.zeros(self.n_components, ) self.intercept_visible_ = np.zeros(X.shape[1], ) self.h_samples_ = np.zeros((self.batch_size, self.n_components)) n_batches = int(np.ceil(float(n_samples) / self.batch_size)) batch_slices = list(gen_even_slices(n_batches * self.batch_size, n_batches, n_samples)) verbose = self.verbose begin = time.time() for iteration in xrange(1, self.n_iter + 1): for batch_slice in batch_slices: self._fit(X[batch_slice], rng) if verbose: end = time.time() print("[%s] Iteration %d, pseudo-likelihood = %.2f," " time = %.2fs" % (type(self).__name__, iteration, self.score_samples(X).mean(), end - begin)) begin = end return self
########################################################################### # (C) Vrije Universiteit, Amsterdam (the Netherlands) # # # # This file is part of AmCAT - The Amsterdam Content Analysis Toolkit # # # # AmCAT is free software: you can redistribute it and/or modify it under # # the terms of the GNU Affero General Public License as published by the # # Free Software Foundation, either version 3 of the License, or (at your # # option) any later version. # # # # AmCAT is distributed in the hope that it will be useful, but WITHOUT # # ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or # # FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public # # License for more details. # # # # You should have received a copy of the GNU Affero General Public # # License along with AmCAT. If not, see <http://www.gnu.org/licenses/>. # ########################################################################### import functools from rest_framework import serializers from amcat.models.task import IN_PROGRESS from amcat.models import Task from amcat.tools import amcattest from api.rest.serializer import AmCATModelSerializer __all__ = ("TaskSerializer", "TaskResultSerializer") class TaskSerializer(AmCATModelSerializer): """Represents a Task object defined in amcat.models.task.Task. Adds two fields to the model: status and ready.""" status = serializers.SerializerMethodField('get_status') ready = serializers.SerializerMethodField('get_ready') progress = serializers.SerializerMethodField('get_progress') def __init__(self, *args, **kwargs): super(TaskSerializer, self).__init__(*args, **kwargs) self._tasks = {} def set_status_ready(self, task): async = task.get_async_result() self._tasks[task] = (async.ready(), async.result, async.status) def get_status_ready(self, task): """Returns tuple with (status, ready) => (str, bool)""" if task not in self._tasks: self.set_status_ready(task) return self._tasks[task] def get_status(self, task): _, _, status = self.get_status_ready(task) return status def get_ready(self, task): ready, _, _ = self.get_status_ready(task) return ready def get_progress(self, task): _, result, status = self.get_status_ready(task) if status == IN_PROGRESS and isinstance(result, dict): return result class Meta: model = Task class TaskResultSerializer(AmCATModelSerializer): result = serializers.SerializerMethodField('get_result') ready = serializers.SerializerMethodField('get_ready') def get_ready(self, task): return task.get_async_result().ready() def get_result(self, task): if not self.get_ready(task): return None return task.get_result() class Meta: model = Task fields = ("uuid", "ready", "result") class TestTaskSerializer(amcattest.AmCATTestCase): def test_order(self): class MockTask: def __init__(self, ready=False, status="PENDING", result=None, callback=None): self._ready = ready self._status = status self._result = result self.callback = callback def ready(self): if self.callback: self.callback("_ready") return self._ready @property def status(self, **kwargs): if self.callback: self.callback("_status") return self._status @property def result(self): if self.callback: self.callback("_result") return self._result def get_async_result(self): return self ts = TaskSerializer() mt = MockTask() mt2 = MockTask(ready=True, status="SUCCESS") mt3 = MockTask() mt4 = MockTask() # Test simple getting / caching self.assertEqual("PENDING", ts.get_status(mt)) self.assertEqual(False, ts.get_ready(mt)) self.assertEqual("SUCCESS", ts.get_status(mt2)) self.assertEqual(True, ts.get_ready(mt2)) # Test order of ready/status/result def _change(task, set_prop, set_value, prop, callprop): if prop == callprop: setattr(task, set_prop, set_value) # Set ready to True when _result is fetched change = functools.partial(_change, mt3, "_ready", True, "_result") mt3.callback = change self.assertEqual("PENDING", ts.get_status(mt3)) self.assertEqual(False, ts.get_ready(mt3)) self.assertEqual(True, mt3._ready) # Set ready to True when _status is fetched change = functools.partial(_change, mt4, "_ready", True, "_status") mt4.callback = change self.assertEqual("PENDING", ts.get_status(mt4)) self.assertEqual(False, ts.get_ready(mt4)) self.assertEqual(True, mt4._ready)
# -*- coding: utf-8 -*- # ------------------------------------------------------------ # streamondemand.- XBMC Plugin # Canal para piratestreaming # http://blog.tvalacarta.info/plugin-xbmc/streamondemand. # ------------------------------------------------------------ import re import urlparse from core import config from core import logger from core import scrapertools from core.item import Item from core.tmdb import infoSod from servers import servertools __channel__ = "piratestreaming" __category__ = "F,S,A" __type__ = "generic" __title__ = "piratestreaming" __language__ = "IT" DEBUG = config.get_setting("debug") host = "http://www.piratestreaming.news" def isGeneric(): return True def mainlist(item): logger.info("[piratestreaming.py] mainlist") itemlist = [Item(channel=__channel__, title="[COLOR azure]Aggiornamenti[/COLOR]", action="peliculas", url="%s/film-aggiornamenti.php" % host, thumbnail="http://orig03.deviantart.net/6889/f/2014/079/7/b/movies_and_popcorn_folder_icon_by_matheusgrilo-d7ay4tw.png"), Item(channel=__channel__, title="[COLOR azure]Contenuti per Genere[/COLOR]", action="categorias", url=host, thumbnail="http://xbmc-repo-ackbarr.googlecode.com/svn/trunk/dev/skin.cirrus%20extended%20v2/extras/moviegenres/All%20Movies%20by%20Genre.png"), Item(channel=__channel__, title="[COLOR yellow]Cerca...[/COLOR]", action="search", thumbnail="http://dc467.4shared.com/img/fEbJqOum/s7/13feaf0c8c0/Search"), Item(channel=__channel__, title="[COLOR azure]Archivio Serie TV[/COLOR]", action="categoryarchive", url="%s/archivio-serietv.php" % host, thumbnail="http://repository-butchabay.googlecode.com/svn/branches/eden/skin.cirrus.extended.v2/extras/moviegenres/TV%20Series.png"), Item(channel=__channel__, title="[COLOR azure]Serie TV[/COLOR]", extra="serie", action="peliculas_tv", url="%s/serietv-aggiornamenti.php" % host, thumbnail="http://xbmc-repo-ackbarr.googlecode.com/svn/trunk/dev/skin.cirrus%20extended%20v2/extras/moviegenres/New%20TV%20Shows.png"), Item(channel=__channel__, title="[COLOR yellow]Cerca Serie TV...[/COLOR]", action="search", extra="serie", thumbnail="http://dc467.4shared.com/img/fEbJqOum/s7/13feaf0c8c0/Search")] return itemlist def peliculas(item): logger.info("streamondemand.piratestreaming peliculas") itemlist = [] # Descarga la pagina data = scrapertools.cache_page(item.url) # Extrae las entradas (carpetas) patron = '<div class="featuredItem">.*?<a href="([^"]+)".*?<img src="([^"]+)".*?<a href=[^>]*>(.*?)</a>' matches = re.compile(patron, re.DOTALL).findall(data) for scrapedurl, scrapedthumbnail, scrapedtitle in matches: scrapedtitle = scrapertools.decodeHtmlentities(scrapedtitle).strip() try: daa = scrapertools.cache_page(scrapedurl) da = daa.split('justify;">') da = da[1].split('</p>') scrapedplot = scrapertools.htmlclean(da[0]).strip() except: scrapedplot = "Trama non disponibile" if DEBUG: logger.info( "title=[" + scrapedtitle + "], url=[" + scrapedurl + "], thumbnail=[" + scrapedthumbnail + "]") itemlist.append(infoSod( Item(channel=__channel__, action="episodios" if item.extra == "serie" else "findvideos", fulltitle=scrapedtitle, show=scrapedtitle, title=scrapedtitle, url=scrapedurl, thumbnail=scrapedthumbnail, plot=scrapedplot, folder=True), tipo='movie')) # Extrae el paginador patronvideos = '<td align="center">[^<]+</td>[^<]+<td align="center">\s*<a href="([^"]+)">[^<]+</a>' matches = re.compile(patronvideos, re.DOTALL).findall(data) if len(matches) > 0: scrapedurl = urlparse.urljoin(item.url, matches[0]) itemlist.append( Item(channel=__channel__, action="HomePage", title="[COLOR yellow]Torna Home[/COLOR]", folder=True)), itemlist.append( Item(channel=__channel__, action="peliculas", title="[COLOR orange]Successivo >>[/COLOR]", url=scrapedurl, thumbnail="http://2.bp.blogspot.com/-fE9tzwmjaeQ/UcM2apxDtjI/AAAAAAAAeeg/WKSGM2TADLM/s1600/pager+old.png", folder=True)) return itemlist def peliculas_tv(item): logger.info("streamondemand.piratestreaming peliculas") itemlist = [] # Descarga la pagina data = scrapertools.cache_page(item.url) # Extrae las entradas (carpetas) patron = '<div class="featuredItem">.*?<a href="([^"]+)".*?<img src="([^"]+)".*?<a href=[^>]*>(.*?)</a>' matches = re.compile(patron, re.DOTALL).findall(data) for scrapedurl, scrapedthumbnail, scrapedtitle in matches: scrapedtitle = scrapertools.decodeHtmlentities(scrapedtitle).strip() try: daa = scrapertools.cache_page(scrapedurl) da = daa.split('justify;">') da = da[1].split('</p>') scrapedplot = scrapertools.htmlclean(da[0]).strip() except: scrapedplot = "Trama non disponibile" if DEBUG: logger.info( "title=[" + scrapedtitle + "], url=[" + scrapedurl + "], thumbnail=[" + scrapedthumbnail + "]") itemlist.append(infoSod( Item(channel=__channel__, action="episodios" if item.extra == "serie" else "findvideos", fulltitle=scrapedtitle, show=scrapedtitle, title=scrapedtitle, url=scrapedurl, thumbnail=scrapedthumbnail, plot=scrapedplot, folder=True), tipo='tv')) # Extrae el paginador patronvideos = '<td align="center">[^<]+</td>[^<]+<td align="center">\s*<a href="([^"]+)">[^<]+</a>' matches = re.compile(patronvideos, re.DOTALL).findall(data) if len(matches) > 0: scrapedurl = urlparse.urljoin(item.url, matches[0]) itemlist.append( Item(channel=__channel__, action="HomePage", title="[COLOR yellow]Torna Home[/COLOR]", folder=True)), itemlist.append( Item(channel=__channel__, action="peliculas_tv", title="[COLOR orange]Successivo >>[/COLOR]", url=scrapedurl, thumbnail="http://2.bp.blogspot.com/-fE9tzwmjaeQ/UcM2apxDtjI/AAAAAAAAeeg/WKSGM2TADLM/s1600/pager+old.png", folder=True)) return itemlist def HomePage(item): import xbmc xbmc.executebuiltin("ReplaceWindow(10024,plugin://plugin.video.streamondemand)") def categorias(item): itemlist = [] data = scrapertools.cache_page(item.url) patron = '<a href="#">Film</a>[^<]+<ul>(.*?)</ul>' data = scrapertools.find_single_match(data, patron) patron = '<li><a href="([^"]+)">([^<]+)</a></li>' matches = re.compile(patron, re.DOTALL).findall(data) for scrapedurl, scrapedtitle in matches: scrapedplot = "" scrapedthumbnail = "" if DEBUG: logger.info( "title=[" + scrapedtitle + "], url=[" + scrapedurl + "], thumbnail=[" + scrapedthumbnail + "]") itemlist.append( Item(channel=__channel__, action="peliculas", title="[COLOR azure]" + scrapedtitle + "[/COLOR]", url=scrapedurl, thumbnail=scrapedthumbnail, plot=scrapedplot, folder=True)) return itemlist def categoryarchive(item): itemlist = [] data = scrapertools.cache_page(item.url) patron = '<b>0-9</b><hr />(.*?)<div class="clear"></div>' data = scrapertools.find_single_match(data, patron) patron = '<a href=([^>]+)>([^<]+)</a><br />' matches = re.compile(patron, re.DOTALL).findall(data) for scrapedurl, scrapedtitle in matches: scrapedplot = "" scrapedthumbnail = "" if DEBUG: logger.info( "title=[" + scrapedtitle + "], url=[" + scrapedurl + "], thumbnail=[" + scrapedthumbnail + "]") itemlist.append( Item(channel=__channel__, action="episodios", fulltitle=scrapedtitle, show=scrapedtitle, title="[COLOR azure]" + scrapedtitle + "[/COLOR]", url=scrapedurl, thumbnail="http://repository-butchabay.googlecode.com/svn/branches/eden/skin.cirrus.extended.v2/extras/moviegenres/TV%20Series.png", plot=scrapedplot, folder=True)) return itemlist def search(item, texto): logger.info("[piratestreaming.py] search " + texto) item.url = host + "/cerca.php?all=" + texto try: return cerca(item) # Se captura la excepcin, para no interrumpir al buscador global si un canal falla except: import sys for line in sys.exc_info(): logger.error("%s" % line) return [] def cerca(item): itemlist = [] # Descarga la pagina data = scrapertools.cache_page(item.url) if item.extra == "serie": data = data.split('Serie TV Complete')[1] patron = '<!-- Featured Item -->(.*?)<!-- End of Content -->' bloque = scrapertools.find_single_match(data, patron) # Extrae las entradas (carpetas) patron = '<img src=(.*?) alt="featured item" style="width:\s+80.8px; height: 109.6px;" /></a>\s*<div class="featuredText">\s*' patron += '<b><a href=([^>]+)>(.*?)</b>' matches = re.compile(patron).findall(bloque) for scrapedthumbnail, scrapedurl, scrapedtitle in matches: scrapedplot = "" scrapedtitle = scrapertools.decodeHtmlentities(scrapedtitle.replace("</a>", "")) if DEBUG: logger.info("title=[" + scrapedtitle + "], url=[" + scrapedurl + "]") itemlist.append(infoSod( Item(channel=__channel__, action="episodios" if item.extra == "serie" else "findvideos", fulltitle=scrapedtitle, show=scrapedtitle, title="[COLOR azure]" + scrapedtitle + "[/COLOR]", url=scrapedurl, thumbnail=scrapedthumbnail, plot=scrapedplot, folder=True), tipo='movie')) # Extrae el paginador patronvideos = '<td align="center">[^<]+</td>[^<]+<td align="center">\s*<a href="([^"]+)">[^<]+</a>' matches = re.compile(patronvideos, re.DOTALL).findall(data) if len(matches) > 0: scrapedurl = urlparse.urljoin(item.url, matches[0]) itemlist.append( Item(channel=__channel__, action="HomePage", title="[COLOR yellow]Torna Home[/COLOR]", folder=True)), itemlist.append( Item(channel=__channel__, action="cerca", title="[COLOR orange]Successivo >>[/COLOR]", url=scrapedurl, thumbnail="http://2.bp.blogspot.com/-fE9tzwmjaeQ/UcM2apxDtjI/AAAAAAAAeeg/WKSGM2TADLM/s1600/pager+old.png", folder=True)) return itemlist def episodios(item): def load_episodios(html, item, itemlist, lang_title): for data in scrapertools.decodeHtmlentities(html).splitlines(): # Extrae las entradas end = data.find('<a ') if end > 0: scrapedtitle = re.sub(r'<[^>]*>', '', data[:end]).strip() else: scrapedtitle = '' if scrapedtitle == '': patron = '<a\s*rel="nofollow"\s*target="_blank"\s*href="[^"]+">([^<]+)</a>' scrapedtitle = scrapertools.find_single_match(data, patron).strip() title = scrapertools.find_single_match(scrapedtitle, '\d+[^\d]+\d+') if title == '': title = scrapedtitle if title != '': itemlist.append( Item(channel=__channel__, action="findvid_serie", title=title + " (" + lang_title + ")", url=item.url, thumbnail=item.thumbnail, extra=data, fulltitle=item.fulltitle, show=item.show)) logger.info("[piratestreaming.py] episodios") itemlist = [] # Descarga la pgina data = scrapertools.cache_page(item.url) start = data.find('<!--googleoff: all-->') end = data.find('<!--googleon: all-->', start) data = data[start:end] lang_titles = [] starts = [] patron = r"(?:STAGIONE|MINISERIE|WEBSERIE|SERIE).*?ITA" matches = re.compile(patron, re.IGNORECASE).finditer(data) for match in matches: season_title = match.group() if season_title != '': lang_titles.append('SUB ITA' if 'SUB' in season_title.upper() else 'ITA') starts.append(match.end()) i = 1 len_lang_titles = len(lang_titles) while i <= len_lang_titles: inizio = starts[i - 1] fine = starts[i] if i < len_lang_titles else -1 html = data[inizio:fine] lang_title = lang_titles[i - 1] load_episodios(html, item, itemlist, lang_title) i += 1 if len(itemlist) == 0: load_episodios(data, item, itemlist, 'ITA') if config.get_library_support() and len(itemlist) != 0: itemlist.append( Item(channel=__channel__, title=item.title, url=item.url, action="add_serie_to_library", extra="episodios", show=item.show)) itemlist.append( Item(channel=item.channel, title="Scarica tutti gli episodi della serie", url=item.url, action="download_all_episodes", extra="episodios", show=item.show)) return itemlist def findvid_serie(item): logger.info("[piratestreaming.py] findvideos") # Descarga la pgina data = item.extra itemlist = servertools.find_video_items(data=data) for videoitem in itemlist: videoitem.title = item.title + videoitem.title videoitem.fulltitle = item.fulltitle videoitem.thumbnail = item.thumbnail videoitem.show = item.show videoitem.plot = item.plot videoitem.channel = __channel__ return itemlist
""" Reorganisation step: cleanup all local data. Cleanup the local data (for the whole data-set) created during copy_to_local step. Configuration variables used: * :reorganisation:copy_to_local section * OUTPUT_FOLDER: destination folder for the local copy """ from datetime import timedelta from textwrap import dedent from airflow import configuration from airflow.operators.bash_operator import BashOperator from common_steps import Step def cleanup_all_local_cfg(dag, upstream_step, step_section=None): cleanup_folder = configuration.get(step_section, "OUTPUT_FOLDER") return cleanup_all_local_step(dag, upstream_step, cleanup_folder) def cleanup_all_local_step(dag, upstream_step, cleanup_folder): cleanup_local_cmd = dedent(""" rm -rf {{ params["cleanup_folder"] }}/* """) cleanup_all_local = BashOperator( task_id='cleanup_all_local', bash_command=cleanup_local_cmd, params={'cleanup_folder': cleanup_folder}, priority_weight=upstream_step.priority_weight, execution_timeout=timedelta(hours=1), dag=dag ) if upstream_step.task: cleanup_all_local.set_upstream(upstream_step.task) cleanup_all_local.doc_md = dedent("""\ # Cleanup all local files Remove locally stored files as they have been already reorganised. """) return Step(cleanup_all_local, cleanup_all_local.task_id, upstream_step.priority_weight + 10)
"""Tests for setuptools.find_packages().""" import os import sys import shutil import tempfile import unittest import platform import setuptools from setuptools import find_packages from setuptools.tests.py26compat import skipIf find_420_packages = setuptools.PEP420PackageFinder.find def has_symlink(): bad_symlink = ( # Windows symlink directory detection is broken on Python 3.2 platform.system() == 'Windows' and sys.version_info[:2] == (3,2) ) return hasattr(os, 'symlink') and not bad_symlink class TestFindPackages(unittest.TestCase): def setUp(self): self.dist_dir = tempfile.mkdtemp() self._make_pkg_structure() def tearDown(self): shutil.rmtree(self.dist_dir) def _make_pkg_structure(self): """Make basic package structure. dist/ docs/ conf.py pkg/ __pycache__/ nspkg/ mod.py subpkg/ assets/ asset __init__.py setup.py """ self.docs_dir = self._mkdir('docs', self.dist_dir) self._touch('conf.py', self.docs_dir) self.pkg_dir = self._mkdir('pkg', self.dist_dir) self._mkdir('__pycache__', self.pkg_dir) self.ns_pkg_dir = self._mkdir('nspkg', self.pkg_dir) self._touch('mod.py', self.ns_pkg_dir) self.sub_pkg_dir = self._mkdir('subpkg', self.pkg_dir) self.asset_dir = self._mkdir('assets', self.sub_pkg_dir) self._touch('asset', self.asset_dir) self._touch('__init__.py', self.sub_pkg_dir) self._touch('setup.py', self.dist_dir) def _mkdir(self, path, parent_dir=None): if parent_dir: path = os.path.join(parent_dir, path) os.mkdir(path) return path def _touch(self, path, dir_=None): if dir_: path = os.path.join(dir_, path) fp = open(path, 'w') fp.close() return path def test_regular_package(self): self._touch('__init__.py', self.pkg_dir) packages = find_packages(self.dist_dir) self.assertEqual(packages, ['pkg', 'pkg.subpkg']) def test_exclude(self): self._touch('__init__.py', self.pkg_dir) packages = find_packages(self.dist_dir, exclude=('pkg.*',)) assert packages == ['pkg'] def test_include_excludes_other(self): """ If include is specified, other packages should be excluded. """ self._touch('__init__.py', self.pkg_dir) alt_dir = self._mkdir('other_pkg', self.dist_dir) self._touch('__init__.py', alt_dir) packages = find_packages(self.dist_dir, include=['other_pkg']) self.assertEqual(packages, ['other_pkg']) def test_dir_with_dot_is_skipped(self): shutil.rmtree(os.path.join(self.dist_dir, 'pkg/subpkg/assets')) data_dir = self._mkdir('some.data', self.pkg_dir) self._touch('__init__.py', data_dir) self._touch('file.dat', data_dir) packages = find_packages(self.dist_dir) self.assertTrue('pkg.some.data' not in packages) def test_dir_with_packages_in_subdir_is_excluded(self): """ Ensure that a package in a non-package such as build/pkg/__init__.py is excluded. """ build_dir = self._mkdir('build', self.dist_dir) build_pkg_dir = self._mkdir('pkg', build_dir) self._touch('__init__.py', build_pkg_dir) packages = find_packages(self.dist_dir) self.assertTrue('build.pkg' not in packages) @skipIf(not has_symlink(), 'Symlink support required') def test_symlinked_packages_are_included(self): """ A symbolically-linked directory should be treated like any other directory when matched as a package. Create a link from lpkg -> pkg. """ self._touch('__init__.py', self.pkg_dir) linked_pkg = os.path.join(self.dist_dir, 'lpkg') os.symlink('pkg', linked_pkg) assert os.path.isdir(linked_pkg) packages = find_packages(self.dist_dir) self.assertTrue('lpkg' in packages) def _assert_packages(self, actual, expected): self.assertEqual(set(actual), set(expected)) def test_pep420_ns_package(self): packages = find_420_packages( self.dist_dir, include=['pkg*'], exclude=['pkg.subpkg.assets']) self._assert_packages(packages, ['pkg', 'pkg.nspkg', 'pkg.subpkg']) def test_pep420_ns_package_no_includes(self): packages = find_420_packages( self.dist_dir, exclude=['pkg.subpkg.assets']) self._assert_packages(packages, ['docs', 'pkg', 'pkg.nspkg', 'pkg.subpkg']) def test_pep420_ns_package_no_includes_or_excludes(self): packages = find_420_packages(self.dist_dir) expected = [ 'docs', 'pkg', 'pkg.nspkg', 'pkg.subpkg', 'pkg.subpkg.assets'] self._assert_packages(packages, expected) def test_regular_package_with_nested_pep420_ns_packages(self): self._touch('__init__.py', self.pkg_dir) packages = find_420_packages( self.dist_dir, exclude=['docs', 'pkg.subpkg.assets']) self._assert_packages(packages, ['pkg', 'pkg.nspkg', 'pkg.subpkg']) def test_pep420_ns_package_no_non_package_dirs(self): shutil.rmtree(self.docs_dir) shutil.rmtree(os.path.join(self.dist_dir, 'pkg/subpkg/assets')) packages = find_420_packages(self.dist_dir) self._assert_packages(packages, ['pkg', 'pkg.nspkg', 'pkg.subpkg'])
print("""Enter 'C' or 'c' for Celsius, 'K' or 'k' for Kelvin, 'F' or 'f' for Fahrenheit\n\n""") converted=0 fr=input("I want converter from: \n") value1=input("Enter value: \n") to=input("to: \n") try: value1=float(value1) if(fr=='C' or fr=='c'): if(to=='F' or to=='f'): converted=value1*1,8+32 elif(to=='K' or to=='k'): converted = value1 + 273.15 else: print("you enter different value\n") exit() elif(fr=='K' or fr=='k'): if(to=='C' or to=='c'): converted=value1-273.15 elif(to=='F' or to=='f'): converted = (value1 - 273.15) * 1.8 + 32 else: print("you enter different value\n") exit() elif(fr=='F' or fr=='f'): if(to=='C' or to=='c'): converted=(value1-32)/1.8 elif(to=='K' or to=='k'): converted = ((f - 32) / 1.8) + 273.15 else: print("you enter different value\n") exit() except ValueError: print("That was no valid number.") print("result = {}".format(converted))
from django.forms import models as model_forms from django.core.exceptions import ImproperlyConfigured from django.http import HttpResponseRedirect from django.views.generic.base import TemplateResponseMixin, View from django.views.generic.detail import (SingleObjectMixin, SingleObjectTemplateResponseMixin, BaseDetailView) class FormMixin(object): """ A mixin that provides a way to show and handle a form in a request. """ initial = {} form_class = None success_url = None def get_initial(self): """ Returns the initial data to use for forms on this view. """ return self.initial def get_form_class(self): """ Returns the form class to use in this view """ return self.form_class def get_form(self, form_class): """ Returns an instance of the form to be used in this view. """ return form_class(**self.get_form_kwargs()) def get_form_kwargs(self): """ Returns the keyword arguments for instanciating the form. """ kwargs = {'initial': self.get_initial()} if self.request.method in ('POST', 'PUT'): kwargs.update({ 'data': self.request.POST, 'files': self.request.FILES, }) return kwargs def get_context_data(self, **kwargs): return kwargs def get_success_url(self): if self.success_url: url = self.success_url else: raise ImproperlyConfigured( "No URL to redirect to. Provide a success_url.") return url def form_valid(self, form): return HttpResponseRedirect(self.get_success_url()) def form_invalid(self, form): return self.render_to_response(self.get_context_data(form=form)) class ModelFormMixin(FormMixin, SingleObjectMixin): """ A mixin that provides a way to show and handle a modelform in a request. """ def get_form_class(self): """ Returns the form class to use in this view """ if self.form_class: return self.form_class else: if self.model is not None: # If a model has been explicitly provided, use it model = self.model elif hasattr(self, 'object') and self.object is not None: # If this view is operating on a single object, use # the class of that object model = self.object.__class__ else: # Try to get a queryset and extract the model class # from that model = self.get_queryset().model return model_forms.modelform_factory(model) def get_form_kwargs(self): """ Returns the keyword arguments for instanciating the form. """ kwargs = super(ModelFormMixin, self).get_form_kwargs() kwargs.update({'instance': self.object}) return kwargs def get_success_url(self): if self.success_url: url = self.success_url % self.object.__dict__ else: try: url = self.object.get_absolute_url() except AttributeError: raise ImproperlyConfigured( "No URL to redirect to. Either provide a url or define" " a get_absolute_url method on the Model.") return url def form_valid(self, form): self.object = form.save() return super(ModelFormMixin, self).form_valid(form) def get_context_data(self, **kwargs): context = kwargs if self.object: context['object'] = self.object context_object_name = self.get_context_object_name(self.object) if context_object_name: context[context_object_name] = self.object return context class ProcessFormView(View): """ A mixin that processes a form on POST. """ def get(self, request, *args, **kwargs): form_class = self.get_form_class() form = self.get_form(form_class) return self.render_to_response(self.get_context_data(form=form)) def post(self, request, *args, **kwargs): form_class = self.get_form_class() form = self.get_form(form_class) if form.is_valid(): return self.form_valid(form) else: return self.form_invalid(form) # PUT is a valid HTTP verb for creating (with a known URL) or editing an # object, note that browsers only support POST for now. def put(self, *args, **kwargs): return self.post(*args, **kwargs) class BaseFormView(FormMixin, ProcessFormView): """ A base view for displaying a form """ class FormView(TemplateResponseMixin, BaseFormView): """ A view for displaying a form, and rendering a template response. """ class BaseCreateView(ModelFormMixin, ProcessFormView): """ Base view for creating an new object instance. Using this base class requires subclassing to provide a response mixin. """ def get(self, request, *args, **kwargs): self.object = None return super(BaseCreateView, self).get(request, *args, **kwargs) def post(self, request, *args, **kwargs): self.object = None return super(BaseCreateView, self).post(request, *args, **kwargs) class CreateView(SingleObjectTemplateResponseMixin, BaseCreateView): """ View for creating an new object instance, with a response rendered by template. """ template_name_suffix = '_form' class BaseUpdateView(ModelFormMixin, ProcessFormView): """ Base view for updating an existing object. Using this base class requires subclassing to provide a response mixin. """ def get(self, request, *args, **kwargs): self.object = self.get_object() return super(BaseUpdateView, self).get(request, *args, **kwargs) def post(self, request, *args, **kwargs): self.object = self.get_object() return super(BaseUpdateView, self).post(request, *args, **kwargs) class UpdateView(SingleObjectTemplateResponseMixin, BaseUpdateView): """ View for updating an object, with a response rendered by template.. """ template_name_suffix = '_form' class DeletionMixin(object): """ A mixin providing the ability to delete objects """ success_url = None def delete(self, request, *args, **kwargs): self.object = self.get_object() self.object.delete() return HttpResponseRedirect(self.get_success_url()) # Add support for browsers which only accept GET and POST for now. def post(self, *args, **kwargs): return self.delete(*args, **kwargs) def get_success_url(self): if self.success_url: return self.success_url else: raise ImproperlyConfigured( "No URL to redirect to. Provide a success_url.") class BaseDeleteView(DeletionMixin, BaseDetailView): """ Base view for deleting an object. Using this base class requires subclassing to provide a response mixin. """ class DeleteView(SingleObjectTemplateResponseMixin, BaseDeleteView): """ View for deleting an object retrieved with `self.get_object()`, with a response rendered by template. """ template_name_suffix = '_confirm_delete'
# Copyright 2015, Google Inc. # All rights reserved. # # Redistribution and use in source and binary forms, with or without # modification, are permitted provided that the following conditions are # met: # # * Redistributions of source code must retain the above copyright # notice, this list of conditions and the following disclaimer. # * Redistributions in binary form must reproduce the above # copyright notice, this list of conditions and the following disclaimer # in the documentation and/or other materials provided with the # distribution. # * Neither the name of Google Inc. nor the names of its # contributors may be used to endorse or promote products derived from # this software without specific prior written permission. # # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS # "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT # LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR # A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT # OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, # SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT # LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, # DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY # THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT # (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE # OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. import collections import threading import grpc from grpc import _common from grpc._cython import cygrpc class AuthMetadataContext( collections.namedtuple('AuthMetadataContext', ( 'service_url', 'method_name',)), grpc.AuthMetadataContext): pass class AuthMetadataPluginCallback(grpc.AuthMetadataContext): def __init__(self, callback): self._callback = callback def __call__(self, metadata, error): self._callback(metadata, error) class _WrappedCygrpcCallback(object): def __init__(self, cygrpc_callback): self.is_called = False self.error = None self.is_called_lock = threading.Lock() self.cygrpc_callback = cygrpc_callback def _invoke_failure(self, error): # TODO(atash) translate different Exception superclasses into different # status codes. self.cygrpc_callback(_common.EMPTY_METADATA, cygrpc.StatusCode.internal, _common.encode(str(error))) def _invoke_success(self, metadata): try: cygrpc_metadata = _common.to_cygrpc_metadata(metadata) except Exception as exception: # pylint: disable=broad-except self._invoke_failure(exception) return self.cygrpc_callback(cygrpc_metadata, cygrpc.StatusCode.ok, b'') def __call__(self, metadata, error): with self.is_called_lock: if self.is_called: raise RuntimeError('callback should only ever be invoked once') if self.error: self._invoke_failure(self.error) return self.is_called = True if error is None: self._invoke_success(metadata) else: self._invoke_failure(error) def notify_failure(self, error): with self.is_called_lock: if not self.is_called: self.error = error class _WrappedPlugin(object): def __init__(self, plugin): self.plugin = plugin def __call__(self, context, cygrpc_callback): wrapped_cygrpc_callback = _WrappedCygrpcCallback(cygrpc_callback) wrapped_context = AuthMetadataContext( _common.decode(context.service_url), _common.decode(context.method_name)) try: self.plugin(wrapped_context, AuthMetadataPluginCallback(wrapped_cygrpc_callback)) except Exception as error: wrapped_cygrpc_callback.notify_failure(error) raise def call_credentials_metadata_plugin(plugin, name): """ Args: plugin: A callable accepting a grpc.AuthMetadataContext object and a callback (itself accepting a list of metadata key/value 2-tuples and a None-able exception value). The callback must be eventually called, but need not be called in plugin's invocation. plugin's invocation must be non-blocking. """ return cygrpc.call_credentials_metadata_plugin( cygrpc.CredentialsMetadataPlugin( _WrappedPlugin(plugin), _common.encode(name)))
# -*- coding: utf-8 -*- from __future__ import unicode_literals from django.apps import apps from django.core import checks from django.core.checks.registry import registry from django.core.management.base import BaseCommand, CommandError class Command(BaseCommand): help = "Checks the entire Django project for potential problems." requires_system_checks = False def add_arguments(self, parser): parser.add_argument('args', metavar='app_label', nargs='*') parser.add_argument('--tag', '-t', action='append', dest='tags', help='Run only checks labeled with given tag.') parser.add_argument('--list-tags', action='store_true', dest='list_tags', help='List available tags.') parser.add_argument('--deploy', action='store_true', dest='deploy', help='Check deployment settings.') def handle(self, *app_labels, **options): include_deployment_checks = options['deploy'] if options.get('list_tags'): self.stdout.write('\n'.join(sorted(registry.tags_available(include_deployment_checks)))) return if app_labels: app_configs = [apps.get_app_config(app_label) for app_label in app_labels] else: app_configs = None tags = options.get('tags') if tags: try: invalid_tag = next( tag for tag in tags if not checks.tag_exists(tag, include_deployment_checks) ) except StopIteration: # no invalid tags pass else: raise CommandError('There is no system check with the "%s" tag.' % invalid_tag) self.check( app_configs=app_configs, tags=tags, display_num_errors=True, include_deployment_checks=include_deployment_checks, )
#!/usr/bin/env python3 import argparse import os import sys import boto3 import json def get_entity_sizes(bucket, prefix): s3 = boto3.client("s3") prefix = f"{prefix}social_network/csv/raw/composite-merged-fk/dynamic/" more = True token = None sizes = {} while more: resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, **({'ContinuationToken': token} if token else {})) for obj in resp["Contents"]: splits = obj["Key"][len(prefix):].split("/", 1) if len(splits) > 1: entity, rest = splits if rest.endswith(".csv"): total, c, m = sizes.get(entity, [0, 0, 0]) sizes[entity] = [total + obj["Size"], c + 1, max(m, obj["Size"])] more = False if 'NextContinuationToken' in resp.keys(): token = resp['NextContinuationToken'] more = True with open(f"sizes.json", "w") as f: json.dump(sizes, f) get_entity_sizes("ldbc-datagen-sf10k-debug", "sf1000/runs/20210412_091530/")
#!/usr/bin/env python3 # Copyright (c) 2015-2018 The Energi Core developers # Distributed under the MIT software license, see the accompanying # file COPYING or http://www.opensource.org/licenses/mit-license.php. # Copyright (c) 2015-2016 The Bitcoin Core developers # Distributed under the MIT software license, see the accompanying # file COPYING or http://www.opensource.org/licenses/mit-license.php. from test_framework.test_framework import ComparisonTestFramework from test_framework.util import * from test_framework.mininode import CTransaction, NetworkThread from test_framework.blocktools import create_coinbase, create_block from test_framework.comptool import TestInstance, TestManager from test_framework.script import CScript from io import BytesIO # A canonical signature consists of: # <30> <total len> <02> <len R> <R> <02> <len S> <S> <hashtype> def unDERify(tx): ''' Make the signature in vin 0 of a tx non-DER-compliant, by adding padding after the S-value. ''' scriptSig = CScript(tx.vin[0].scriptSig) newscript = [] for i in scriptSig: if (len(newscript) == 0): newscript.append(i[0:-1] + b'\0' + i[-1:]) else: newscript.append(i) tx.vin[0].scriptSig = CScript(newscript) ''' This test is meant to exercise BIP66 (DER SIG). Connect to a single node. Mine 2 (version 2) blocks (save the coinbases for later). Generate 98 more version 2 blocks, verify the node accepts. Mine 749 version 3 blocks, verify the node accepts. Check that the new DERSIG rules are not enforced on the 750th version 3 block. Check that the new DERSIG rules are enforced on the 751st version 3 block. Mine 199 new version blocks. Mine 1 old-version block. Mine 1 new version block. Mine 1 old version block, see that the node rejects. ''' class BIP66Test(ComparisonTestFramework): def __init__(self): super().__init__() self.num_nodes = 1 def setup_network(self): # Must set the blockversion for this test self.nodes = start_nodes(self.num_nodes, self.options.tmpdir, extra_args=[['-debug', '-whitelist=127.0.0.1', '-blockversion=2']], binary=[self.options.testbinary]) def run_test(self): test = TestManager(self, self.options.tmpdir) test.add_all_connections(self.nodes) NetworkThread().start() # Start up network handling in another thread test.run() def create_transaction(self, node, coinbase, to_address, amount): from_txid = node.getblock(coinbase)['tx'][0] inputs = [{ "txid" : from_txid, "vout" : 0}] outputs = { to_address : amount } rawtx = node.createrawtransaction(inputs, outputs) signresult = node.signrawtransaction(rawtx) tx = CTransaction() f = BytesIO(hex_str_to_bytes(signresult['hex'])) tx.deserialize(f) return tx def get_tests(self): self.coinbase_blocks = self.nodes[0].generate(2) height = 3 # height of the next block to build self.tip = int("0x" + self.nodes[0].getbestblockhash(), 0) self.nodeaddress = self.nodes[0].getnewaddress() self.last_block_time = get_mocktime() + 1 ''' 298 more version 2 blocks ''' test_blocks = [] for i in range(298): block = create_block(self.tip, create_coinbase(height), self.last_block_time + 1) block.nVersion = 2 block.rehash() block.solve() test_blocks.append([block, True]) self.last_block_time += 1 self.tip = block.sha256 height += 1 yield TestInstance(test_blocks, sync_every_block=False) ''' Mine 749 version 3 blocks ''' test_blocks = [] for i in range(749): block = create_block(self.tip, create_coinbase(height), self.last_block_time + 1) block.nVersion = 3 block.rehash() block.solve() test_blocks.append([block, True]) self.last_block_time += 1 self.tip = block.sha256 height += 1 yield TestInstance(test_blocks, sync_every_block=False) ''' Check that the new DERSIG rules are not enforced in the 750th version 3 block. ''' spendtx = self.create_transaction(self.nodes[0], self.coinbase_blocks[0], self.nodeaddress, 1.0) unDERify(spendtx) spendtx.rehash() block = create_block(self.tip, create_coinbase(height), self.last_block_time + 1) block.nVersion = 3 block.vtx.append(spendtx) block.hashMerkleRoot = block.calc_merkle_root() block.rehash() block.solve() self.last_block_time += 1 self.tip = block.sha256 height += 1 yield TestInstance([[block, True]]) ''' Mine 199 new version blocks on last valid tip ''' test_blocks = [] for i in range(199): block = create_block(self.tip, create_coinbase(height), self.last_block_time + 1) block.nVersion = 3 block.rehash() block.solve() test_blocks.append([block, True]) self.last_block_time += 1 self.tip = block.sha256 height += 1 yield TestInstance(test_blocks, sync_every_block=False) ''' Mine 1 old version block ''' block = create_block(self.tip, create_coinbase(height), self.last_block_time + 1) block.nVersion = 2 block.rehash() block.solve() self.last_block_time += 1 self.tip = block.sha256 height += 1 yield TestInstance([[block, True]]) ''' Mine 1 new version block ''' block = create_block(self.tip, create_coinbase(height), self.last_block_time + 1) block.nVersion = 3 block.rehash() block.solve() self.last_block_time += 1 self.tip = block.sha256 height += 1 yield TestInstance([[block, True]]) ''' Check that the new DERSIG rules are enforced in the 951st version 3 block. ''' spendtx = self.create_transaction(self.nodes[0], self.coinbase_blocks[1], self.nodeaddress, 1.0) unDERify(spendtx) spendtx.rehash() block = create_block(self.tip, create_coinbase(height), self.last_block_time + 1) block.nVersion = 3 block.vtx.append(spendtx) block.hashMerkleRoot = block.calc_merkle_root() block.rehash() block.solve() self.last_block_time += 1 yield TestInstance([[block, False]]) ''' Mine 1 old version block, should be invalid ''' block = create_block(self.tip, create_coinbase(height), self.last_block_time + 1) block.nVersion = 2 block.rehash() block.solve() self.last_block_time += 1 yield TestInstance([[block, False]]) if __name__ == '__main__': BIP66Test().main()
#!/usr/bin/env python # vim: sts=4 sw=4 et # This is a component of EMC # probe.py Copyright 2010 Michael Haberler # # # This program is free software; you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation; either version 2 of the License, or # (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program; if not, write to the Free Software # Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA'''''' ''' gladevcp probe demo example Michael Haberler 11/2010 ''' import os,sys from gladevcp.persistence import IniFile,widget_defaults,set_debug,select_widgets import hal import hal_glib import gtk import glib import linuxcnc debug = 0 class EmcInterface(object): def __init__(self): try: self.s = linuxcnc.stat(); self.c = linuxcnc.command() except Exception, msg: print "cant initialize EmcInterface: %s - EMC not running?" %(msg) def running(self,do_poll=True): if do_poll: self.s.poll() return self.s.task_mode == linuxcnc.MODE_AUTO and self.s.interp_state != linuxcnc.INTERP_IDLE def manual_ok(self,do_poll=True): if do_poll: self.s.poll() if self.s.task_state != linuxcnc.STATE_ON: return False return self.s.interp_state == linuxcnc.INTERP_IDLE def ensure_mode(self,m, *p): ''' If emc is not already in one of the modes given, switch it to the first mode example: ensure_mode(linuxcnc.MODE_MDI) ensure_mode(linuxcnc.MODE_AUTO, linuxcnc.MODE_MDI) ''' self.s.poll() if self.s.task_mode == m or self.s.task_mode in p: return True if self.running(do_poll=False): return False self.c.mode(m) self.c.wait_complete() return True def active_codes(self): self.s.poll() return self.s.gcodes def get_current_system(self): for i in self.active_codes(): if i >= 540 and i <= 590: return i/10 - 53 elif i >= 590 and i <= 593: return i - 584 return 1 def mdi_command(self,command, wait=True): #ensure_mode(emself.c.MODE_MDI) self.c.mdi(command) if wait: self.c.wait_complete() def emc_status(self): ''' return tuple (task mode, task state, exec state, interp state) as strings ''' self.s.poll() task_mode = ['invalid', 'MANUAL', 'AUTO', 'MDI'][self.s.task_mode] task_state = ['invalid', 'ESTOP', 'ESTOP_RESET', 'OFF', 'ON'][self.s.task_state] exec_state = ['invalid', 'ERROR', 'DONE', 'WAITING_FOR_MOTION', 'WAITING_FOR_MOTION_QUEUE', 'WAITING_FOR_IO', 'WAITING_FOR_PAUSE', 'WAITING_FOR_MOTION_AND_IO', 'WAITING_FOR_DELAY', 'WAITING_FOR_SYSTEM_CMD' ][self.s.exec_state] interp_state = ['invalid', 'IDLE', 'READING', 'PAUSED', 'WAITING'][self.s.interp_state] return (task_mode, task_state, exec_state, interp_state) class HandlerClass: def on_manual_mode(self,widget,data=None): if self.e.ensure_mode(linuxcnc.MODE_MANUAL): print "switched to manual mode" else: print "cant switch to manual in this state" def on_mdi_mode(self,widget,data=None): if self.e.ensure_mode(linuxcnc.MODE_MDI): print "switched to MDI mode" else: print "cant switch to MDI in this state" def _query_emc_status(self,data=None): (task_mode, task_state, exec_state, interp_state) = self.e.emc_status() self.builder.get_object('task_mode').set_label("Task mode: " + task_mode) self.builder.get_object('task_state').set_label("Task state: " + task_state) self.builder.get_object('exec_state').set_label("Exec state: " + exec_state) self.builder.get_object('interp_state').set_label("Interp state: " + interp_state) return True def on_probe(self,widget,data=None): label = widget.get_label() axis = ord(label[0].lower()) - ord('x') direction = 1.0 if label[1] == '-': direction = -1.0 self.e.s.poll() self.start_feed = self.e.s.settings[1] # determine system we are touching off - 1...g54 etc self.current_system = self.e.get_current_system() # remember current abs or rel mode - g91 self.start_relative = (910 in self.e.active_codes()) self.previous_mode = self.e.s.task_mode if self.e.s.task_state != linuxcnc.STATE_ON: print "machine not turned on" return if not self.e.s.homed[axis]: print "%s axis not homed" %(chr(axis + ord('X'))) return if self.e.running(do_poll=False): print "cant do that now - intepreter running" return self.e.ensure_mode(linuxcnc.MODE_MDI) self.e.mdi_command("#<_Probe_System> = %d " % (self.current_system ),wait=False) self.e.mdi_command("#<_Probe_Axis> = %d " % (axis),wait=False) self.e.mdi_command("#<_Probe_Speed> = %s " % (self.builder.get_object('probe_feed').get_value()),wait=False) self.e.mdi_command("#<_Probe_Diameter> = %s " % (self.builder.get_object('probe_diameter').get_value() ),wait=False) self.e.mdi_command("#<_Probe_Distance> = %s " % (self.builder.get_object('probe_travel').get_value() * direction),wait=False) self.e.mdi_command("#<_Probe_Retract> = %s " % (self.builder.get_object('retract').get_value() * direction * -1.0),wait=False) self.e.mdi_command("O<probe> call",wait=False) self.e.mdi_command('F%f' % (self.start_feed),wait=False) self.e.mdi_command('G91' if self.start_relative else 'G90',wait=False) # self.e.ensure_mode(self.previous_mode) def on_destroy(self,obj,data=None): self.ini.save_state(self) def on_restore_defaults(self,button,data=None): ''' example callback for 'Reset to defaults' button currently unused ''' self.ini.create_default_ini() self.ini.restore_state(self) def __init__(self, halcomp,builder,useropts): self.halcomp = halcomp self.builder = builder self.ini_filename = __name__ + '.ini' self.defaults = { IniFile.vars: dict(), IniFile.widgets : widget_defaults(select_widgets(self.builder.get_objects(), hal_only=False,output_only = True)) } self.ini = IniFile(self.ini_filename,self.defaults,self.builder) self.ini.restore_state(self) self.e = EmcInterface() glib.timeout_add_seconds(1, self._query_emc_status) def get_handlers(halcomp,builder,useropts): global debug for cmd in useropts: exec cmd in globals() set_debug(debug) return [HandlerClass(halcomp,builder,useropts)]
#!/usr/bin/env python # -*- coding: utf-8 -*- """Tests haystack.utils .""" from __future__ import print_function import logging import mmap import os import struct import unittest from haystack import listmodel from haystack import target from haystack.mappings.base import AMemoryMapping from haystack.mappings.process import make_local_memory_handler from haystack.mappings import folder from test.haystack import SrcTests log = logging.getLogger('test_memory_mapping') class TestMmapHack(unittest.TestCase): def setUp(self): pass def tearDown(self): pass def test_mmap_hack64(self): my_target = target.TargetPlatform.make_target_linux_64() my_ctypes = my_target.get_target_ctypes() my_utils = my_target.get_target_ctypes_utils() real_ctypes_long = my_ctypes.get_real_ctypes_member('c_ulong') fname = os.path.normpath(os.path.abspath(__file__)) fin = open(fname, 'rb') local_mmap_bytebuffer = mmap.mmap(fin.fileno(), 1024, access=mmap.ACCESS_READ) # yeap, that right, I'm stealing the pointer value. DEAL WITH IT. heapmap = struct.unpack('L', real_ctypes_long.from_address(id(local_mmap_bytebuffer) + 2 * (my_ctypes.sizeof(real_ctypes_long))))[0] log.debug('MMAP HACK: heapmap: 0x%0.8x' % heapmap) handler = make_local_memory_handler(force=True) ret = [m for m in handler.get_mappings() if heapmap in m] if len(ret) == 0: for m in handler.get_mappings(): print(m) # heapmap is a pointer value in local memory self.assertEqual(len(ret), 1) # heapmap is a pointer value to this executable? self.assertEqual(ret[0].pathname, fname) self.assertIn('CTypesProxy-8:8:16', str(my_ctypes)) fin.close() fin = None def test_mmap_hack32(self): my_target = target.TargetPlatform.make_target_linux_32() my_ctypes = my_target.get_target_ctypes() my_utils = my_target.get_target_ctypes_utils() real_ctypes_long = my_ctypes.get_real_ctypes_member('c_ulong') fname = os.path.normpath(os.path.abspath(__file__)) fin = open(fname, 'rb') local_mmap_bytebuffer = mmap.mmap(fin.fileno(), 1024, access=mmap.ACCESS_READ) # yeap, that right, I'm stealing the pointer value. DEAL WITH IT. heapmap = struct.unpack('L', real_ctypes_long.from_address(id(local_mmap_bytebuffer) + 2 * (my_ctypes.sizeof(real_ctypes_long))))[0] log.debug('MMAP HACK: heapmap: 0x%0.8x', heapmap) maps = make_local_memory_handler(force=True) # print 'MMAP HACK: heapmap: 0x%0.8x' % heapmap # for m in maps: # print m ret = [m for m in maps if heapmap in m] # heapmap is a pointer value in local memory self.assertEqual(len(ret), 1) # heapmap is a pointer value to this executable? self.assertEqual(ret[0].pathname, fname) self.assertIn('CTypesProxy-4:4:12', str(my_ctypes)) fin.close() fin = None class TestMappingsLinux(SrcTests): @classmethod def setUpClass(cls): cls.memory_handler = folder.load('test/dumps/ssh/ssh.1') @classmethod def tearDownClass(cls): cls.memory_handler.reset_mappings() cls.memory_handler = None def test_get_mapping(self): self.assertEqual(len(self.memory_handler._get_mapping('[heap]')), 1) self.assertEqual(len(self.memory_handler._get_mapping('None')), 9) def test_get_mapping_for_address(self): finder = self.memory_handler.get_heap_finder() walker = finder.list_heap_walkers()[0] self.assertEqual(walker.get_heap_address(), self.memory_handler.get_mapping_for_address(0xb84e02d3).start) def test_contains(self): for m in self.memory_handler: self.assertTrue(m.start in self.memory_handler) self.assertTrue((m.end - 1) in self.memory_handler) def test_len(self): self.assertEqual(len(self.memory_handler), 70) def test_getitem(self): self.assertTrue(isinstance(self.memory_handler[0], AMemoryMapping)) self.assertTrue( isinstance(self.memory_handler[len(self.memory_handler) - 1], AMemoryMapping)) with self.assertRaises(IndexError): self.memory_handler[0x0005c000] def test_iter(self): mps = [m for m in self.memory_handler] mps2 = [m for m in self.memory_handler.get_mappings()] self.assertEqual(mps, mps2) def test_setitem(self): with self.assertRaises(NotImplementedError): self.memory_handler[0x0005c000] = 1 def test_get_os_name(self): x = self.memory_handler.get_target_platform().get_os_name() self.assertEqual(x, 'linux') def test_get_cpu_bits(self): x = self.memory_handler.get_target_platform().get_cpu_bits() self.assertEqual(x, 32) class TestMappingsLinuxAddresses32(SrcTests): @classmethod def setUpClass(cls): cls.memory_handler = folder.load('test/src/test-ctypes5.32.dump') cls.my_target = cls.memory_handler.get_target_platform() cls.my_ctypes = cls.my_target.get_target_ctypes() cls.my_utils = cls.my_target.get_target_ctypes_utils() cls.my_model = cls.memory_handler.get_model() cls.ctypes5_gen32 = cls.my_model.import_module("test.src.ctypes5_gen32") cls.validator = listmodel.ListModel(cls.memory_handler, None) def setUp(self): self._load_offsets_values('test/src/test-ctypes5.32.dump') @classmethod def tearDownClass(cls): cls.memory_handler = None cls.my_target = None cls.my_ctypes = None cls.my_utils = None cls.my_model = None cls.ctypes5_gen32 = None pass def test_is_valid_address(self): offset = self.offsets['struct_d'][0] m = self.memory_handler.get_mapping_for_address(offset) d = m.read_struct(offset, self.ctypes5_gen32.struct_d) ret = self.validator.load_members(d, 10) self.assertTrue(self.memory_handler.is_valid_address(d.a)) self.assertTrue(self.memory_handler.is_valid_address(d.b)) self.assertTrue(self.memory_handler.is_valid_address(d.d)) self.assertTrue(self.memory_handler.is_valid_address(d.h)) pass def test_is_valid_address_value(self): offset = self.offsets['struct_d'][0] m = self.memory_handler.get_mapping_for_address(offset) d = m.read_struct(offset, self.ctypes5_gen32.struct_d) ret = self.validator.load_members(d, 10) self.assertTrue(self.memory_handler.is_valid_address(d.a.value)) self.assertTrue(self.memory_handler.is_valid_address(d.b.value)) self.assertTrue(self.memory_handler.is_valid_address(d.d.value)) self.assertTrue(self.memory_handler.is_valid_address(d.h.value)) pass class TestMappingsWin32(unittest.TestCase): @classmethod def setUpClass(cls): cls.memory_handler = folder.load('test/dumps/putty/putty.1.dump') cls.my_target = cls.memory_handler.get_target_platform() cls.my_ctypes = cls.my_target.get_target_ctypes() cls.my_utils = cls.my_target.get_target_ctypes_utils() @classmethod def tearDownClass(cls): cls.memory_handler.reset_mappings() cls.memory_handler = None cls.my_target = None cls.my_ctypes = None cls.my_utils = None def test_get_mapping(self): # FIXME: remove with self.assertRaises(IndexError): self.assertEqual(len(self.memory_handler._get_mapping('[heap]')), 1) self.assertEqual(len(self.memory_handler._get_mapping('None')), 71) def test_get_mapping_for_address(self): m = self.memory_handler.get_mapping_for_address(0x005c0000) self.assertNotEquals(m, False) self.assertEqual(m.start, 0x005c0000) self.assertEqual(m.end, 0x00619000) def test_contains(self): for m in self.memory_handler: self.assertTrue(m.start in self.memory_handler) self.assertTrue((m.end - 1) in self.memory_handler) def test_len(self): self.assertEqual(len(self.memory_handler), 403) def test_getitem(self): self.assertTrue(isinstance(self.memory_handler[0], AMemoryMapping)) self.assertTrue( isinstance(self.memory_handler[len(self.memory_handler) - 1], AMemoryMapping)) with self.assertRaises(IndexError): self.memory_handler[0x0005c000] def test_iter(self): mps = [m for m in self.memory_handler] mps2 = [m for m in self.memory_handler.get_mappings()] self.assertEqual(mps, mps2) def test_setitem(self): with self.assertRaises(NotImplementedError): self.memory_handler[0x0005c000] = 1 def test_get_os_name(self): x = self.memory_handler.get_target_platform().get_os_name() self.assertEqual(x, 'win7') def test_get_cpu_bits(self): x = self.memory_handler.get_target_platform().get_cpu_bits() self.assertEqual(x, 32) class TestReferenceBook(unittest.TestCase): """Test the reference book.""" def setUp(self): self.memory_handler = folder.load('test/src/test-ctypes6.32.dump') def tearDown(self): self.memory_handler.reset_mappings() self.memory_handler = None def test_keepRef(self): self.assertEqual(len(self.memory_handler.getRefByAddr(0xcafecafe)), 0) self.assertEqual(len(self.memory_handler.getRefByAddr(0xdeadbeef)), 0) # same address, same type self.memory_handler.keepRef(1, int, 0xcafecafe) self.memory_handler.keepRef(2, int, 0xcafecafe) self.memory_handler.keepRef(3, int, 0xcafecafe) me = self.memory_handler.getRefByAddr(0xcafecafe) # only one ref ( the first) self.assertEqual(len(me), 1) # different type, same address self.memory_handler.keepRef('4', str, 0xcafecafe) me = self.memory_handler.getRefByAddr(0xcafecafe) # multiple refs self.assertEqual(len(me), 2) return def test_hasRef(self): self.assertEqual(len(self.memory_handler.getRefByAddr(0xcafecafe)), 0) self.assertEqual(len(self.memory_handler.getRefByAddr(0xdeadbeef)), 0) # same address, different types self.memory_handler.keepRef(1, int, 0xcafecafe) self.memory_handler.keepRef(2, float, 0xcafecafe) self.memory_handler.keepRef(3, str, 0xcafecafe) self.assertTrue(self.memory_handler.hasRef(int, 0xcafecafe)) self.assertTrue(self.memory_handler.hasRef(float, 0xcafecafe)) self.assertTrue(self.memory_handler.hasRef(str, 0xcafecafe)) self.assertFalse(self.memory_handler.hasRef(int, 0xdeadbeef)) me = self.memory_handler.getRefByAddr(0xcafecafe) # multiple refs self.assertEqual(len(me), 3) def test_getRef(self): self.assertEqual(len(self.memory_handler.getRefByAddr(0xcafecafe)), 0) self.assertEqual(len(self.memory_handler.getRefByAddr(0xdeadbeef)), 0) self.memory_handler.keepRef(1, int, 0xcafecafe) self.memory_handler.keepRef(2, float, 0xcafecafe) self.assertEqual(self.memory_handler.getRef(int, 0xcafecafe), 1) self.assertEqual(self.memory_handler.getRef(float, 0xcafecafe), 2) self.assertIsNone(self.memory_handler.getRef(str, 0xcafecafe)) self.assertIsNone(self.memory_handler.getRef(str, 0xdeadbeef)) self.assertIsNone(self.memory_handler.getRef(int, 0xdeadbeef)) def test_delRef(self): self.assertEqual(len(self.memory_handler.getRefByAddr(0xcafecafe)), 0) self.assertEqual(len(self.memory_handler.getRefByAddr(0xdeadbeef)), 0) self.memory_handler.keepRef(1, int, 0xcafecafe) self.memory_handler.keepRef(2, float, 0xcafecafe) self.memory_handler.keepRef(3, str, 0xcafecafe) self.assertTrue(self.memory_handler.hasRef(int, 0xcafecafe)) self.assertTrue(self.memory_handler.hasRef(float, 0xcafecafe)) self.assertTrue(self.memory_handler.hasRef(str, 0xcafecafe)) # del one type self.memory_handler.delRef(str, 0xcafecafe) self.assertTrue(self.memory_handler.hasRef(int, 0xcafecafe)) self.assertTrue(self.memory_handler.hasRef(float, 0xcafecafe)) self.assertFalse(self.memory_handler.hasRef(str, 0xcafecafe)) # try harder, same type, same result self.memory_handler.delRef(str, 0xcafecafe) self.assertTrue(self.memory_handler.hasRef(int, 0xcafecafe)) self.assertTrue(self.memory_handler.hasRef(float, 0xcafecafe)) self.assertFalse(self.memory_handler.hasRef(str, 0xcafecafe)) self.memory_handler.delRef(int, 0xcafecafe) self.assertFalse(self.memory_handler.hasRef(int, 0xcafecafe)) self.assertTrue(self.memory_handler.hasRef(float, 0xcafecafe)) self.assertFalse(self.memory_handler.hasRef(str, 0xcafecafe)) self.memory_handler.delRef(float, 0xcafecafe) self.assertFalse(self.memory_handler.hasRef(int, 0xcafecafe)) self.assertFalse(self.memory_handler.hasRef(float, 0xcafecafe)) self.assertFalse(self.memory_handler.hasRef(str, 0xcafecafe)) if __name__ == '__main__': # logging.basicConfig(level=logging.DEBUG) logging.basicConfig(level=logging.INFO) # logging.getLogger('memory_mapping').setLevel(logging.DEBUG) # logging.getLogger('basicmodel').setLevel(logging.INFO) # logging.getLogger('model').setLevel(logging.INFO) # logging.getLogger('listmodel').setLevel(logging.INFO) unittest.main(verbosity=2)
#!/usr/bin/env python """ file: tests/CMIP5/db_fixture.py author: Scott Wales <scott.wales@unimelb.edu.au> Copyright 2015 ARC Centre of Excellence for Climate Systems Science Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. """ from __future__ import print_function import pytest from ARCCSSive import CMIP5 from ARCCSSive.CMIP5.Model import * from sqlalchemy.orm.exc import NoResultFound from datetime import date def insert_unique(db, klass, **kwargs): """ Insert an item into the DB if it can't be found """ try: value = db.query(klass).filter_by(**kwargs).one() except NoResultFound: value = klass(**kwargs) db.add(value) db.commit() return value def retrieve_item(db, klass, **kwargs): """ Retrieve an item into the DB if it can be found """ try: value = db.query(klass).filter_by(**kwargs).one() except NoResultFound: print( "Cannot find fixture with ", kwargs) return value def add_instance_item(db, variable, mip, model, experiment, ensemble, realm): """ Add a new test instance item to the DB """ instance = insert_unique(db, Instance, variable = variable, mip = mip, model = model, experiment = experiment, ensemble = ensemble, realm = realm) return instance.id def add_version_item(db, instance_id, path, is_latest, checked_on, to_update, dataset_id, version): #def add_version_item(db, **kwargs): """ Add a new test version item to the DB """ #version = insert_unique(db, Version,**kwargs) version = insert_unique(db, Version, instance_id = instance_id, path = path, is_latest = is_latest, checked_on = checked_on, to_update = to_update, dataset_id = dataset_id, version = version) return version.id def add_warning_item(db, version_id, warning, added_by, added_on): """ Add a new test warning item to the DB """ warning = insert_unique(db, VersionWarning, version_id = version_id, warning = warning, added_on = added_on, added_by = added_by) def add_file_item(db, version_id, filename, md5, sha256): """ Add a new test file item to the DB """ afile = insert_unique(db, VersionFile, version_id = version_id, filename = filename, md5 = md5, sha256 = sha256) @pytest.fixture(scope="module") def session(request, tmpdir_factory): session = CMIP5.connect('sqlite:///:memory:') dira = tmpdir_factory.mktemp('a') dirb = tmpdir_factory.mktemp('b') # Create some example entries db = session.session added_on=date.today() inst1_id = add_instance_item(db, variable = 'a', mip = '6hrLev', model = 'c', experiment = 'd', ensemble = 'e', realm = 'realm') v11_id = add_version_item(db, instance_id = inst1_id, path = dira.strpath, is_latest = False, checked_on = added_on, to_update = False, dataset_id = 'someid', version = 'v20111201') v12_id = add_version_item(db, instance_id = inst1_id, path = dira.strpath, is_latest = False, checked_on = added_on, to_update = False, dataset_id = 'someid', version = 'v20120101') v13_id = add_version_item(db, instance_id = inst1_id, path = dira.strpath, is_latest = False, checked_on = added_on, to_update = False, dataset_id = 'someid', version = 'NA') inst2_id = add_instance_item(db, variable = 'f', mip = 'cfMon', model = 'c', experiment = 'd', ensemble = 'e', realm = 'realm') v21_id = add_version_item(db, instance_id = inst2_id, path = dirb.strpath, is_latest = True, checked_on = added_on, to_update = False, dataset_id = 'someid', version = 'v20111201') v22_id = add_version_item(db, instance_id = inst2_id, path = dirb.strpath, is_latest = False, checked_on = added_on, to_update = False, dataset_id = 'someid', version = 'v20120101') add_warning_item(db, version_id = v11_id, warning = 'Test warning for inst1 v20111201', added_by = 'someone@example.com', added_on = added_on) add_warning_item(db, version_id = v12_id, warning = 'Test warning for inst1 v20120101', added_by = 'someone@example.com', added_on = added_on) add_file_item(db, version_id = v22_id, filename = 'Somefilename', md5 = 'Somemd5', sha256 = 'Somesha256') add_file_item(db, version_id = v22_id, filename = 'Anotherfilename', md5 = 'Anothermd5', sha256 = 'Anothersha256') add_warning_item(db, version_id = v21_id, warning = 'Test warning for inst2 v20111201', added_by = 'anyone@example.com', added_on = added_on) inst = add_instance_item(db, variable = 'tas', mip = 'Amon', model = 'ACCESS1-3', experiment = 'rcp45', ensemble = 'r1i1p1', realm = 'realm') vers = add_version_item(db, instance_id = inst, path = dirb.strpath, is_latest = False, checked_on = added_on, to_update = False, dataset_id = 'someid', version = 'v20130507') add_file_item(db, version_id = vers, filename = 'example.nc', md5 = None, sha256 = None) # add more instances to test unique function inst0 = add_instance_item(db, variable = 'tas', mip = 'Amon', model = 'ACCESS1-3', experiment = 'rcp26', ensemble = 'r1i1p1', realm = 'realm') inst0 = add_instance_item(db, variable = 'a', mip = 'Amon', model = 'MIROC5', experiment = 'rcp26', ensemble = 'r1i1p1', realm = 'realm') inst0 = add_instance_item(db, variable = 'a', mip = '6hrLev', model = 'MIROC5', experiment = 'rcp45', ensemble = 'r2i1p1', realm = 'realm') inst0 = add_instance_item(db, variable = 'tas', mip = 'cfMon', model = 'MIROC5', experiment = 'rcp45', ensemble = 'r2i1p1', realm = 'realm') db.commit() # Close the session def fin(): db.close() request.addfinalizer(fin) return session
# -*- coding: utf-8 -*- ############################################################################## # # OpenERP, Open Source Management Solution # Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>). # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## import base64 from tempfile import TemporaryFile from openerp import tools from openerp.osv import osv, fields class base_language_import(osv.osv_memory): """ Language Import """ _name = "base.language.import" _description = "Language Import" _columns = { 'name': fields.char('Language Name', required=True), 'code': fields.char('ISO Code', size=5, help="ISO Language and Country code, e.g. en_US", required=True), 'data': fields.binary('File', required=True), 'overwrite': fields.boolean('Overwrite Existing Terms', help="If you enable this option, existing translations (including custom ones) " "will be overwritten and replaced by those in this file"), } def import_lang(self, cr, uid, ids, context=None): if context is None: context = {} this = self.browse(cr, uid, ids[0]) if this.overwrite: context = dict(context, overwrite=True) fileobj = TemporaryFile('w+') try: fileobj.write(base64.decodestring(this.data)) # now we determine the file format fileobj.seek(0) first_line = fileobj.readline().strip().replace('"', '').replace(' ', '') fileformat = first_line.endswith("type,name,res_id,src,value") and 'csv' or 'po' fileobj.seek(0) tools.trans_load_data(cr, fileobj, fileformat, this.code, lang_name=this.name, context=context) finally: fileobj.close() return True # vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
from pip.req import InstallRequirement, RequirementSet, parse_requirements from pip.basecommand import Command from pip.exceptions import InstallationError class UninstallCommand(Command): """ Uninstall packages. pip is able to uninstall most installed packages. Known exceptions are: - Pure distutils packages installed with ``python setup.py install``, which leave behind no metadata to determine what files were installed. - Script wrappers installed by ``python setup.py develop``. """ name = 'uninstall' usage = """ %prog [options] <package> ... %prog [options] -r <requirements file> ...""" summary = 'Uninstall packages.' def __init__(self, *args, **kw): super(UninstallCommand, self).__init__(*args, **kw) self.cmd_opts.add_option( '-r', '--requirement', dest='requirements', action='append', default=[], metavar='file', help='Uninstall all the packages listed in the given requirements ' 'file. This option can be used multiple times.', ) self.cmd_opts.add_option( '-y', '--yes', dest='yes', action='store_true', help="Don't ask for confirmation of uninstall deletions.") self.parser.insert_option_group(0, self.cmd_opts) def run(self, options, args): session = self._build_session(options) requirement_set = RequirementSet( build_dir=None, src_dir=None, download_dir=None, session=session, ) for name in args: requirement_set.add_requirement( InstallRequirement.from_line(name)) for filename in options.requirements: for req in parse_requirements( filename, options=options, session=session): requirement_set.add_requirement(req) if not requirement_set.has_requirements: raise InstallationError( 'You must give at least one requirement to %(name)s (see "pip ' 'help %(name)s")' % dict(name=self.name) ) requirement_set.uninstall(auto_confirm=options.yes)
# Copyright 2014 IBM Corp. # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. """ Tests for volume replication API code. """ import json import mock from oslo_config import cfg import webob from cinder import context from cinder import test from cinder.tests.unit.api import fakes from cinder.tests.unit import utils as tests_utils CONF = cfg.CONF def app(): # no auth, just let environ['cinder.context'] pass through api = fakes.router.APIRouter() mapper = fakes.urlmap.URLMap() mapper['/v2'] = api return mapper class VolumeReplicationAPITestCase(test.TestCase): """Test Cases for replication API.""" def setUp(self): super(VolumeReplicationAPITestCase, self).setUp() self.ctxt = context.RequestContext('admin', 'fake', True) self.volume_params = { 'host': CONF.host, 'size': 1} def _get_resp(self, operation, volume_id, xml=False): """Helper for a replication action req for the specified volume_id.""" req = webob.Request.blank('/v2/fake/volumes/%s/action' % volume_id) req.method = 'POST' if xml: body = '<os-%s-replica/>' % operation req.headers['Content-Type'] = 'application/xml' req.headers['Accept'] = 'application/xml' req.body = body else: body = {'os-%s-replica' % operation: ''} req.headers['Content-Type'] = 'application/json' req.body = json.dumps(body) req.environ['cinder.context'] = context.RequestContext('admin', 'fake', True) res = req.get_response(app()) return req, res def test_promote_bad_id(self): (req, res) = self._get_resp('promote', 'fake') msg = ("request: %s\nresult: %s" % (req, res)) self.assertEqual(404, res.status_int, msg) def test_promote_bad_id_xml(self): (req, res) = self._get_resp('promote', 'fake', xml=True) msg = ("request: %s\nresult: %s" % (req, res)) self.assertEqual(404, res.status_int, msg) def test_promote_volume_not_replicated(self): volume = tests_utils.create_volume( self.ctxt, **self.volume_params) (req, res) = self._get_resp('promote', volume['id']) msg = ("request: %s\nresult: %s" % (req, res)) self.assertEqual(400, res.status_int, msg) def test_promote_volume_not_replicated_xml(self): volume = tests_utils.create_volume( self.ctxt, **self.volume_params) (req, res) = self._get_resp('promote', volume['id'], xml=True) msg = ("request: %s\nresult: %s" % (req, res)) self.assertEqual(400, res.status_int, msg) @mock.patch('cinder.volume.rpcapi.VolumeAPI.promote_replica') def test_promote_replication_volume_status(self, _rpcapi_promote): for status in ['error', 'in-use']: volume = tests_utils.create_volume(self.ctxt, status = status, replication_status = 'active', **self.volume_params) (req, res) = self._get_resp('promote', volume['id']) msg = ("request: %s\nresult: %s" % (req, res)) self.assertEqual(400, res.status_int, msg) for status in ['available']: volume = tests_utils.create_volume(self.ctxt, status = status, replication_status = 'active', **self.volume_params) (req, res) = self._get_resp('promote', volume['id']) msg = ("request: %s\nresult: %s" % (req, res)) self.assertEqual(202, res.status_int, msg) @mock.patch('cinder.volume.rpcapi.VolumeAPI.promote_replica') def test_promote_replication_volume_status_xml(self, _rpcapi_promote): for status in ['error', 'in-use']: volume = tests_utils.create_volume(self.ctxt, status = status, replication_status = 'active', **self.volume_params) (req, res) = self._get_resp('promote', volume['id'], xml=True) msg = ("request: %s\nresult: %s" % (req, res)) self.assertEqual(400, res.status_int, msg) for status in ['available']: volume = tests_utils.create_volume(self.ctxt, status = status, replication_status = 'active', **self.volume_params) (req, res) = self._get_resp('promote', volume['id'], xml=True) msg = ("request: %s\nresult: %s" % (req, res)) self.assertEqual(202, res.status_int, msg) @mock.patch('cinder.volume.rpcapi.VolumeAPI.promote_replica') def test_promote_replication_replication_status(self, _rpcapi_promote): for status in ['error', 'copying', 'inactive']: volume = tests_utils.create_volume(self.ctxt, status = 'available', replication_status = status, **self.volume_params) (req, res) = self._get_resp('promote', volume['id']) msg = ("request: %s\nresult: %s" % (req, res)) self.assertEqual(400, res.status_int, msg) for status in ['active', 'active-stopped']: volume = tests_utils.create_volume(self.ctxt, status = 'available', replication_status = status, **self.volume_params) (req, res) = self._get_resp('promote', volume['id']) msg = ("request: %s\nresult: %s" % (req, res)) self.assertEqual(202, res.status_int, msg) @mock.patch('cinder.volume.rpcapi.VolumeAPI.promote_replica') def test_promote_replication_replication_status_xml(self, _rpcapi_promote): for status in ['error', 'copying', 'inactive']: volume = tests_utils.create_volume(self.ctxt, status = 'available', replication_status = status, **self.volume_params) (req, res) = self._get_resp('promote', volume['id'], xml=True) msg = ("request: %s\nresult: %s" % (req, res)) self.assertEqual(400, res.status_int, msg) for status in ['active', 'active-stopped']: volume = tests_utils.create_volume(self.ctxt, status = 'available', replication_status = status, **self.volume_params) (req, res) = self._get_resp('promote', volume['id'], xml=True) msg = ("request: %s\nresult: %s" % (req, res)) self.assertEqual(202, res.status_int, msg) def test_reenable_bad_id(self): (req, res) = self._get_resp('reenable', 'fake') msg = ("request: %s\nresult: %s" % (req, res)) self.assertEqual(404, res.status_int, msg) def test_reenable_bad_id_xml(self): (req, res) = self._get_resp('reenable', 'fake', xml=True) msg = ("request: %s\nresult: %s" % (req, res)) self.assertEqual(404, res.status_int, msg) def test_reenable_volume_not_replicated(self): volume = tests_utils.create_volume( self.ctxt, **self.volume_params) (req, res) = self._get_resp('reenable', volume['id']) msg = ("request: %s\nresult: %s" % (req, res)) self.assertEqual(400, res.status_int, msg) def test_reenable_volume_not_replicated_xml(self): volume = tests_utils.create_volume( self.ctxt, **self.volume_params) (req, res) = self._get_resp('reenable', volume['id'], xml=True) msg = ("request: %s\nresult: %s" % (req, res)) self.assertEqual(400, res.status_int, msg) @mock.patch('cinder.volume.rpcapi.VolumeAPI.reenable_replication') def test_reenable_replication_replication_status(self, _rpcapi_promote): for status in ['active', 'copying']: volume = tests_utils.create_volume(self.ctxt, status = 'available', replication_status = status, **self.volume_params) (req, res) = self._get_resp('reenable', volume['id']) msg = ("request: %s\nresult: %s" % (req, res)) self.assertEqual(400, res.status_int, msg) for status in ['inactive', 'active-stopped', 'error']: volume = tests_utils.create_volume(self.ctxt, status = 'available', replication_status = status, **self.volume_params) (req, res) = self._get_resp('reenable', volume['id']) msg = ("request: %s\nresult: %s" % (req, res)) self.assertEqual(202, res.status_int, msg) @mock.patch('cinder.volume.rpcapi.VolumeAPI.reenable_replication') def test_reenable_replication_replication_status_xml(self, _rpcapi_promote): for status in ['active', 'copying']: volume = tests_utils.create_volume(self.ctxt, status = 'available', replication_status = status, **self.volume_params) (req, res) = self._get_resp('reenable', volume['id'], xml=True) msg = ("request: %s\nresult: %s" % (req, res)) self.assertEqual(400, res.status_int, msg) for status in ['inactive', 'active-stopped', 'error']: volume = tests_utils.create_volume(self.ctxt, status = 'available', replication_status = status, **self.volume_params) (req, res) = self._get_resp('reenable', volume['id'], xml=True) msg = ("request: %s\nresult: %s" % (req, res)) self.assertEqual(202, res.status_int, msg)
# Copyright (c) 2012 Cloudera, Inc. All rights reserved. # Unit tests for the test file parser # import logging import pytest from tests.util.test_file_parser import * from tests.common.base_test_suite import BaseTestSuite test_text = """ # Text before in the header (before the first ====) should be ignored # so put this here to test it out. ==== ---- QUERY # comment SELECT blah from Foo s ---- RESULTS 'Hi' ---- TYPES string ==== ---- QUERY SELECT 2 ---- RESULTS 'Hello' ---- TYPES string #==== # SHOULD PARSE COMMENTED OUT TEST PROPERLY #---- QUERY: TEST_WORKLOAD_Q2 #SELECT int_col from Bar #---- RESULTS #231 #---- TYPES #int ==== ---- QUERY: TEST_WORKLOAD_Q2 SELECT int_col from Bar ---- RESULTS 231 ---- TYPES int ==== """ VALID_SECTIONS = ['QUERY', 'RESULTS', 'TYPES'] class TestTestFileParser(BaseTestSuite): def test_valid_parse(self): results = parse_test_file_text(test_text, VALID_SECTIONS) assert len(results) == 3 print results[0] expected_results = {'QUERY': '# comment\nSELECT blah from Foo\ns', 'TYPES': 'string', 'RESULTS': "'Hi'"} assert results[0] == expected_results def test_invalid_section(self): # Restrict valid sections to exclude one of the section names. valid_sections = ['QUERY', 'RESULTS'] results = parse_test_file_text(test_text, valid_sections, skip_unknown_sections=True) assert len(results) == 3 expected_results = {'QUERY': '# comment\nSELECT blah from Foo\ns', 'RESULTS': "'Hi'"} assert results[0] == expected_results # In this case, instead of ignoring the invalid section we should get an error try: results = parse_test_file_text(test_text, valid_sections, skip_unknown_sections=False) assert 0, 'Expected error due to invalid section' except RuntimeError as re: assert re.message == "Unknown subsection: TYPES" def test_parse_query_name(self): results = parse_test_file_text(test_text, VALID_SECTIONS, False) assert len(results) == 3 expected_results = {'QUERY': 'SELECT int_col from Bar', 'TYPES': 'int', 'RESULTS': '231', 'QUERY_NAME': 'TEST_WORKLOAD_Q2'} assert results[2] == expected_results def test_parse_commented_out_test_as_comment(self): results = parse_test_file_text(test_text, VALID_SECTIONS) assert len(results) == 3 expected_results = {'QUERY': 'SELECT 2', 'RESULTS': "'Hello'", 'TYPES': "string\n#====\n"\ "# SHOULD PARSE COMMENTED OUT TEST PROPERLY\n"\ "#---- QUERY: TEST_WORKLOAD_Q2\n"\ "#SELECT int_col from Bar\n"\ "#---- RESULTS\n#231\n#---- TYPES\n#int"} print expected_results print results[1] assert results[1] == expected_results
# This Source Code Form is subject to the terms of the Mozilla Public # License, v. 2.0. If a copy of the MPL was not distributed with this # file, You can obtain one at http://mozilla.org/MPL/2.0/. import datetime from email.utils import formatdate import time from django.conf import settings from django.core.exceptions import MiddlewareNotUsed from django_statsd.middleware import GraphiteRequestTimingMiddleware class CacheMiddleware(object): def process_response(self, request, response): cache = (request.method != 'POST' and response.status_code != 404 and 'Cache-Control' not in response) if cache: d = datetime.datetime.now() + datetime.timedelta(minutes=10) stamp = time.mktime(d.timetuple()) response['Cache-Control'] = 'max-age=600' response['Expires'] = formatdate(timeval=stamp, localtime=False, usegmt=True) return response class MozorgRequestTimingMiddleware(GraphiteRequestTimingMiddleware): def process_view(self, request, view, view_args, view_kwargs): if hasattr(view, 'page_name'): request._view_module = 'page' request._view_name = view.page_name.replace('/', '.') request._start_time = time.time() else: f = super(MozorgRequestTimingMiddleware, self) f.process_view(request, view, view_args, view_kwargs) class ClacksOverheadMiddleware(object): # bug 1144901 @staticmethod def process_response(request, response): if response.status_code == 200: response['X-Clacks-Overhead'] = 'GNU Terry Pratchett' return response class HostnameMiddleware(object): def __init__(self): if not settings.ENABLE_HOSTNAME_MIDDLEWARE: raise MiddlewareNotUsed values = [getattr(settings, x) for x in ['HOSTNAME', 'DEIS_APP', 'DEIS_DOMAIN']] self.backend_server = '.'.join(x for x in values if x) def process_response(self, request, response): response['X-Backend-Server'] = self.backend_server return response class VaryNoCacheMiddleware(object): def __init__(self): if not settings.ENABLE_VARY_NOCACHE_MIDDLEWARE: raise MiddlewareNotUsed @staticmethod def process_response(request, response): if 'vary' in response: path = request.path if path != '/' and not any(path.startswith(x) for x in settings.VARY_NOCACHE_EXEMPT_URL_PREFIXES): del response['vary'] del response['expires'] response['Cache-Control'] = 'max-age=0' return response
# -*- coding:utf-8 -*- from mako import runtime, filters, cache UNDEFINED = runtime.UNDEFINED STOP_RENDERING = runtime.STOP_RENDERING __M_dict_builtin = dict __M_locals_builtin = locals _magic_number = 10 _modified_time = 1443802885.4031692 _enable_loop = True _template_filename = '/usr/local/lib/python3.4/dist-packages/nikola/data/themes/base/templates/comments_helper_googleplus.tmpl' _template_uri = 'comments_helper_googleplus.tmpl' _source_encoding = 'utf-8' _exports = ['comment_link_script', 'comment_form', 'comment_link'] def render_body(context,**pageargs): __M_caller = context.caller_stack._push_frame() try: __M_locals = __M_dict_builtin(pageargs=pageargs) __M_writer = context.writer() __M_writer('\n\n') __M_writer('\n\n') __M_writer('\n') return '' finally: context.caller_stack._pop_frame() def render_comment_link_script(context): __M_caller = context.caller_stack._push_frame() try: __M_writer = context.writer() __M_writer('\n') return '' finally: context.caller_stack._pop_frame() def render_comment_form(context,url,title,identifier): __M_caller = context.caller_stack._push_frame() try: __M_writer = context.writer() __M_writer('\n<script src="https://apis.google.com/js/plusone.js"></script>\n<div class="g-comments"\n data-href="') __M_writer(str(url)) __M_writer('"\n data-first_party_property="BLOGGER"\n data-view_type="FILTERED_POSTMOD">\n</div>\n') return '' finally: context.caller_stack._pop_frame() def render_comment_link(context,link,identifier): __M_caller = context.caller_stack._push_frame() try: __M_writer = context.writer() __M_writer('\n<div class="g-commentcount" data-href="') __M_writer(str(link)) __M_writer('"></div>\n<script src="https://apis.google.com/js/plusone.js"></script>\n') return '' finally: context.caller_stack._pop_frame() """ __M_BEGIN_METADATA {"uri": "comments_helper_googleplus.tmpl", "source_encoding": "utf-8", "filename": "/usr/local/lib/python3.4/dist-packages/nikola/data/themes/base/templates/comments_helper_googleplus.tmpl", "line_map": {"33": 16, "39": 2, "57": 12, "43": 2, "44": 5, "45": 5, "16": 0, "51": 11, "21": 9, "22": 14, "23": 17, "56": 12, "55": 11, "29": 16, "63": 57}} __M_END_METADATA """
# Copyright 2012 Google Inc. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS-IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. """Generic object editor view that uses REST services.""" __author__ = 'Pavel Simakov (psimakov@google.com)' import os import urllib import appengine_config from common import jinja_utils from common import schema_fields from common import tags from controllers import utils import jinja2 from models import custom_modules from models import transforms import webapp2 # a set of YUI and inputex modules required by the editor COMMON_REQUIRED_MODULES = [ 'inputex-group', 'inputex-form', 'inputex-jsonschema'] ALL_MODULES = [ 'querystring-stringify-simple', 'inputex-select', 'inputex-string', 'inputex-radio', 'inputex-date', 'inputex-datepicker', 'inputex-checkbox', 'inputex-list', 'inputex-color', 'gcb-rte', 'inputex-textarea', 'inputex-url', 'inputex-uneditable', 'inputex-integer', 'inputex-hidden', 'inputex-file', 'io-upload-iframe'] class ObjectEditor(object): """Generic object editor powered by jsonschema.""" @classmethod def get_html_for( cls, handler, schema_json, annotations, object_key, rest_url, exit_url, extra_args=None, save_method='put', delete_url=None, delete_message=None, delete_method='post', auto_return=False, read_only=False, required_modules=None, extra_js_files=None, delete_button_caption='Delete', save_button_caption='Save', exit_button_caption='Close'): """Creates an HTML code needed to embed and operate this form. This method creates an HTML, JS and CSS required to embed JSON schema-based object editor into a view. Args: handler: a BaseHandler class, which will host this HTML, JS and CSS schema_json: a text of JSON schema for the object being edited annotations: schema annotations dictionary object_key: a key of an object being edited rest_url: a REST endpoint for object GET/PUT operation exit_url: a URL to go to after the editor form is dismissed extra_args: extra request params passed back in GET and POST save_method: how the data should be saved to the server (put|upload) delete_url: optional URL for delete operation delete_message: string. Optional custom delete confirmation message delete_method: optional HTTP method for delete operation auto_return: whether to return to the exit_url on successful save read_only: optional flag; if set, removes Save and Delete operations required_modules: list of inputex modules required for this editor extra_js_files: list of extra JS files to be included delete_button_caption: string. A caption for the 'Delete' button save_button_caption: a caption for the 'Save' button exit_button_caption: a caption for the 'Close' button Returns: The HTML, JS and CSS text that will instantiate an object editor. """ required_modules = required_modules or ALL_MODULES if not delete_message: kind = transforms.loads(schema_json).get('description') if not kind: kind = 'Generic Object' delete_message = 'Are you sure you want to delete this %s?' % kind # construct parameters get_url = rest_url get_args = {'key': object_key} post_url = rest_url post_args = {'key': object_key} if extra_args: get_args.update(extra_args) post_args.update(extra_args) if read_only: post_url = '' post_args = '' custom_rte_tag_icons = [] for tag, tag_class in tags.get_tag_bindings().items(): custom_rte_tag_icons.append({ 'name': tag, 'iconUrl': tag_class().get_icon_url()}) template_values = { 'enabled': custom_module.enabled, 'schema': schema_json, 'get_url': '%s?%s' % (get_url, urllib.urlencode(get_args, True)), 'save_url': post_url, 'save_args': transforms.dumps(post_args), 'exit_button_caption': exit_button_caption, 'exit_url': exit_url, 'required_modules': COMMON_REQUIRED_MODULES + required_modules, 'extra_js_files': extra_js_files or [], 'schema_annotations': [ (item[0], transforms.dumps(item[1])) for item in annotations], 'save_method': save_method, 'auto_return': auto_return, 'delete_button_caption': delete_button_caption, 'save_button_caption': save_button_caption, 'custom_rte_tag_icons': transforms.dumps(custom_rte_tag_icons), 'delete_message': delete_message, } if delete_url and not read_only: template_values['delete_url'] = delete_url if delete_method: template_values['delete_method'] = delete_method if appengine_config.BUNDLE_LIB_FILES: template_values['bundle_lib_files'] = True return jinja2.utils.Markup(handler.get_template( 'oeditor.html', [os.path.dirname(__file__)] ).render(template_values)) class PopupHandler(webapp2.RequestHandler, utils.ReflectiveRequestHandler): """A handler to serve the content of the popup subeditor.""" default_action = 'custom_tag' get_actions = ['edit_custom_tag', 'add_custom_tag'] post_actions = [] def get_template(self, template_name, dirs): """Sets up an environment and Gets jinja template.""" return jinja_utils.get_template( template_name, dirs + [os.path.dirname(__file__)]) def get_edit_custom_tag(self): """Return the the page used to edit a custom HTML tag in a popup.""" tag_name = self.request.get('tag_name') tag_bindings = tags.get_tag_bindings() tag_class = tag_bindings[tag_name] schema = tag_class().get_schema(self) if schema.has_subregistries(): raise NotImplementedError() template_values = {} template_values['form_html'] = ObjectEditor.get_html_for( self, schema.get_json_schema(), schema.get_schema_dict(), None, None, None) self.response.out.write( self.get_template('popup.html', []).render(template_values)) def get_add_custom_tag(self): """Return the page for the popup used to add a custom HTML tag.""" tag_name = self.request.get('tag_name') tag_bindings = tags.get_tag_bindings() select_data = [] for name in tag_bindings.keys(): clazz = tag_bindings[name] select_data.append((name, '%s: %s' % ( clazz.vendor(), clazz.name()))) select_data = sorted(select_data, key=lambda pair: pair[1]) if tag_name: tag_class = tag_bindings[tag_name] else: tag_class = tag_bindings[select_data[0][0]] tag_schema = tag_class().get_schema(self) schema = schema_fields.FieldRegistry('Add a Component') type_select = schema.add_sub_registry('type', 'Component Type') type_select.add_property(schema_fields.SchemaField( 'tag', 'Name', 'string', select_data=select_data)) schema.add_sub_registry('attributes', registry=tag_schema) template_values = {} template_values['form_html'] = ObjectEditor.get_html_for( self, schema.get_json_schema(), schema.get_schema_dict(), None, None, None, required_modules=tag_class.required_modules(), extra_js_files=['add_custom_tag.js']) self.response.out.write( self.get_template('popup.html', []).render(template_values)) def create_bool_select_annotation( keys_list, label, true_label, false_label, class_name=None, description=None): """Creates inputex annotation to display bool type as a select.""" properties = { 'label': label, 'choices': [ {'value': True, 'label': true_label}, {'value': False, 'label': false_label}]} if class_name: properties['className'] = class_name if description: properties['description'] = description return (keys_list, {'type': 'select', '_inputex': properties}) custom_module = None def register_module(): """Registers this module in the registry.""" from controllers import sites # pylint: disable-msg=g-import-not-at-top yui_handlers = [ ('/static/inputex-3.1.0/(.*)', sites.make_zip_handler( os.path.join( appengine_config.BUNDLE_ROOT, 'lib/inputex-3.1.0.zip'))), ('/static/yui_3.6.0/(.*)', sites.make_zip_handler( os.path.join( appengine_config.BUNDLE_ROOT, 'lib/yui_3.6.0.zip'))), ('/static/2in3/(.*)', sites.make_zip_handler( os.path.join( appengine_config.BUNDLE_ROOT, 'lib/yui_2in3-2.9.0.zip')))] if appengine_config.BUNDLE_LIB_FILES: yui_handlers += [ ('/static/combo/inputex', sites.make_css_combo_zip_handler( os.path.join( appengine_config.BUNDLE_ROOT, 'lib/inputex-3.1.0.zip'), '/static/inputex-3.1.0/')), ('/static/combo/yui', sites.make_css_combo_zip_handler( os.path.join(appengine_config.BUNDLE_ROOT, 'lib/yui_3.6.0.zip'), '/yui/')), ('/static/combo/2in3', sites.make_css_combo_zip_handler( os.path.join( appengine_config.BUNDLE_ROOT, 'lib/yui_2in3-2.9.0.zip'), '/static/2in3/'))] oeditor_handlers = [('/oeditorpopup', PopupHandler)] global custom_module custom_module = custom_modules.Module( 'Object Editor', 'A visual editor for editing various types of objects.', yui_handlers, oeditor_handlers) return custom_module
from django import http, test from django.conf import settings from django.core.cache import cache from django.utils import translation import caching import pytest import amo from access.models import Group, GroupUser from translations.hold import clean_translations from users.models import UserProfile @pytest.fixture(autouse=True) def mock_inline_css(monkeypatch): """Mock jingo_minify.helpers.is_external: don't break on missing files. When testing, we don't want nor need the bundled/minified css files, so pretend that all the css files are external. Mocking this will prevent amo.helpers.inline_css to believe it should bundle the css. """ import amo.helpers monkeypatch.setattr(amo.helpers, 'is_external', lambda css: True) def prefix_indexes(config): """Prefix all ES index names and cache keys with `test_` and, if running under xdist, the ID of the current slave.""" if hasattr(config, 'slaveinput'): prefix = 'test_{[slaveid]}'.format(config.slaveinput) else: prefix = 'test' from django.conf import settings # Ideally, this should be a session-scoped fixture that gets injected into # any test that requires ES. This would be especially useful, as it would # allow xdist to transparently group all ES tests into a single process. # Unfurtunately, it's surprisingly difficult to achieve with our current # unittest-based setup. for key, index in settings.ES_INDEXES.items(): if not index.startswith(prefix): settings.ES_INDEXES[key] = '{prefix}_amo_{index}'.format( prefix=prefix, index=index) settings.CACHE_PREFIX = 'amo:{0}:'.format(prefix) settings.KEY_PREFIX = settings.CACHE_PREFIX def pytest_configure(config): prefix_indexes(config) @pytest.fixture(autouse=True, scope='session') def instrument_jinja(): """Make sure the "templates" list in a response is properly updated, even though we're using Jinja2 and not the default django template engine.""" import jinja2 old_render = jinja2.Template.render def instrumented_render(self, *args, **kwargs): context = dict(*args, **kwargs) test.signals.template_rendered.send( sender=self, template=self, context=context) return old_render(self, *args, **kwargs) jinja2.Template.render = instrumented_render def default_prefixer(): """Make sure each test starts with a default URL prefixer.""" request = http.HttpRequest() request.META['SCRIPT_NAME'] = '' prefixer = amo.urlresolvers.Prefixer(request) prefixer.app = settings.DEFAULT_APP prefixer.locale = settings.LANGUAGE_CODE amo.urlresolvers.set_url_prefix(prefixer) @pytest.fixture(autouse=True) def test_pre_setup(): cache.clear() # Override django-cache-machine caching.base.TIMEOUT because it's # computed too early, before settings_test.py is imported. caching.base.TIMEOUT = settings.CACHE_COUNT_TIMEOUT translation.trans_real.deactivate() # Django fails to clear this cache. translation.trans_real._translations = {} translation.trans_real.activate(settings.LANGUAGE_CODE) # Reset the prefixer. default_prefixer() @pytest.fixture(autouse=True) def test_post_teardown(): amo.set_user(None) clean_translations(None) # Make sure queued translations are removed. # Make sure we revert everything we might have changed to prefixers. amo.urlresolvers.clean_url_prefixes() @pytest.fixture def admin_group(db): """Create the Admins group.""" return Group.objects.create(name='Admins', rules='*:*') @pytest.fixture def mozilla_user(admin_group): """Create a "Mozilla User".""" user = UserProfile.objects.create(pk=settings.TASK_USER_ID, email='admin@mozilla.com', username='admin') user.set_password('password') user.save() GroupUser.objects.create(user=user, group=admin_group) return user
# Status: ported. # Base revision: 45462 # # Copyright 2003 Dave Abrahams # Copyright 2002, 2003, 2004, 2005 Vladimir Prus # Distributed under the Boost Software License, Version 1.0. # (See accompanying file LICENSE_1_0.txt or http://www.boost.org/LICENSE_1_0.txt) # Implements scanners: objects that compute implicit dependencies for # files, such as includes in C++. # # Scanner has a regular expression used to find dependencies, some # data needed to interpret those dependencies (for example, include # paths), and a code which actually established needed relationship # between actual jam targets. # # Scanner objects are created by actions, when they try to actualize # virtual targets, passed to 'virtual-target.actualize' method and are # then associated with actual targets. It is possible to use # several scanners for a virtual-target. For example, a single source # might be used by to compile actions, with different include paths. # In this case, two different actual targets will be created, each # having scanner of its own. # # Typically, scanners are created from target type and action's # properties, using the rule 'get' in this module. Directly creating # scanners is not recommended, because it might create many equvivalent # but different instances, and lead in unneeded duplication of # actual targets. However, actions can also create scanners in a special # way, instead of relying on just target type. import property import bjam import os from b2.manager import get_manager from b2.util import is_iterable_typed def reset (): """ Clear the module state. This is mainly for testing purposes. """ global __scanners, __rv_cache, __scanner_cache # Maps registered scanner classes to relevant properties __scanners = {} # A cache of scanners. # The key is: class_name.properties_tag, where properties_tag is the concatenation # of all relevant properties, separated by '-' __scanner_cache = {} reset () def register(scanner_class, relevant_properties): """ Registers a new generator class, specifying a set of properties relevant to this scanner. Ctor for that class should have one parameter: list of properties. """ assert issubclass(scanner_class, Scanner) assert isinstance(relevant_properties, basestring) __scanners[str(scanner_class)] = relevant_properties def registered(scanner_class): """ Returns true iff a scanner of that class is registered """ return str(scanner_class) in __scanners def get(scanner_class, properties): """ Returns an instance of previously registered scanner with the specified properties. """ assert issubclass(scanner_class, Scanner) assert is_iterable_typed(properties, basestring) scanner_name = str(scanner_class) if not registered(scanner_name): raise BaseException ("attempt to get unregisted scanner: %s" % scanner_name) relevant_properties = __scanners[scanner_name] r = property.select(relevant_properties, properties) scanner_id = scanner_name + '.' + '-'.join(r) if scanner_id not in __scanner_cache: __scanner_cache[scanner_id] = scanner_class(r) return __scanner_cache[scanner_id] class Scanner: """ Base scanner class. """ def __init__ (self): pass def pattern (self): """ Returns a pattern to use for scanning. """ raise BaseException ("method must be overriden") def process (self, target, matches, binding): """ Establish necessary relationship between targets, given actual target beeing scanned, and a list of pattern matches in that file. """ raise BaseException ("method must be overriden") # Common scanner class, which can be used when there's only one # kind of includes (unlike C, where "" and <> includes have different # search paths). class CommonScanner(Scanner): def __init__ (self, includes): Scanner.__init__(self) self.includes = includes def process(self, target, matches, binding): target_path = os.path.normpath(os.path.dirname(binding[0])) bjam.call("mark-included", target, matches) get_manager().engine().set_target_variable(matches, "SEARCH", [target_path] + self.includes) get_manager().scanners().propagate(self, matches) class ScannerRegistry: def __init__ (self, manager): self.manager_ = manager self.count_ = 0 self.exported_scanners_ = {} def install (self, scanner, target, vtarget): """ Installs the specified scanner on actual target 'target'. vtarget: virtual target from which 'target' was actualized. """ assert isinstance(scanner, Scanner) assert isinstance(target, basestring) assert isinstance(vtarget, basestring) engine = self.manager_.engine() engine.set_target_variable(target, "HDRSCAN", scanner.pattern()) if scanner not in self.exported_scanners_: exported_name = "scanner_" + str(self.count_) self.count_ = self.count_ + 1 self.exported_scanners_[scanner] = exported_name bjam.import_rule("", exported_name, scanner.process) else: exported_name = self.exported_scanners_[scanner] engine.set_target_variable(target, "HDRRULE", exported_name) # scanner reflects difference in properties affecting # binding of 'target', which will be known when processing # includes for it, will give information on how to # interpret quoted includes. engine.set_target_variable(target, "HDRGRIST", str(id(scanner))) pass def propagate(self, scanner, targets): assert isinstance(scanner, Scanner) assert is_iterable_typed(targets, basestring) or isinstance(targets, basestring) engine = self.manager_.engine() engine.set_target_variable(targets, "HDRSCAN", scanner.pattern()) engine.set_target_variable(targets, "HDRRULE", self.exported_scanners_[scanner]) engine.set_target_variable(targets, "HDRGRIST", str(id(scanner)))
import numpy as np import os import sys import HTSeq from IntervalTree import IntervalTree ########################## # Input of this script # ########################## # This script input a count table: # chr start end junctiontype count1 count2 ... countn # and a repeatitive region file in gtf format # specify minimum circular RNA length class Circfilter(object): def __init__(self, length, countthreshold, replicatethreshold, tmp_dir): ''' counttable: the circular RNA count file, typically generated by findcircRNA.py: chr start end junctiontype count1 count2 ... countn rep_file: the gtf file to specify the region of repeatitive reagion of analyzed genome length: the minimum length of circular RNAs countthreshold: the minimum expression level of junction type 1 circular RNAs ''' # self.counttable = counttable # self.rep_file = rep_file self.length = int(length) # self.level0 = int(level0) self.countthreshold = int(countthreshold) # self.threshold0 = int(threshold0) self.replicatethreshold = int(replicatethreshold) self.tmp_dir = tmp_dir # Read circRNA count and coordinates information to numpy array def readcirc(self, countfile, coordinates): # Read the circRNA count file circ = open(countfile, 'r') coor = open(coordinates, 'r') count = [] indx = [] for line in circ: fields = line.split('\t') # row_indx = [str(itm) for itm in fields[0:4]] # print row_indx try: row_count = [int(itm) for itm in fields[4:]] except ValueError: row_count = [float(itm) for itm in fields[4:]] count.append(row_count) # indx.append(row_indx) for line in coor: fields = line.split('\t') row_indx = [str(itm).strip() for itm in fields[0:6]] indx.append(row_indx) count = np.array(count) indx = np.array(indx) circ.close() return count, indx # Do filtering def filtercount(self, count, indx): print 'Filtering by read counts' sel = [] # store the passed filtering rows for itm in range(len(count)): if indx[itm][4] == '0': # if sum( count[itm] >= self.level0 ) >= self.threshold0: # sel.append(itm) pass elif indx[itm][4] != '0': if sum(count[itm] >= self.countthreshold) >= self.replicatethreshold: sel.append(itm) # splicing the passed filtering rows if len(sel) == 0: sys.exit("No circRNA passed the expression threshold filtering.") return count[sel], indx[sel] def read_rep_region(self, regionfile): regions = HTSeq.GFF_Reader(regionfile, end_included=True) rep_tree = IntervalTree() for feature in regions: iv = feature.iv rep_tree.insert(iv, annotation='.') return rep_tree def filter_nonrep(self, regionfile, indx0, count0): if not regionfile is None: rep_tree = self.read_rep_region(regionfile) def numpy_array_2_GenomiInterval(array): left = HTSeq.GenomicInterval(str(array[0]), int(array[1]), int(array[1]) + self.length, str(array[5])) right = HTSeq.GenomicInterval(str(array[0]), int(array[2]) - self.length, int(array[2]), str(array[5])) return left, right keep_index = [] for i, j in enumerate(indx0): out = [] left, right = numpy_array_2_GenomiInterval(j) rep_tree.intersect(left, lambda x: out.append(x)) rep_tree.intersect(right, lambda x: out.append(x)) if not out: # not in repetitive region keep_index.append(i) indx0 = indx0[keep_index] count0 = count0[keep_index] nonrep = np.column_stack((indx0, count0)) # write the result np.savetxt(self.tmp_dir + 'tmp_unsortedWithChrM', nonrep, delimiter='\t', newline='\n', fmt='%s') def dummy_filter(self, indx0, count0): nonrep = np.column_stack((indx0, count0)) # write the result np.savetxt(self.tmp_dir + 'tmp_unsortedWithChrM', nonrep, delimiter='\t', newline='\n', fmt='%s') def removeChrM(self, withChrM): print 'Remove ChrM' unremoved = open(withChrM, 'r').readlines() removed = [] for lines in unremoved: if not lines.startswith('chrM') and not lines.startswith('MT'): removed.append(lines) removedfile = open(self.tmp_dir + 'tmp_unsortedNoChrM', 'w') removedfile.writelines(removed) removedfile.close() def sortOutput(self, unsorted, outCount, outCoordinates, samplelist=None): # Sample list is a string with sample names seperated by \t. # Split used to split if coordinates information and count information are integrated count = open(outCount, 'w') coor = open(outCoordinates, 'w') if samplelist: count.write('Chr\tStart\tEnd\t' + samplelist + '\n') lines = open(unsorted).readlines() for line in lines: linesplit = [x.strip() for x in line.split('\t')] count.write('\t'.join(linesplit[0:3] + list(linesplit[6:])) + '\n') coor.write('\t'.join(linesplit[0:6]) + '\n') coor.close() count.close() def remove_tmp(self): try: os.remove(self.tmp_dir + 'tmp_left') os.remove(self.tmp_dir + 'tmp_right') os.remove(self.tmp_dir + 'tmp_unsortedWithChrM') os.remove(self.tmp_dir + 'tmp_unsortedNoChrM') except OSError: pass
# -*- coding: utf-8 -*- from __future__ import unicode_literals from django.template.defaultfilters import lower from django.test import SimpleTestCase from django.utils.safestring import mark_safe from ..utils import setup class LowerTests(SimpleTestCase): @setup({'lower01': '{% autoescape off %}{{ a|lower }} {{ b|lower }}{% endautoescape %}'}) def test_lower01(self): output = self.engine.render_to_string('lower01', {"a": "Apple & banana", "b": mark_safe("Apple &amp; banana")}) self.assertEqual(output, "apple & banana apple &amp; banana") @setup({'lower02': '{{ a|lower }} {{ b|lower }}'}) def test_lower02(self): output = self.engine.render_to_string('lower02', {"a": "Apple & banana", "b": mark_safe("Apple &amp; banana")}) self.assertEqual(output, "apple &amp; banana apple &amp; banana") class FunctionTests(SimpleTestCase): def test_lower(self): self.assertEqual(lower('TEST'), 'test') def test_unicode(self): # uppercase E umlaut self.assertEqual(lower('\xcb'), '\xeb') def test_non_string_input(self): self.assertEqual(lower(123), '123')
"Build up the sphinx autodoc file for the python code" import os import sys docs_folder = os.path.dirname(__file__) pywin_folder = os.path.dirname(docs_folder) sys.path.append(pywin_folder) pywin_folder = os.path.join(pywin_folder, "pywinauto") excluded_dirs = ["unittests"] excluded_files = [ "_menux.py", "__init__.py", "win32defines.py", "win32structures.py", "win32functions.py"] output_folder = os.path.join(docs_folder, "code") try: os.mkdir(output_folder) except WindowsError: pass module_docs = [] for root, dirs, files in os.walk(pywin_folder): # Skip over directories we don't want to document for i, d in enumerate(dirs): if d in excluded_dirs: del dirs[i] py_files = [f for f in files if f.endswith(".py")] for py_filename in py_files: # skip over py files we don't want to document if py_filename in excluded_files: continue py_filepath = os.path.join(root, py_filename) # find the last instance of 'pywinauto' to make a module name from # the path modulename = 'pywinauto' + py_filepath.rsplit("pywinauto", 1)[1] modulename = os.path.splitext(modulename)[0] modulename = modulename.replace('\\', '.') # the final doc name is the modulename + .txt doc_source_filename = os.path.join(output_folder, modulename + ".txt") # skip files that are already generated if os.path.exists(doc_source_filename): continue print py_filename out = open(doc_source_filename, "w") out.write(modulename + "\n") out.write("-" * len(modulename) + "\n") out.write(" .. automodule:: %s\n"% modulename) out.write(" :members:\n") out.write(" :undoc-members:\n\n") #out.write(" :inherited-members:\n") #out.write(" .. autoattribute:: %s\n"% modulename) out.close() module_docs.append(doc_source_filename) # This section needs to be updated - I should idealy parse the # existing file to see if any new docs have been added, if not then # I should just leave the file alone rathre than re-create. # #c = open(os.path.join(output_folder, "code.txt"), "w") #c.write("Source Code\n") #c.write("=" * 30 + "\n") # #c.write(".. toctree::\n") #c.write(" :maxdepth: 3\n\n") #for doc in module_docs: # c.write(" " + doc + "\n") # #c.close()
# -*- coding: utf-8 -*- # # NESTServerClient.py # # This file is part of NEST. # # Copyright (C) 2004 The NEST Initiative # # NEST is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 2 of the License, or # (at your option) any later version. # # NEST is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with NEST. If not, see <http://www.gnu.org/licenses/>. import requests from werkzeug.exceptions import BadRequest __all__ = [ 'NESTServerClient', ] def encode(response): if response.ok: return response.json() elif response.status_code == 400: raise BadRequest(response.text) class NESTServerClient(object): def __init__(self, host='localhost', port=5000): self.url = 'http://{}:{}/'.format(host, port) self.headers = {'Content-type': 'application/json', 'Accept': 'text/plain'} def __getattr__(self, call): def method(*args, **kwargs): kwargs.update({'args': args}) response = requests.post(self.url + 'api/' + call, json=kwargs, headers=self.headers) return encode(response) return method def exec_script(self, source, return_vars=None): params = { 'source': source, 'return': return_vars, } response = requests.post(self.url + 'exec', json=params, headers=self.headers) return encode(response) def from_file(self, filename, return_vars=None): with open(filename, 'r') as f: lines = f.readlines() script = ''.join(lines) print('Execute script code of {}'.format(filename)) print('Return variables: {}'.format(return_vars)) print(20*'-') print(script) print(20*'-') return self.exec_script(script, return_vars)
# Copyright (c) 2012 Google Inc. All rights reserved. # Use of this source code is governed by a BSD-style license that can be # found in the LICENSE file. """Visual Studio user preferences file writer.""" import os import re import socket # for gethostname import gyp.common import gyp.easy_xml as easy_xml #------------------------------------------------------------------------------ def _FindCommandInPath(command): """If there are no slashes in the command given, this function searches the PATH env to find the given command, and converts it to an absolute path. We have to do this because MSVS is looking for an actual file to launch a debugger on, not just a command line. Note that this happens at GYP time, so anything needing to be built needs to have a full path.""" if '/' in command or '\\' in command: # If the command already has path elements (either relative or # absolute), then assume it is constructed properly. return command else: # Search through the path list and find an existing file that # we can access. paths = os.environ.get('PATH','').split(os.pathsep) for path in paths: item = os.path.join(path, command) if os.path.isfile(item) and os.access(item, os.X_OK): return item return command def _QuoteWin32CommandLineArgs(args): new_args = [] for arg in args: # Replace all double-quotes with double-double-quotes to escape # them for cmd shell, and then quote the whole thing if there # are any. if arg.find('"') != -1: arg = '""'.join(arg.split('"')) arg = '"%s"' % arg # Otherwise, if there are any spaces, quote the whole arg. elif re.search(r'[ \t\n]', arg): arg = '"%s"' % arg new_args.append(arg) return new_args class Writer(object): """Visual Studio XML user user file writer.""" def __init__(self, user_file_path, version, name): """Initializes the user file. Args: user_file_path: Path to the user file. version: Version info. name: Name of the user file. """ self.user_file_path = user_file_path self.version = version self.name = name self.configurations = {} def AddConfig(self, name): """Adds a configuration to the project. Args: name: Configuration name. """ self.configurations[name] = ['Configuration', {'Name': name}] def AddDebugSettings(self, config_name, command, environment = {}, working_directory=""): """Adds a DebugSettings node to the user file for a particular config. Args: command: command line to run. First element in the list is the executable. All elements of the command will be quoted if necessary. working_directory: other files which may trigger the rule. (optional) """ command = _QuoteWin32CommandLineArgs(command) abs_command = _FindCommandInPath(command[0]) if environment and isinstance(environment, dict): env_list = ['%s="%s"' % (key, val) for (key,val) in environment.iteritems()] environment = ' '.join(env_list) else: environment = '' n_cmd = ['DebugSettings', {'Command': abs_command, 'WorkingDirectory': working_directory, 'CommandArguments': " ".join(command[1:]), 'RemoteMachine': socket.gethostname(), 'Environment': environment, 'EnvironmentMerge': 'true', # Currently these are all "dummy" values that we're just setting # in the default manner that MSVS does it. We could use some of # these to add additional capabilities, I suppose, but they might # not have parity with other platforms then. 'Attach': 'false', 'DebuggerType': '3', # 'auto' debugger 'Remote': '1', 'RemoteCommand': '', 'HttpUrl': '', 'PDBPath': '', 'SQLDebugging': '', 'DebuggerFlavor': '0', 'MPIRunCommand': '', 'MPIRunArguments': '', 'MPIRunWorkingDirectory': '', 'ApplicationCommand': '', 'ApplicationArguments': '', 'ShimCommand': '', 'MPIAcceptMode': '', 'MPIAcceptFilter': '' }] # Find the config, and add it if it doesn't exist. if config_name not in self.configurations: self.AddConfig(config_name) # Add the DebugSettings onto the appropriate config. self.configurations[config_name].append(n_cmd) def WriteIfChanged(self): """Writes the user file.""" configs = ['Configurations'] for config, spec in sorted(self.configurations.iteritems()): configs.append(spec) content = ['VisualStudioUserFile', {'Version': self.version.ProjectVersion(), 'Name': self.name }, configs] easy_xml.WriteXmlIfChanged(content, self.user_file_path, encoding="Windows-1252")
# coding:utf-8 from django.conf.urls import url from django.urls import path from .. import views app_name = 'perms' urlpatterns = [ # asset-permission path('asset-permission/', views.AssetPermissionListView.as_view(), name='asset-permission-list'), path('asset-permission/create/', views.AssetPermissionCreateView.as_view(), name='asset-permission-create'), path('asset-permission/<uuid:pk>/update/', views.AssetPermissionUpdateView.as_view(), name='asset-permission-update'), path('asset-permission/<uuid:pk>/', views.AssetPermissionDetailView.as_view(),name='asset-permission-detail'), path('asset-permission/<uuid:pk>/delete/', views.AssetPermissionDeleteView.as_view(), name='asset-permission-delete'), path('asset-permission/<uuid:pk>/user/', views.AssetPermissionUserView.as_view(), name='asset-permission-user-list'), path('asset-permission/<uuid:pk>/asset/', views.AssetPermissionAssetView.as_view(), name='asset-permission-asset-list'), # remote-app-permission path('remote-app-permission/', views.RemoteAppPermissionListView.as_view(), name='remote-app-permission-list'), path('remote-app-permission/create/', views.RemoteAppPermissionCreateView.as_view(), name='remote-app-permission-create'), path('remote-app-permission/<uuid:pk>/update/', views.RemoteAppPermissionUpdateView.as_view(), name='remote-app-permission-update'), path('remote-app-permission/<uuid:pk>/', views.RemoteAppPermissionDetailView.as_view(), name='remote-app-permission-detail'), path('remote-app-permission/<uuid:pk>/user/', views.RemoteAppPermissionUserView.as_view(), name='remote-app-permission-user-list'), path('remote-app-permission/<uuid:pk>/remote-app/', views.RemoteAppPermissionRemoteAppView.as_view(), name='remote-app-permission-remote-app-list'), # database-app-permission path('database-app-permission/', views.DatabaseAppPermissionListView.as_view(), name='database-app-permission-list'), path('database-app-permission/create/', views.DatabaseAppPermissionCreateView.as_view(), name='database-app-permission-create'), path('database-app-permission/<uuid:pk>/update/', views.DatabaseAppPermissionUpdateView.as_view(), name='database-app-permission-update'), path('database-app-permission/<uuid:pk>/', views.DatabaseAppPermissionDetailView.as_view(), name='database-app-permission-detail'), path('database-app-permission/<uuid:pk>/user/', views.DatabaseAppPermissionUserView.as_view(), name='database-app-permission-user-list'), path('database-app-permission/<uuid:pk>/database-app/', views.DatabaseAppPermissionDatabaseAppView.as_view(), name='database-app-permission-database-app-list'), ]
#!/usr/bin/python # # Copyright 2015 Google Inc. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. """This code example approves a single proposal. To determine which proposals exist, run get_all_proposals.py.""" __author__ = 'Nicholas Chen' # Import appropriate modules from the client library. from googleads import dfp PROPOSAL_ID = 'INSERT_PROPOSAL_ID_HERE' def main(client, proposal_id): # Initialize appropriate service. proposal_service = client.GetService('ProposalService', version='v201505') # Create query. values = [{ 'key': 'proposalId', 'value': { 'xsi_type': 'TextValue', 'value': proposal_id } }] query = 'WHERE id = :proposalId' # Create a filter statement. statement = dfp.FilterStatement(query, values) proposals_approved = 0 # Get proposals by statement. while True: response = proposal_service.getProposalsByStatement(statement.ToStatement()) if 'results' in response: # Display results. for proposal in response['results']: print ('Proposal with id \'%s\', name \'%s\', and status \'%s\' will be' ' approved.' % (proposal['id'], proposal['name'], proposal['status'])) # Perform action. result = proposal_service.performProposalAction( {'xsi_type': 'SubmitProposalsForApproval'}, statement.ToStatement()) if result and int(result['numChanges']) > 0: proposals_approved += int(result['numChanges']) statement.offset += dfp.SUGGESTED_PAGE_LIMIT else: break # Display results. if proposals_approved > 0: print '\nNumber of proposals approved: %s' % proposals_approved else: print '\nNo proposals were approved.' if __name__ == '__main__': # Initialize client object. dfp_client = dfp.DfpClient.LoadFromStorage() main(dfp_client, PROPOSAL_ID)
""" This module implements a VERY limited parser that finds <link> tags in the head of HTML or XHTML documents and parses out their attributes according to the OpenID spec. It is a liberal parser, but it requires these things from the data in order to work: - There must be an open <html> tag - There must be an open <head> tag inside of the <html> tag - Only <link>s that are found inside of the <head> tag are parsed (this is by design) - The parser follows the OpenID specification in resolving the attributes of the link tags. This means that the attributes DO NOT get resolved as they would by an XML or HTML parser. In particular, only certain entities get replaced, and href attributes do not get resolved relative to a base URL. From http://openid.net/specs.bml#linkrel: - The openid.server URL MUST be an absolute URL. OpenID consumers MUST NOT attempt to resolve relative URLs. - The openid.server URL MUST NOT include entities other than &amp;, &lt;, &gt;, and &quot;. The parser ignores SGML comments and <![CDATA[blocks]]>. Both kinds of quoting are allowed for attributes. The parser deals with invalid markup in these ways: - Tag names are not case-sensitive - The <html> tag is accepted even when it is not at the top level - The <head> tag is accepted even when it is not a direct child of the <html> tag, but a <html> tag must be an ancestor of the <head> tag - <link> tags are accepted even when they are not direct children of the <head> tag, but a <head> tag must be an ancestor of the <link> tag - If there is no closing tag for an open <html> or <head> tag, the remainder of the document is viewed as being inside of the tag. If there is no closing tag for a <link> tag, the link tag is treated as a short tag. Exceptions to this rule are that <html> closes <html> and <body> or <head> closes <head> - Attributes of the <link> tag are not required to be quoted. - In the case of duplicated attribute names, the attribute coming last in the tag will be the value returned. - Any text that does not parse as an attribute within a link tag will be ignored. (e.g. <link pumpkin rel='openid.server' /> will ignore pumpkin) - If there are more than one <html> or <head> tag, the parser only looks inside of the first one. - The contents of <script> tags are ignored entirely, except unclosed <script> tags. Unclosed <script> tags are ignored. - Any other invalid markup is ignored, including unclosed SGML comments and unclosed <![CDATA[blocks. """ __all__ = ['parseLinkAttrs'] import re flags = ( re.DOTALL # Match newlines with '.' | re.IGNORECASE | re.VERBOSE # Allow comments and whitespace in patterns | re.UNICODE # Make \b respect Unicode word boundaries ) # Stuff to remove before we start looking for tags removed_re = re.compile(r''' # Comments <!--.*?--> # CDATA blocks | <!\[CDATA\[.*?\]\]> # script blocks | <script\b # make sure script is not an XML namespace (?!:) [^>]*>.*?</script> ''', flags) tag_expr = r''' # Starts with the tag name at a word boundary, where the tag name is # not a namespace <%(tag_name)s\b(?!:) # All of the stuff up to a ">", hopefully attributes. (?P<attrs>[^>]*?) (?: # Match a short tag /> | # Match a full tag > (?P<contents>.*?) # Closed by (?: # One of the specified close tags </?%(closers)s\s*> # End of the string | \Z ) ) ''' def tagMatcher(tag_name, *close_tags): if close_tags: options = '|'.join((tag_name,) + close_tags) closers = '(?:%s)' % (options,) else: closers = tag_name expr = tag_expr % locals() return re.compile(expr, flags) # Must contain at least an open html and an open head tag html_find = tagMatcher('html') head_find = tagMatcher('head', 'body') link_find = re.compile(r'<link\b(?!:)', flags) attr_find = re.compile(r''' # Must start with a sequence of word-characters, followed by an equals sign (?P<attr_name>\w+)= # Then either a quoted or unquoted attribute (?: # Match everything that\'s between matching quote marks (?P<qopen>["\'])(?P<q_val>.*?)(?P=qopen) | # If the value is not quoted, match up to whitespace (?P<unq_val>(?:[^\s<>/]|/(?!>))+) ) | (?P<end_link>[<>]) ''', flags) # Entity replacement: replacements = { 'amp':'&', 'lt':'<', 'gt':'>', 'quot':'"', } ent_replace = re.compile(r'&(%s);' % '|'.join(replacements.keys())) def replaceEnt(mo): "Replace the entities that are specified by OpenID" return replacements.get(mo.group(1), mo.group()) def parseLinkAttrs(html): """Find all link tags in a string representing a HTML document and return a list of their attributes. @param html: the text to parse @type html: str or unicode @return: A list of dictionaries of attributes, one for each link tag @rtype: [[(type(html), type(html))]] """ stripped = removed_re.sub('', html) html_mo = html_find.search(stripped) if html_mo is None or html_mo.start('contents') == -1: return [] start, end = html_mo.span('contents') head_mo = head_find.search(stripped, start, end) if head_mo is None or head_mo.start('contents') == -1: return [] start, end = head_mo.span('contents') link_mos = link_find.finditer(stripped, head_mo.start(), head_mo.end()) matches = [] for link_mo in link_mos: start = link_mo.start() + 5 link_attrs = {} for attr_mo in attr_find.finditer(stripped, start): if attr_mo.lastgroup == 'end_link': break # Either q_val or unq_val must be present, but not both # unq_val is a True (non-empty) value if it is present attr_name, q_val, unq_val = attr_mo.group( 'attr_name', 'q_val', 'unq_val') attr_val = ent_replace.sub(replaceEnt, unq_val or q_val) link_attrs[attr_name] = attr_val matches.append(link_attrs) return matches def relMatches(rel_attr, target_rel): """Does this target_rel appear in the rel_str?""" # XXX: TESTME rels = rel_attr.strip().split() for rel in rels: rel = rel.lower() if rel == target_rel: return 1 return 0 def linkHasRel(link_attrs, target_rel): """Does this link have target_rel as a relationship?""" # XXX: TESTME rel_attr = link_attrs.get('rel') return rel_attr and relMatches(rel_attr, target_rel) def findLinksRel(link_attrs_list, target_rel): """Filter the list of link attributes on whether it has target_rel as a relationship.""" # XXX: TESTME matchesTarget = lambda attrs: linkHasRel(attrs, target_rel) return filter(matchesTarget, link_attrs_list) def findFirstHref(link_attrs_list, target_rel): """Return the value of the href attribute for the first link tag in the list that has target_rel as a relationship.""" # XXX: TESTME matches = findLinksRel(link_attrs_list, target_rel) if not matches: return None first = matches[0] return first.get('href')
import datrie from data_utils import Vocabulary, Dataset import string import re from flask import Flask from flask_restful import Resource, Api import traceback import time import sys #import thriftpy import os from flask import Flask, request, redirect, url_for from werkzeug.utils import secure_filename from newPyClient import computeKSR import json import numpy as np import time import tensorflow as tf from data_utils import Vocabulary, Dataset from language_model import LM from common import CheckpointLoader import heapq UPLOAD_FOLDER = '/data/ngramTest/uploads' UPLOAD_FOLDER = './' top_k = 3 pattern = re.compile('[\w+]') p_punc = re.compile('(\.|\"|,|\?|\!)') hps = LM.get_default_hparams() vocab = Vocabulary.from_file("1b_word_vocab.txt") with tf.variable_scope("model"): hps.num_sampled = 0 # Always using full softmax at evaluation. run out of memory hps.keep_prob = 1.0 hps.num_gpus = 1 model = LM(hps,"predict_next", "/cpu:0") if hps.average_params: print("Averaging parameters for evaluation.") saver = tf.train.Saver(model.avg_dict) else: saver = tf.train.Saver() # Use only 4 threads for the evaluation. config = tf.ConfigProto(allow_soft_placement=True, intra_op_parallelism_threads=20, inter_op_parallelism_threads=1) config.gpu_options.allow_growth=True sess = tf.Session(config=config) ckpt_loader = CheckpointLoader(saver, model.global_step, "log.txt/train") saver.restore(sess,"log.txt/train/model.ckpt-742996") app = Flask(__name__) api = Api(app) ''' #build vocab trie trie = datrie.new(string.printable) cnt = 0 vocab_size = 140000 for i in range(vocab_size): word = vocab.get_token(i) trie[word] = i for key in trie.keys(u"pre"): print key,trie[key] trie.save("data/vocab_trie") ''' trie = datrie.Trie.load("data/vocab_trie") class ngramPredict(Resource): def get(self,input): input = input.decode("utf-8") #print "input:",input input_words = input if input_words.find('<S>')!=0: input_words = '<S> ' + input isCompletion = False if input_words[-1] == ' ': #print "Predict:" prefix_input = [vocab.get_id(w) for w in input_words.split()] else: #print "Compeletion:" isCompletion = True prefix_input = [vocab.get_id(w) for w in input_words.split()[:-1]] prefix = input_words.split()[-1] #print "prefix:",prefix,type(prefix) #print("input:",input,"pre:",prefix_input,"len:",len(prefix_input)) w = np.zeros([1, len(prefix_input)], np.uint8) w[:] =1 inputs = np.zeros([hps.batch_size*hps.num_gpus,hps.num_steps]) weights = np.zeros([hps.batch_size*hps.num_gpus,hps.num_steps]) inputs[0,:len(prefix_input)] = prefix_input[:] weights[0,:len(prefix_input)] = w[:] words = [] with sess.as_default(): #ckpt_loader.load_checkpoint() # FOR ONLY ONE CHECKPOINT sess.run(tf.local_variables_initializer()) words = [] if not isCompletion: indexes = sess.run([model.index],{model.x:inputs, model.w:weights}) indexes = np.reshape(indexes,[hps.num_steps,hps.arg_max]) for j in range(hps.arg_max): word = vocab.get_token(indexes[len(prefix_input)-1][j]) if not p_punc.match(word)==None: words += [word] continue if pattern.match(word)==None: continue words += [word] else: prob = sess.run([model.logits],{model.x:inputs, model.w:weights}) prob = np.reshape(prob,[hps.num_steps,hps.vocab_size]) prob = prob[len(prefix_input)-1] # the last prefix_input step prob is the predict one #print "prob:", len(prob) #print "prefix:",trie.keys(prefix) cand = [trie[cand_index] for cand_index in trie.keys(prefix)] #print "cand:", cand #print "prefix:", prefix cand_prob = [prob[pb] for pb in cand] ins = heapq.nlargest(top_k, range(len(cand_prob)), cand_prob.__getitem__) for j in ins: word = vocab.get_token(cand[j]) words += [word] #print words return words[:top_k] @app.route('/ngramfile/', methods=['GET', 'POST']) def upload_file(): if request.method == 'POST': doc = request.json if doc: doc = doc['text'] ngramClient = ngramPredict() res = computeKSR(ngramClient,doc) return json.dumps(res) if 'text' not in request.files: return "{\"ret\":-1}" file = request.files['text'] if file.filename == '': return "{\"ret\":-2}" filename = secure_filename(file.filename) uploadFilePath = os.path.join(UPLOAD_FOLDER, filename) file.save(uploadFilePath) doc = "" with open(uploadFilePath, 'rb') as textFile: doc = textFile.read() ngramClient = ngramPredict() res = computeKSR(ngramClient,doc) #print("res:",res) #TODO #return json.dumps(res) api.add_resource(ngramPredict, '/ngram/<input>') #predictClient = PredictClient() if __name__ == '__main__': ''' ngrampredict = ngramPredict() ngrampredict.get("how are") ngrampredict.get("what the") ngrampredict.get("i am") ngrampredict.get("how do") ''' #print('test for grep ksr') app.run(host = "0",port=9898)
# Copyright 2016 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== """Softplus bijector.""" from __future__ import absolute_import from __future__ import division from __future__ import print_function from tensorflow.python.framework import ops from tensorflow.python.ops import check_ops from tensorflow.python.ops import control_flow_ops from tensorflow.python.ops import math_ops from tensorflow.python.ops import nn_ops from tensorflow.python.ops.distributions import bijector from tensorflow.python.ops.distributions import util as distribution_util from tensorflow.python.util import deprecation __all__ = [ "Softplus", ] class Softplus(bijector.Bijector): """Bijector which computes `Y = g(X) = Log[1 + exp(X)]`. The softplus `Bijector` has the following two useful properties: * The domain is the positive real numbers * `softplus(x) approx x`, for large `x`, so it does not overflow as easily as the `Exp` `Bijector`. The optional nonzero `hinge_softness` parameter changes the transition at zero. With `hinge_softness = c`, the bijector is: ```f_c(x) := c * g(x / c) = c * Log[1 + exp(x / c)].``` For large `x >> 1`, `c * Log[1 + exp(x / c)] approx c * Log[exp(x / c)] = x`, so the behavior for large `x` is the same as the standard softplus. As `c > 0` approaches 0 from the right, `f_c(x)` becomes less and less soft, approaching `max(0, x)`. * `c = 1` is the default. * `c > 0` but small means `f(x) approx ReLu(x) = max(0, x)`. * `c < 0` flips sign and reflects around the `y-axis`: `f_{-c}(x) = -f_c(-x)`. * `c = 0` results in a non-bijective transformation and triggers an exception. Example Use: ```python # Create the Y=g(X)=softplus(X) transform which works only on Tensors with 1 # batch ndim and 2 event ndims (i.e., vector of matrices). softplus = Softplus() x = [[[1., 2], [3, 4]], [[5, 6], [7, 8]]] log(1 + exp(x)) == softplus.forward(x) log(exp(x) - 1) == softplus.inverse(x) ``` Note: log(.) and exp(.) are applied element-wise but the Jacobian is a reduction over the event space. """ @distribution_util.AppendDocstring( kwargs_dict={ "hinge_softness": ( "Nonzero floating point `Tensor`. Controls the softness of what " "would otherwise be a kink at the origin. Default is 1.0")}) @deprecation.deprecated( "2018-10-01", "The TensorFlow Distributions library has moved to " "TensorFlow Probability " "(https://github.com/tensorflow/probability). You " "should update all references to use `tfp.distributions` " "instead of `tf.contrib.distributions`.", warn_once=True) def __init__(self, hinge_softness=None, validate_args=False, name="softplus"): with ops.name_scope(name, values=[hinge_softness]): if hinge_softness is not None: self._hinge_softness = ops.convert_to_tensor( hinge_softness, name="hinge_softness") else: self._hinge_softness = None if validate_args: nonzero_check = check_ops.assert_none_equal( ops.convert_to_tensor( 0, dtype=self.hinge_softness.dtype), self.hinge_softness, message="hinge_softness must be non-zero") self._hinge_softness = control_flow_ops.with_dependencies( [nonzero_check], self.hinge_softness) super(Softplus, self).__init__( forward_min_event_ndims=0, validate_args=validate_args, name=name) def _forward(self, x): if self.hinge_softness is None: return nn_ops.softplus(x) hinge_softness = math_ops.cast(self.hinge_softness, x.dtype) return hinge_softness * nn_ops.softplus(x / hinge_softness) def _inverse(self, y): if self.hinge_softness is None: return distribution_util.softplus_inverse(y) hinge_softness = math_ops.cast(self.hinge_softness, y.dtype) return hinge_softness * distribution_util.softplus_inverse( y / hinge_softness) def _inverse_log_det_jacobian(self, y): # Could also do: # ildj = math_ops.reduce_sum(y - distribution_util.softplus_inverse(y), # axis=event_dims) # but the following is more numerically stable. Ie, # Y = Log[1 + exp{X}] ==> X = Log[exp{Y} - 1] # ==> dX/dY = exp{Y} / (exp{Y} - 1) # = 1 / (1 - exp{-Y}), # which is the most stable for large Y > 0. For small Y, we use # 1 - exp{-Y} approx Y. if self.hinge_softness is not None: y /= math_ops.cast(self.hinge_softness, y.dtype) return -math_ops.log(-math_ops.expm1(-y)) def _forward_log_det_jacobian(self, x): if self.hinge_softness is not None: x /= math_ops.cast(self.hinge_softness, x.dtype) return -nn_ops.softplus(-x) @property def hinge_softness(self): return self._hinge_softness
#!/usr/bin/env python # -*- coding: utf-8 -*- # Copyright (c) 2014, 2015, 2019 Adam.Dybbroe # Author(s): # Adam.Dybbroe <a000680@c14526.ad.smhi.se> # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # You should have received a copy of the GNU General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. """Make quick look images of the ctth composite """ import argparse from datetime import datetime import numpy as np import xarray as xr from trollimage.xrimage import XRImage from mesan_compositer import ctth_height from mesan_compositer.netcdf_io import ncCTTHComposite from mesan_compositer import get_config from satpy.composites import ColormapCompositor import sys import os from logging import handlers import logging LOG = logging.getLogger(__name__) #: Default time format _DEFAULT_TIME_FORMAT = '%Y-%m-%d %H:%M:%S' #: Default log format _DEFAULT_LOG_FORMAT = '[%(levelname)s: %(asctime)s : %(name)s] %(message)s' def get_arguments(): """ Get command line arguments args.logging_conf_file, args.config_file, obs_time, area_id, wsize Return File path of the logging.ini file File path of the application configuration file Observation/Analysis time Area id Window size """ parser = argparse.ArgumentParser() parser.add_argument('--datetime', '-d', help='Date and time of observation - yyyymmddhh', required=True) parser.add_argument('--area_id', '-a', help='Area id', required=True) parser.add_argument('-c', '--config_file', type=str, dest='config_file', required=True, help="The file containing configuration parameters e.g. mesan_sat_config.yaml") parser.add_argument("-l", "--logging", help="The path to the log-configuration file (e.g. './logging.ini')", dest="logging_conf_file", type=str, required=False) parser.add_argument("-v", "--verbose", help="print debug messages too", action="store_true") args = parser.parse_args() tanalysis = datetime.strptime(args.datetime, '%Y%m%d%H') area_id = args.area_id if 'template' in args.config_file: print("Template file given as master config, aborting!") sys.exit() return args.logging_conf_file, args.config_file, tanalysis, area_id if __name__ == "__main__": (logfile, config_filename, time_of_analysis, areaid) = get_arguments() if logfile: logging.config.fileConfig(logfile) handler = logging.StreamHandler(sys.stderr) formatter = logging.Formatter(fmt=_DEFAULT_LOG_FORMAT, datefmt=_DEFAULT_TIME_FORMAT) handler.setFormatter(formatter) handler.setLevel(logging.DEBUG) logging.getLogger('').addHandler(handler) logging.getLogger('').setLevel(logging.DEBUG) logging.getLogger('satpy').setLevel(logging.INFO) LOG = logging.getLogger('ctth_quicklooks') log_handlers = logging.getLogger('').handlers for log_handle in log_handlers: if type(log_handle) is handlers.SMTPHandler: LOG.debug("Mail notifications to: %s", str(log_handle.toaddrs)) OPTIONS = get_config(config_filename) values = {"area": areaid, } bname = time_of_analysis.strftime(OPTIONS['ctth_composite_filename']) % values path = OPTIONS['composite_output_dir'] filename = os.path.join(path, bname) + '.nc' if not os.path.exists(filename): LOG.error("File " + str(filename) + " does not exist!") sys.exit(-1) comp = ncCTTHComposite() comp.load(filename) palette = ctth_height() ctth_data = comp.height.data ctth_data = ctth_data / 500.0 + 1 ctth_data = ctth_data.astype(np.uint8) cmap = ColormapCompositor('mesan_cloudheight_composite') colors, sqpal = cmap.build_colormap(palette, np.uint8, {}) attrs = {'_FillValue': 0} xdata = xr.DataArray(ctth_data, dims=['y', 'x'], attrs=attrs).astype('uint8') pimage = XRImage(xdata) pimage.palettize(colors) pimage.save(filename.strip('.nc') + '_height.png')
from dopamine.adapters import Adapter import numpy as np class Explorer(Adapter): # define the conditions of the environment inConditions = {} # define the conditions of the environment outConditions = {} def __init__(self): Adapter.__init__(self) # set this to False to turn off exploration self.active = True def applyAction(self, action): """ apply transformations to action and return it. """ if self.active: action = self._explore(action) # tell agent the action that was executed (for the history) self.experiment.agent.action = action return action def _explore(self, action): return action class DecayExplorer(Explorer): finalFactor = 0.001 def __init__(self, epsilon, episodeCount=None, actionCount=None): """ DecayExplorer is an explorer base class that has exploration decay, i.e. the amount of exploration weakens exponentially over time. epsilon is the initial parameter (can mean different things for different explorers), which reduced over time. if episodeCount is given, epsilon reduces to 1/1000 of the initial value in the given number of episodes. if actionCount is given, epsilon reduces to 1/1000 of the initial value in the given number of actions executed. actionCount takes priority if both values are given. In either case, after epsilon is 1/1000 of its initial value, exploration automatically deactivates. """ Explorer.__init__(self) self.episodeCount = episodeCount self.actionCount = actionCount self.epsilon = epsilon self.initialEpsilon = epsilon if self.episodeCount: self.decay = np.power(self.finalFactor, 1./self.episodeCount) if self.actionCount: self.decay = np.power(self.finalFactor, 1./self.actionCount) self.episodeCount = None def resetExploration(self): self.epsilon = self.initialEpsilon def applyAction(self, action): action = Explorer.applyAction(self, action) if self.actionCount and self.active: self.epsilon *= self.decay if self.epsilon <= self.initialEpsilon * self.finalFactor: self.active = False return action def applyEpisodeFinished(self, episodeFinished): if episodeFinished and self.episodeCount and self.active: self.epsilon *= self.decay return episodeFinished
import os from .base import NullBrowser, ExecutorBrowser, require_arg from .base import get_timeout_multiplier # noqa: F401 from ..executors import executor_kwargs as base_executor_kwargs from ..executors.executorservo import ServoTestharnessExecutor, ServoRefTestExecutor, ServoWdspecExecutor # noqa: F401 here = os.path.join(os.path.split(__file__)[0]) __wptrunner__ = { "product": "servo", "check_args": "check_args", "browser": "ServoBrowser", "executor": { "testharness": "ServoTestharnessExecutor", "reftest": "ServoRefTestExecutor", "wdspec": "ServoWdspecExecutor", }, "browser_kwargs": "browser_kwargs", "executor_kwargs": "executor_kwargs", "env_extras": "env_extras", "env_options": "env_options", "timeout_multiplier": "get_timeout_multiplier", "update_properties": "update_properties", } def check_args(**kwargs): require_arg(kwargs, "binary") def browser_kwargs(test_type, run_info_data, config, **kwargs): return { "binary": kwargs["binary"], "debug_info": kwargs["debug_info"], "binary_args": kwargs["binary_args"], "user_stylesheets": kwargs.get("user_stylesheets"), "ca_certificate_path": config.ssl_config["ca_cert_path"], } def executor_kwargs(test_type, server_config, cache_manager, run_info_data, **kwargs): rv = base_executor_kwargs(test_type, server_config, cache_manager, run_info_data, **kwargs) rv["pause_after_test"] = kwargs["pause_after_test"] if test_type == "wdspec": rv["capabilities"] = {} return rv def env_extras(**kwargs): return [] def env_options(): return {"server_host": "127.0.0.1", "bind_address": False, "testharnessreport": "testharnessreport-servo.js", "supports_debugger": True} def update_properties(): return ["debug", "os", "version", "processor", "bits"], None class ServoBrowser(NullBrowser): def __init__(self, logger, binary, debug_info=None, binary_args=None, user_stylesheets=None, ca_certificate_path=None): NullBrowser.__init__(self, logger) self.binary = binary self.debug_info = debug_info self.binary_args = binary_args or [] self.user_stylesheets = user_stylesheets or [] self.ca_certificate_path = ca_certificate_path def executor_browser(self): return ExecutorBrowser, { "binary": self.binary, "debug_info": self.debug_info, "binary_args": self.binary_args, "user_stylesheets": self.user_stylesheets, "ca_certificate_path": self.ca_certificate_path, }
# symcompartment.py --- # # Filename: symcompartment.py # Description: # Author: # Maintainer: # Created: Thu Jun 20 17:47:10 2013 (+0530) # Version: # Last-Updated: Wed Jun 26 11:43:47 2013 (+0530) # By: subha # Update #: 90 # URL: # Keywords: # Compatibility: # # # Commentary: # # # # # Change log: # # # # # This program is free software; you can redistribute it and/or # modify it under the terms of the GNU General Public License as # published by the Free Software Foundation; either version 3, or # (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU # General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program; see the file COPYING. If not, write to # the Free Software Foundation, Inc., 51 Franklin Street, Fifth # Floor, Boston, MA 02110-1301, USA. # # # Code: import numpy as np import pylab import moose simdt = 1e-6 simtime = 100e-3 def test_symcompartment(): model = moose.Neutral('model') soma = moose.SymCompartment('%s/soma' % (model.path)) soma.Em = -60e-3 soma.Rm = 1e9 soma.Cm = 1e-11 soma.Ra = 1e6 d1 = moose.SymCompartment('%s/d1' % (model.path)) d1.Rm = 1e8 d1.Cm = 1e-10 d1.Ra = 1e7 d2 = moose.SymCompartment('%s/d2' % (model.path)) d2.Rm = 1e8 d2.Cm = 1e-10 d2.Ra = 2e7 moose.connect(d1, 'proximal', soma, 'distal') moose.connect(d2, 'proximal', soma, 'distal') moose.connect(d1, 'sibling', d2, 'sibling') pg = moose.PulseGen('/model/pulse') pg.delay[0] = 10e-3 pg.width[0] = 20e-3 pg.level[0] = 1e-6 pg.delay[1] = 1e9 moose.connect(pg, 'output', d1, 'injectMsg') data = moose.Neutral('/data') tab_soma = moose.Table('%s/soma_Vm' % (data.path)) tab_d1 = moose.Table('%s/d1_Vm' % (data.path)) tab_d2 = moose.Table('%s/d2_Vm' % (data.path)) moose.connect(tab_soma, 'requestOut', soma, 'getVm') moose.connect(tab_d1, 'requestOut', d1, 'getVm') moose.connect(tab_d2, 'requestOut', d2, 'getVm') moose.setClock(0, simdt) moose.setClock(1, simdt) moose.setClock(2, simdt) moose.useClock(0, '/model/##[ISA=Compartment]', 'init') # This is allowed because SymCompartment is a subclass of Compartment moose.useClock(1, '/model/##', 'process') moose.useClock(2, '/data/##[ISA=Table]', 'process') moose.reinit() moose.start(simtime) t = np.linspace(0, simtime, len(tab_soma.vector)) data_matrix = np.vstack((t, tab_soma.vector, tab_d1.vector, tab_d2.vector)) np.savetxt('symcompartment.txt', data_matrix.transpose()) pylab.plot(t, tab_soma.vector, label='Vm_soma') pylab.plot(t, tab_d1.vector, label='Vm_d1') pylab.plot(t, tab_d2.vector, label='Vm_d2') pylab.show() if __name__ == '__main__': test_symcompartment() # # symcompartment.py ends here
from functools import wraps import sys import warnings from django.core.exceptions import ObjectDoesNotExist, ImproperlyConfigured # NOQA from django.db.models.query import Q, QuerySet, Prefetch # NOQA from django.db.models.expressions import F # NOQA from django.db.models.manager import Manager # NOQA from django.db.models.base import Model # NOQA from django.db.models.aggregates import * # NOQA from django.db.models.fields import * # NOQA from django.db.models.fields.subclassing import SubfieldBase # NOQA from django.db.models.fields.files import FileField, ImageField # NOQA from django.db.models.fields.related import ( # NOQA ForeignKey, ForeignObject, OneToOneField, ManyToManyField, ManyToOneRel, ManyToManyRel, OneToOneRel) from django.db.models.fields.proxy import OrderWrt # NOQA from django.db.models.deletion import ( # NOQA CASCADE, PROTECT, SET, SET_NULL, SET_DEFAULT, DO_NOTHING, ProtectedError) from django.db.models.lookups import Lookup, Transform # NOQA from django.db.models import signals # NOQA from django.utils.deprecation import RemovedInDjango19Warning def permalink(func): """ Decorator that calls urlresolvers.reverse() to return a URL using parameters returned by the decorated function "func". "func" should be a function that returns a tuple in one of the following formats: (viewname, viewargs) (viewname, viewargs, viewkwargs) """ from django.core.urlresolvers import reverse @wraps(func) def inner(*args, **kwargs): bits = func(*args, **kwargs) return reverse(bits[0], None, *bits[1:3]) return inner # Deprecated aliases for functions were exposed in this module. def make_alias(function_name): # Close function_name. def alias(*args, **kwargs): warnings.warn( "django.db.models.%s is deprecated." % function_name, RemovedInDjango19Warning, stacklevel=2) # This raises a second warning. from . import loading return getattr(loading, function_name)(*args, **kwargs) alias.__name__ = function_name return alias this_module = sys.modules['django.db.models'] for function_name in ('get_apps', 'get_app_path', 'get_app_paths', 'get_app', 'get_models', 'get_model', 'register_models'): setattr(this_module, function_name, make_alias(function_name)) del this_module, make_alias, function_name
# -*- coding: utf-8 -*- import sys from django.core.management.base import BaseCommand, CommandError class SubcommandsCommand(BaseCommand): subcommands = {} command_name = '' def __init__(self): super(SubcommandsCommand, self).__init__() for name, subcommand in self.subcommands.items(): subcommand.command_name = '%s %s' % (self.command_name, name) def handle(self, *args, **options): stderr = getattr(self, 'stderr', sys.stderr) stdout = getattr(self, 'stdout', sys.stdout) if len(args) > 0: if args[0] in self.subcommands.keys(): handle_command = self.subcommands.get(args[0])() handle_command.stdout = stdout handle_command.stderr = stderr handle_command.handle(*args[1:], **options) else: stderr.write("%r is not a valid subcommand for %r\n" % (args[0], self.command_name)) stderr.write("Available subcommands are:\n") for subcommand in sorted(self.subcommands.keys()): stderr.write(" %r\n" % subcommand) raise CommandError('Invalid subcommand %r for %r' % (args[0], self.command_name)) else: stderr.write("%r must be called with at least one argument, it's subcommand.\n" % self.command_name) stderr.write("Available subcommands are:\n") for subcommand in sorted(self.subcommands.keys()): stderr.write(" %r\n" % subcommand) raise CommandError('No subcommand given for %r' % self.command_name)
import wx class Frame(wx.Frame): def __init__(self, parent, id, title): wx.Frame.__init__(self, parent, id, title, wx.DefaultPosition, wx.Size(450, 350)) hbox = wx.BoxSizer(wx.HORIZONTAL) vbox = wx.BoxSizer(wx.VERTICAL) panel1 = wx.Panel(self, -1) panel2 = wx.Panel(self, -1) self.tree = wx.TreeCtrl(panel1, 1, wx.DefaultPosition, (-1,-1), wx.TR_HIDE_ROOT|wx.TR_HAS_BUTTONS) root = self.tree.AddRoot('weaver Data') os = self.tree.AppendItem(root, 'Gis data') pl = self.tree.AppendItem(root, 'Time Series') cl = self.tree.AppendItem(pl, 'Integrated Data') sl = self.tree.AppendItem(pl, 'world data') self.tree.AppendItem(cl, 'Plants') self.tree.Bind(wx.EVT_TREE_SEL_CHANGED, self.OnSelChanged, id=1) self.display = wx.StaticText(panel2, -1, '',(10,10), style=wx.ALIGN_CENTRE) vbox.Add(self.tree, 1, wx.EXPAND) hbox.Add(panel1, 1, wx.EXPAND) hbox.Add(panel2, 1, wx.EXPAND) panel1.SetSizer(vbox) self.SetSizer(hbox) self.Centre() def OnSelChanged(self, event): item = event.GetItem() self.display.SetLabel(self.tree.GetItemText(item)) class App(wx.App): def OnInit(self): frame = Frame(None, -1, 'treectrl.py') frame.Show(True) self.SetTopWindow(frame) return True
# # Licensed to the Apache Software Foundation (ASF) under one or more # contributor license agreements. See the NOTICE file distributed with # this work for additional information regarding copyright ownership. # The ASF licenses this file to You under the Apache License, Version 2.0 # (the "License"); you may not use this file except in compliance with # the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # """Unit tests for the windowing classes.""" import unittest from apache_beam.runners import pipeline_context from apache_beam.testing.test_pipeline import TestPipeline from apache_beam.testing.util import assert_that, equal_to from apache_beam.transforms import CombinePerKey from apache_beam.transforms import combiners from apache_beam.transforms import core from apache_beam.transforms import Create from apache_beam.transforms import GroupByKey from apache_beam.transforms import Map from apache_beam.transforms import WindowInto from apache_beam.transforms.core import Windowing from apache_beam.transforms.trigger import AccumulationMode from apache_beam.transforms.trigger import AfterCount from apache_beam.transforms.window import FixedWindows from apache_beam.transforms.window import GlobalWindow from apache_beam.transforms.window import GlobalWindows from apache_beam.transforms.window import IntervalWindow from apache_beam.transforms.window import TimestampCombiner from apache_beam.transforms.window import Sessions from apache_beam.transforms.window import SlidingWindows from apache_beam.transforms.window import TimestampedValue from apache_beam.transforms.window import WindowedValue from apache_beam.transforms.window import WindowFn from apache_beam.utils.timestamp import MAX_TIMESTAMP from apache_beam.utils.timestamp import MIN_TIMESTAMP def context(element, timestamp): return WindowFn.AssignContext(timestamp, element) sort_values = Map(lambda (k, vs): (k, sorted(vs))) class ReifyWindowsFn(core.DoFn): def process(self, element, window=core.DoFn.WindowParam): key, values = element yield "%s @ %s" % (key, window), values reify_windows = core.ParDo(ReifyWindowsFn()) class WindowTest(unittest.TestCase): def test_timestamped_value_cmp(self): self.assertEqual(TimestampedValue('a', 2), TimestampedValue('a', 2)) self.assertEqual(TimestampedValue('a', 2), TimestampedValue('a', 2.0)) self.assertNotEqual(TimestampedValue('a', 2), TimestampedValue('a', 2.1)) self.assertNotEqual(TimestampedValue('a', 2), TimestampedValue('b', 2)) def test_global_window(self): self.assertEqual(GlobalWindow(), GlobalWindow()) self.assertNotEqual(GlobalWindow(), IntervalWindow(MIN_TIMESTAMP, MAX_TIMESTAMP)) self.assertNotEqual(IntervalWindow(MIN_TIMESTAMP, MAX_TIMESTAMP), GlobalWindow()) def test_fixed_windows(self): # Test windows with offset: 2, 7, 12, 17, ... windowfn = FixedWindows(size=5, offset=2) self.assertEqual([IntervalWindow(7, 12)], windowfn.assign(context('v', 7))) self.assertEqual([IntervalWindow(7, 12)], windowfn.assign(context('v', 11))) self.assertEqual([IntervalWindow(12, 17)], windowfn.assign(context('v', 12))) # Test windows without offset: 0, 5, 10, 15, ... windowfn = FixedWindows(size=5) self.assertEqual([IntervalWindow(5, 10)], windowfn.assign(context('v', 5))) self.assertEqual([IntervalWindow(5, 10)], windowfn.assign(context('v', 9))) self.assertEqual([IntervalWindow(10, 15)], windowfn.assign(context('v', 10))) # Test windows with offset out of range. windowfn = FixedWindows(size=5, offset=12) self.assertEqual([IntervalWindow(7, 12)], windowfn.assign(context('v', 11))) def test_sliding_windows_assignment(self): windowfn = SlidingWindows(size=15, period=5, offset=2) expected = [IntervalWindow(7, 22), IntervalWindow(2, 17), IntervalWindow(-3, 12)] self.assertEqual(expected, windowfn.assign(context('v', 7))) self.assertEqual(expected, windowfn.assign(context('v', 8))) self.assertEqual(expected, windowfn.assign(context('v', 11))) def test_sliding_windows_assignment_fraction(self): windowfn = SlidingWindows(size=3.5, period=2.5, offset=1.5) self.assertEqual([IntervalWindow(1.5, 5.0), IntervalWindow(-1.0, 2.5)], windowfn.assign(context('v', 1.7))) self.assertEqual([IntervalWindow(1.5, 5.0)], windowfn.assign(context('v', 3))) def test_sliding_windows_assignment_fraction_large_offset(self): windowfn = SlidingWindows(size=3.5, period=2.5, offset=4.0) self.assertEqual([IntervalWindow(1.5, 5.0), IntervalWindow(-1.0, 2.5)], windowfn.assign(context('v', 1.7))) self.assertEqual([IntervalWindow(4.0, 7.5), IntervalWindow(1.5, 5.0)], windowfn.assign(context('v', 4.5))) def test_sessions_merging(self): windowfn = Sessions(10) def merge(*timestamps): windows = [windowfn.assign(context(None, t)) for t in timestamps] running = set() class TestMergeContext(WindowFn.MergeContext): def __init__(self): super(TestMergeContext, self).__init__(running) def merge(self, to_be_merged, merge_result): for w in to_be_merged: if w in running: running.remove(w) running.add(merge_result) for ws in windows: running.update(ws) windowfn.merge(TestMergeContext()) windowfn.merge(TestMergeContext()) return sorted(running) self.assertEqual([IntervalWindow(2, 12)], merge(2)) self.assertEqual([IntervalWindow(2, 12), IntervalWindow(19, 29)], merge(2, 19)) self.assertEqual([IntervalWindow(2, 19)], merge(2, 9)) self.assertEqual([IntervalWindow(2, 19)], merge(9, 2)) self.assertEqual([IntervalWindow(2, 19), IntervalWindow(19, 29)], merge(2, 9, 19)) self.assertEqual([IntervalWindow(2, 19), IntervalWindow(19, 29)], merge(19, 9, 2)) self.assertEqual([IntervalWindow(2, 25)], merge(2, 15, 10)) def timestamped_key_values(self, pipeline, key, *timestamps): return (pipeline | 'start' >> Create(timestamps) | Map(lambda x: WindowedValue((key, x), x, [GlobalWindow()]))) def test_sliding_windows(self): p = TestPipeline() pcoll = self.timestamped_key_values(p, 'key', 1, 2, 3) result = (pcoll | 'w' >> WindowInto(SlidingWindows(period=2, size=4)) | GroupByKey() | reify_windows) expected = [('key @ [-2.0, 2.0)', [1]), ('key @ [0.0, 4.0)', [1, 2, 3]), ('key @ [2.0, 6.0)', [2, 3])] assert_that(result, equal_to(expected)) p.run() def test_sessions(self): p = TestPipeline() pcoll = self.timestamped_key_values(p, 'key', 1, 2, 3, 20, 35, 27) result = (pcoll | 'w' >> WindowInto(Sessions(10)) | GroupByKey() | sort_values | reify_windows) expected = [('key @ [1.0, 13.0)', [1, 2, 3]), ('key @ [20.0, 45.0)', [20, 27, 35])] assert_that(result, equal_to(expected)) p.run() def test_timestamped_value(self): p = TestPipeline() result = (p | 'start' >> Create([(k, k) for k in range(10)]) | Map(lambda (x, t): TimestampedValue(x, t)) | 'w' >> WindowInto(FixedWindows(5)) | Map(lambda v: ('key', v)) | GroupByKey()) assert_that(result, equal_to([('key', [0, 1, 2, 3, 4]), ('key', [5, 6, 7, 8, 9])])) p.run() def test_rewindow(self): p = TestPipeline() result = (p | Create([(k, k) for k in range(10)]) | Map(lambda (x, t): TimestampedValue(x, t)) | 'window' >> WindowInto(SlidingWindows(period=2, size=6)) # Per the model, each element is now duplicated across # three windows. Rewindowing must preserve this duplication. | 'rewindow' >> WindowInto(FixedWindows(5)) | 'rewindow2' >> WindowInto(FixedWindows(5)) | Map(lambda v: ('key', v)) | GroupByKey()) assert_that(result, equal_to([('key', sorted([0, 1, 2, 3, 4] * 3)), ('key', sorted([5, 6, 7, 8, 9] * 3))])) p.run() def test_timestamped_with_combiners(self): p = TestPipeline() result = (p # Create some initial test values. | 'start' >> Create([(k, k) for k in range(10)]) # The purpose of the WindowInto transform is to establish a # FixedWindows windowing function for the PCollection. # It does not bucket elements into windows since the timestamps # from Create are not spaced 5 ms apart and very likely they all # fall into the same window. | 'w' >> WindowInto(FixedWindows(5)) # Generate timestamped values using the values as timestamps. # Now there are values 5 ms apart and since Map propagates the # windowing function from input to output the output PCollection # will have elements falling into different 5ms windows. | Map(lambda (x, t): TimestampedValue(x, t)) # We add a 'key' to each value representing the index of the # window. This is important since there is no guarantee of # order for the elements of a PCollection. | Map(lambda v: (v / 5, v))) # Sum all elements associated with a key and window. Although it # is called CombinePerKey it is really CombinePerKeyAndWindow the # same way GroupByKey is really GroupByKeyAndWindow. sum_per_window = result | CombinePerKey(sum) # Compute mean per key and window. mean_per_window = result | combiners.Mean.PerKey() assert_that(sum_per_window, equal_to([(0, 10), (1, 35)]), label='assert:sum') assert_that(mean_per_window, equal_to([(0, 2.0), (1, 7.0)]), label='assert:mean') p.run() class RunnerApiTest(unittest.TestCase): def test_windowfn_encoding(self): for window_fn in (GlobalWindows(), FixedWindows(37), SlidingWindows(2, 389), Sessions(5077)): context = pipeline_context.PipelineContext() self.assertEqual( window_fn, WindowFn.from_runner_api(window_fn.to_runner_api(context), context)) def test_windowing_encoding(self): for windowing in ( Windowing(GlobalWindows()), Windowing(FixedWindows(1, 3), AfterCount(6), accumulation_mode=AccumulationMode.ACCUMULATING), Windowing(SlidingWindows(10, 15, 21), AfterCount(28), timestamp_combiner=TimestampCombiner.OUTPUT_AT_LATEST, accumulation_mode=AccumulationMode.DISCARDING)): context = pipeline_context.PipelineContext() self.assertEqual( windowing, Windowing.from_runner_api(windowing.to_runner_api(context), context)) if __name__ == '__main__': unittest.main()
"""yue URL Configuration The `urlpatterns` list routes URLs to views. For more information please see: https://docs.djangoproject.com/en/1.9/topics/http/urls/ Examples: Function views 1. Add an import: from my_app import views 2. Add a URL to urlpatterns: url(r'^$', views.home, name='home') Class-based views 1. Add an import: from other_app.views import Home 2. Add a URL to urlpatterns: url(r'^$', Home.as_view(), name='home') Including another URLconf 1. Import the include() function: from django.conf.urls import url, include 2. Add a URL to urlpatterns: url(r'^blog/', include('blog.urls')) """ from django.conf.urls import url from django.contrib.staticfiles.urls import static from django.conf import settings # from django.contrib import admin from django.contrib.staticfiles.urls import staticfiles_urlpatterns from api import views as api_views from admin import views as admin_views from guest import views as guest_views from login import views as login_views urlpatterns = [ url(r'^login', login_views.login, name='login'), url(r'^logout', login_views.logout, name='logout'), url(r'^admin', admin_views.index, name='admin'), url(r'^changepwd', admin_views.changepwd, name='changepwd'), url(r'^configure', admin_views.configure, name='configure'), url(r'^checkenv', admin_views.checkenv, name='checkenv'), url(r'^storage', admin_views.storage, name='storage'), url(r'^network', admin_views.network, name='network'), url(r'^vms', admin_views.vm, name='vm'), url(r'^vm/status', admin_views.vm_status, name='vm_status'), url(r'^vm/edit', admin_views.vm_edit, name='vm_edit'), url(r'^vm/delete', admin_views.vm_delete, name='vm_delete'), url(r'^vm/start', admin_views.vm_start, name='vm_start'), url(r'^vm/stop', admin_views.vm_stop, name='vm_stop'), url(r'^vm/template', admin_views.template, name='template'), url(r'^vm/snapshot', admin_views.snapshot, name='snapshot'), url(r'^vm/conninfo', admin_views.connect_info, name='conninfo'), url(r'^iso', admin_views.iso, name='iso'), url(r'^users', login_views.users, name='users'), url(r'^user/create', login_views.create_user, name='create_user'), url(r'^user/delete', login_views.delete_user, name='delete_user'), url(r'^user/edit', login_views.edit_user, name='edit_user'), url(r'^guest', guest_views.index, name='guest'), url(r'^get_vms_by_user', admin_views.get_vms_by_user, name='get_vms_by_user'), url(r'^api', api_views.index), url(r'^index', login_views.index, name='index'), url(r'^', login_views.index, name='index'), ] urlpatterns += staticfiles_urlpatterns()
# coding: utf-8 # # Copyright 2014 The Oppia Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS-IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. """Provides a seam for transaction services.""" __author__ = 'Sean Lip' from google.appengine.ext import ndb def run_in_transaction(fn, *args, **kwargs): """Run a function in a transaction.""" return ndb.transaction( lambda: fn(*args, **kwargs), xg=True, propagation=ndb.TransactionOptions.ALLOWED, ) # The NDB toplevel() function. For more details, see # https://developers.google.com/appengine/docs/python/ndb/async#intro toplevel_wrapper = ndb.toplevel
""" Master Boot Record. """ # cfdisk uses the following algorithm to compute the geometry: # 0. Use the values given by the user. # 1. Try to guess the geometry from the partition table: # if all the used partitions end at the same head H and the # same sector S, then there are (H+1) heads and S sectors/cylinder. # 2. Ask the system (ioctl/HDIO_GETGEO). # 3. 255 heads and 63 sectors/cylinder. from lib.hachoir_parser import Parser from lib.hachoir_core.field import (FieldSet, Enum, Bits, UInt8, UInt16, UInt32, RawBytes) from lib.hachoir_core.endian import LITTLE_ENDIAN from lib.hachoir_core.tools import humanFilesize from lib.hachoir_core.text_handler import textHandler, hexadecimal BLOCK_SIZE = 512 # bytes class CylinderNumber(Bits): def __init__(self, parent, name, description=None): Bits.__init__(self, parent, name, 10, description) def createValue(self): i = self.parent.stream.readInteger( self.absolute_address, False, self._size, self.parent.endian) return i >> 2 | i % 4 << 8 class PartitionHeader(FieldSet): static_size = 16*8 # taken from the source of cfdisk: # sed -n 's/.*{\(.*\), N_(\(.*\))}.*/ \1: \2,/p' i386_sys_types.c system_name = { 0x00: "Empty", 0x01: "FAT12", 0x02: "XENIX root", 0x03: "XENIX usr", 0x04: "FAT16 <32M", 0x05: "Extended", 0x06: "FAT16", 0x07: "HPFS/NTFS", 0x08: "AIX", 0x09: "AIX bootable", 0x0a: "OS/2 Boot Manager", 0x0b: "W95 FAT32", 0x0c: "W95 FAT32 (LBA)", 0x0e: "W95 FAT16 (LBA)", 0x0f: "W95 Ext'd (LBA)", 0x10: "OPUS", 0x11: "Hidden FAT12", 0x12: "Compaq diagnostics", 0x14: "Hidden FAT16 <32M", 0x16: "Hidden FAT16", 0x17: "Hidden HPFS/NTFS", 0x18: "AST SmartSleep", 0x1b: "Hidden W95 FAT32", 0x1c: "Hidden W95 FAT32 (LBA)", 0x1e: "Hidden W95 FAT16 (LBA)", 0x24: "NEC DOS", 0x39: "Plan 9", 0x3c: "PartitionMagic recovery", 0x40: "Venix 80286", 0x41: "PPC PReP Boot", 0x42: "SFS", 0x4d: "QNX4.x", 0x4e: "QNX4.x 2nd part", 0x4f: "QNX4.x 3rd part", 0x50: "OnTrack DM", 0x51: "OnTrack DM6 Aux1", 0x52: "CP/M", 0x53: "OnTrack DM6 Aux3", 0x54: "OnTrackDM6", 0x55: "EZ-Drive", 0x56: "Golden Bow", 0x5c: "Priam Edisk", 0x61: "SpeedStor", 0x63: "GNU HURD or SysV", 0x64: "Novell Netware 286", 0x65: "Novell Netware 386", 0x70: "DiskSecure Multi-Boot", 0x75: "PC/IX", 0x80: "Old Minix", 0x81: "Minix / old Linux", 0x82: "Linux swap / Solaris", 0x83: "Linux (ext2/ext3)", 0x84: "OS/2 hidden C: drive", 0x85: "Linux extended", 0x86: "NTFS volume set", 0x87: "NTFS volume set", 0x88: "Linux plaintext", 0x8e: "Linux LVM", 0x93: "Amoeba", 0x94: "Amoeba BBT", 0x9f: "BSD/OS", 0xa0: "IBM Thinkpad hibernation", 0xa5: "FreeBSD", 0xa6: "OpenBSD", 0xa7: "NeXTSTEP", 0xa8: "Darwin UFS", 0xa9: "NetBSD", 0xab: "Darwin boot", 0xb7: "BSDI fs", 0xb8: "BSDI swap", 0xbb: "Boot Wizard hidden", 0xbe: "Solaris boot", 0xbf: "Solaris", 0xc1: "DRDOS/sec (FAT-12)", 0xc4: "DRDOS/sec (FAT-16 < 32M)", 0xc6: "DRDOS/sec (FAT-16)", 0xc7: "Syrinx", 0xda: "Non-FS data", 0xdb: "CP/M / CTOS / ...", 0xde: "Dell Utility", 0xdf: "BootIt", 0xe1: "DOS access", 0xe3: "DOS R/O", 0xe4: "SpeedStor", 0xeb: "BeOS fs", 0xee: "EFI GPT", 0xef: "EFI (FAT-12/16/32)", 0xf0: "Linux/PA-RISC boot", 0xf1: "SpeedStor", 0xf4: "SpeedStor", 0xf2: "DOS secondary", 0xfd: "Linux raid autodetect", 0xfe: "LANstep", 0xff: "BBT" } def createFields(self): yield UInt8(self, "bootable", "Bootable flag (true if equals to 0x80)") if self["bootable"].value not in (0x00, 0x80): self.warning("Stream doesn't look like master boot record (partition bootable error)!") yield UInt8(self, "start_head", "Starting head number of the partition") yield Bits(self, "start_sector", 6, "Starting sector number of the partition") yield CylinderNumber(self, "start_cylinder", "Starting cylinder number of the partition") yield Enum(UInt8(self, "system", "System indicator"), self.system_name) yield UInt8(self, "end_head", "Ending head number of the partition") yield Bits(self, "end_sector", 6, "Ending sector number of the partition") yield CylinderNumber(self, "end_cylinder", "Ending cylinder number of the partition") yield UInt32(self, "LBA", "LBA (number of sectors before this partition)") yield UInt32(self, "size", "Size (block count)") def isUsed(self): return self["system"].value != 0 def createDescription(self): desc = "Partition header: " if self.isUsed(): system = self["system"].display size = self["size"].value * BLOCK_SIZE desc += "%s, %s" % (system, humanFilesize(size)) else: desc += "(unused)" return desc class MasterBootRecord(FieldSet): static_size = 512*8 def createFields(self): yield RawBytes(self, "program", 446, "Boot program (Intel x86 machine code)") yield PartitionHeader(self, "header[0]") yield PartitionHeader(self, "header[1]") yield PartitionHeader(self, "header[2]") yield PartitionHeader(self, "header[3]") yield textHandler(UInt16(self, "signature", "Signature (0xAA55)"), hexadecimal) def _getPartitions(self): return ( self[index] for index in xrange(1,5) ) headers = property(_getPartitions) class Partition(FieldSet): def createFields(self): mbr = MasterBootRecord(self, "mbr") yield mbr # No error if we only want to analyse a backup of a mbr if self.eof: return for start, index, header in sorted((hdr["LBA"].value, index, hdr) for index, hdr in enumerate(mbr.headers) if hdr.isUsed()): # Seek to the beginning of the partition padding = self.seekByte(start * BLOCK_SIZE, "padding[]") if padding: yield padding # Content of the partition name = "partition[%u]" % index size = BLOCK_SIZE * header["size"].value desc = header["system"].display if header["system"].value == 5: yield Partition(self, name, desc, size * 8) else: yield RawBytes(self, name, size, desc) # Padding at the end if self.current_size < self._size: yield self.seekBit(self._size, "end") class MSDos_HardDrive(Parser, Partition): endian = LITTLE_ENDIAN MAGIC = "\x55\xAA" PARSER_TAGS = { "id": "msdos_harddrive", "category": "file_system", "description": "MS-DOS hard drive with Master Boot Record (MBR)", "min_size": 512*8, "file_ext": ("",), # "magic": ((MAGIC, 510*8),), } def validate(self): if self.stream.readBytes(510*8, 2) != self.MAGIC: return "Invalid signature" used = False for hdr in self["mbr"].headers: if hdr["bootable"].value not in (0x00, 0x80): return "Wrong boot flag" used |= hdr.isUsed() return used or "No partition found"
#!/usr/bin/env python ## ## Copyright 2016 SRI International ## See COPYING file distributed along with the package for the copyright and license terms. ## import pandas import Rwrapper # # Variables from surveys needed for CTQ # # LimeSurvey field names lime_fields = [ "ctq_set1 [ctq1]", "ctq_set1 [ctq2]", "ctq_set1 [ctq3]", "ctq_set1 [ctq4]", "ctq_set1 [ctq5]", "ctq_set1 [ctq6]", "ctq_set1 [ctq7]", "ctq_set2 [ctq8]", "ctq_set2 [ctq9]", "ctq_set2 [ct10]", "ctq_set2 [ct11]", "ctq_set2 [ct12]", "ctq_set2 [ct13]", "ctq_set2 [ct14]", "ctq_set3 [ctq15]", "ctq_set3 [ctq16]", "ctq_set3 [ctq17]", "ctq_set3 [ctq18]", "ctq_set3 [ctq19]", "ctq_set3 [ctq20]", "ctq_set3 [ctq21]", "ctq_set4 [ctq22]", "ctq_set4 [ctq23]", "ctq_set4 [ctq24]", "ctq_set4 [ctq25]", "ctq_set4 [ctq26]", "ctq_set4 [ctq27]", "ctq_set4 [ctq28]" ] # Dictionary to recover LimeSurvey field names from REDCap names rc2lime = dict() for field in lime_fields: rc2lime[Rwrapper.label_to_sri( 'youthreport2', field )] = field # REDCap fields names input_fields = { 'mrireport' : [ 'youth_report_2_complete', 'youthreport2_missing' ] + rc2lime.keys() } # # This determines the name of the form in REDCap where the results are posted. # output_form = 'clinical' # # CTQ field names mapping from R to REDCap # R2rc = { 'Emotional Abuse Scale Total Score' : 'ctq_ea', 'Physical Abuse Scale Total Score' : 'ctq_pa', 'Sexual Abuse Scale Total Score' : 'ctq_sa', 'Emotional Neglect Scale Total Score' : 'ctq_en', 'Physical Neglect Scale Total Score' : 'ctq_pn', 'Minimization/Denial Scale Total Score' : 'ctq_minds' } # # Scoring function - take requested data (as requested by "input_fields") for each (subject,event), and demographics (date of birth, gender) for each subject. # def compute_scores( data, demographics ): # Get rid of all records that don't have YR2 data.dropna( axis=1, subset=['youth_report_2_complete'] ) data = data[ data['youth_report_2_complete'] > 0 ] data = data[ ~(data['youthreport2_missing'] > 0) ] # If no records to score, return empty DF if len( data ) == 0: return pandas.DataFrame() # Replace all column labels with the original LimeSurvey names data.columns = Rwrapper.map_labels( data.columns, rc2lime ) # Call the scoring function for all table rows scores = data.apply( Rwrapper.runscript, axis=1, Rscript='ctq/CTQ.R', scores_key='CTQ.ary' ) # Replace all score columns with REDCap field names scores.columns = Rwrapper.map_labels( scores.columns, R2rc ) # Simply copy completion status from the input surveys scores['ctq_complete'] = data['youth_report_2_complete'].map( int ) # Make a proper multi-index for the scores table scores.index = pandas.MultiIndex.from_tuples(scores.index) scores.index.names = ['study_id', 'redcap_event_name'] # Return the computed scores - this is what will be imported back into REDCap outfield_list = [ 'ctq_complete' ] + R2rc.values() return scores[ outfield_list ]
import win32security,win32file,win32api,ntsecuritycon,win32con from security_enums import TRUSTEE_TYPE,TRUSTEE_FORM,ACE_FLAGS,ACCESS_MODE new_privs = ((win32security.LookupPrivilegeValue('',ntsecuritycon.SE_SECURITY_NAME),win32con.SE_PRIVILEGE_ENABLED), (win32security.LookupPrivilegeValue('',ntsecuritycon.SE_CREATE_PERMANENT_NAME),win32con.SE_PRIVILEGE_ENABLED), (win32security.LookupPrivilegeValue('','SeEnableDelegationPrivilege'),win32con.SE_PRIVILEGE_ENABLED) ##doesn't seem to be in ntsecuritycon.py ? ) ph = win32api.GetCurrentProcess() th = win32security.OpenProcessToken(ph,win32security.TOKEN_ALL_ACCESS) ##win32con.TOKEN_ADJUST_PRIVILEGES) win32security.AdjustTokenPrivileges(th,0,new_privs) policy_handle = win32security.GetPolicyHandle('',win32security.POLICY_ALL_ACCESS) tmp_sid = win32security.LookupAccountName('','tmp')[0] privs=[ntsecuritycon.SE_DEBUG_NAME,ntsecuritycon.SE_TCB_NAME,ntsecuritycon.SE_RESTORE_NAME,ntsecuritycon.SE_REMOTE_SHUTDOWN_NAME] win32security.LsaAddAccountRights(policy_handle,tmp_sid,privs) privlist=win32security.LsaEnumerateAccountRights(policy_handle,tmp_sid) for priv in privlist: print priv privs=[ntsecuritycon.SE_DEBUG_NAME,ntsecuritycon.SE_TCB_NAME] win32security.LsaRemoveAccountRights(policy_handle,tmp_sid,0,privs) privlist=win32security.LsaEnumerateAccountRights(policy_handle,tmp_sid) for priv in privlist: print priv win32security.LsaClose(policy_handle)
import src.irulez.util as util import src.irulez.topic_factory as topic_factory import src.irulez.log as log import paho.mqtt.client as mqtt import uuid logger = log.get_logger('dimmer_mqtt_sender') class MqttSender: def __init__(self, client: mqtt.Client): self.__client = client def publish_dimming_action_to_timer(self, dimming_action_id: uuid.UUID, delay: int): publish_topic = topic_factory.create_timer_dimmer_timer_fired_topic() topic_name = topic_factory.create_timer_dimmer_timer_fired_response_topic() payload = util.serialize_json({ 'topic': topic_name, 'payload': str(dimming_action_id), 'delay': delay }) logger.debug(f"Publishing: {publish_topic}{payload}") self.__client.publish(publish_topic, payload, 0, False) def publish_dimming_action_to_arduino(self, arduino_name: str, pin_number: int, dim_value: int): publish_topic = topic_factory.create_arduino_dim_action_topic(arduino_name, pin_number) logger.debug(f"Publishing: {publish_topic} / {dim_value}") self.__client.publish(publish_topic, dim_value, 0, False)
#!/usr/bin/python # Copyright (C) Vladimir Prus 2006. # Distributed under the Boost Software License, Version 1.0. (See # accompanying file LICENSE_1_0.txt or copy at # http://www.boost.org/LICENSE_1_0.txt) import BoostBuild t = BoostBuild.Tester(use_test_config=False) t.write("a.cpp", "int main() {}\n") t.write("jamroot.jam", "exe a : a.cpp sub1//sub1 sub2//sub2 sub3//sub3 ;") t.write("sub1/jamfile.jam", """\ lib sub1 : sub1.cpp sub1_2 ../sub2//sub2 ; lib sub1_2 : sub1_2.cpp ; """) t.write("sub1/sub1.cpp", """\ #ifdef _WIN32 __declspec(dllexport) #endif void sub1() {} """) t.write("sub1/sub1_2.cpp", """\ #ifdef _WIN32 __declspec(dllexport) #endif void sub1() {} """) t.write("sub2/jamfile.jam", "lib sub2 : sub2.cpp ;") t.write("sub2/sub2.cpp", """\ #ifdef _WIN32 __declspec(dllexport) #endif void sub2() {} """) t.write("sub3/jamroot.jam", "lib sub3 : sub3.cpp ;") t.write("sub3/sub3.cpp", """\ #ifdef _WIN32 __declspec(dllexport) #endif void sub3() {} """) # 'clean' should not remove files under separate jamroot.jam. t.run_build_system() t.run_build_system(["--clean"]) t.expect_removal("bin/$toolset/debug/a.obj") t.expect_removal("sub1/bin/$toolset/debug/sub1.obj") t.expect_removal("sub1/bin/$toolset/debug/sub1_2.obj") t.expect_removal("sub2/bin/$toolset/debug/sub2.obj") t.expect_nothing("sub3/bin/$toolset/debug/sub3.obj") # 'clean-all' removes everything it can reach. t.run_build_system() t.run_build_system(["--clean-all"]) t.expect_removal("bin/$toolset/debug/a.obj") t.expect_removal("sub1/bin/$toolset/debug/sub1.obj") t.expect_removal("sub1/bin/$toolset/debug/sub1_2.obj") t.expect_removal("sub2/bin/$toolset/debug/sub2.obj") t.expect_nothing("sub3/bin/$toolset/debug/sub3.obj") # 'clean' together with project target removes only under that project. t.run_build_system() t.run_build_system(["sub1", "--clean"]) t.expect_nothing("bin/$toolset/debug/a.obj") t.expect_removal("sub1/bin/$toolset/debug/sub1.obj") t.expect_removal("sub1/bin/$toolset/debug/sub1_2.obj") t.expect_nothing("sub2/bin/$toolset/debug/sub2.obj") t.expect_nothing("sub3/bin/$toolset/debug/sub3.obj") # 'clean-all' removes everything. t.run_build_system() t.run_build_system(["sub1", "--clean-all"]) t.expect_nothing("bin/$toolset/debug/a.obj") t.expect_removal("sub1/bin/$toolset/debug/sub1.obj") t.expect_removal("sub1/bin/$toolset/debug/sub1_2.obj") t.expect_removal("sub2/bin/$toolset/debug/sub2.obj") t.expect_nothing("sub3/bin/$toolset/debug/sub3.obj") # If main target is explicitly named, we should not remove files from other # targets. t.run_build_system() t.run_build_system(["sub1//sub1", "--clean"]) t.expect_removal("sub1/bin/$toolset/debug/sub1.obj") t.expect_nothing("sub1/bin/$toolset/debug/sub1_2.obj") t.expect_nothing("sub2/bin/$toolset/debug/sub2.obj") t.expect_nothing("sub3/bin/$toolset/debug/sub3.obj") # Regression test: sources of the 'cast' rule were mistakenly deleted. t.rm(".") t.write("jamroot.jam", """\ import cast ; cast a cpp : a.h ; """) t.write("a.h", "") t.run_build_system(["--clean"]) t.expect_nothing("a.h") t.cleanup()
#!/usr/bin/env python # Test whether a client sends a correct PUBLISH to a topic with QoS 2. # The client should connect to port 1888 with keepalive=60, clean session set, # and client id publish-qos2-test # The test will send a CONNACK message to the client with rc=0. Upon receiving # the CONNACK the client should verify that rc==0. If not, it should exit with # return code=1. # On a successful CONNACK, the client should send a PUBLISH message with topic # "pub/qos2/test", payload "message" and QoS=2. # The test will not respond to the first PUBLISH message, so the client must # resend the PUBLISH message with dup=1. Note that to keep test durations low, a # message retry timeout of less than 10 seconds is required for this test. # On receiving the second PUBLISH message, the test will send the correct # PUBREC response. On receiving the correct PUBREC response, the client should # send a PUBREL message. # The test will not respond to the first PUBREL message, so the client must # resend the PUBREL message with dup=1. On receiving the second PUBREL message, # the test will send the correct PUBCOMP response. On receiving the correct # PUBCOMP response, the client should send a DISCONNECT message. import inspect import os import subprocess import socket import sys import time # From http://stackoverflow.com/questions/279237/python-import-a-module-from-a-folder cmd_subfolder = os.path.realpath(os.path.abspath(os.path.join(os.path.split(inspect.getfile( inspect.currentframe() ))[0],".."))) if cmd_subfolder not in sys.path: sys.path.insert(0, cmd_subfolder) import mosq_test rc = 1 keepalive = 60 connect_packet = mosq_test.gen_connect("publish-qos2-test", keepalive=keepalive) connack_packet = mosq_test.gen_connack(rc=0) disconnect_packet = mosq_test.gen_disconnect() mid = 1 publish_packet = mosq_test.gen_publish("pub/qos2/test", qos=2, mid=mid, payload="message") publish_dup_packet = mosq_test.gen_publish("pub/qos2/test", qos=2, mid=mid, payload="message", dup=True) pubrec_packet = mosq_test.gen_pubrec(mid) pubrel_packet = mosq_test.gen_pubrel(mid) pubcomp_packet = mosq_test.gen_pubcomp(mid) sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) sock.settimeout(10) sock.bind(('', 1888)) sock.listen(5) client_args = sys.argv[1:] env = dict(os.environ) env['LD_LIBRARY_PATH'] = '../../lib:../../lib/cpp' try: pp = env['PYTHONPATH'] except KeyError: pp = '' env['PYTHONPATH'] = '../../lib/python:'+pp client = mosq_test.start_client(filename=sys.argv[1].replace('/', '-'), cmd=client_args, env=env) try: (conn, address) = sock.accept() conn.settimeout(10) if mosq_test.expect_packet(conn, "connect", connect_packet): conn.send(connack_packet) if mosq_test.expect_packet(conn, "publish", publish_packet): conn.send(pubrec_packet) if mosq_test.expect_packet(conn, "pubrel", pubrel_packet): conn.send(pubcomp_packet) if mosq_test.expect_packet(conn, "disconnect", disconnect_packet): rc = 0 conn.close() finally: client.terminate() client.wait() sock.close() exit(rc)
# Copyright 2017 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== """Tests for `tf.data.Dataset.interleave()`.""" from __future__ import absolute_import from __future__ import division from __future__ import print_function import multiprocessing from absl.testing import parameterized import numpy as np from tensorflow.python.data.kernel_tests import test_base from tensorflow.python.data.ops import dataset_ops from tensorflow.python.framework import errors from tensorflow.python.framework import sparse_tensor from tensorflow.python.framework import test_util from tensorflow.python.ops import array_ops from tensorflow.python.ops import sparse_ops from tensorflow.python.platform import test def _interleave(lists, cycle_length, block_length): """Reference implementation of interleave used for testing. Args: lists: a list of lists to interleave cycle_length: the length of the interleave cycle block_length: the length of the interleave block Yields: Elements of `lists` interleaved in the order determined by `cycle_length` and `block_length`. """ num_open = 0 # `all_iterators` acts as a queue of iterators over each element of `lists`. all_iterators = [iter(l) for l in lists] # `open_iterators` are the iterators whose elements are currently being # interleaved. open_iterators = [] if cycle_length == dataset_ops.AUTOTUNE: cycle_length = multiprocessing.cpu_count() for i in range(cycle_length): if all_iterators: open_iterators.append(all_iterators.pop(0)) num_open += 1 else: open_iterators.append(None) while num_open or all_iterators: for i in range(cycle_length): if open_iterators[i] is None: if all_iterators: open_iterators[i] = all_iterators.pop(0) num_open += 1 else: continue for _ in range(block_length): try: yield next(open_iterators[i]) except StopIteration: open_iterators[i] = None num_open -= 1 break def _repeat(values, count): """Produces a list of lists suitable for testing interleave. Args: values: for each element `x` the result contains `[x] * x` count: determines how many times to repeat `[x] * x` in the result Returns: A list of lists of values suitable for testing interleave. """ return [[value] * value for value in np.tile(values, count)] @test_util.run_all_in_graph_and_eager_modes class InterleaveTest(test_base.DatasetTestBase, parameterized.TestCase): @parameterized.named_parameters( ("1", [4, 5, 6], 1, 1, [ 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6 ]), ("2", [4, 5, 6], 2, 1, [ 4, 5, 4, 5, 4, 5, 4, 5, 5, 6, 6, 4, 6, 4, 6, 4, 6, 4, 6, 5, 6, 5, 6, 5, 6, 5, 6, 5, 6, 6 ]), ("3", [4, 5, 6], 2, 3, [ 4, 4, 4, 5, 5, 5, 4, 5, 5, 6, 6, 6, 4, 4, 4, 6, 6, 6, 4, 5, 5, 5, 6, 6, 6, 5, 5, 6, 6, 6 ]), ("4", [4, 5, 6], 7, 2, [ 4, 4, 5, 5, 6, 6, 4, 4, 5, 5, 6, 6, 4, 4, 5, 5, 6, 6, 4, 4, 5, 5, 6, 6, 5, 6, 6, 5, 6, 6 ]), ("5", [4, 0, 6], 2, 1, [4, 4, 6, 4, 6, 4, 6, 6, 4, 6, 4, 6, 4, 4, 6, 6, 6, 6, 6, 6]), ) def testPythonImplementation(self, input_values, cycle_length, block_length, expected_elements): input_lists = _repeat(input_values, 2) for expected, produced in zip( expected_elements, _interleave(input_lists, cycle_length, block_length)): self.assertEqual(expected, produced) @parameterized.named_parameters( ("1", np.int64([4, 5, 6]), 1, 3, None), ("2", np.int64([4, 5, 6]), 1, 3, 1), ("3", np.int64([4, 5, 6]), 2, 1, None), ("4", np.int64([4, 5, 6]), 2, 1, 1), ("5", np.int64([4, 5, 6]), 2, 1, 2), ("6", np.int64([4, 5, 6]), 2, 3, None), ("7", np.int64([4, 5, 6]), 2, 3, 1), ("8", np.int64([4, 5, 6]), 2, 3, 2), ("9", np.int64([4, 5, 6]), 7, 2, None), ("10", np.int64([4, 5, 6]), 7, 2, 1), ("11", np.int64([4, 5, 6]), 7, 2, 3), ("12", np.int64([4, 5, 6]), 7, 2, 5), ("13", np.int64([4, 5, 6]), 7, 2, 7), ("14", np.int64([4, 5, 6]), dataset_ops.AUTOTUNE, 3, None), ("15", np.int64([4, 5, 6]), dataset_ops.AUTOTUNE, 3, 1), ("16", np.int64([]), 2, 3, None), ("17", np.int64([0, 0, 0]), 2, 3, None), ("18", np.int64([4, 0, 6]), 2, 3, None), ("19", np.int64([4, 0, 6]), 2, 3, 1), ("20", np.int64([4, 0, 6]), 2, 3, 2), ) def testInterleaveDataset(self, input_values, cycle_length, block_length, num_parallel_calls): count = 2 dataset = dataset_ops.Dataset.from_tensor_slices(input_values).repeat( count).interleave( lambda x: dataset_ops.Dataset.from_tensors(x).repeat(x), cycle_length, block_length, num_parallel_calls) expected_output = [ element for element in _interleave( _repeat(input_values, count), cycle_length, block_length) ] self.assertDatasetProduces(dataset, expected_output) @parameterized.named_parameters( ("1", np.float32([1., np.nan, 2., np.nan, 3.]), 1, 3, None), ("2", np.float32([1., np.nan, 2., np.nan, 3.]), 1, 3, 1), ("3", np.float32([1., np.nan, 2., np.nan, 3.]), 2, 1, None), ("4", np.float32([1., np.nan, 2., np.nan, 3.]), 2, 1, 1), ("5", np.float32([1., np.nan, 2., np.nan, 3.]), 2, 1, 2), ("6", np.float32([1., np.nan, 2., np.nan, 3.]), 2, 3, None), ("7", np.float32([1., np.nan, 2., np.nan, 3.]), 2, 3, 1), ("8", np.float32([1., np.nan, 2., np.nan, 3.]), 2, 3, 2), ("9", np.float32([1., np.nan, 2., np.nan, 3.]), 7, 2, None), ("10", np.float32([1., np.nan, 2., np.nan, 3.]), 7, 2, 1), ("11", np.float32([1., np.nan, 2., np.nan, 3.]), 7, 2, 3), ("12", np.float32([1., np.nan, 2., np.nan, 3.]), 7, 2, 5), ("13", np.float32([1., np.nan, 2., np.nan, 3.]), 7, 2, 7), ) def testInterleaveDatasetError(self, input_values, cycle_length, block_length, num_parallel_calls): dataset = dataset_ops.Dataset.from_tensor_slices(input_values).map( lambda x: array_ops.check_numerics(x, "message")).interleave( dataset_ops.Dataset.from_tensors, cycle_length, block_length, num_parallel_calls) get_next = self.getNext(dataset) for value in input_values: if np.isnan(value): with self.assertRaises(errors.InvalidArgumentError): self.evaluate(get_next()) else: self.assertEqual(value, self.evaluate(get_next())) with self.assertRaises(errors.OutOfRangeError): self.evaluate(get_next()) def testInterleaveSparse(self): def _map_fn(i): return sparse_tensor.SparseTensorValue( indices=[[0, 0], [1, 1]], values=(i * [1, -1]), dense_shape=[2, 2]) def _interleave_fn(x): return dataset_ops.Dataset.from_tensor_slices( sparse_ops.sparse_to_dense(x.indices, x.dense_shape, x.values)) dataset = dataset_ops.Dataset.range(10).map(_map_fn).interleave( _interleave_fn, cycle_length=1) get_next = self.getNext(dataset) for i in range(10): for j in range(2): expected = [i, 0] if j % 2 == 0 else [0, -i] self.assertAllEqual(expected, self.evaluate(get_next())) with self.assertRaises(errors.OutOfRangeError): self.evaluate(get_next()) with self.assertRaises(errors.OutOfRangeError): self.evaluate(get_next()) @parameterized.named_parameters( ("1", np.int64([4, 5, 6]), 1, 3, 1), ("2", np.int64([4, 5, 6]), 2, 1, 1), ("3", np.int64([4, 5, 6]), 2, 1, 2), ("4", np.int64([4, 5, 6]), 2, 3, 1), ("5", np.int64([4, 5, 6]), 2, 3, 2), ("6", np.int64([4, 5, 6]), 7, 2, 1), ("7", np.int64([4, 5, 6]), 7, 2, 3), ("8", np.int64([4, 5, 6]), 7, 2, 5), ("9", np.int64([4, 5, 6]), 7, 2, 7), ("10", np.int64([4, 5, 6]), dataset_ops.AUTOTUNE, 3, 1), ("11", np.int64([4, 0, 6]), 2, 3, 1), ("12", np.int64([4, 0, 6]), 2, 3, 2), ) def testSloppyInterleaveDataset(self, input_values, cycle_length, block_length, num_parallel_calls): count = 2 dataset = dataset_ops.Dataset.from_tensor_slices(input_values).repeat( count).interleave( lambda x: dataset_ops.Dataset.from_tensors(x).repeat(x), cycle_length, block_length, num_parallel_calls) options = dataset_ops.Options() options.experimental_deterministic = False dataset = dataset.with_options(options) expected_output = [ element for element in _interleave( _repeat(input_values, count), cycle_length, block_length) ] get_next = self.getNext(dataset) actual_output = [] for _ in range(len(expected_output)): actual_output.append(self.evaluate(get_next())) self.assertAllEqual(expected_output.sort(), actual_output.sort()) def testInterleaveMap(self): dataset = dataset_ops.Dataset.range(100) def interleave_fn(x): dataset = dataset_ops.Dataset.from_tensors(x) return dataset.map(lambda x: x + x) dataset = dataset.interleave(interleave_fn, cycle_length=5) dataset = dataset.interleave(interleave_fn, cycle_length=5) self.assertDatasetProduces(dataset, [4 * x for x in range(100)]) if __name__ == "__main__": test.main()
# coding=utf-8 # Copyright 2018 Pants project contributors (see CONTRIBUTORS.md). # Licensed under the Apache License, Version 2.0 (see LICENSE). from __future__ import absolute_import, division, print_function, unicode_literals import logging from builtins import object from pants.contrib.node.subsystems.command import command_gen LOG = logging.getLogger(__name__) PACKAGE_MANAGER_NPM = 'npm' PACKAGE_MANAGER_YARNPKG = 'yarnpkg' PACKAGE_MANAGER_YARNPKG_ALIAS = 'yarn' VALID_PACKAGE_MANAGERS = [PACKAGE_MANAGER_NPM, PACKAGE_MANAGER_YARNPKG, PACKAGE_MANAGER_YARNPKG_ALIAS] # TODO: Change to enum type when migrated to Python 3.4+ class PackageInstallationTypeOption(object): PROD = 'prod' DEV = 'dev' PEER = 'peer' BUNDLE = 'bundle' OPTIONAL = 'optional' NO_SAVE = 'not saved' class PackageInstallationVersionOption(object): EXACT = 'exact' TILDE = 'tilde' class PackageManager(object): """Defines node package manager functionalities.""" def __init__(self, name, tool_installations): self.name = name self.tool_installations = tool_installations def _get_installation_args(self, install_optional, production_only, force, frozen_lockfile): """Returns command line args for installing package. :param install_optional: True to request install optional dependencies. :param production_only: True to only install production dependencies, i.e. ignore devDependencies. :param force: True to force re-download dependencies. :param frozen_lockfile: True to disallow automatic update of lock files. :rtype: list of strings """ raise NotImplementedError def _get_run_script_args(self): """Returns command line args to run a package.json script. :rtype: list of strings """ raise NotImplementedError def _get_add_package_args(self, package, type_option, version_option): """Returns command line args to add a node pacakge. :rtype: list of strings """ raise NotImplementedError() def run_command(self, args=None, node_paths=None): """Returns a command that when executed will run an arbitury command via package manager.""" return command_gen( self.tool_installations, self.name, args=args, node_paths=node_paths ) def install_module( self, install_optional=False, production_only=False, force=False, frozen_lockfile=True, node_paths=None): """Returns a command that when executed will install node package. :param install_optional: True to install optional dependencies. :param production_only: True to only install production dependencies, i.e. ignore devDependencies. :param force: True to force re-download dependencies. :param frozen_lockfile: True to disallow automatic update of lock files. :param node_paths: A list of path that should be included in $PATH when running installation. """ args=self._get_installation_args( install_optional=install_optional, production_only=production_only, force=force, frozen_lockfile=frozen_lockfile) return self.run_command(args=args, node_paths=node_paths) def run_script(self, script_name, script_args=None, node_paths=None): """Returns a command to execute a package.json script. :param script_name: Name of the script to name. Note that script name 'test' can be used to run node tests. :param script_args: Args to be passed to package.json script. :param node_paths: A list of path that should be included in $PATH when running the script. """ # TODO: consider add a pants.util function to manipulate command line. package_manager_args = self._get_run_script_args() package_manager_args.append(script_name) if script_args: package_manager_args.append('--') package_manager_args.extend(script_args) return self.run_command(args=package_manager_args, node_paths=node_paths) def add_package( self, package, node_paths=None, type_option=PackageInstallationTypeOption.PROD, version_option=None): """Returns a command that when executed will add a node package to current node module. :param package: string. A valid npm/yarn package description. The accepted forms are package-name, package-name@version, package-name@tag, file:/folder, file:/path/to.tgz https://url/to.tgz :param node_paths: A list of path that should be included in $PATH when running the script. :param type_option: A value from PackageInstallationTypeOption that indicates the type of package to be installed. Default to 'prod', which is a production dependency. :param version_option: A value from PackageInstallationVersionOption that indicates how to match version. Default to None, which uses package manager default. """ args=self._get_add_package_args( package, type_option=type_option, version_option=version_option) return self.run_command(args=args, node_paths=node_paths) def run_cli(self, cli, args=None, node_paths=None): """Returns a command that when executed will run an installed cli via package manager.""" cli_args = [cli] if args: cli_args.append('--') cli_args.extend(args) return self.run_command(args=cli_args, node_paths=node_paths) class PackageManagerYarnpkg(PackageManager): def __init__(self, tool_installation): super(PackageManagerYarnpkg, self).__init__(PACKAGE_MANAGER_YARNPKG, tool_installation) def _get_run_script_args(self): return ['run'] def _get_installation_args(self, install_optional, production_only, force, frozen_lockfile): return_args = ['--non-interactive'] if not install_optional: return_args.append('--ignore-optional') if production_only: return_args.append('--production=true') if force: return_args.append('--force') if frozen_lockfile: return_args.append('--frozen-lockfile') return return_args def _get_add_package_args(self, package, type_option, version_option): return_args = ['add', package] package_type_option = { PackageInstallationTypeOption.PROD: '', # Yarn save production is the default. PackageInstallationTypeOption.DEV: '--dev', PackageInstallationTypeOption.PEER: '--peer', PackageInstallationTypeOption.OPTIONAL: '--optional', PackageInstallationTypeOption.BUNDLE: None, PackageInstallationTypeOption.NO_SAVE: None, }.get(type_option) if package_type_option is None: LOG.warning('{} does not support {} packages, ignored.'.format(self.name, type_option)) elif package_type_option: # Skip over '' entries return_args.append(package_type_option) package_version_option = { PackageInstallationVersionOption.EXACT: '--exact', PackageInstallationVersionOption.TILDE: '--tilde', }.get(version_option) if package_version_option is None: LOG.warning( '{} does not support install with {} version, ignored'.format(self.name, version_option)) elif package_version_option: # Skip over '' entries return_args.append(package_version_option) return return_args class PackageManagerNpm(PackageManager): def __init__(self, tool_installation): super(PackageManagerNpm, self).__init__(PACKAGE_MANAGER_NPM, tool_installation) def _get_run_script_args(self): return ['run-script'] def _get_installation_args(self, install_optional, production_only, force, frozen_lockfile): return_args = ['install'] if not install_optional: return_args.append('--no-optional') if production_only: return_args.append('--production') if force: return_args.append('--force') if frozen_lockfile: LOG.warning('{} does not support frozen lockfile option. Ignored.'.format(self.name)) return return_args def _get_add_package_args(self, package, type_option, version_option): return_args = ['install', package] package_type_option = { PackageInstallationTypeOption.PROD: '--save-prod', PackageInstallationTypeOption.DEV: '--save-dev', PackageInstallationTypeOption.PEER: None, PackageInstallationTypeOption.OPTIONAL: '--save-optional', PackageInstallationTypeOption.BUNDLE: '--save-bundle', PackageInstallationTypeOption.NO_SAVE: '--no-save', }.get(type_option) if package_type_option is None: LOG.warning('{} does not support {} packages, ignored.'.format(self.name, type_option)) elif package_type_option: # Skip over '' entries return_args.append(package_type_option) package_version_option = { PackageInstallationVersionOption.EXACT: '--save-exact', PackageInstallationVersionOption.TILDE: None, }.get(version_option) if package_version_option is None: LOG.warning( '{} does not support install with {} version, ignored.'.format(self.name, version_option)) elif package_version_option: # Skip over '' entries return_args.append(package_version_option) return return_args def run_cli(self, cli, args=None, node_paths=None): raise RuntimeError('npm does not support run cli directly. Please use Yarn instead.')
# -*- coding: utf-8 -*- # # Configuration file for the Sphinx documentation builder. # # This file does only contain a selection of the most common options. For a # full list see the documentation: # http://www.sphinx-doc.org/en/stable/config # -- Path setup -------------------------------------------------------------- # If extensions (or modules to document with autodoc) are in another directory, # add these directories to sys.path here. If the directory is relative to the # documentation root, use os.path.abspath to make it absolute, like shown here. # import os import sys import sphinx_bootstrap_theme # sys.path.insert(0, os.path.abspath('.')) sys.path.insert(0, os.path.abspath('../..')) # -- Project information ----------------------------------------------------- project = 'blenderseed' copyright = '2010-2018, The appleseedhq Organization' author = 'The appleseedhq Organization' # The short X.Y version version = '2.0.0-beta' # The full version, including alpha/beta/rc tags release = '2.0.0-beta' # -- General configuration --------------------------------------------------- # If your documentation needs a minimal Sphinx version, state it here. # # needs_sphinx = '1.0' # Add any Sphinx extension module names here, as strings. They can be # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom # ones. extensions = ['sphinx.ext.intersphinx'] intersphinx_mapping = {'appleseed_maya': ('http://appleseed.readthedocs.io/projects/appleseed-maya/en/latest/', None)} # Add any paths that contain templates here, relative to this directory. templates_path = ['_templates'] # The suffix(es) of source filenames. # You can specify multiple suffix as a list of string: # # source_suffix = ['.rst', '.md'] source_suffix = '.rst' # The master toctree document. master_doc = 'index' # The language for content autogenerated by Sphinx. Refer to documentation # for a list of supported languages. # # This is also used if you do content translation via gettext catalogs. # Usually you set "language" from the command line for these cases. language = 'en' # List of patterns, relative to source directory, that match files and # directories to ignore when looking for source files. # This pattern also affects html_static_path and html_extra_path . exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store'] # The name of the Pygments (syntax highlighting) style to use. pygments_style = 'sphinx' # -- Options for HTML output ------------------------------------------------- # The theme to use for HTML and HTML Help pages. See the documentation for # a list of builtin themes. # html_theme = 'bootstrap' html_theme_path = sphinx_bootstrap_theme.get_html_theme_path() # Theme options are theme-specific and customize the look and feel of a theme # further. For a list of options available for each theme, see the # documentation. # html_theme_options = { # Navigation bar title. (Default: ``project`` value) 'navbar_title': "blenderseed", # Tab name for entire site. (Default: "Site") 'navbar_site_name': "Site", # A list of tuples containing pages or urls to link to. # Valid tuples should be in the following forms: # (name, page) # a link to a page # (name, "/aa/bb", 1) # a link to an arbitrary relative url # (name, "http://example.com", True) # arbitrary absolute url # Note the "1" or "True" value above as the third argument to indicate # an arbitrary url. 'navbar_links': [ ("Features", "features"), ("Installation", "installation"), ("Reference", "reference"), ("About", "about"), ("Tutorials", "tutorials"), ("appleseedhq", "https://appleseedhq.net", True), # ("vimeo", "https://vimeo.com/appleseedhq", True) ], # Render the next and previous page links in navbar. (Default: true) 'navbar_sidebarrel': False, # Render the current pages TOC in the navbar. (Default: true) 'navbar_pagenav': False, # Tab name for the current pages TOC. (Default: "Page") # 'navbar_pagenav_name': "Page", # Global TOC depth for "site" navbar tab. (Default: 1) # Switching to -1 shows all levels. 'globaltoc_depth': -1, # Include hidden TOCs in Site navbar? # # Note: If this is "false", you cannot have mixed ``:hidden:`` and # non-hidden ``toctree`` directives in the same page, or else the build # will break. # # Values: "true" (default) or "false" 'globaltoc_includehidden': "false", # HTML navbar class (Default: "navbar") to attach to <div> element. # For black navbar, do "navbar navbar-inverse" # 'navbar_class': "navbar navbar-inverse", 'navbar_class': "navbar navbar", # Fix navigation bar to top of page? # Values: "true" (default) or "false" 'navbar_fixed_top': "true", # Location of link to source. # Options are "nav" (default), "footer" or anything else to exclude. 'source_link_position': "footer", # Bootswatch (http://bootswatch.com/) theme. # # Options are nothing (default) or the name of a valid theme # such as "amelia" or "cosmo". # 'bootswatch_theme': "lumen", # 'bootswatch_theme': "sandstone", # 'bootswatch_theme': "readable", 'bootswatch_theme': "yeti", # Choose Bootstrap version. # Values: "3" (default) or "2" (in quotes) 'bootstrap_version': "3", } def setup(app): app.add_stylesheet("css/blockquote_custom1.css") html_logo = "_static/appleseed-logo.png" # The name of an image file (within the static path) to use as favicon of the # docs. This file should be a Windows icon file (.ico) being 16x16 or 32x32 # pixels large. html_favicon = "_static/appleseed-favicon.ico" # Add any paths that contain custom static files (such as style sheets) here, # relative to this directory. They are copied after the builtin static files, # so a file named "default.css" will overwrite the builtin "default.css". html_static_path = ['_static'] # Custom sidebar templates, must be a dictionary that maps document names # to template names. # # The default sidebars (for documents that don't match any pattern) are # defined by theme itself. Builtin themes are using these templates by # default: ``['localtoc.html', 'relations.html', 'sourcelink.html', # 'searchbox.html']``. # html_sidebars = { '**': ['localtoc.html', 'searchbox.html'], 'using/windows': ['windowssidebar.html', 'searchbox.html'], } # -- Options for HTMLHelp output --------------------------------------------- # Output file base name for HTML help builder. htmlhelp_basename = 'blenderseedManualdoc' # -- Options for LaTeX output ------------------------------------------------ latex_elements = { # The paper size ('letterpaper' or 'a4paper'). # # 'papersize': 'letterpaper', # The font size ('10pt', '11pt' or '12pt'). # # 'pointsize': '10pt', # Additional stuff for the LaTeX preamble. # # 'preamble': '', # Latex figure (float) alignment # # 'figure_align': 'htbp', } # Grouping the document tree into LaTeX files. List of tuples # (source start file, target name, title, # author, documentclass [howto, manual, or own class]). latex_documents = [ (master_doc, 'blenderseed.tex', 'blenderseed Documentation', 'blenderseed Manual', 'manual'), ] # -- Options for manual page output ------------------------------------------ # One entry per manual page. List of tuples # (source start file, name, description, authors, manual section). man_pages = [ (master_doc, 'blenderseed', 'blenderseed Documentation', [author], 1) ] # -- Options for Texinfo output ---------------------------------------------- # Grouping the document tree into Texinfo files. List of tuples # (source start file, target name, title, author, # dir menu entry, description, category) texinfo_documents = [ (master_doc, 'blenderseed', 'blenderseed Documentation', author, 'blenderseed', 'appleseed plugin for Blender', 'Miscellaneous'), ]
# Import the WebIDL module, so we can do isinstance checks and whatnot import WebIDL def WebIDLTest(parser, harness): # Basic functionality threw = False try: parser.parse(""" A implements B; interface B { attribute long x; }; interface A { attribute long y; }; """) results = parser.finish() except: threw = True harness.ok(not threw, "Should not have thrown on implements statement " "before interfaces") harness.check(len(results), 3, "We have three statements") harness.ok(isinstance(results[1], WebIDL.IDLInterface), "B is an interface") harness.check(len(results[1].members), 1, "B has one member") A = results[2] harness.ok(isinstance(A, WebIDL.IDLInterface), "A is an interface") harness.check(len(A.members), 2, "A has two members") harness.check(A.members[0].identifier.name, "y", "First member is 'y'") harness.check(A.members[1].identifier.name, "x", "Second member is 'x'") # Duplicated member names not allowed threw = False try: parser.parse(""" C implements D; interface D { attribute long x; }; interface C { attribute long x; }; """) parser.finish() except: threw = True harness.ok(threw, "Should have thrown on implemented interface duplicating " "a name on base interface") # Same, but duplicated across implemented interfaces threw = False try: parser.parse(""" E implements F; E implements G; interface F { attribute long x; }; interface G { attribute long x; }; interface E {}; """) parser.finish() except: threw = True harness.ok(threw, "Should have thrown on implemented interfaces " "duplicating each other's member names") # Same, but duplicated across indirectly implemented interfaces threw = False try: parser.parse(""" H implements I; H implements J; I implements K; interface K { attribute long x; }; interface L { attribute long x; }; interface I {}; interface J : L {}; interface H {}; """) parser.finish() except: threw = True harness.ok(threw, "Should have thrown on indirectly implemented interfaces " "duplicating each other's member names") # Same, but duplicated across an implemented interface and its parent threw = False try: parser.parse(""" M implements N; interface O { attribute long x; }; interface N : O { attribute long x; }; interface M {}; """) parser.finish() except: threw = True harness.ok(threw, "Should have thrown on implemented interface and its " "ancestor duplicating member names") # Reset the parser so we can actually find things where we expect # them in the list parser = parser.reset() # Diamonds should be allowed threw = False try: parser.parse(""" P implements Q; P implements R; Q implements S; R implements S; interface Q {}; interface R {}; interface S { attribute long x; }; interface P {}; """) results = parser.finish() except: threw = True harness.ok(not threw, "Diamond inheritance is fine") harness.check(results[6].identifier.name, "S", "We should be looking at 'S'") harness.check(len(results[6].members), 1, "S should have one member") harness.check(results[6].members[0].identifier.name, "x", "S's member should be 'x'") parser = parser.reset() threw = False try: parser.parse(""" interface TestInterface { }; callback interface TestCallbackInterface { }; TestInterface implements TestCallbackInterface; """) results = parser.finish() except: threw = True harness.ok(threw, "Should not allow callback interfaces on the right-hand side " "of 'implements'") parser = parser.reset() threw = False try: parser.parse(""" interface TestInterface { }; callback interface TestCallbackInterface { }; TestCallbackInterface implements TestInterface; """) results = parser.finish() except: threw = True harness.ok(threw, "Should not allow callback interfaces on the left-hand side of " "'implements'") parser = parser.reset() threw = False try: parser.parse(""" interface TestInterface { }; dictionary Dict { }; Dict implements TestInterface; """) results = parser.finish() except: threw = True harness.ok(threw, "Should not allow non-interfaces on the left-hand side " "of 'implements'") parser = parser.reset() threw = False try: parser.parse(""" interface TestInterface { }; dictionary Dict { }; TestInterface implements Dict; """) results = parser.finish() except: threw = True harness.ok(threw, "Should not allow non-interfaces on the right-hand side " "of 'implements'")
#!/usr/bin/env python3 # # linearize-hashes.py: List blocks in a linear, no-fork version of the chain. # # Copyright (c) 2013-2016 The Bitcoin Core developers # Distributed under the MIT software license, see the accompanying # file COPYING or http://www.opensource.org/licenses/mit-license.php. # from __future__ import print_function try: # Python 3 import http.client as httplib except ImportError: # Python 2 import httplib import json import re import base64 import sys import os import os.path settings = {} ##### Switch endian-ness ##### def hex_switchEndian(s): """ Switches the endianness of a hex string (in pairs of hex chars) """ pairList = [s[i:i+2].encode() for i in range(0, len(s), 2)] return b''.join(pairList[::-1]).decode() class BitcoinRPC: def __init__(self, host, port, username, password): authpair = "%s:%s" % (username, password) authpair = authpair.encode('utf-8') self.authhdr = b"Basic " + base64.b64encode(authpair) self.conn = httplib.HTTPConnection(host, port=port, timeout=30) def execute(self, obj): try: self.conn.request('POST', '/', json.dumps(obj), { 'Authorization' : self.authhdr, 'Content-type' : 'application/json' }) except ConnectionRefusedError: print('RPC connection refused. Check RPC settings and the server status.', file=sys.stderr) return None resp = self.conn.getresponse() if resp is None: print("JSON-RPC: no response", file=sys.stderr) return None body = resp.read().decode('utf-8') resp_obj = json.loads(body) return resp_obj @staticmethod def build_request(idx, method, params): obj = { 'version' : '1.1', 'method' : method, 'id' : idx } if params is None: obj['params'] = [] else: obj['params'] = params return obj @staticmethod def response_is_error(resp_obj): return 'error' in resp_obj and resp_obj['error'] is not None def get_block_hashes(settings, max_blocks_per_call=10000): rpc = BitcoinRPC(settings['host'], settings['port'], settings['rpcuser'], settings['rpcpassword']) height = settings['min_height'] while height < settings['max_height']+1: num_blocks = min(settings['max_height']+1-height, max_blocks_per_call) batch = [] for x in range(num_blocks): batch.append(rpc.build_request(x, 'getblockhash', [height + x])) reply = rpc.execute(batch) if reply is None: print('Cannot continue. Program will halt.') return None for x,resp_obj in enumerate(reply): if rpc.response_is_error(resp_obj): print('JSON-RPC: error at height', height+x, ': ', resp_obj['error'], file=sys.stderr) exit(1) assert(resp_obj['id'] == x) # assume replies are in-sequence if settings['rev_hash_bytes'] == 'true': resp_obj['result'] = hex_switchEndian(resp_obj['result']) print(resp_obj['result']) height += num_blocks def get_rpc_cookie(): # Open the cookie file with open(os.path.join(os.path.expanduser(settings['datadir']), '.cookie'), 'r') as f: combined = f.readline() combined_split = combined.split(":") settings['rpcuser'] = combined_split[0] settings['rpcpassword'] = combined_split[1] if __name__ == '__main__': if len(sys.argv) != 2: print("Usage: linearize-hashes.py CONFIG-FILE") sys.exit(1) f = open(sys.argv[1]) for line in f: # skip comment lines m = re.search('^\s*#', line) if m: continue # parse key=value lines m = re.search('^(\w+)\s*=\s*(\S.*)$', line) if m is None: continue settings[m.group(1)] = m.group(2) f.close() if 'host' not in settings: settings['host'] = '127.0.0.1' if 'port' not in settings: settings['port'] = 8332 if 'min_height' not in settings: settings['min_height'] = 0 if 'max_height' not in settings: settings['max_height'] = 313000 if 'rev_hash_bytes' not in settings: settings['rev_hash_bytes'] = 'false' use_userpass = True use_datadir = False if 'rpcuser' not in settings or 'rpcpassword' not in settings: use_userpass = False if 'datadir' in settings and not use_userpass: use_datadir = True if not use_userpass and not use_datadir: print("Missing datadir or username and/or password in cfg file", file=stderr) sys.exit(1) settings['port'] = int(settings['port']) settings['min_height'] = int(settings['min_height']) settings['max_height'] = int(settings['max_height']) # Force hash byte format setting to be lowercase to make comparisons easier. settings['rev_hash_bytes'] = settings['rev_hash_bytes'].lower() # Get the rpc user and pass from the cookie if the datadir is set if use_datadir: get_rpc_cookie() get_block_hashes(settings)
# -*- coding: utf-8 -*- """ requests.exceptions ~~~~~~~~~~~~~~~~~~~ This module contains the set of Requests' exceptions. """ from .packages.urllib3.exceptions import HTTPError as BaseHTTPError class RequestException(IOError): """There was an ambiguous exception that occurred while handling your request.""" def __init__(self, *args, **kwargs): """ Initialize RequestException with `request` and `response` objects. """ response = kwargs.pop('response', None) self.response = response self.request = kwargs.pop('request', None) if (response is not None and not self.request and hasattr(response, 'request')): self.request = self.response.request super(RequestException, self).__init__(*args, **kwargs) class HTTPError(RequestException): """An HTTP error occurred.""" class ConnectionError(RequestException): """A Connection error occurred.""" class ProxyError(ConnectionError): """A proxy error occurred.""" class SSLError(ConnectionError): """An SSL error occurred.""" class Timeout(RequestException): """The request timed out.""" class URLRequired(RequestException): """A valid URL is required to make a request.""" class TooManyRedirects(RequestException): """Too many redirects.""" class MissingSchema(RequestException, ValueError): """The URL schema (e.g. http or https) is missing.""" class InvalidSchema(RequestException, ValueError): """See defaults.py for valid schemas.""" class InvalidURL(RequestException, ValueError): """ The URL provided was somehow invalid. """ class ChunkedEncodingError(RequestException): """The server declared chunked encoding but sent an invalid chunk.""" class ContentDecodingError(RequestException, BaseHTTPError): """Failed to decode response content"""
# # Copyright (c) 2009 Dr. D Studios. (Please refer to license for details) # SVN_META_HEADURL = "$HeadURL: $" # SVN_META_ID = "$Id: keyword.py 9408 2010-03-03 22:35:49Z brobison $" # from sqlalchemy import Column, Table, types, ForeignKey, Index from sqlalchemy.orm import relation, backref from ..config import mapper, metadata from .asset import Asset class Keyword( object ): def __init__( self ): self.keybachkeyword = None self.name = None @property def asset_count(self): return 0 #len(self.assets) def __repr__( self ): return '<%s:%s:%s>' % ( self.__class__.__name__, self.keybachkeyword, self.name ) table = Table( 'bachkeyword', metadata, Column( 'keybachkeyword', types.Integer, primary_key=True ), Column( 'name', types.String, nullable=False ) ) join_table = Table( 'bachkeywordmap', metadata, Column( 'fkeybachkeyword', types.Integer, ForeignKey( 'bachkeyword.keybachkeyword' ) ), Column( 'fkeybachasset', types.Integer, ForeignKey( 'bachasset.keybachasset' ) ) ) mapper( Keyword, table, properties={ 'assets':relation( Asset, secondary=join_table, # backref='buckets' ), } )
#!/usr/bin/env python import urwid from trello import TrelloClient import models import settings from models import Board, get_session, TrelloList from trellointerface import create_dbcard_and_ensure_checklist class RemoveOrgUser(object): def __init__(self, parent): self.items = [urwid.Text("foo"), urwid.Text("bar")] self.main_content = urwid.SimpleListWalker( [urwid.AttrMap(w, None, 'reveal focus') for w in self.items]) self.parent = parent self.listbox = urwid.ListBox(self.main_content) @property def widget(self): return urwid.AttrWrap(self.listbox, 'body') def handle_input(self, k): if k in ('u', 'U'): self.parent.set_view(Top) class NoRefocusColumns(urwid.Columns): def keypress(self, size, key): return key class NoRefocusPile(urwid.Pile): def keypress(self, size, key): return key class TrelloCard(object): def __init__(self, card, trello): self.card = card self.trello = trello self.initialize() def initialize(self): raise NotImplementedError() @property def trellocard(self): return self.trello.get_card(self.card.id) @property def url(self): return self.trellocard.url @property def id(self): return self.card.id @property def name(self): return self.card.name class Story(TrelloCard): def initialize(self): pass @property def meta_checklist(self): tc = self.trellocard tc.fetch(eager=True) return [ checklist for checklist in tc.checklists if checklist.id == self.card.magic_checklist_id ][0] def connect_to(self, epic): self.meta_checklist.add_checklist_item("Epic Connection: {}: {}".format(epic.id, epic.url)) epic.story_checklist.add_checklist_item("{}: {}".format(self.id, self.url)) self.card.connected_to_id = epic.id @property def more_info_area(self): if self.card.connected_to is None: return "No connection" else: epic = self.card.connected_to return "Connected to {}".format(epic.name) class Epic(TrelloCard): def initialize(self): pass @property def story_checklist(self): tc = self.trellocard tc.fetch(eager=True) return [ checklist for checklist in tc.checklists if checklist.id == self.card.magic_checklist_id ][0] @property def more_info_area(self): return "" class Panel(object): def __init__(self, parent, board, card_cls): self.db_session = parent.db_session self.trello = parent.trello self.parent = parent self.board = board self.card_list_ptr = 0 self.card_lists = self.db_session.query(TrelloList).filter_by(board=self.board) self.card_cls = card_cls self.content = urwid.SimpleListWalker( [urwid.AttrMap(w, None, 'reveal focus') for w in self.items]) self.listbox = urwid.ListBox(self.content) def reset_content(self): while self.content: self.content.pop() self.content += [ urwid.AttrMap(w, None, 'reveal focus') for w in self.items] if len(self.items) >2: self.listbox.set_focus(2) # TODO: this doesn't belong here exactly. self.parent.more_info_area.set_text(self.card.more_info_area) @property def trelloboard(self): return self.trello.get_board(self.board.id) @property def items(self): items = [urwid.Text(self.card_list.name), urwid.Text('-=-=-=-=-=-=-=-=-=-')] items += [urwid.Text("{}] {}".format(i, card.name)) for i, card in enumerate(self.get_cards())] return items @property def card_list(self): return self.card_lists[self.card_list_ptr] def get_cards(self): db_cards = self.db_session.query(models.Card).filter_by(trellolist=self.card_list) self.cards = [ self.card_cls(card, self.trello) for card in db_cards] return self.cards def set_focus(self, idx): self.listbox.set_focus(idx) @property def card(self): return self.cards[self.listbox.get_focus()[1] - 2] def go_left(self): if self.card_list_ptr > 0: self.card_list_ptr -= 1 self.reset_content() def go_right(self): if self.card_list_ptr < self.card_lists.count() - 1: self.card_list_ptr += 1 self.reset_content() def move_up(self): focus_widget, idx = self.listbox.get_focus() if idx > 2: idx = idx - 1 self.listbox.set_focus(idx) self.parent.more_info_area.set_text(self.card.more_info_area) def move_down(self): focus_widget, idx = self.listbox.get_focus() if idx < len(self.content) - 1: idx = idx + 1 self.listbox.set_focus(idx) self.parent.more_info_area.set_text(self.card.more_info_area) class Connect(object): def __init__(self, parent): self.db_session = get_session()() self.mid_cmd = self.old_focus = None self.parent = parent self.story_board = self.db_session.query(Board).filter_by(story_board=True).first() self.epic_board = self.db_session.query(Board).filter_by(epic_board=True).first() self.future_story_boards = self.db_session.query(Board).filter_by(future_story_board=True) self.panels = [Panel(self, board=self.story_board, card_cls=Story)] self.panels += [ Panel(self, board=fsb, card_cls=Story) for fsb in self.future_story_boards] self.left_panel_idx = 0 self.left_panel = self.panels[self.left_panel_idx] self.right_panel = Panel(self, board=self.epic_board, card_cls=Epic) self.left_panel.set_focus(2) self.columns = NoRefocusColumns([self.left_panel.listbox, self.right_panel.listbox], focus_column=0) self.more_info_area = urwid.Text(self.left_panel.card.more_info_area) self.command_area = urwid.Edit(caption="") self.edit_area_listbox = urwid.ListBox([urwid.Text("-=-=-=-=-=-=-=-"), self.more_info_area, self.command_area]) #urwid.AttrMap(self.command_area, "notfocus", "focus")]) self.frame = NoRefocusPile([self.columns, self.edit_area_listbox], focus_item=0) @property def widget(self): return urwid.AttrWrap(self.frame, 'body') @property def trello(self): return self.parent.trelloclient def _complete(self): self.command_area.set_edit_text("") self.mid_cmd = False self.left_panel.listbox.set_focus(self.old_focus) self.frame.set_focus(0) def complete_n(self): output = self.command_area.get_edit_text().strip() card_list = self.right_panel.card_list trello_list = self.right_panel.trelloboard.get_list(card_list.id) card = trello_list.add_card(output) db_card = create_dbcard_and_ensure_checklist(self.db_session, card, prefetch_checklists=True) self.db_session.commit() self.db_session = get_session()() self.left_panel.card.connect_to(Epic(db_card, self.trello)) self.right_panel.reset_content() self._complete() def complete_c(self): output = int(self.command_area.get_edit_text()) self._complete() self.left_panel.card.connect_to(self.right_panel.cards[output]) self.db_session.commit() def switch_story_boards(self): self.left_panel_idx += 1 if self.left_panel_idx == len(self.panels): self.left_panel_idx = 0 self.left_panel = self.panels[self.left_panel_idx] self.columns.widget_list = [self.left_panel.listbox, self.right_panel.listbox] self.columns.set_focus(0) self.left_panel.listbox.set_focus(2) def handle_input(self, k): if self.mid_cmd: if k == 'esc': self._complete() if k == 'enter': if self.mid_cmd == 'n': self.complete_n() elif self.mid_cmd == 'c': self.complete_c() else: self.command_area.keypress([0], k) return if k in ('u', 'U'): self.parent.set_view(Top) if k == 's': self.switch_story_boards() if k == 'c': self.frame.set_focus(1) self.command_area.set_edit_pos(0) self.mid_cmd = 'c' self.old_focus = self.left_panel.listbox.get_focus()[1] if k == 'n': self.frame.set_focus(1) self.command_area.set_edit_pos(0) self.mid_cmd = 'n' self.old_focus = self.left_panel.listbox.get_focus()[1] # navigation elif k == 'j': self.right_panel.go_left() elif k == 'l': self.right_panel.go_right() elif k == 'a': self.left_panel.go_left() elif k == 'd': self.left_panel.go_right() elif k == 'up': self.left_panel.move_up() elif k == 'down': self.left_panel.move_down() VIEWS = { "Remove Organization User": RemoveOrgUser, "Connect Stories to Epics": Connect, } class Top(object): def __init__(self, parent): self.commands = [urwid.Text(text) for text in VIEWS.keys() ] self.main_content = urwid.SimpleListWalker( [urwid.Text('Commands'), urwid.Text('-=-=-=-=-=-=-=-=-=-')]+ [urwid.AttrMap(w, None, 'reveal focus') for w in self.commands]) self.listbox = urwid.ListBox(self.main_content) self.listbox.set_focus(2) self.parent = parent @property def widget(self): return urwid.AttrWrap(self.listbox, 'body') def enter_command(self): focus_widget, idx = self.listbox.get_focus() item = self.main_content[idx].original_widget.text view = VIEWS[item] self.parent.set_view(view) # focus_widget, idx = self.listbox.get_focus() # item = self.main_content[idx].original_widget.text # new_item = urwid.Text(item) # self.commands.append(new_item) # self.main_content.append(urwid.AttrMap(new_item, None, 'reveal focus')) def handle_input(self, k): if k == 'up': focus_widget, idx = self.listbox.get_focus() if idx > 2: idx = idx - 1 self.listbox.set_focus(idx) elif k == 'down': focus_widget, idx = self.listbox.get_focus() if idx < len(self.main_content) - 1: idx = idx + 1 self.listbox.set_focus(idx) elif k == 'enter': self.enter_command() class TrelloTraceability: palette = [ ('body', 'black', 'light gray'), ('focus', 'light gray', 'dark blue', 'standout'), ('head', 'yellow', 'black', 'standout'), ('foot', 'light gray', 'black'), ('key', 'light cyan', 'black','underline'), ('title', 'white', 'black', 'bold'), ('flag', 'dark gray', 'light gray'), ('error', 'dark red', 'light gray'), ] cmdstrings = [ ('title', "Commands"), " ", ('key', "Q"), ' ', ('key', 'U') ] def __init__(self): self.current_view = Top(self) # header and footer self.header = urwid.Text( "Trello Traceability" ) self.cmds = urwid.AttrWrap(urwid.Text(self.cmdstrings), 'foot') self.view = urwid.Frame( self.current_view.widget, header=urwid.AttrWrap(self.header, 'head' ), footer=self.cmds ) self.trelloclient = TrelloClient( api_key=settings.TRELLO_API_KEY, api_secret=settings.TRELLO_API_SECRET, token=settings.TRELLO_OAUTH_TOKEN, ) self.organization = self.trelloclient.get_organization(settings.TRELLO_ORGANIZATION_ID) def set_view(self, cls): self.current_view = cls(self) self.view.body = self.current_view.widget def main(self): """Run the program.""" self.loop = urwid.MainLoop(self.view, self.palette, unhandled_input=self.unhandled_input) self.loop.run() def unhandled_input(self, k): if k in ('q','Q'): raise urwid.ExitMainLoop() else: self.current_view.handle_input(k) def main(): TrelloTraceability().main() if __name__=="__main__": main()
## This file is part of Invenio. ## Copyright (C) 2013 CERN. ## ## Invenio is free software; you can redistribute it and/or ## modify it under the terms of the GNU General Public License as ## published by the Free Software Foundation; either version 2 of the ## License, or (at your option) any later version. ## ## Invenio is distributed in the hope that it will be useful, but ## WITHOUT ANY WARRANTY; without even the implied warranty of ## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU ## General Public License for more details. ## ## You should have received a copy of the GNU General Public License ## along with Invenio; if not, write to the Free Software Foundation, Inc., ## 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA. def _convert_x_to_10(x): if x != 'X': return int(x) else: return 10 def is_type_isbn10(val): """ Test if argument is an ISBN-10 number Courtesy Wikipedia: http://en.wikipedia.org/wiki/International_Standard_Book_Number """ val = val.replace("-", "").replace(" ", "") if len(val) != 10: return False r = sum([(10 - i) * (_convert_x_to_10(x)) for i, x in enumerate(val)]) return not (r % 11) def is_type_isbn13(val): """ Test if argument is an ISBN-13 number Courtesy Wikipedia: http://en.wikipedia.org/wiki/International_Standard_Book_Number """ val = val.replace("-", "").replace(" ", "") if len(val) != 13: return False total = sum([int(num) * weight for num, weight in zip(val, (1, 3) * 6)]) ck = (10 - total) % 10 return ck == int(val[-1]) def is_type_isbn(val): """ Test if argument is an ISBN-10 or ISBN-13 number """ try: return is_type_isbn10(val) or is_type_isbn13(val) except: return False
# -*- coding: utf-8 -*- ############################################################################## # # OpenERP, Open Source Management Solution # Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>). # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## import mrp_repair import wizard # vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
# Copyright (c) 2013 Hewlett-Packard Development Company, L.P. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or # implied. # See the License for the specific language governing permissions and # limitations under the License. # # Copyright (C) 2013 Association of Universities for Research in Astronomy # (AURA) # # Redistribution and use in source and binary forms, with or without # modification, are permitted provided that the following conditions are met: # # 1. Redistributions of source code must retain the above copyright # notice, this list of conditions and the following disclaimer. # # 2. Redistributions in binary form must reproduce the above # copyright notice, this list of conditions and the following # disclaimer in the documentation and/or other materials provided # with the distribution. # # 3. The name of AURA and its representatives may not be used to # endorse or promote products derived from this software without # specific prior written permission. # # THIS SOFTWARE IS PROVIDED BY AURA ``AS IS'' AND ANY EXPRESS OR IMPLIED # WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF # MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE # DISCLAIMED. IN NO EVENT SHALL AURA BE LIABLE FOR ANY DIRECT, INDIRECT, # INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, # BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS import glob import os import tarfile import fixtures from pbr.tests import base class TestCore(base.BaseTestCase): cmd_names = ('pbr_test_cmd', 'pbr_test_cmd_with_class') def check_script_install(self, install_stdout): for cmd_name in self.cmd_names: install_txt = 'Installing %s script to %s' % (cmd_name, self.temp_dir) self.assertIn(install_txt, install_stdout) cmd_filename = os.path.join(self.temp_dir, cmd_name) script_txt = open(cmd_filename, 'r').read() self.assertNotIn('pkg_resources', script_txt) stdout, _, return_code = self._run_cmd(cmd_filename) self.assertIn("PBR", stdout) def test_setup_py_keywords(self): """setup.py --keywords. Test that the `./setup.py --keywords` command returns the correct value without balking. """ self.run_setup('egg_info') stdout, _, _ = self.run_setup('--keywords') assert stdout == 'packaging,distutils,setuptools' def test_sdist_extra_files(self): """Test that the extra files are correctly added.""" stdout, _, return_code = self.run_setup('sdist', '--formats=gztar') # There can be only one try: tf_path = glob.glob(os.path.join('dist', '*.tar.gz'))[0] except IndexError: assert False, 'source dist not found' tf = tarfile.open(tf_path) names = ['/'.join(p.split('/')[1:]) for p in tf.getnames()] self.assertIn('extra-file.txt', names) def test_console_script_install(self): """Test that we install a non-pkg-resources console script.""" if os.name == 'nt': self.skipTest('Windows support is passthrough') stdout, _, return_code = self.run_setup( 'install_scripts', '--install-dir=%s' % self.temp_dir) self.useFixture( fixtures.EnvironmentVariable('PYTHONPATH', '.')) self.check_script_install(stdout) def test_console_script_develop(self): """Test that we develop a non-pkg-resources console script.""" if os.name == 'nt': self.skipTest('Windows support is passthrough') self.useFixture( fixtures.EnvironmentVariable( 'PYTHONPATH', ".:%s" % self.temp_dir)) stdout, _, return_code = self.run_setup( 'develop', '--install-dir=%s' % self.temp_dir) self.check_script_install(stdout) class TestGitSDist(base.BaseTestCase): def setUp(self): super(TestGitSDist, self).setUp() stdout, _, return_code = self._run_cmd('git', ('init',)) if return_code: self.skipTest("git not installed") stdout, _, return_code = self._run_cmd('git', ('add', '.')) stdout, _, return_code = self._run_cmd( 'git', ('commit', '-m', 'Turn this into a git repo')) stdout, _, return_code = self.run_setup('sdist', '--formats=gztar') def test_sdist_git_extra_files(self): """Test that extra files found in git are correctly added.""" # There can be only one tf_path = glob.glob(os.path.join('dist', '*.tar.gz'))[0] tf = tarfile.open(tf_path) names = ['/'.join(p.split('/')[1:]) for p in tf.getnames()] self.assertIn('git-extra-file.txt', names)
# Copyright (c) 2015 Mirantis, Inc. # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from oslo_log import log from oslo_serialization import jsonutils from pecan import hooks from neutron.api.v2 import attributes as v2_attributes from neutron.api.v2 import base as v2_base LOG = log.getLogger(__name__) class BodyValidationHook(hooks.PecanHook): priority = 120 def before(self, state): if state.request.method not in ('POST', 'PUT'): return resource = state.request.context.get('resource') collection = state.request.context.get('collection') neutron_context = state.request.context['neutron_context'] is_create = state.request.method == 'POST' if not resource: return try: json_data = jsonutils.loads(state.request.body) except ValueError: LOG.debug("No JSON Data in %(method)s request for %(collection)s", {'method': state.request.method, 'collections': collection}) return # Raw data are consumed by member actions such as add_router_interface state.request.context['request_data'] = json_data if not (resource in json_data or collection in json_data): # there is no resource in the request. This can happen when a # member action is being processed or on agent scheduler operations return # Prepare data to be passed to the plugin from request body data = v2_base.Controller.prepare_request_body( neutron_context, json_data, is_create, resource, v2_attributes.get_collection_info(collection), allow_bulk=is_create) if collection in data: state.request.context['resources'] = [item[resource] for item in data[collection]] else: state.request.context['resources'] = [data[resource]]
# -*- coding: utf-8 -*- ############################################################################## # # OpenERP, Open Source Management Solution # Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>). # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## from openerp.osv import osv class pos_confirm(osv.osv_memory): _name = 'pos.confirm' _description = 'Post POS Journal Entries' def action_confirm(self, cr, uid, ids, context=None): order_obj = self.pool.get('pos.order') ids = order_obj.search(cr, uid, [('state','=','paid')], context=context) for order in order_obj.browse(cr, uid, ids, context=context): todo = True for line in order.statement_ids: if line.statement_id.state != 'confirm': todo = False break if todo: order.signal_workflow('done') # Check if there is orders to reconcile their invoices ids = order_obj.search(cr, uid, [('state','=','invoiced'),('invoice_id.state','=','open')], context=context) for order in order_obj.browse(cr, uid, ids, context=context): invoice = order.invoice_id data_lines = [x.id for x in invoice.move_id.line_id if x.account_id.id == invoice.account_id.id] for st in order.statement_ids: for move in st.move_ids: data_lines += [x.id for x in move.line_id if x.account_id.id == invoice.account_id.id] self.pool.get('account.move.line').reconcile(cr, uid, data_lines, context=context) return {} # vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
# -*- coding: utf-8 -*- ############################################################################## # # OpenERP, Open Source Management Solution # Copyright (C) 2015 Odoo.com. # Copyright (C) 2015 Openies.com. # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## { 'name': 'Allotment on sale orders', 'version': '8.0.1.1.0', 'category': 'Sales', 'summary': "Separate the shipment according to allotment partner", 'author': u'Openies,Numrigraphe,Odoo Community Association (OCA)', 'website': 'http://www.Openies.com/', 'depends': ['sale_stock'], 'data': [ 'views/sale_order_line_view.xml' ], 'installable': True, 'auto_install': False, 'license': 'AGPL-3', }
# Copyright 2015 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== """Benchmark for split and grad of split.""" from __future__ import absolute_import from __future__ import division from __future__ import print_function import numpy as np import tensorflow as tf from tensorflow.python.platform import benchmark from tensorflow.python.platform import tf_logging as logging def build_graph(device, input_shape, output_sizes, axis): """Build a graph containing a sequence of batch normalizations. Args: device: string, the device to run on. input_shape: shape of the input tensor. output_sizes: size of each output along axis. axis: axis to be split along. Returns: An array of tensors to run() """ with tf.device("/%s:0" % device): inp = tf.zeros(input_shape) outputs = [] for _ in range(100): outputs.extend(tf.split_v(inp, output_sizes, axis)) return tf.group(*outputs) class SplitBenchmark(tf.test.Benchmark): """Benchmark split!""" def _run_graph(self, device, output_shape, variable, num_outputs, axis): """Run the graph and print its execution time. Args: device: string, the device to run on. output_shape: shape of each output tensors. variable: whether or not the output shape should be fixed num_outputs: the number of outputs to split the input into axis: axis to be split Returns: The duration of the run in seconds. """ graph = tf.Graph() with graph.as_default(): if not variable: if axis == 0: input_shape = [output_shape[0] * num_outputs, output_shape[1]] sizes = [output_shape[0] for _ in range(num_outputs)] else: input_shape = [output_shape[0], output_shape[1] * num_outputs] sizes = [output_shape[1] for _ in range(num_outputs)] else: sizes = np.random.randint( low=max(1, output_shape[axis] - 2), high=output_shape[axis] + 2, size=num_outputs) total_size = np.sum(sizes) if axis == 0: input_shape = [total_size, output_shape[1]] else: input_shape = [output_shape[0], total_size] outputs = build_graph(device, input_shape, sizes, axis) config = tf.ConfigProto(graph_options=tf.GraphOptions( optimizer_options=tf.OptimizerOptions( opt_level=tf.OptimizerOptions.L0))) with tf.Session(graph=graph, config=config) as session: logging.set_verbosity("info") tf.global_variables_initializer().run() bench = benchmark.TensorFlowBenchmark() bench.run_op_benchmark( session, outputs, mbs=input_shape[0] * input_shape[1] * 4 * 2 * 100 / 1e6, extras={ "input_shape": input_shape, "variable": variable, "axis": axis }) def benchmark_split(self): print("Forward vs backward concat") shapes = [[2000, 8], [8, 2000], [100, 18], [1000, 18], [10000, 18], [100, 97], [1000, 97], [10000, 1], [1, 10000]] axis_ = [1] # 0 is very fast because it doesn't actually do any copying num_outputs = 100 variable = [False, True] # fixed input size or not for shape in shapes: for axis in axis_: for v in variable: self._run_graph("gpu", shape, v, num_outputs, axis) if __name__ == "__main__": tf.test.main()
#!/usr/bin/python -tt # Control BZFlag tanks remotely with synchronous communication. #################################################################### # NOTE TO STUDENTS: # You CAN and probably SHOULD modify this code. Just because it is # in a separate file does not mean that you can ignore it or that # you have to leave it alone. Treat it as your code. You are # required to understand it enough to be able to modify it if you # find something wrong. This is merely a help to get you started # on interacting with BZRC. It is provided AS IS, with NO WARRANTY, # express or implied. #################################################################### from __future__ import division import math import sys import socket import time class BZRC: """Class handles queries and responses with remote controled tanks.""" def __init__(self, host, port, debug=False): """Given a hostname and port number, connect to the RC tanks.""" self.debug = debug # Note that AF_INET and SOCK_STREAM are defaults. sock = socket.socket() sock.connect((host, port)) # Make a line-buffered "file" from the socket. self.conn = sock.makefile(bufsize=1) self.handshake() def handshake(self): """Perform the handshake with the remote tanks.""" self.expect(('bzrobots', '1'), True) print >>self.conn, 'agent 1' def close(self): """Close the socket.""" self.conn.close() def read_arr(self): """Read a response from the RC tanks as an array split on whitespace. """ try: line = self.conn.readline() except socket.error: print 'Server Shut down. Aborting' sys.exit(1) if self.debug: print 'Received: %s' % line.split() return line.split() def sendline(self, line): """Send a line to the RC tanks.""" print >>self.conn, line def die_confused(self, expected, got_arr): """When we think the RC tanks should have responded differently, call this method with a string explaining what should have been sent and with the array containing what was actually sent. """ raise UnexpectedResponse(expected, ' '.join(got_arr)) def expect(self, expected, full=False): """Verify that server's response is as expected.""" if isinstance(expected, str): expected = (expected,) line = self.read_arr() good = True if full and len(expected) != len(line): good = False else: for a,b in zip(expected,line): if a!=b: good = False break if not good: self.die_confused(' '.join(expected), line) if full: return True return line[len(expected):] def expect_multi(self, *expecteds, **kwds): """Verify the server's response looks like one of several possible responses. Return the index of the matched response, and the server's line response. """ line = self.read_arr() for i,expected in enumerate(expecteds): for a,b in zip(expected, line): if a!=b: break else: if not kwds.get('full',False) or len(expected) == len(line): break else: self.die_confused(' or '.join(' '.join(one) for one in expecteds), line) return i, line[len(expected):] def read_ack(self): """Expect an "ack" line from the remote tanks. Raise an UnexpectedResponse exception if we get something else. """ self.expect('ack') def read_bool(self): """Expect a boolean response from the remote tanks. Return True or False in accordance with the response. Raise an UnexpectedResponse exception if we get something else. """ i, rest = self.expect_multi(('ok',),('fail',)) return (True, False)[i] def read_teams(self): """Get team information.""" self.expect('begin') teams = [] while True: i, rest = self.expect_multi(('team',),('end',)) if i == 1: break team = Answer() team.color = rest[0] team.count = float(rest[1]) team.base = [(float(x), float(y)) for (x, y) in zip(rest[2:10:2], rest[3:10:2])] teams.append(team) return teams def read_obstacles(self): """Get obstacle information.""" self.expect('begin') obstacles = [] while True: i, rest = self.expect_multi(('obstacle',),('end',)) if i == 1: break obstacle = [(float(x), float(y)) for (x, y) in zip(rest[::2], rest[1::2])] obstacles.append(obstacle) return obstacles def read_occgrid(self): """Read grid.""" response = self.read_arr() if 'fail' in response: return None pos = tuple(int(a) for a in self.expect('at')[0].split(',')) size = tuple(int(a) for a in self.expect('size')[0].split('x')) grid = [[0 for i in range(size[1])] for j in range(size[0])] for x in range(size[0]): line = self.read_arr()[0] for y in range(size[1]): if line[y] == '1': grid[x][y] = 1 self.expect('end', True) return pos, grid def read_flags(self): """Get flag information.""" line = self.read_arr() if line[0] != 'begin': self.die_confused('begin', line) flags = [] while True: line = self.read_arr() if line[0] == 'flag': flag = Answer() flag.color = line[1] flag.poss_color = line[2] flag.x = float(line[3]) flag.y = float(line[4]) flags.append(flag) elif line[0] == 'end': break else: self.die_confused('flag or end', line) return flags def read_shots(self): """Get shot information.""" line = self.read_arr() if line[0] != 'begin': self.die_confused('begin', line) shots = [] while True: line = self.read_arr() if line[0] == 'shot': shot = Answer() shot.x = float(line[1]) shot.y = float(line[2]) shot.vx = float(line[3]) shot.vy = float(line[4]) shots.append(shot) elif line[0] == 'end': break else: self.die_confused('shot or end', line) return shots def read_mytanks(self): """Get friendly tank information.""" line = self.read_arr() if line[0] != 'begin': self.die_confused('begin', line) tanks = [] while True: line = self.read_arr() if line[0] == 'mytank': tank = Answer() tank.index = int(line[1]) tank.callsign = line[2] tank.status = line[3] tank.shots_avail = int(line[4]) tank.time_to_reload = float(line[5]) tank.flag = line[6] tank.x = float(line[7]) tank.y = float(line[8]) tank.angle = float(line[9]) tank.vx = float(line[10]) tank.vy = float(line[11]) tank.angvel = float(line[12]) tanks.append(tank) elif line[0] == 'end': break else: self.die_confused('mytank or end', line) return tanks def read_othertanks(self): """Get enemy tank information.""" line = self.read_arr() if line[0] != 'begin': self.die_confused('begin', line) tanks = [] while True: line = self.read_arr() if line[0] == 'othertank': tank = Answer() tank.callsign = line[1] tank.color = line[2] tank.status = line[3] tank.flag = line[4] tank.x = float(line[5]) tank.y = float(line[6]) tank.angle = float(line[7]) tanks.append(tank) elif line[0] == 'end': break else: self.die_confused('othertank or end', line) return tanks def read_bases(self): """Get base information.""" bases = [] line = self.read_arr() if line[0] != 'begin': self.die_confused('begin', line) while True: line = self.read_arr() if line[0] == 'base': base = Answer() base.color = line[1] base.corner1_x = float(line[2]) base.corner1_y = float(line[3]) base.corner2_x = float(line[4]) base.corner2_y = float(line[5]) base.corner3_x = float(line[6]) base.corner3_y = float(line[7]) base.corner4_x = float(line[8]) base.corner4_y = float(line[9]) bases.append(base) elif line[0] == 'end': break else: self.die_confused('othertank or end', line) return bases def read_constants(self): """Get constants.""" line = self.read_arr() if line[0] != 'begin': self.die_confused('begin', line) constants = {} while True: line = self.read_arr() if line[0] == 'constant': constants[line[1]] = line[2] elif line[0] == 'end': break else: self.die_confused('constant or end', line) return constants # Commands: def shoot(self, index): """Perform a shoot request.""" self.sendline('shoot %s' % index) self.read_ack() return self.read_bool() def speed(self, index, value): """Set the desired speed to the specified value.""" self.sendline('speed %s %s' % (index, value)) self.read_ack() return self.read_bool() def angvel(self, index, value): """Set the desired angular velocity to the specified value.""" self.sendline('angvel %s %s' % (index, value)) self.read_ack() return self.read_bool() # Information Requests: def get_teams(self): """Request a list of teams.""" self.sendline('teams') self.read_ack() return self.read_teams() def get_obstacles(self): """Request a list of obstacles.""" self.sendline('obstacles') self.read_ack() return self.read_obstacles() def get_occgrid(self, tankid): """Request an occupancy grid for a tank""" self.sendline('occgrid %d' % tankid) self.read_ack() return self.read_occgrid() def get_flags(self): """Request a list of flags.""" self.sendline('flags') self.read_ack() return self.read_flags() def get_shots(self): """Request a list of shots.""" self.sendline('shots') self.read_ack() return self.read_shots() def get_mytanks(self): """Request a list of our tanks.""" self.sendline('mytanks') self.read_ack() return self.read_mytanks() def get_othertanks(self): """Request a list of tanks that aren't ours.""" self.sendline('othertanks') self.read_ack() return self.read_othertanks() def get_bases(self): """Request a list of bases.""" self.sendline('bases') self.read_ack() return self.read_bases() def get_constants(self): """Request a dictionary of game constants.""" self.sendline('constants') self.read_ack() return self.read_constants() # Optimized queries def get_lots_o_stuff(self): """Network-optimized request for mytanks, othertanks, flags, and shots. Returns a tuple with the four results. """ self.sendline('mytanks') self.sendline('othertanks') self.sendline('flags') self.sendline('shots') self.read_ack() mytanks = self.read_mytanks() self.read_ack() othertanks = self.read_othertanks() self.read_ack() flags = self.read_flags() self.read_ack() shots = self.read_shots() return (mytanks, othertanks, flags, shots) def do_commands(self, commands): """Send commands for a bunch of tanks in a network-optimized way.""" for cmd in commands: self.sendline('speed %s %s' % (cmd.index, cmd.speed)) self.sendline('angvel %s %s' % (cmd.index, cmd.angvel)) if cmd.shoot: self.sendline('shoot %s' % cmd.index) results = [] for cmd in commands: self.read_ack() result_speed = self.read_bool() self.read_ack() result_angvel = self.read_bool() if cmd.shoot: self.read_ack() result_shoot = self.read_bool() else: result_shoot = False results.append((result_speed, result_angvel, result_shoot)) return results class Answer(object): """BZRC returns an Answer for things like tanks, obstacles, etc. You should probably write your own code for this sort of stuff. We created this class just to keep things short and sweet. """ pass class Command(object): """Class for setting a command for a tank.""" def __init__(self, index, speed, angvel, shoot): self.index = index self.speed = speed self.angvel = angvel self.shoot = shoot class UnexpectedResponse(Exception): """Exception raised when the BZRC gets confused by a bad response.""" def __init__(self, expected, got): self.expected = expected self.got = got def __str__(self): return 'BZRC: Expected "%s". Instead got "%s".' % (self.expected, self.got) # vim: et sw=4 sts=4
#!/usr/bin/env python # $Id: tls_ftpd.py 977 2012-01-22 23:05:09Z g.rodola $ # pyftpdlib is released under the MIT license, reproduced below: # ====================================================================== # Copyright (C) 2007-2012 Giampaolo Rodola' <g.rodola@gmail.com> # # All Rights Reserved # # Permission is hereby granted, free of charge, to any person # obtaining a copy of this software and associated documentation # files (the "Software"), to deal in the Software without # restriction, including without limitation the rights to use, # copy, modify, merge, publish, distribute, sublicense, and/or sell # copies of the Software, and to permit persons to whom the # Software is furnished to do so, subject to the following # conditions: # # The above copyright notice and this permission notice shall be # included in all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, # EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES # OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND # NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT # HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, # WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING # FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR # OTHER DEALINGS IN THE SOFTWARE. # # ====================================================================== """An RFC-4217 asynchronous FTPS server supporting both SSL and TLS. Requires PyOpenSSL module (http://pypi.python.org/pypi/pyOpenSSL). """ import os from pyftpdlib import ftpserver from pyftpdlib.contrib.handlers import TLS_FTPHandler CERTFILE = os.path.abspath(os.path.join(os.path.dirname(__file__), "keycert.pem")) def main(): authorizer = ftpserver.DummyAuthorizer() authorizer.add_user('user', '12345', '.', perm='elradfmw') authorizer.add_anonymous('.') ftp_handler = TLS_FTPHandler ftp_handler.certfile = CERTFILE ftp_handler.authorizer = authorizer # requires SSL for both control and data channel #ftp_handler.tls_control_required = True #ftp_handler.tls_data_required = True ftpd = ftpserver.FTPServer(('', 8021), ftp_handler) ftpd.serve_forever() if __name__ == '__main__': main()
""" Checks that reversed() receive proper argument """ # pylint: disable=missing-docstring # pylint: disable=too-few-public-methods,no-self-use,no-absolute-import from collections import deque __revision__ = 0 class GoodReversed(object): """ Implements __reversed__ """ def __reversed__(self): return [1, 2, 3] class SecondGoodReversed(object): """ Implements __len__ and __getitem__ """ def __len__(self): return 3 def __getitem__(self, index): return index class BadReversed(object): """ implements only len() """ def __len__(self): return 3 class SecondBadReversed(object): """ implements only __getitem__ """ def __getitem__(self, index): return index class ThirdBadReversed(dict): """ dict subclass """ def uninferable(seq): """ This can't be infered at this moment, make sure we don't have a false positive. """ return reversed(seq) def test(path): """ test function """ seq = reversed() # No argument given seq = reversed(None) # [bad-reversed-sequence] seq = reversed([1, 2, 3]) seq = reversed((1, 2, 3)) seq = reversed(set()) # [bad-reversed-sequence] seq = reversed({'a': 1, 'b': 2}) # [bad-reversed-sequence] seq = reversed(iter([1, 2, 3])) # [bad-reversed-sequence] seq = reversed(GoodReversed()) seq = reversed(SecondGoodReversed()) seq = reversed(BadReversed()) # [bad-reversed-sequence] seq = reversed(SecondBadReversed()) # [bad-reversed-sequence] seq = reversed(range(100)) seq = reversed(ThirdBadReversed()) # [bad-reversed-sequence] seq = reversed(lambda: None) # [bad-reversed-sequence] seq = reversed(deque([])) seq = reversed("123") seq = uninferable([1, 2, 3]) seq = reversed(path.split("/")) return seq def test_dict_ancestor_and_reversed(): """Don't emit for subclasses of dict, with __reversed__ implemented.""" from collections import OrderedDict class Child(dict): def __reversed__(self): return reversed(range(10)) seq = reversed(OrderedDict()) return reversed(Child()), seq
# # Copyright 2018 Analytics Zoo Authors. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # import torch import torch.nn as nn from zoo.automl.model.base_pytorch_model import PytorchBaseModel, \ PYTORCH_REGRESSION_LOSS_MAP import numpy as np class LSTMSeq2Seq(nn.Module): def __init__(self, input_feature_num, future_seq_len, output_feature_num, lstm_hidden_dim=128, lstm_layer_num=2, dropout=0.25, teacher_forcing=False): super(LSTMSeq2Seq, self).__init__() self.lstm_encoder = nn.LSTM(input_size=input_feature_num, hidden_size=lstm_hidden_dim, num_layers=lstm_layer_num, dropout=dropout, batch_first=True) self.lstm_decoder = nn.LSTM(input_size=output_feature_num, hidden_size=lstm_hidden_dim, num_layers=lstm_layer_num, dropout=dropout, batch_first=True) self.fc = nn.Linear(in_features=lstm_hidden_dim, out_features=output_feature_num) self.future_seq_len = future_seq_len self.output_feature_num = output_feature_num self.teacher_forcing = teacher_forcing def forward(self, input_seq, target_seq=None): x, (hidden, cell) = self.lstm_encoder(input_seq) # input feature order should have target dimensions in the first decoder_input = input_seq[:, -1, :self.output_feature_num] decoder_input = decoder_input.unsqueeze(1) decoder_output = [] for i in range(self.future_seq_len): decoder_output_step, (hidden, cell) = self.lstm_decoder(decoder_input, (hidden, cell)) out_step = self.fc(decoder_output_step) decoder_output.append(out_step) if not self.teacher_forcing or target_seq is None: # no teaching force decoder_input = out_step else: # with teaching force decoder_input = target_seq[:, i:i+1, :] decoder_output = torch.cat(decoder_output, dim=1) return decoder_output def model_creator(config): return LSTMSeq2Seq(input_feature_num=config["input_feature_num"], output_feature_num=config["output_feature_num"], future_seq_len=config["future_seq_len"], lstm_hidden_dim=config.get("lstm_hidden_dim", 128), lstm_layer_num=config.get("lstm_layer_num", 2), dropout=config.get("dropout", 0.25), teacher_forcing=config.get("teacher_forcing", False)) def optimizer_creator(model, config): return getattr(torch.optim, config.get("optim", "Adam"))(model.parameters(), lr=config.get("lr", 0.001)) def loss_creator(config): loss_name = config.get("loss", "mse") if loss_name in PYTORCH_REGRESSION_LOSS_MAP: loss_name = PYTORCH_REGRESSION_LOSS_MAP[loss_name] else: raise RuntimeError(f"Got \"{loss_name}\" for loss name,\ where \"mse\", \"mae\" or \"huber_loss\" is expected") return getattr(torch.nn, loss_name)() class Seq2SeqPytorch(PytorchBaseModel): def __init__(self, check_optional_config=False): super().__init__(model_creator=model_creator, optimizer_creator=optimizer_creator, loss_creator=loss_creator, check_optional_config=check_optional_config) def _input_check(self, x, y): if len(x.shape) < 3: raise RuntimeError(f"Invalid data x with {len(x.shape)} dim where 3 dim is required.") if len(y.shape) < 3: raise RuntimeError(f"Invalid data y with {len(y.shape)} dim where 3 dim is required.") if y.shape[-1] > x.shape[-1]: raise RuntimeError(f"output dim should not larger than input dim,\ while we get {y.shape[-1]} > {x.shape[-1]}.") def _forward(self, x, y): self._input_check(x, y) return self.model(x, y) def _get_required_parameters(self): return { "input_feature_num", "future_seq_len", "output_feature_num" } def _get_optional_parameters(self): return { "lstm_hidden_dim", "lstm_layer_num", "teacher_forcing" } | super()._get_optional_parameters()
#!/usr/bin/env python # # Copyright 2008, Google Inc. # All rights reserved. # # Redistribution and use in source and binary forms, with or without # modification, are permitted provided that the following conditions are # met: # # * Redistributions of source code must retain the above copyright # notice, this list of conditions and the following disclaimer. # * Redistributions in binary form must reproduce the above # copyright notice, this list of conditions and the following disclaimer # in the documentation and/or other materials provided with the # distribution. # * Neither the name of Google Inc. nor the names of its # contributors may be used to endorse or promote products derived from # this software without specific prior written permission. # # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS # "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT # LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR # A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT # OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, # SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT # LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, # DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY # THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT # (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE # OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. """Tests the text output of Google C++ Mocking Framework. SYNOPSIS gmock_output_test.py --build_dir=BUILD/DIR --gengolden # where BUILD/DIR contains the built gmock_output_test_ file. gmock_output_test.py --gengolden gmock_output_test.py """ __author__ = 'wan@google.com (Zhanyong Wan)' import os import re import sys import gmock_test_utils # The flag for generating the golden file GENGOLDEN_FLAG = '--gengolden' PROGRAM_PATH = gmock_test_utils.GetTestExecutablePath('gmock_output_test_') COMMAND = [PROGRAM_PATH, '--gtest_stack_trace_depth=0', '--gtest_print_time=0'] GOLDEN_NAME = 'gmock_output_test_golden.txt' GOLDEN_PATH = os.path.join(gmock_test_utils.GetSourceDir(), GOLDEN_NAME) def ToUnixLineEnding(s): """Changes all Windows/Mac line endings in s to UNIX line endings.""" return s.replace('\r\n', '\n').replace('\r', '\n') def RemoveReportHeaderAndFooter(output): """Removes Google Test result report's header and footer from the output.""" output = re.sub(r'.*gtest_main.*\n', '', output) output = re.sub(r'\[.*\d+ tests.*\n', '', output) output = re.sub(r'\[.* test environment .*\n', '', output) output = re.sub(r'\[=+\] \d+ tests .* ran.*', '', output) output = re.sub(r'.* FAILED TESTS\n', '', output) return output def RemoveLocations(output): """Removes all file location info from a Google Test program's output. Args: output: the output of a Google Test program. Returns: output with all file location info (in the form of 'DIRECTORY/FILE_NAME:LINE_NUMBER: 'or 'DIRECTORY\\FILE_NAME(LINE_NUMBER): ') replaced by 'FILE:#: '. """ return re.sub(r'.*[/\\](.+)(\:\d+|\(\d+\))\:', 'FILE:#:', output) def NormalizeErrorMarker(output): """Normalizes the error marker, which is different on Windows vs on Linux.""" return re.sub(r' error: ', ' Failure\n', output) def RemoveMemoryAddresses(output): """Removes memory addresses from the test output.""" return re.sub(r'@\w+', '@0x#', output) def RemoveTestNamesOfLeakedMocks(output): """Removes the test names of leaked mock objects from the test output.""" return re.sub(r'\(used in test .+\) ', '', output) def GetLeakyTests(output): """Returns a list of test names that leak mock objects.""" # findall() returns a list of all matches of the regex in output. # For example, if '(used in test FooTest.Bar)' is in output, the # list will contain 'FooTest.Bar'. return re.findall(r'\(used in test (.+)\)', output) def GetNormalizedOutputAndLeakyTests(output): """Normalizes the output of gmock_output_test_. Args: output: The test output. Returns: A tuple (the normalized test output, the list of test names that have leaked mocks). """ output = ToUnixLineEnding(output) output = RemoveReportHeaderAndFooter(output) output = NormalizeErrorMarker(output) output = RemoveLocations(output) output = RemoveMemoryAddresses(output) return (RemoveTestNamesOfLeakedMocks(output), GetLeakyTests(output)) def GetShellCommandOutput(cmd): """Runs a command in a sub-process, and returns its STDOUT in a string.""" return gmock_test_utils.Subprocess(cmd, capture_stderr=False).output def GetNormalizedCommandOutputAndLeakyTests(cmd): """Runs a command and returns its normalized output and a list of leaky tests. Args: cmd: the shell command. """ # Disables exception pop-ups on Windows. os.environ['GTEST_CATCH_EXCEPTIONS'] = '1' return GetNormalizedOutputAndLeakyTests(GetShellCommandOutput(cmd)) class GMockOutputTest(gmock_test_utils.TestCase): def testOutput(self): (output, leaky_tests) = GetNormalizedCommandOutputAndLeakyTests(COMMAND) golden_file = open(GOLDEN_PATH, 'rb') golden = golden_file.read() golden_file.close() # The normalized output should match the golden file. self.assertEquals(golden, output) # The raw output should contain 2 leaked mock object errors for # test GMockOutputTest.CatchesLeakedMocks. self.assertEquals(['GMockOutputTest.CatchesLeakedMocks', 'GMockOutputTest.CatchesLeakedMocks'], leaky_tests) if __name__ == '__main__': if sys.argv[1:] == [GENGOLDEN_FLAG]: (output, _) = GetNormalizedCommandOutputAndLeakyTests(COMMAND) golden_file = open(GOLDEN_PATH, 'wb') golden_file.write(output) golden_file.close() else: gmock_test_utils.Main()
# -*- coding: utf-8 -*- """ *************************************************************************** SextanteToolsTest.py --------------------- Date : April 2013 Copyright : (C) 2013 by Victor Olaya Email : volayaf at gmail dot com *************************************************************************** * * * This program is free software; you can redistribute it and/or modify * * it under the terms of the GNU General Public License as published by * * the Free Software Foundation; either version 2 of the License, or * * (at your option) any later version. * * * *************************************************************************** """ __author__ = 'Victor Olaya' __date__ = 'April 2013' __copyright__ = '(C) 2013, Victor Olaya' # This will get replaced with a git SHA1 when you do a git archive __revision__ = '$Format:%H$' import sextante import unittest from sextante.tests.TestData import points, points2, polygons, polygons2, lines, union,\ table, polygonsGeoJson, raster from sextante.core import Sextante from sextante.tools.vector import values from sextante.tools.general import getfromname class SextanteToolsTest(unittest.TestCase): '''tests the method imported when doing an "import sextante", and also in sextante.tools. They are mostly convenience tools''' def test_getobject(self): layer = sextante.getobject(points()); self.assertIsNotNone(layer) layer = sextante.getobject("points"); self.assertIsNotNone(layer) def test_runandload(self): sextante.runandload("qgis:countpointsinpolygon",polygons(),points(),"NUMPOINTS", None) layer = getfromname("Result") self.assertIsNotNone(layer) def test_featuresWithoutSelection(self): layer = sextante.getobject(points()) features = sextante.getfeatures(layer) self.assertEqual(12, len(features)) def test_featuresWithSelection(self): layer = sextante.getobject(points()) feature = layer.getFeatures().next() selected = [feature.id()] layer.setSelectedFeatures(selected) features = sextante.getfeatures(layer) self.assertEqual(1, len(features)) layer.setSelectedFeatures([]) def test_attributeValues(self): layer = sextante.getobject(points()) attributeValues = values(layer, "ID") i = 1 for value in attributeValues['ID']: self.assertEqual(int(i), int(value)) i+=1 self.assertEquals(13,i) def test_extent(self): pass def suite(): suite = unittest.makeSuite(SextanteToolsTest, 'test') return suite def runtests(): result = unittest.TestResult() testsuite = suite() testsuite.run(result) return result
#!/usr/bin/env python # -*- coding: utf-8 -*- # @Author: largelymfs # @Date: 2014-12-23 20:06:10 # @Last Modified by: largelymfs # @Last Modified time: 2014-12-23 23:18:08 import numpy as np import numpy.linalg as LA class TFIDF: def __init__(self, filename): #get the self.vocab self.vocab = {} id = 0 doc = 0 with open(filename) as fin: self.doc = [l.strip().split() for l in fin] for words in self.doc: for word in words: if word not in self.vocab: self.vocab[word] = id id+=1 doc+=1 self.word_number = id self.doc_number = doc self.matrix = np.zeros((self.doc_number, self.word_number)) self.idf = {} for words in self.doc: now = set(words) for word in now: word_id = self.vocab[word] if word_id not in self.idf: self.idf[word_id] = 1 else: self.idf[word_id] +=1 for k in self.idf.keys(): self.idf[k] = np.log((float(self.doc_number) / float(self.idf[k]))) id = 0 for words in self.doc: total = 0.0 for word in words: self.matrix[id][self.vocab[word]] +=1.0 total +=1.0 if total==0: print words continue self.matrix[id] = self.matrix[id] * (1./total) id+=1 self.matrix = self.matrix.T for i in range(self.word_number): self.matrix[i] = self.matrix[i] * (self.idf[i]) self.matrix = self.matrix.T def get_score(self, v1, v2): return np.dot(v1, v2)/(LA.norm(v1) * LA.norm(v2)) def querry(self, q): vector = np.zeros(self.word_number) total = 0.0 for w in q: if w in self.vocab: vector[self.vocab[w]]+=1.0 total +=1.0 vector = vector * (1./total) for i in range(self.word_number): vector[i] *= (self.idf[i]) result = [(i, self.get_score(self.matrix[i], vector)) for i in range(self.doc_number)] result = sorted(result, cmp=lambda x, y:-cmp(x[1],y[1]))[:10] for (id, score) in result: print id, "".join(self.doc[id]) if __name__=='__main__': model = TFIDF("./../../data/demo.txt.out") model.querry(["", "", ""])
#!/usr/local/bin/python2.7 from flask import Flask import sys from flask_frozen import Freezer from upload_s3 import set_metadata from config import AWS_DIRECTORY from query import get_slugs app = Flask(__name__) app.config.from_object('config') from views import * # Serving from s3 leads to some complications in how static files are served if len(sys.argv) > 1 and sys.argv[1] == 'build': PROJECT_ROOT = '/' + AWS_DIRECTORY else: PROJECT_ROOT = '/' class WebFactionMiddleware(object): def __init__(self, app): self.app = app def __call__(self, environ, start_response): environ['SCRIPT_NAME'] = PROJECT_ROOT return self.app(environ, start_response) app.wsgi_app = WebFactionMiddleware(app.wsgi_app) freezer = Freezer(app) @freezer.register_generator def post(): slugs, links = get_slugs(title=False) for i in slugs: yield {'title': i} if __name__ == '__main__': if len(sys.argv) > 1 and sys.argv[1] == 'build': app.debug = True freezer.freeze() set_metadata() else: app.run(debug=True)
# -*- coding: utf-8 -*- #import my_math from my_math import factorial import os #import my_math def test_db(): return def test_network(): return def test_exception(): # opening file failed try: fi = open("testfile", 'r') fh = open("testfile", "w") fh.write("This is my test file for exception handling!!") except IOError: print "Error: can\'t find file or read data" else: print fi.read() print "Written content in the file successfully" fh.close() fi.close() return def test_module(): print '10! = %d'%(factorial(10)) return class Employee: 'Common base class for all employees' empCount = 0 def __init__(self, name, salary): self.name = name self.salary = salary Employee.empCount += 1 def displayCount(self): print "Total Employee %d" % Employee.empCount def displayEmployee(self): print "Name : ", self.name, ", Salary: ", self.salary def test_class(): "This would create first object of Employee class" emp1 = Employee("Zara", 2000) "This would create second object of Employee class" emp2 = Employee("Manni", 5000) emp1.displayEmployee() emp2.displayEmployee() print "Total Employee %d" % Employee.empCount print emp1.empCount # inheritence # overiding # operator overloading return def fib_1(n): """Print a Fibonacci series up to n.""" a, b = 0, 1 while b < n: print b a, b = b, a+b return cnt = 0 fib_tmp = {}# make fib faster def fib_2(n): """return the nth fib num""" global cnt cnt += 1 if n == 0: return 0 elif n == 1: return 1 elif n > 1: return fib_2(n-1) + fib_2(n-2) else: print 'invalid input' return None def simple_func(a, b, c): return a + b + c**3 def test_function(): print simple_func(1, 2, 3) fib_1(100) print fib_2(5) print 'fib_2 is called %d times'%(cnt) return def test_generator(): l1 = range(100) print l1 # the first 100 odd numbers l2 = [2*x+1 for x in range(100)] print l2 # gen a dict # gen a ascii code table dict1 = {x:chr(x) for x in range(128)} print dict1 # gen a 10*10 array l3 = [[10*x+y for y in range(10)] for x in range(10)] print l3 # cross product vec1 = [2, 4, 6] vec2 = [1, 3, 5] cross_product = [x*y for x in vec1 for y in vec2] print cross_product # using if vec_if = [x for x in l1 if x % 7 == 0] print vec_if print len(vec_if) return def test_file_io(): # write to a file fo = open('testfile', 'wt') for x in range(20): fo.write(str(x) + ',') fo.close() # read from a file fi = open('testfile', 'rt') # read as much as possible at one time! contents = fi.read() print contents list_num = contents.split(',') # read a line at a time # reset file obj position fi.seek(0) for line in fi: print line fi.seek(10) print fi.read(10) # tell the current position print fi.tell() fi.close() # create a dir import os os.mkdir("test_dir") # return def test_io(): # print function a = ['hello', 'this is fun', 'I love wargames'] for item in a: print item, len(item) # get input from keyboard # raw_input, get a line of input from keyboard as string x = str(raw_input("enter something:")) print x # input x = input("input your python expression: ") print x return def test_loops(): # for loops, break, continue # problem: check prime n = 23 prime = True for x in range(2, n): if n % x == 0: print '%d is not a prime since it has a factor %d'%(n, x) prime = False break if prime: print '%d is a prime'%(n) # using while loop do the same prime = True x = 2 while x < n: if n % x == 0: print '%d is not a prime since it has a factor %d'%(n, x) prime = False break x += 1 if prime: print '%d is a prime'%(n) # do while? n = 1 while True: if n < 10: print n n += 1 return def test_control_flow(): # get input from keyboard #x = int(raw_input("Please enter #:")) x = 5 if x < 0: x = 0 print 'Negative changed to zero' elif x == 0: print 'Zero' elif x == 1: print 'Single' else: print 'More' # no case statement return def test_dictionary(): # create a dictionary dict_1 = {'Alice': '2341', 'Beth': '9102', 'Cecil': '3258'} print dict_1 dict_2 = {x:x*'a' for x in range(10)} print dict_2 # add a new entry dict_1['newguy'] = '2323' print dict_1 # del a entry del dict_1['Beth'] print dict_1 # check for existance print dict_1.has_key('Beth') print 'Beth' in dict_1 print 'Alice' in dict_1 # update dict print dict_1['Alice'] dict_1['Alice'] = '323232' print dict_1['Alice'] # no duplicates! # make a copy copy_dict_1 = dict_1.copy() print copy_dict_1 # clear the dict dict_1.clear() print dict_1 return def test_list(): # items are ordered # items in list can be heterogeneous a = ['spam', 'eggs', 100, 1234, 2*2] b = [1, 2 ,3, 4] c = range(12) print a print b print c # access list elements print a[0] for num in b: num += 1 print b for i in range(len(b)): b[i] += 1 print b # loop through a list for item in a: print item # add a new item to a list b.append(6) print b # delete a item based on location del b[0] del b[-1] print b #check membership if 'spam' in a: print 'got it' else: print 'spam is not in list a' # lists cancatenation d = a + b + c print d # list repetiion print 2*a # nested list print max(a) a.sort() print a a.reverse() print a # index function print index('spam') return def test_str(): """play with string""" str_1 = "hacking is fun" print str_1 + 16*'a' print str_1 + 16*'\x61' print len(str_1) # take a substring # str[left:right] print str_1[:] print str_1[:5] # do not modify char in a string #str_1[0] = 'H' # print the last char print str_1[-1] # check a string's hex print str_1.encode('hex') # copy a string str_2 = str_1 str_3 = str_1[:-1] print id(str_2) == id(str_1) print id(str_3) == id(str_1) print str_1 print str_2 return def test_var(): a = 5 b = 1.2 c = 0xdeadbeef d = u'\xde\xad\xbe\xef' e = 8 * '\x00' f = 'abcd' ff = '\x61\x62\x63\x64' kk = u'' g = True h = False j = 0x61 print not g print a, b, c print a+b print type(c) print type(a) print type(d) print hex(c) print f, ff print chr(j) print kk.encode('utf-8') print d.encode('utf-8') # the id function # global var return if __name__ == "__main__": #test_var() test_str() #test_list() #test_dictionary() #test_control_flow() #test_loops() #test_function() #test_generator() #test_module() #test_io() #test_file_io() #test_class() #test_exception()
# Copyright (c) 2006,2007 Mitch Garnaat http://garnaat.org/ # # Permission is hereby granted, free of charge, to any person obtaining a # copy of this software and associated documentation files (the # "Software"), to deal in the Software without restriction, including # without limitation the rights to use, copy, modify, merge, publish, dis- # tribute, sublicense, and/or sell copies of the Software, and to permit # persons to whom the Software is furnished to do so, subject to the fol- # lowing conditions: # # The above copyright notice and this permission notice shall be included # in all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS # OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL- # ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT # SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, # WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS # IN THE SOFTWARE. from boto.s3.user import User CannedACLStrings = ['private', 'public-read', 'public-read-write', 'authenticated-read', 'bucket-owner-read', 'bucket-owner-full-control'] class Policy: def __init__(self, parent=None): self.parent = parent self.acl = None def __repr__(self): grants = [] for g in self.acl.grants: if g.id == self.owner.id: grants.append("%s (owner) = %s" % (g.display_name, g.permission)) else: if g.type == 'CanonicalUser': u = g.display_name elif g.type == 'Group': u = g.uri else: u = g.email_address grants.append("%s = %s" % (u, g.permission)) return "<Policy: %s>" % ", ".join(grants) def startElement(self, name, attrs, connection): if name == 'Owner': self.owner = User(self) return self.owner elif name == 'AccessControlList': self.acl = ACL(self) return self.acl else: return None def endElement(self, name, value, connection): if name == 'Owner': pass elif name == 'AccessControlList': pass else: setattr(self, name, value) def to_xml(self): s = '<AccessControlPolicy>' s += self.owner.to_xml() s += self.acl.to_xml() s += '</AccessControlPolicy>' return s class ACL: def __init__(self, policy=None): self.policy = policy self.grants = [] def add_grant(self, grant): self.grants.append(grant) def add_email_grant(self, permission, email_address): grant = Grant(permission=permission, type='AmazonCustomerByEmail', email_address=email_address) self.grants.append(grant) def add_user_grant(self, permission, user_id, display_name=None): grant = Grant(permission=permission, type='CanonicalUser', id=user_id, display_name=display_name) self.grants.append(grant) def startElement(self, name, attrs, connection): if name == 'Grant': self.grants.append(Grant(self)) return self.grants[-1] else: return None def endElement(self, name, value, connection): if name == 'Grant': pass else: setattr(self, name, value) def to_xml(self): s = '<AccessControlList>' for grant in self.grants: s += grant.to_xml() s += '</AccessControlList>' return s class Grant: NameSpace = 'xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"' def __init__(self, permission=None, type=None, id=None, display_name=None, uri=None, email_address=None): self.permission = permission self.id = id self.display_name = display_name self.uri = uri self.email_address = email_address self.type = type def startElement(self, name, attrs, connection): if name == 'Grantee': self.type = attrs['xsi:type'] return None def endElement(self, name, value, connection): if name == 'ID': self.id = value elif name == 'DisplayName': self.display_name = value elif name == 'URI': self.uri = value elif name == 'EmailAddress': self.email_address = value elif name == 'Grantee': pass elif name == 'Permission': self.permission = value else: setattr(self, name, value) def to_xml(self): s = '<Grant>' s += '<Grantee %s xsi:type="%s">' % (self.NameSpace, self.type) if self.type == 'CanonicalUser': s += '<ID>%s</ID>' % self.id s += '<DisplayName>%s</DisplayName>' % self.display_name elif self.type == 'Group': s += '<URI>%s</URI>' % self.uri else: s += '<EmailAddress>%s</EmailAddress>' % self.email_address s += '</Grantee>' s += '<Permission>%s</Permission>' % self.permission s += '</Grant>' return s
import xmlrpclib, logging from urlparse import urlparse """ author: msune, CarolinaFernandez Server monitoring thread """ class XmlRpcClient(): """ Calling a remote method with variable number of parameters """ @staticmethod def callRPCMethodBasicAuth(url,userName,password,methodName,*params): result = None #Incrust basic authentication parsed = urlparse(url) newUrl = parsed.scheme+"://"+userName+":"+password+"@"+parsed.netloc+parsed.path if not parsed.query == "": newUrl += "?"+parsed.query try: result = XmlRpcClient.callRPCMethod(newUrl,methodName,*params) except Exception: raise return result @staticmethod def callRPCMethod(url,methodName,*params): result = None try: server = xmlrpclib.Server(url) result = getattr(server,methodName)(*params) except Exception as e: turl=url.split('@') if len(turl)>1: url = turl[0].split('//')[0]+'//'+turl[-1] te =str(e) if '@' in te: e=te[0:te.find('for ')]+te[te.find('@')+1:] logging.error("XMLRPC Client error: can't connect to method %s at %s" % (methodName, url)) logging.error(e) raise Exception("XMLRPC Client error: can't connect to method %s at %s\n" % (methodName, url) + str(e)) return result
""" This is the Django template system. How it works: The Lexer.tokenize() function converts a template string (i.e., a string containing markup with custom template tags) to tokens, which can be either plain text (TOKEN_TEXT), variables (TOKEN_VAR) or block statements (TOKEN_BLOCK). The Parser() class takes a list of tokens in its constructor, and its parse() method returns a compiled template -- which is, under the hood, a list of Node objects. Each Node is responsible for creating some sort of output -- e.g. simple text (TextNode), variable values in a given context (VariableNode), results of basic logic (IfNode), results of looping (ForNode), or anything else. The core Node types are TextNode, VariableNode, IfNode and ForNode, but plugin modules can define their own custom node types. Each Node has a render() method, which takes a Context and returns a string of the rendered node. For example, the render() method of a Variable Node returns the variable's value as a string. The render() method of an IfNode returns the rendered output of whatever was inside the loop, recursively. The Template class is a convenient wrapper that takes care of template compilation and rendering. Usage: The only thing you should ever use directly in this file is the Template class. Create a compiled template object with a template_string, then call render() with a context. In the compilation stage, the TemplateSyntaxError exception will be raised if the template doesn't have proper syntax. Sample code: >>> from django import template >>> s = u'<html>{% if test %}<h1>{{ varvalue }}</h1>{% endif %}</html>' >>> t = template.Template(s) (t is now a compiled template, and its render() method can be called multiple times with multiple contexts) >>> c = template.Context({'test':True, 'varvalue': 'Hello'}) >>> t.render(c) u'<html><h1>Hello</h1></html>' >>> c = template.Context({'test':False, 'varvalue': 'Hello'}) >>> t.render(c) u'<html></html>' """ # Template lexing symbols from django.template.base import (ALLOWED_VARIABLE_CHARS, BLOCK_TAG_END, BLOCK_TAG_START, COMMENT_TAG_END, COMMENT_TAG_START, FILTER_ARGUMENT_SEPARATOR, FILTER_SEPARATOR, SINGLE_BRACE_END, SINGLE_BRACE_START, TOKEN_BLOCK, TOKEN_COMMENT, TOKEN_TEXT, TOKEN_VAR, TRANSLATOR_COMMENT_MARK, UNKNOWN_SOURCE, VARIABLE_ATTRIBUTE_SEPARATOR, VARIABLE_TAG_END, VARIABLE_TAG_START, filter_re, tag_re) # Exceptions from django.template.base import (ContextPopException, InvalidTemplateLibrary, TemplateDoesNotExist, TemplateEncodingError, TemplateSyntaxError, VariableDoesNotExist) # Template parts from django.template.base import (Context, FilterExpression, Lexer, Node, NodeList, Parser, RequestContext, Origin, StringOrigin, Template, TextNode, Token, TokenParser, Variable, VariableNode, constant_string, filter_raw_string) # Compiling templates from django.template.base import (compile_string, resolve_variable, unescape_string_literal, generic_tag_compiler) # Library management from django.template.base import (Library, add_to_builtins, builtins, get_library, get_templatetags_modules, get_text_list, import_library, libraries) __all__ = ('Template', 'Context', 'RequestContext', 'compile_string')
#!/usr/bin/env python # Let S_m = (x_1, x_2, ... , x_m) be the m-tuple of positive real # numbers with x_1 + x_2 + ... + x_m = m for which # P_m = x_1 * x_2^2 * ... * x_m^m is maximised. # For example, it can be verified that [P_10] = 4112 # ([] is the integer part function). # Find SUM[P_m] for 2 <= m <= 15. # -------- LAGRANGE -------- # maximize f(x,...) given g(x,....) = c # set ratio of partials equal to lambda # Since g = x_1 + ... + x_m # We need d(P_m)/d(x_i) = i P_m/x_i = lambda # Hence i/x_i = 1/x_1, x_i = i*x_1 # m = x_1(1 + ... + m) = x_1(m)(m+1)/2 # x_1 = 2/(m + 1) # P_m = (2/m+1)**(m*(m+1)/2)*(1*2**2*...*m**m) # P_10 = (2/11)**(55)*(1*4*...*(10**10)) = 4112.0850028536197 import operator from math import floor from python.decorators import euler_timer def P(m): return reduce(operator.mul, [((2 * n) / (1.0 * (m + 1))) ** n for n in range(1, m + 1)]) def main(verbose=False): return int(sum(floor(P(n)) for n in range(2, 16))) if __name__ == '__main__': print euler_timer(190)(main)(verbose=True)
""" Test suite for the code in msilib """ import unittest import os from test_support import run_unittest, import_module msilib = import_module('msilib') class Test_make_id(unittest.TestCase): #http://msdn.microsoft.com/en-us/library/aa369212(v=vs.85).aspx """The Identifier data type is a text string. Identifiers may contain the ASCII characters A-Z (a-z), digits, underscores (_), or periods (.). However, every identifier must begin with either a letter or an underscore. """ def test_is_no_change_required(self): self.assertEqual( msilib.make_id("short"), "short") self.assertEqual( msilib.make_id("nochangerequired"), "nochangerequired") self.assertEqual( msilib.make_id("one.dot"), "one.dot") self.assertEqual( msilib.make_id("_"), "_") self.assertEqual( msilib.make_id("a"), "a") #self.assertEqual( # msilib.make_id(""), "") def test_invalid_first_char(self): self.assertEqual( msilib.make_id("9.short"), "_9.short") self.assertEqual( msilib.make_id(".short"), "_.short") def test_invalid_any_char(self): self.assertEqual( msilib.make_id(".s\x82ort"), "_.s_ort") self.assertEqual ( msilib.make_id(".s\x82o?*+rt"), "_.s_o___rt") def test_main(): run_unittest(__name__) if __name__ == '__main__': test_main()
__author__ = 'huanpc' CPU_THRESHOLD_UP = 0.1 CPU_THRESHOLD_DOWN = 0.001 MEM_THRESHOLD_UP = 15700000.0 MEM_THRESHOLD_DOWN = 2097152.0 HOST = '25.22.28.94' PORT = 8086 USER = 'root' PASS = 'root' DATABASE = 'cadvisor' SELECT_CPU = 'derivative(cpu_cumulative_usage)' SELECT_MEMORY = 'median(memory_usage)' SERIES = '"stats"' APP_NAME = 'demo-server' NAME = '' WHERE_BEGIN = 'container_name =~ /.*' WHERE_END = '.*/ and time>now()-5m' GROUP_BY = "time(10s), container_name" CONDITION = " limit 1 " JSON_APP_DEFINE = './demo_web_server.json' APP_ID = 'demo-server' MARATHON_URI = 'localhost:8080' HEADER = {'Content-Type': 'application/json'} # scale SCALE_LINK = '/v2/apps/' + APP_ID + '?force=true' TIME_DELAY_LONG = 15 TIME_DELAY_SORT = 5 ROOT_PASSWORD = '444455555' MODEL_ENGINE = 'mysql+pymysql://root:autoscaling@secret@127.0.0.1:3306/policydb' SCHEMA = ''' # PolicyDB # apps.enabled: 0-not scaled, 1-scaled # apps.locked: 0-unlocked, 1-locked # apps.next_time: time in the future the app'll be checked for scaling # next_time = last success caused by policyX + policyX.cooldown_period # policies.metric_type: 0-CPU, 1-memory # policies.cooldown_period: in second # policies.measurement_period: in second # deleted: 0-active, 1-deleted DROP DATABASE IF EXISTS policydb; CREATE DATABASE policydb; USE policydb; CREATE TABLE apps(\ Id INT AUTO_INCREMENT PRIMARY KEY, \ app_uuid VARCHAR(255), \ name VARCHAR(255), \ min_instances SMALLINT UNSIGNED, \ max_instances SMALLINT UNSIGNED, \ enabled TINYINT UNSIGNED, \ locked TINYINT UNSIGNED, \ next_time INT \ ); CREATE TABLE policies(\ Id INT AUTO_INCREMENT PRIMARY KEY, \ app_uuid VARCHAR(255), \ policy_uuid VARCHAR(255), \ metric_type TINYINT UNSIGNED, \ upper_threshold FLOAT, \ lower_threshold FLOAT, \ instances_out SMALLINT UNSIGNED, \ instances_in SMALLINT UNSIGNED, \ cooldown_period SMALLINT UNSIGNED, \ measurement_period SMALLINT UNSIGNED, \ deleted TINYINT UNSIGNED \ ); # tuna CREATE TABLE crons(\ Id INT AUTO_INCREMENT PRIMARY KEY, \ app_uuid VARCHAR(255), \ cron_uuid VARCHAR(255), \ min_instances SMALLINT UNSIGNED, \ max_instances SMALLINT UNSIGNED, \ cron_string VARCHAR(255), \ deleted TINYINT UNSIGNED \ ); # end tuna ----- # Test data # Stresser INSERT INTO apps(app_uuid, name, min_instances, max_instances, enabled, locked, next_time) \ VALUES ("f5bfcbad-7daa-4317-97cc-e42ae46b6ad1", "java-allocateMemory", 1, 5, 1, 0, 0); INSERT INTO policies(app_uuid, policy_uuid, metric_type, upper_threshold, lower_threshold, instances_out, instances_in, cooldown_period, measurement_period, deleted) \ VALUES ("f5bfcbad-7daa-4317-97cc-e42ae46b6ad1", "b3da4493-58f1-4d65-bf43-e52e7de62151", 1, 0.7, 0.3, 1, 1, 30, 10, 0); # INSERT INTO policies(app_uuid, policy_uuid, metric_type, upper_threshold, lower_threshold, instances_out, instances_in, cooldown_period, measurement_period, deleted) \ # VALUES ("f5bfcbad-7daa-4317-97cc-e42ae46b6ad1", "b3da4493-58f1-4d65-bf43-e52e7dpolicy", 1, 0.7, 0.3, 1, 1, 30, 10, 0); INSERT INTO crons(app_uuid, cron_uuid, min_instances, max_instances, cron_string, deleted) \ VALUES ("f5bfcbad-7daa-4317-97cc-e42ae46b6ad1", "b3da4493-58f1-4d65-bf43-e52eacascron", 1, 10, "* * * * * *", false); '''
from gps import * import time import datetime import threading import math class GpsUtils(): MPS_TO_MPH = 2.2369362920544 @staticmethod def latLongToXY(lat, lon): rMajor = 6378137 # Equatorial Radius, WGS84 shift = math.pi * rMajor x = lon * shift / 180 y = math.log(math.tan((90 + lat) * math.pi / 360)) / (math.pi / 180) y = y * shift / 180 return x,y class GpsController(threading.Thread): def __init__(self): threading.Thread.__init__(self) self.gpsd = gps(mode=WATCH_ENABLE) #starting the stream of info self.running = False def run(self): self.running = True while self.running: # grab EACH set of gpsd info to clear the buffer self.gpsd.next() def stopController(self): self.running = False @property def fix(self): return self.gpsd.fix @property def utc(self): return self.gpsd.utc @property def satellites(self): return self.gpsd.satellites @property def fixdatetime(self): #return None if we cant get a time UTCTime = None try: # have we got a fix? if self.fix.mode != 1: #strip time from utc UTCTime = time.strptime(self.utc, "%Y-%m-%dT%H:%M:%S.%fz") #convert time struct to datetime UTCTime = datetime.datetime.fromtimestamp(time.mktime(UTCTime)) except: #return None if we get an error UTCTime = None return UTCTime if __name__ == '__main__': gpsc = GpsController() # create the thread try: gpsc.start() # start it up while True: print "latitude ", gpsc.fix.latitude print "longitude ", gpsc.fix.longitude print "time utc ", gpsc.utc, " + ", gpsc.gpsd.fix.time print "altitude (m)", gpsc.fix.altitude #print "eps ", gpsc.gpsd.fix.eps #print "epx ", gpsc.gpsd.fix.epx #print "epv ", gpsc.gpsd.fix.epv #print "ept ", gpsc.gpsd.fix.ept print "speed (m/s) ", gpsc.fix.speed print "track ", gpsc.gpsd.fix.track print "mode ", gpsc.gpsd.fix.mode #print "sats ", gpsc.satellites print "climb ", gpsc.fix.climb print gpsc.fixdatetime x,y = GpsUtils.latLongToXY(gpsc.fix.latitude, gpsc.fix.longitude) print "x", x print "y", y time.sleep(0.5) #Ctrl C except KeyboardInterrupt: print "User cancelled" #Error except: print "Unexpected error:", sys.exc_info()[0] raise finally: print "Stopping gps controller" gpsc.stopController() #wait for the tread to finish gpsc.join() print "Done"
#!/usr/bin/env python import datetime import six from sqlalchemy import Column, MetaData, Table, create_engine from sqlalchemy import BigInteger, Boolean, Date, DateTime, Float, Integer, String, Time from sqlalchemy.schema import CreateTable NoneType = type(None) DIALECTS = { 'access': 'access.base', 'firebird': 'firebird.kinterbasdb', 'informix': 'informix.informixdb', 'maxdb': 'maxdb.sapdb', 'mssql': 'mssql.pyodbc', 'mysql': 'mysql.mysqlconnector', 'oracle': 'oracle.cx_oracle', 'postgresql': 'postgresql.psycopg2', 'sqlite': 'sqlite.pysqlite', 'sybase': 'sybase.pyodbc' } NULL_COLUMN_MAX_LENGTH = 32 SQL_INTEGER_MAX = 2147483647 SQL_INTEGER_MIN = -2147483647 def make_column(column, no_constraints=False): """ Creates a sqlalchemy column from a csvkit Column. """ sql_column_kwargs = {} sql_type_kwargs = {} column_types = { bool: Boolean, #int: Integer, see special case below float: Float, datetime.datetime: DateTime, datetime.date: Date, datetime.time: Time, NoneType: String, six.text_type: String } if column.type in column_types: sql_column_type = column_types[column.type] elif column.type is int: column_max = max([v for v in column if v is not None]) column_min = min([v for v in column if v is not None]) if column_max > SQL_INTEGER_MAX or column_min < SQL_INTEGER_MIN: sql_column_type = BigInteger else: sql_column_type = Integer else: raise ValueError('Unexpected normalized column type: %s' % column.type) if no_constraints is False: if column.type is NoneType: sql_type_kwargs['length'] = NULL_COLUMN_MAX_LENGTH elif column.type is six.text_type: sql_type_kwargs['length'] = column.max_length() sql_column_kwargs['nullable'] = column.has_nulls() return Column(column.name, sql_column_type(**sql_type_kwargs), **sql_column_kwargs) def get_connection(connection_string): engine = create_engine(connection_string) metadata = MetaData(engine) return engine, metadata def make_table(csv_table, name='table_name', no_constraints=False, db_schema=None, metadata=None): """ Creates a sqlalchemy table from a csvkit Table. """ if not metadata: metadata = MetaData() sql_table = Table(csv_table.name, metadata, schema=db_schema) for column in csv_table: sql_table.append_column(make_column(column, no_constraints)) return sql_table def make_create_table_statement(sql_table, dialect=None): """ Generates a CREATE TABLE statement for a sqlalchemy table. """ if dialect: module = __import__('sqlalchemy.dialects.%s' % DIALECTS[dialect], fromlist=['dialect']) sql_dialect = module.dialect() else: sql_dialect = None return six.text_type(CreateTable(sql_table).compile(dialect=sql_dialect)).strip() + ';'
# Copyright (c) 2011 OpenStack Foundation. # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import logging import six from cinder.openstack.common.scheduler import filters from cinder.openstack.common.scheduler.filters import extra_specs_ops LOG = logging.getLogger(__name__) class CapabilitiesFilter(filters.BaseHostFilter): """HostFilter to work with resource (instance & volume) type records.""" def _satisfies_extra_specs(self, capabilities, resource_type): """Check that the capabilities provided by the services satisfy the extra specs associated with the resource type. """ extra_specs = resource_type.get('extra_specs', []) if not extra_specs: return True for key, req in six.iteritems(extra_specs): # Either not scope format, or in capabilities scope scope = key.split(':') if len(scope) > 1 and scope[0] != "capabilities": continue elif scope[0] == "capabilities": del scope[0] cap = capabilities for index in range(len(scope)): try: cap = cap.get(scope[index]) except AttributeError: return False if cap is None: return False if not extra_specs_ops.match(cap, req): LOG.debug("extra_spec requirement '%(req)s' " "does not match '%(cap)s'", {'req': req, 'cap': cap}) return False return True def host_passes(self, host_state, filter_properties): """Return a list of hosts that can create resource_type.""" # Note(zhiteng) Currently only Cinder and Nova are using # this filter, so the resource type is either instance or # volume. resource_type = filter_properties.get('resource_type') if not self._satisfies_extra_specs(host_state.capabilities, resource_type): LOG.debug("%(host_state)s fails resource_type extra_specs " "requirements", {'host_state': host_state}) return False return True
# -*- coding: utf-8 -*- """ pygments.styles.fruity ~~~~~~~~~~~~~~~~~~~~~~ pygments version of my "fruity" vim theme. :copyright: Copyright 2006-2013 by the Pygments team, see AUTHORS. :license: BSD, see LICENSE for details. """ from pygments.style import Style from pygments.token import Token, Comment, Name, Keyword, \ Generic, Number, String, Whitespace class FruityStyle(Style): """ Pygments version of the "native" vim theme. """ background_color = '#111111' highlight_color = '#333333' styles = { Whitespace: '#888888', Token: '#ffffff', Generic.Output: '#444444 bg:#222222', Keyword: '#fb660a bold', Keyword.Pseudo: 'nobold', Number: '#0086f7 bold', Name.Tag: '#fb660a bold', Name.Variable: '#fb660a', Comment: '#008800 bg:#0f140f italic', Name.Attribute: '#ff0086 bold', String: '#0086d2', Name.Function: '#ff0086 bold', Generic.Heading: '#ffffff bold', Keyword.Type: '#cdcaa9 bold', Generic.Subheading: '#ffffff bold', Name.Constant: '#0086d2', Comment.Preproc: '#ff0007 bold' }
# -*- coding: utf-8 -*- from __future__ import unicode_literals from django.db import migrations, models import django.utils.timezone from django.conf import settings import model_utils.fields import django.core.validators from openedx.core.djangoapps.xmodule_django.models import CourseKeyField class Migration(migrations.Migration): dependencies = [ migrations.swappable_dependency(settings.AUTH_USER_MODEL), ] operations = [ migrations.CreateModel( name='UserCourseTag', fields=[ ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)), ('key', models.CharField(max_length=255, db_index=True)), ('course_id', CourseKeyField(max_length=255, db_index=True)), ('value', models.TextField()), ('user', models.ForeignKey(related_name='+', to=settings.AUTH_USER_MODEL)), ], ), migrations.CreateModel( name='UserOrgTag', fields=[ ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)), ('created', model_utils.fields.AutoCreatedField(default=django.utils.timezone.now, verbose_name='created', editable=False)), ('modified', model_utils.fields.AutoLastModifiedField(default=django.utils.timezone.now, verbose_name='modified', editable=False)), ('key', models.CharField(max_length=255, db_index=True)), ('org', models.CharField(max_length=255, db_index=True)), ('value', models.TextField()), ('user', models.ForeignKey(related_name='+', to=settings.AUTH_USER_MODEL)), ], ), migrations.CreateModel( name='UserPreference', fields=[ ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)), ('key', models.CharField(db_index=True, max_length=255, validators=[django.core.validators.RegexValidator(b'[-_a-zA-Z0-9]+')])), ('value', models.TextField()), ('user', models.ForeignKey(related_name='preferences', to=settings.AUTH_USER_MODEL)), ], ), migrations.AlterUniqueTogether( name='userpreference', unique_together=set([('user', 'key')]), ), migrations.AlterUniqueTogether( name='userorgtag', unique_together=set([('user', 'org', 'key')]), ), migrations.AlterUniqueTogether( name='usercoursetag', unique_together=set([('user', 'course_id', 'key')]), ), ]
from __future__ import division from __future__ import absolute_import from __future__ import print_function from __future__ import unicode_literals import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.calibration import CalibratedClassifierCV from sklearn.metrics import f1_score import argparse from os import path import os from hyperopt import fmin, tpe, hp, STATUS_OK, Trials from utils import * import pickle np.random.seed(54568464) def parse_args(): parser = argparse.ArgumentParser() parser.add_argument('--yix', type=int, default=0) return parser.parse_args() # functions for hyperparameters optimization class Score: def __init__(self, X, y): self.y = y self.X = X def get_score(self, params): params['n_estimators'] = int(params['n_estimators']) params['max_depth'] = int(params['max_depth']) params['min_samples_split'] = int(params['min_samples_split']) params['min_samples_leaf'] = int(params['min_samples_leaf']) params['n_estimators'] = int(params['n_estimators']) print('Training with params:') print(params) # cross validation here scores = [] for train_ix, test_ix in makeKFold(5, self.y, 1): X_train, y_train = self.X[train_ix, :], self.y[train_ix] X_test, y_test = self.X[test_ix, :], self.y[test_ix] weight = y_train.shape[0] / (2 * np.bincount(y_train)) sample_weight = np.array([weight[i] for i in y_train]) clf = RandomForestClassifier(**params) cclf = CalibratedClassifierCV(base_estimator=clf, method='isotonic', cv=makeKFold(3, y_train, 1)) cclf.fit(X_train, y_train, sample_weight) pred = cclf.predict(X_test) scores.append(f1_score(y_true=y_test, y_pred=pred)) print(scores) score = np.mean(scores) print(score) return {'loss': -score, 'status': STATUS_OK} def optimize(trials, X, y, max_evals): space = { 'n_estimators': hp.quniform('n_estimators', 100, 500, 50), 'criterion': hp.choice('criterion', ['gini', 'entropy']), 'max_depth': hp.quniform('max_depth', 1, 7, 1), 'min_samples_split': hp.quniform('min_samples_split', 1, 9, 2), 'min_samples_leaf': hp.quniform('min_samples_leaf', 1, 5, 1), 'bootstrap': True, 'oob_score': True, 'n_jobs': -1 } s = Score(X, y) best = fmin(s.get_score, space, algo=tpe.suggest, trials=trials, max_evals=max_evals ) best['n_estimators'] = int(best['n_estimators']) best['max_depth'] = int(best['max_depth']) best['min_samples_split'] = int(best['min_samples_split']) best['min_samples_leaf'] = int(best['min_samples_leaf']) best['n_estimators'] = int(best['n_estimators']) best['criterion'] = ['gini', 'entropy'][best['criterion']] best['bootstrap'] = True best['oob_score'] = True best['n_jobs'] = -1 del s return best def out_fold_pred(params, X, y): # cross validation here preds = np.zeros((y.shape[0])) for train_ix, test_ix in makeKFold(5, y, 1): X_train, y_train = X[train_ix, :], y[train_ix] X_test = X[test_ix, :] weight = y_train.shape[0] / (2 * np.bincount(y_train)) sample_weight = np.array([weight[i] for i in y_train]) clf = RandomForestClassifier(**params) cclf = CalibratedClassifierCV(base_estimator=clf, method='isotonic', cv=makeKFold(3, y_train, 1)) cclf.fit(X_train, y_train, sample_weight) pred = cclf.predict_proba(X_test)[:, 1] preds[test_ix] = pred return preds def get_model(params, X, y): clf = RandomForestClassifier(**params) cclf = CalibratedClassifierCV(base_estimator=clf, method='isotonic', cv=makeKFold(3, y, 1)) weight = y.shape[0] / (2 * np.bincount(y)) sample_weight = np.array([weight[i] for i in y]) cclf.fit(X, y, sample_weight) return cclf args = parse_args() data_dir = '../level3-feature/' + str(args.yix) X_train = np.load(path.join(data_dir, 'X_train.npy')) X_test = np.load(path.join(data_dir, 'X_test.npy')) y_train = np.load(path.join(data_dir, 'y_train.npy')) print(X_train.shape, X_test.shape, y_train.shape) X_train_ext = np.load('../extra_ftrs/' + str(args.yix) + '/X_train_ext.npy') X_test_ext = np.load('../extra_ftrs/' + str(args.yix) + '/X_test_ext.npy') print(X_train_ext.shape, X_test_ext.shape) X_train = np.hstack((X_train, X_train_ext)) X_test = np.hstack((X_test, X_test_ext)) print('Add Extra') print(X_train.shape, X_test.shape, y_train.shape) # Now we have X_train, X_test, y_train trials = Trials() params = optimize(trials, X_train, y_train, 50) out_fold = out_fold_pred(params, X_train, y_train) clf = get_model(params, X_train, y_train) preds = clf.predict_proba(X_test)[:, 1] save_dir = '../level3-model-final/' + str(args.yix) print(save_dir) if not path.exists(save_dir): os.makedirs(save_dir) # save model, parameter, outFold_pred, pred with open(path.join(save_dir, 'model_rf.pkl'), 'wb') as f_model: pickle.dump(clf.calibrated_classifiers_, f_model) with open(path.join(save_dir, 'param_rf.pkl'), 'wb') as f_param: pickle.dump(params, f_param) np.save(path.join(save_dir, 'pred_rf.npy'), preds) np.save(path.join(save_dir, 'outFold_rf.npy'), out_fold)
# BS mark.1-55 # /* coding: utf-8 */ # BlackSmith plugin # This should be rewritten! BLACK_LIST = 'dynamic/blacklist.txt' CHAT_CACHE = {} AMSGBL = [] DirtyChats = [] def handler_chat_cache(stanza, ltype, source, body): try: subject = stanza.getTag('subject') except: subject = False if ltype != 'public' or subject or not source[2]: return header = u'[%s] %s ' % (time.strftime('%H:%M:%S (%d.%m.%Y) GMT', time.gmtime()), source[2]) CHAT_CACHE[source[1]]['1'] = CHAT_CACHE[source[1]]['2'] if len(body) > 256: body = body[:256]+'[...]' CHAT_CACHE[source[1]]['2'] = header+body def handler_clean(mType, source, body): if source[1] in GROUPCHATS: if source[1] in DirtyChats: DirtyChats.remove(source[1]) if mType != "private": change_bot_status(source[1], u"...", "dnd") zero = xmpp.Message(source[1], "", typ = "groupchat") zero.setTag("body") count = 24 if check_number(body): number = int(body) if number < 51: count = number for msg in xrange(count): try: jClient.send(zero) except IOError: return INFO['outmsg'] += 1 if (msg != count): time.sleep(1.4) if mType != "private": message = STATUS[source[1]]["message"] status = STATUS[source[1]]["status"] change_bot_status(source[1], message, status) CHAT_CACHE[source[1]] = {"1": "", "2": ""} DirtyChats.append(source[1]) else: reply(mType, source, " .") else: reply(mType, source, "  !") def last_chat_cache(type, source, body): confs = sorted(GROUPCHATS.keys()) if body: body = body.lower() if body in confs: conf = body elif check_number(body): number = int(body) - 1 if number >= 0 and number <= len(confs): conf = confs[number] else: conf = False else: conf = False if conf: cache = '' if CHAT_CACHE[conf]['1']: cache += '\n'+CHAT_CACHE[conf]['1'] if CHAT_CACHE[conf]['2']: cache += '\n'+CHAT_CACHE[conf]['2'] if not cache: cache = u'' reply(type, source, cache) else: reply(type, source, u'  !') else: col, list = 0, '' for conf in confs: col = col + 1 list += u'\n '+str(col)+'. - '+conf reply(type, source, list) def handler_test(type, source, body): if time.localtime()[1:3] == (4, 1): testfr = [u" !!11", u"11111", u" 1111", u"!!111", u"!!!!" u"111", u" ......", u" .", u" !1", u" 11"] answer = random.choice(testfr) else: testfr = {0: u"  ! ( )", 1: u"-  (1 )", 2: u"-  ... (2 )", 3: u"-       (3 ) (!)", 4: u"-    (4 ) (!!)", "more": u"     (%d ) (!!!)"} Error = len(ERRORS.keys()) # meet you here! answer = testfr.get(Error, testfr["more"] % Error) reply(type, source, answer + (' (PID: %s)' % str(BOT_PID))) def handler_admin_message(type, source, body): if body: args = body.split() if len(args) >= 2: jid = args[0].strip() if "@" in jid and "." in jid: inst = jid.split('/')[0].lower() if "@conf" in jid and inst not in GROUPCHATS: reply(type, source, u'    .') else: mess = body[(body.find(' ') + 1):].strip() if len(mess) <= 1024: msg(jid, u'  '+source[2]+': '+mess) reply(type, source, u'') else: reply(type, source, u'  !') else: reply(type, source, u'   JabberID!') else: reply(type, source, u'  -?') def handler_admin_say(type, source, body): if body: if len(body) <= 256: msg(source[1], body) else: msg(source[1], body[:256]) else: reply(type, source, u'  ?') def handler_global_message(type, source, body): if body: for conf in GROUPCHATS.keys(): msg(conf, u'###   '+source[2]+':\n'+body) reply(type, source, u'  .') else: reply(type, source, u'  -?') def handler_auto_message(type, source, body): if body: jid = handler_jid(source[0]) if jid in AMSGBL: reply(type, source, u'    .') elif len(body) <= 1024: delivery(u'  '+source[2]+' ('+jid+'): '+body) reply(type, source, u'') else: reply(type, source, u'  !') else: reply(type, source, u'  ?') def handler_amsg_blacklist(type, source, body): if body: args = body.split() if len(args) == 2: jid = args[1].strip() if "@" in jid and "." in jid: check = args[0].strip() if check == '+': if jid not in AMSGBL: AMSGBL.append(jid) write_file(BLACK_LIST, str(AMSGBL)) repl = u' %s   ' % (jid) else: repl = u'    ' elif check == '-': if jid in AMSGBL: AMSGBL.remove(jid) write_file(BLACK_LIST, str(AMSGBL)) repl = u' %s   ' % (jid) else: repl = u'     ' else: repl = u' ' else: repl = u' ,    !' else: repl = u' ' else: repl, col = u' :', 0 for jid in AMSGBL: col = col + 1 repl += '\n'+str(col)+'. '+jid if col == 0: repl = u'  ' reply(type, source, repl) def amsg_blacklist_init(): if initialize_file(BLACK_LIST, '[]'): globals()['AMSGBL'] = eval(read_file(BLACK_LIST)) else: Print('\n\nError: can`t create black list file!', color2) def chat_cache_init(chat): CHAT_CACHE[chat] = {'1': '', '2': ''} DirtyChats.append(chat) handler_register("01eh", handler_chat_cache) command_handler(handler_clean, 15, "collect") command_handler(last_chat_cache, 20, "collect") command_handler(handler_test, 10, "collect") command_handler(handler_admin_message, 100, "collect") command_handler(handler_admin_say, 20, "collect") command_handler(handler_global_message, 100, "collect") command_handler(handler_auto_message, 10, "collect") command_handler(handler_amsg_blacklist, 100, "collect") handler_register("00si", amsg_blacklist_init) handler_register("01si", chat_cache_init)
# Copyright (C) 1998-2015 by the Free Software Foundation, Inc. # # This file is part of GNU Mailman. # # GNU Mailman is free software: you can redistribute it and/or modify it under # the terms of the GNU General Public License as published by the Free # Software Foundation, either version 3 of the License, or (at your option) # any later version. # # GNU Mailman is distributed in the hope that it will be useful, but WITHOUT # ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or # FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for # more details. # # You should have received a copy of the GNU General Public License along with # GNU Mailman. If not, see <http://www.gnu.org/licenses/>. """Send an acknowledgment of the successful post to the sender. This only happens if the sender has set their AcknowledgePosts attribute. """ __all__ = [ 'Acknowledge', ] from mailman.core.i18n import _ from mailman.email.message import UserNotification from mailman.interfaces.handler import IHandler from mailman.interfaces.languages import ILanguageManager from mailman.utilities.i18n import make from mailman.utilities.string import oneline from zope.component import getUtility from zope.interface import implementer @implementer(IHandler) class Acknowledge: """Send an acknowledgment.""" name = 'acknowledge' description = _("""Send an acknowledgment of a posting.""") def process(self, mlist, msg, msgdata): """See `IHandler`.""" # Extract the sender's address and find them in the user database sender = msgdata.get('original_sender', msg.sender) member = mlist.members.get_member(sender) if member is None or not member.acknowledge_posts: # Either the sender is not a member, in which case we can't know # whether they want an acknowlegment or not, or they are a member # who definitely does not want an acknowlegment. return # Okay, they are a member that wants an acknowledgment of their post. # Give them their original subject. BAW: do we want to use the # decoded header? original_subject = msgdata.get( 'origsubj', msg.get('subject', _('(no subject)'))) # Get the user's preferred language. language_manager = getUtility(ILanguageManager) language = (language_manager[msgdata['lang']] if 'lang' in msgdata else member.preferred_language) # Now get the acknowledgement template. display_name = mlist.display_name text = make('postack.txt', mailing_list=mlist, language=language.code, wrap=False, subject=oneline(original_subject, in_unicode=True), list_name=mlist.list_name, display_name=display_name, listinfo_url=mlist.script_url('listinfo'), optionsurl=member.options_url, ) # Craft the outgoing message, with all headers and attributes # necessary for general delivery. Then enqueue it to the outgoing # queue. subject = _('$display_name post acknowledgment') usermsg = UserNotification(sender, mlist.bounces_address, subject, text, language) usermsg.send(mlist)
from time import strptime from dependencies.dependency import ClassSecurityInfo from dependencies.dependency import DateTime, safelocaltime from dependencies.dependency import DateTimeError from dependencies.dependency import registerField from dependencies.dependency import IDateTimeField from dependencies.dependency import * from dependencies.dependency import DateTimeField as DTF from lims import logger from dependencies.dependency import implements class DateTimeField(DTF): """A field that stores dates and times This is identical to the AT widget on which it's based, but it checks the i18n translation values for date formats. This does not specifically check the date_format_short_datepicker, so this means that date_formats should be identical between the python strftime and the jquery version. """ _properties = Field._properties.copy() _properties.update({ 'type': 'datetime', 'widget': CalendarWidget, }) implements(IDateTimeField) security = ClassSecurityInfo() security.declarePrivate('set') def set(self, instance, value, **kwargs): """ Check if value is an actual date/time value. If not, attempt to convert it to one; otherwise, set to None. Assign all properties passed as kwargs to object. """ val = value if not value: val = None elif not isinstance(value, DateTime): for fmt in ['date_format_long', 'date_format_short']: fmtstr = instance.translate(fmt, domain='bika', mapping={}) fmtstr = fmtstr.replace(r"${", '%').replace('}', '') try: val = strptime(value, fmtstr) except ValueError: continue try: val = DateTime(*list(val)[:-6]) except DateTimeError: val = None if val.timezoneNaive(): # Use local timezone for tz naive strings # see http://dev.plone.org/plone/ticket/10141 zone = val.localZone(safelocaltime(val.timeTime())) parts = val.parts()[:-1] + (zone,) val = DateTime(*parts) break else: logger.warning("DateTimeField failed to format date " "string '%s' with '%s'" % (value, fmtstr)) super(DateTimeField, self).set(instance, val, **kwargs) registerField(DateTimeField, title='Date Time', description='Used for storing date/time')
#!/usr/bin/env python # -*- Python -*- # -*- coding: utf-8 -*- '''rtcshell Copyright (C) 2009-2010 Geoffrey Biggs RT-Synthesis Research Group Intelligent Systems Research Institute, National Institute of Advanced Industrial Science and Technology (AIST), Japan All rights reserved. Licensed under the Eclipse Public License -v 1.0 (EPL) http://www.opensource.org/licenses/eclipse-1.0.txt File: rtmgr.py Implementation of the command for controlling managers. ''' # $Source$ from optparse import OptionParser, OptionError import os from rtctree.exceptions import RtcTreeError, FailedToLoadModuleError, \ FailedToUnloadModuleError, \ FailedToCreateComponentError, \ FailedToDeleteComponentError from rtctree.tree import create_rtctree, InvalidServiceError, \ FailedToNarrowRootNamingError, \ NonRootPathError from rtctree.path import parse_path import sys from rtcshell import RTSH_PATH_USAGE, RTSH_VERSION from rtcshell.path import cmd_path_to_full_path def get_manager(cmd_path, full_path, tree=None): path, port = parse_path(full_path) if port: # Can't configure a port print >>sys.stderr, '{0}: Cannot access {1}: No such \ object.'.format(sys.argv[0], cmd_path) return None if not path[-1]: # There was a trailing slash - ignore it path = path[:-1] if not tree: tree = create_rtctree(paths=path) if not tree: return None object = tree.get_node(path) if not object: print >>sys.stderr, '{0}: Cannot access {1}: No such \ object.'.format(sys.argv[0], cmd_path) return tree, None if not object.is_manager: print >>sys.stderr, '{0}: Cannot access {1}: Not a \ manager.'.format(sys.argv[0], cmd_path) return tree, None return tree, object def load_module(cmd_path, full_path, module_path, init_func, tree=None): tree, mgr = get_manager(cmd_path, full_path, tree) if not mgr: return 1 try: mgr.load_module(module_path, init_func) except FailedToLoadModuleError: print >>sys.stderr, '{0}: Failed to load module {1}'.format(\ sys.argv[0], module_path) return 1 return 0 def unload_module(cmd_path, full_path, module_path, tree=None): tree, mgr = get_manager(cmd_path, full_path, tree) if not mgr: return 1 try: mgr.unload_module(module_path) except FailedToUnloadModuleError: print >>sys.stderr, '{0}: Failed to unload module {1}'.format(\ sys.argv[0], module_path) return 1 return 0 def create_component(cmd_path, full_path, module_name, tree=None): tree, mgr = get_manager(cmd_path, full_path, tree) if not mgr: return 1 try: mgr.create_component(module_name) except FailedToCreateComponentError: print >>sys.stderr, '{0}: Failed to create component from module \ {1}'.format(sys.argv[0], module_name) return 1 return 0 def delete_component(cmd_path, full_path, instance_name, tree=None): tree, mgr = get_manager(cmd_path, full_path, tree) if not mgr: return 1 try: mgr.delete_component(instance_name) except FailedToDeleteComponentError, e: print >>sys.stderr, '{0}: Failed to delete component {1}'.format(\ sys.argv[0], instance_name) return 1 return 0 def main(argv=None, tree=None): usage = '''Usage: %prog [options] <path> <command> [args] Control a manager, adding and removing shared libraries and components. To set a mananger's configuration, use rtconf. A command should be one of: load, unload, create, delete load <file system path> <init function> Load a shared library (DLL file or .so file) into the manager. unload <file system path> Unload a shared library (DLL file or .so file) from the manager. create <module name> Create a new component instance from a loaded shared library. Properties of the new component can be set by specifying them as part of the module name argument, prefixed by a question mark. For example, to set the instance name of a new component of type ConsoleIn, use: rtmgr manager.mgr create ConsoleIn?instance_name=blag delete <instance name> Delete a component instance from the manager, destroying it. ''' + RTSH_PATH_USAGE version = RTSH_VERSION parser = OptionParser(usage=usage, version=version) parser.add_option('-d', '--debug', dest='debug', action='store_true', default=False, help='Print debugging information. \ [Default: %default]') if argv: sys.argv = [sys.argv[0]] + argv try: options, args = parser.parse_args() except OptionError, e: print 'OptionError:', e return 1 if len(args) > 2: cmd_path = args[0] cmd = args[1] args = args[2:] else: print >>sys.stderr, usage return 1 full_path = cmd_path_to_full_path(cmd_path) if cmd == 'load': if len(args) != 2: print >>sys.stderr, '{0}: Incorrect number of arguments for load \ command.'.format(sys.argv[0]) return 1 return load_module(cmd_path, full_path, args[0], args[1], tree) elif cmd == 'unload': if len(args) != 1: print >>sys.stderr, '{0}: Incorrect number of arguments for \ unload command.'.format(sys.argv[0]) return 1 return unload_module(cmd_path, full_path, args[0], tree) elif cmd == 'create': if len(args) != 1: print >>sys.stderr, '{0}: Incorrect number of arguments for \ create command.'.format(sys.argv[0]) return 1 return create_component(cmd_path, full_path, args[0], tree) elif cmd == 'delete': if len(args) != 1: print >>sys.stderr, '{0}: Incorrect number of arguments for \ delete command.'.format(sys.argv[0]) return 1 return delete_component(cmd_path, full_path, args[0], tree) print >>sys.stderr, usage return 1 # vim: tw=79
# -*- encoding: utf-8 -*- """ Tests for django.core.servers. """ from __future__ import unicode_literals import os import socket from django.core.exceptions import ImproperlyConfigured from django.test import LiveServerTestCase from django.test import override_settings from django.utils.http import urlencode from django.utils.six.moves.urllib.error import HTTPError from django.utils.six.moves.urllib.request import urlopen from django.utils._os import upath from .models import Person TEST_ROOT = os.path.dirname(upath(__file__)) TEST_SETTINGS = { 'MEDIA_URL': '/media/', 'MEDIA_ROOT': os.path.join(TEST_ROOT, 'media'), 'STATIC_URL': '/static/', 'STATIC_ROOT': os.path.join(TEST_ROOT, 'static'), } @override_settings(ROOT_URLCONF='servers.urls') class LiveServerBase(LiveServerTestCase): available_apps = [ 'servers', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', ] fixtures = ['testdata.json'] @classmethod def setUpClass(cls): # Override settings cls.settings_override = override_settings(**TEST_SETTINGS) cls.settings_override.enable() super(LiveServerBase, cls).setUpClass() @classmethod def tearDownClass(cls): # Restore original settings cls.settings_override.disable() super(LiveServerBase, cls).tearDownClass() def urlopen(self, url): return urlopen(self.live_server_url + url) class LiveServerAddress(LiveServerBase): """ Ensure that the address set in the environment variable is valid. Refs #2879. """ @classmethod def setUpClass(cls): # Backup original environment variable address_predefined = 'DJANGO_LIVE_TEST_SERVER_ADDRESS' in os.environ old_address = os.environ.get('DJANGO_LIVE_TEST_SERVER_ADDRESS') # Just the host is not accepted cls.raises_exception('localhost', ImproperlyConfigured) # The host must be valid cls.raises_exception('blahblahblah:8081', socket.error) # The list of ports must be in a valid format cls.raises_exception('localhost:8081,', ImproperlyConfigured) cls.raises_exception('localhost:8081,blah', ImproperlyConfigured) cls.raises_exception('localhost:8081-', ImproperlyConfigured) cls.raises_exception('localhost:8081-blah', ImproperlyConfigured) cls.raises_exception('localhost:8081-8082-8083', ImproperlyConfigured) # Restore original environment variable if address_predefined: os.environ['DJANGO_LIVE_TEST_SERVER_ADDRESS'] = old_address else: del os.environ['DJANGO_LIVE_TEST_SERVER_ADDRESS'] @classmethod def tearDownClass(cls): # skip it, as setUpClass doesn't call its parent either pass @classmethod def raises_exception(cls, address, exception): os.environ['DJANGO_LIVE_TEST_SERVER_ADDRESS'] = address try: super(LiveServerAddress, cls).setUpClass() raise Exception("The line above should have raised an exception") except exception: pass finally: super(LiveServerAddress, cls).tearDownClass() def test_test_test(self): # Intentionally empty method so that the test is picked up by the # test runner and the overridden setUpClass() method is executed. pass class LiveServerViews(LiveServerBase): def test_404(self): """ Ensure that the LiveServerTestCase serves 404s. Refs #2879. """ try: self.urlopen('/') except HTTPError as err: self.assertEqual(err.code, 404, 'Expected 404 response') else: self.fail('Expected 404 response') def test_view(self): """ Ensure that the LiveServerTestCase serves views. Refs #2879. """ f = self.urlopen('/example_view/') self.assertEqual(f.read(), b'example view') def test_static_files(self): """ Ensure that the LiveServerTestCase serves static files. Refs #2879. """ f = self.urlopen('/static/example_static_file.txt') self.assertEqual(f.read().rstrip(b'\r\n'), b'example static file') def test_no_collectstatic_emulation(self): """ Test that LiveServerTestCase reports a 404 status code when HTTP client tries to access a static file that isn't explicitly put under STATIC_ROOT. """ try: self.urlopen('/static/another_app/another_app_static_file.txt') except HTTPError as err: self.assertEqual(err.code, 404, 'Expected 404 response') else: self.fail('Expected 404 response (got %d)' % err.code) def test_media_files(self): """ Ensure that the LiveServerTestCase serves media files. Refs #2879. """ f = self.urlopen('/media/example_media_file.txt') self.assertEqual(f.read().rstrip(b'\r\n'), b'example media file') def test_environ(self): f = self.urlopen('/environ_view/?%s' % urlencode({'q': ''})) self.assertIn(b"QUERY_STRING: 'q=%D1%82%D0%B5%D1%81%D1%82'", f.read()) class LiveServerDatabase(LiveServerBase): def test_fixtures_loaded(self): """ Ensure that fixtures are properly loaded and visible to the live server thread. Refs #2879. """ f = self.urlopen('/model_view/') self.assertEqual(f.read().splitlines(), [b'jane', b'robert']) def test_database_writes(self): """ Ensure that data written to the database by a view can be read. Refs #2879. """ self.urlopen('/create_model_instance/') self.assertQuerysetEqual( Person.objects.all().order_by('pk'), ['jane', 'robert', 'emily'], lambda b: b.name )
from benchmark.test_types.framework_test_type import FrameworkTestType from benchmark.test_types.verifications import ( basic_body_verification, verify_headers, verify_helloworld_object ) import json class JsonTestType(FrameworkTestType): def __init__(self): kwargs = { 'name': 'json', 'accept_header': self.accept('json'), 'requires_db': False, 'args': ['json_url'] } FrameworkTestType.__init__(self, **kwargs) def get_url(self): return self.json_url def verify(self, base_url): '''Validates the response is a JSON object of { 'message' : 'hello, world!' }. Case insensitive and quoting style is ignored ''' url = base_url + self.json_url headers, body = self.request_headers_and_body(url) response, problems = basic_body_verification(body, url) if len(problems) > 0: return problems problems += verify_helloworld_object(response, url) problems += verify_headers(headers, url, should_be='json') if len(problems) > 0: return problems else: return [('pass', '', url)]
# -*- coding: utf-8 -*- # # This file is part of Linux Show Player # # Copyright 2012-2016 Francesco Ceruti <ceppofrancy@gmail.com> # # Linux Show Player is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Linux Show Player is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Linux Show Player. If not, see <http://www.gnu.org/licenses/>. from PyQt5.QtCore import QT_TRANSLATE_NOOP from lisp.backend.media_element import ElementType, MediaType from lisp.core.has_properties import Property from lisp.modules.gst_backend.gi_repository import Gst from lisp.modules.gst_backend.gst_element import GstMediaElement class UserElement(GstMediaElement): ElementType = ElementType.Plugin MediaType = MediaType.Audio Name = QT_TRANSLATE_NOOP('MediaElementName', 'Custom Element') bin = Property(default='') def __init__(self, pipeline): super().__init__() self.pipeline = pipeline self.audio_convert_sink = Gst.ElementFactory.make("audioconvert", None) # A default assignment for the bin self.gst_bin = Gst.ElementFactory.make("identity", None) self.gst_bin.set_property("signal-handoffs", False) self.audio_convert_src = Gst.ElementFactory.make("audioconvert", None) pipeline.add(self.audio_convert_sink) pipeline.add(self.gst_bin) pipeline.add(self.audio_convert_src) self.audio_convert_sink.link(self.gst_bin) self.gst_bin.link(self.audio_convert_src) self._old_bin = self.gst_bin self.changed('bin').connect(self.__prepare_bin) def sink(self): return self.audio_convert_sink def src(self): return self.audio_convert_src def __prepare_bin(self, value): if value != '' and value != self._old_bin: self._old_bin = value # If in playing we need to restart the pipeline after unblocking playing = self.gst_bin.current_state == Gst.State.PLAYING # Block the stream pad = self.audio_convert_sink.sinkpads[0] probe = pad.add_probe(Gst.PadProbeType.BLOCK, lambda *a: 0, "") # Unlink the components self.audio_convert_sink.unlink(self.gst_bin) self.gst_bin.unlink(self.audio_convert_src) self.pipeline.remove(self.gst_bin) # Create the bin, when fail use a do-nothing element try: self.gst_bin = Gst.parse_bin_from_description(value, True) except Exception: self.gst_bin = Gst.ElementFactory.make("identity", None) self.gst_bin.set_property("signal-handoffs", False) # Link the components self.pipeline.add(self.gst_bin) self.audio_convert_sink.link(self.gst_bin) self.gst_bin.link(self.audio_convert_src) # Unblock the stream pad.remove_probe(probe) if playing: self.pipeline.set_state(Gst.State.PLAYING)
import numpy as np import scipy from matplotlib import pyplot as plt from numpy import pi as pi # Plotting logic switches time_plot = True freq_plot = True # Oversample to make things look purty oversample = 100 # Frequencies to simulate f_min = 5 #[Hz] f_max = 10 #[Hz] f_list = np.arange(f_min,f_max) # Note: arange does not include the stop pt # Time array t_start = 0 #[s] t_stop = oversample/f_min #[s] f_samp = oversample*f_max #[Hz] t_step = 1/f_samp #[s] # Create a time span, but do not care about the number of points. # This will likely create sinc functions in the FFT. #t = np.arange(t_start,t_stop,t_step) # Use N points to make a faster FFT and to avoid # the addition of zeros at the end of the FFT array. # The addition of zeros will result in the mulitplication # of a box filter in the time domain, which results in # a sinc function in the frequency domain N = int(np.power(2,np.ceil(np.log2(t_stop/t_step)))) # Create a time span, but care about the number of points such that # the signal does not look like a sinc function in the freq. domain. # Source: U of RI ECE, ELE 436: Comm. Sys., FFT Tutorial t = np.linspace(t_start,t_stop,num=N,endpoint=True) # Create random amplitudes a_list = [np.random.randint(1,10) for i in f_list] # Create a time signal with random amplitudes for each frequency x = 0 for a,f in zip(a_list,f_list): x += a*np.sin(2*pi*f*t) # Take the FFT of the signal # Normalize by the size of x due to how a DTFT is taken # Take absoulte value because we only care about the real part # of the signal. X = np.abs(np.fft.fft(x)/x.size) # Get the labels for the frequencies, num pts and delta between them freq_labels = np.fft.fftfreq(N,t[1]-t[0]) # Plot the time signal if time_plot and not freq_plot: plt.figure('Time Domain View') plt.title("Time domain view of signal x") plt.plot(t,x) plt.xlim([0,5/f_min]) plt.xlabel("Time [s]") plt.ylabel("Amplitude") plt.show() # Or plot the frequecy if freq_plot and not time_plot: plt.figure('Frequency Domain View') plt.title("Frequency domain view of signal x") plt.plot(freq_labels,X) plt.xlim([-f_max,f_max]) plt.show() # Or plot both if freq_plot and time_plot: plt.subplot(211) plt.title("Time and frequency domain view of real signal x") plt.plot(t,x) plt.xlim([0,5/f_min]) # Limit the time shown to a small amount plt.xlabel("Time [s]") plt.ylabel("Amplitude") plt.subplot(212) plt.plot(freq_labels,X) plt.xlim([-f_max,f_max]) # Limit the freq shown to a small amount plt.xlabel("Frequency [Hz]") plt.ylabel("Magnitude (linear)") plt.show()
from PyQt4.QtGui import QMainWindow, QApplication, QDockWidget from PyQt4 import QtGui, QtCore from PyQt4.QtCore import Qt import os from .qtviewer import app from .qchemlabwidget import QChemlabWidget from .. import resources import numpy as np resources_dir = os.path.dirname(resources.__file__) class PlayStopButton(QtGui.QPushButton): play = QtCore.pyqtSignal() pause = QtCore.pyqtSignal() def __init__(self): css = ''' PlayStopButton { width: 30px; height: 30px; } ''' super(PlayStopButton, self).__init__() self.setStyleSheet(css) icon = QtGui.QIcon(os.path.join(resources_dir, 'play_icon.svg')) self.setIcon(icon) self.status = 'paused' self.clicked.connect(self.on_click) def on_click(self): if self.status == 'paused': self.status = 'playing' icon = QtGui.QIcon(os.path.join(resources_dir, 'pause_icon.svg')) self.setIcon(icon) self.play.emit() else: self.status = 'paused' icon = QtGui.QIcon(os.path.join(resources_dir, 'play_icon.svg')) self.setIcon(icon) self.pause.emit() def set_pause(self): self.status = 'paused' icon = QtGui.QIcon(os.path.join(resources_dir, 'play_icon.svg')) self.setIcon(icon) def set_play(self): self.status = 'playing' icon = QtGui.QIcon(os.path.join(resources_dir, 'pause_icon.svg')) self.setIcon(icon) class AnimationSlider(QtGui.QSlider): def __init__(self): super(AnimationSlider, self).__init__(Qt.Horizontal) self._cursor_adjustment = 7 #px def mousePressEvent(self, event): if event.button() == Qt.LeftButton: value = self.__pixelPosToRangeValue(event.x()-self._cursor_adjustment) self.setValue(value) event.accept() super(AnimationSlider, self).mousePressEvent(event) def __pixelPosToRangeValue(self, pos): opt = QtGui.QStyleOptionSlider() self.initStyleOption(opt) style = QtGui.QApplication.style() gr = style.subControlRect(style.CC_Slider, opt, style.SC_SliderGroove, self) sr = style.subControlRect(style.CC_Slider, opt, style.SC_SliderHandle, self) if self.orientation() == QtCore.Qt.Horizontal: slider_length = sr.width() slider_min = gr.x() slider_max = gr.right() - slider_length + 1 else: slider_length = sr.height() slider_min = gr.y() slider_max = gr.bottom() - slider_length + 1 return style.sliderValueFromPosition(self.minimum(), self.maximum(), pos-slider_min, slider_max-slider_min, opt.upsideDown) class TrajectoryControls(QtGui.QWidget): play = QtCore.pyqtSignal() pause = QtCore.pyqtSignal() frame_changed = QtCore.pyqtSignal(int) speed_changed = QtCore.pyqtSignal() def __init__(self, parent=None): super(TrajectoryControls, self).__init__(parent) self.current_index = 0 self.max_index = 0 self._timer = QtCore.QTimer(self) self._timer.timeout.connect(self.do_update) containerhb2 = QtGui.QWidget(parent) hb = QtGui.QHBoxLayout() # For controls vb = QtGui.QVBoxLayout() hb2 = QtGui.QHBoxLayout() # For settings vb.addWidget(containerhb2) vb.addLayout(hb) containerhb2.setLayout(hb2) containerhb2.setSizePolicy(QtGui.QSizePolicy.Minimum, QtGui.QSizePolicy.Minimum) hb2.addWidget(QtGui.QLabel('Speed')) self._speed_slider = QtGui.QSlider(Qt.Horizontal) self._speed_slider.resize(100, self._speed_slider.height()) self._speed_slider.setSizePolicy(QtGui.QSizePolicy.Fixed, QtGui.QSizePolicy.Fixed) self.speeds = np.linspace(15, 250, 11).astype(int) self.speeds = self.speeds.tolist() self.speeds.reverse() self._speed_slider.setMaximum(10) self._speed_slider.setValue(7) #self._speed_slider.valueChanged.connect(self.on_speed_changed) hb2.addWidget(self._speed_slider) hb2.addStretch(1) # Control buttons self.play_stop = PlayStopButton() hb.addWidget(self.play_stop) self.slider = AnimationSlider() hb.addWidget(self.slider, 2) self._label_tmp = '<b><FONT SIZE=30>{}</b>' self.timelabel = QtGui.QLabel(self._label_tmp.format('0.0')) hb.addWidget(self.timelabel) self._settings_button = QtGui.QPushButton() self._settings_button.setStyleSheet(''' QPushButton { width: 30px; height: 30px; }''') icon = QtGui.QIcon(os.path.join(resources_dir, 'settings_icon.svg')) self._settings_button.setIcon(icon) self._settings_button.clicked.connect(self._toggle_settings) hb.addWidget(self._settings_button) self.play_stop.setFocus() vb.setSizeConstraint(QtGui.QLayout.SetMaximumSize) containerhb2.setVisible(False) self._settings_pan = containerhb2 self.setLayout(vb) self.speed = self.speeds[self._speed_slider.value()] # Connecting all the signals self.play_stop.play.connect(self.on_play) self.play_stop.pause.connect(self.on_pause) self.slider.valueChanged.connect(self.on_slider_change) self.slider.sliderPressed.connect(self.on_slider_down) self.play_stop.setFocus() def _toggle_settings(self): self._settings_pan.setVisible(not self._settings_pan.isVisible()) def on_play(self): if self.current_index == self.max_index - 1: # Restart self.current_index = 0 self._timer.start(self.speed) def do_update(self): if self.current_index >= self.max_index: self.current_index = self.max_index - 1 self._timer.stop() self.play_stop.set_pause() else: self.current_index += 1 # This triggers on_slider_change self.slider.setSliderPosition(self.current_index) def next(self, skip=1): if self.current_index + skip >= self.max_index - 1: raise StopIteration else: self.slider.setValue(self.current_index + skip) # The current_index is changes def goto_frame(self, framenum): self.slider.setValue(framenum) def on_pause(self): self._timer.stop() def on_slider_change(self, value): #print 'Slider moved', value self.current_index = value self.frame_changed.emit(self.current_index) def on_slider_down(self): self._timer.stop() self.play_stop.set_pause() def on_speed_changed(self, index): self.speed = self.speeds[index] if self._timer.isActive(): self._timer.stop() self._timer.start(self.speed) def set_ticks(self, number): '''Set the number of frames to animate. ''' self.max_index = number self.current_index = 0 self.slider.setMaximum(self.max_index-1) self.slider.setMinimum(0) self.slider.setPageStep(1) def set_time(self, t): stime = format_time(t) label_tmp = '<b><FONT SIZE=30>{}</b>' self.timelabel.setText(label_tmp.format(stime)) class QtTrajectoryViewer(QMainWindow): """Bases: `PyQt4.QtGui.QMainWindow` Interface for viewing trajectory. It provides interface elements to play/pause and set the speed of the animation. **Example** To set up a QtTrajectoryViewer you have to add renderers to the scene, set the number of frames present in the animation by calling ;py:meth:`~chemlab.graphics.QtTrajectoryViewer.set_ticks` and define an update function. Below is an example taken from the function :py:func:`chemlab.graphics.display_trajectory`:: from chemlab.graphics import QtTrajectoryViewer # sys = some System # coords_list = some list of atomic coordinates v = QtTrajectoryViewer() sr = v.add_renderer(AtomRenderer, sys.r_array, sys.type_array, backend='impostors') br = v.add_renderer(BoxRenderer, sys.box_vectors) v.set_ticks(len(coords_list)) @v.update_function def on_update(index): sr.update_positions(coords_list[index]) br.update(sys.box_vectors) v.set_text(format_time(times[index])) v.widget.repaint() v.run() .. warning:: Use with caution, the API for this element is not fully stabilized and may be subject to change. """ def __init__(self): super(QtTrajectoryViewer, self).__init__() self.controls = QDockWidget() # Eliminate the dock titlebar title_widget = QtGui.QWidget(self) self.controls.setTitleBarWidget(title_widget) traj_controls = TrajectoryControls(self) self.controls.setWidget(traj_controls) # Molecular viewer self.widget = QChemlabWidget(self) self.setCentralWidget(self.widget) self.addDockWidget(Qt.DockWidgetArea(Qt.BottomDockWidgetArea), self.controls) self.show() # Replace in this way traj_controls.frame_changed.connect(self.on_frame_changed) self.traj_controls = traj_controls def set_ticks(self, number): self.traj_controls.set_ticks(number) def set_text(self, text): '''Update the time indicator in the interface. ''' self.traj_controls.timelabel.setText(self.traj_controls._label_tmp.format(text)) def on_frame_changed(self, index): self._update_function(index) def on_pause(self): self._timer.stop() def on_slider_change(self, value): self.current_index = value self._update_function(self.current_index) def on_slider_down(self): self._timer.stop() self.play_stop.set_pause() def on_speed_changed(self, index): self.speed = self.speeds[index] if self._timer.isActive(): self._timer.stop() self._timer.start(self.speed) def add_renderer(self, klass, *args, **kwargs): '''The behaviour of this function is the same as :py:meth:`chemlab.graphics.QtViewer.add_renderer`. ''' renderer = klass(self.widget, *args, **kwargs) self.widget.renderers.append(renderer) return renderer def add_ui(self, klass, *args, **kwargs): '''Add an UI element for the current scene. The approach is the same as renderers. .. warning:: The UI api is not yet finalized ''' ui = klass(self.widget, *args, **kwargs) self.widget.uis.append(ui) return ui def add_post_processing(self, klass, *args, **kwargs): pp = klass(self.widget, *args, **kwargs) self.widget.post_processing.append(pp) return pp def run(self): app.exec_() def update_function(self, func, frames=None): '''Set the function to be called when it's time to display a frame. *func* should be a function that takes one integer argument that represents the frame that has to be played:: def func(index): # Update the renderers to match the # current animation index ''' # Back-compatibility if frames is not None: self.traj_controls.set_ticks(frames) self._update_function = func def _toggle_settings(self): self._settings_pan.setVisible(not self._settings_pan.isVisible()) def format_time(t): if 0.0 <= t < 100.0: return '%.1f ps' % t elif 100.0 <= t < 1.0e5: return '%.1f ns' % (t/1e3) elif 1.0e5 <= t < 1.0e8: return '%.1f us' % (t/1e6) elif 1.0e8 <= t < 1.0e12: return '%.1f ms' % (t/1e9) elif 1.0e12 <= t < 1.0e15: return '%.1f s' % (t/1e12) if __name__ == '__main__': v = QtTrajectoryViewer() v.show() app.exec_()
# Copyright 2015 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== """Tests for tensorflow.ops.numerics.""" from __future__ import absolute_import from __future__ import division from __future__ import print_function import numpy as np from tensorflow.python.framework import constant_op from tensorflow.python.framework import dtypes from tensorflow.python.framework import ops from tensorflow.python.framework import test_util from tensorflow.python.ops import array_ops from tensorflow.python.ops import control_flow_ops from tensorflow.python.ops import math_ops from tensorflow.python.ops import numerics from tensorflow.python.platform import test class VerifyTensorAllFiniteTest(test.TestCase): def testVerifyTensorAllFiniteSucceeds(self): x_shape = [5, 4] x = np.random.random_sample(x_shape).astype(np.float32) with test_util.use_gpu(): t = constant_op.constant(x, shape=x_shape, dtype=dtypes.float32) t_verified = numerics.verify_tensor_all_finite(t, "Input is not a number.") self.assertAllClose(x, self.evaluate(t_verified)) def testVerifyTensorAllFiniteFails(self): x_shape = [5, 4] x = np.random.random_sample(x_shape).astype(np.float32) my_msg = "Input is not a number." # Test NaN. x[0] = np.nan with test_util.use_gpu(): with self.assertRaisesOpError(my_msg): t = constant_op.constant(x, shape=x_shape, dtype=dtypes.float32) t_verified = numerics.verify_tensor_all_finite(t, my_msg) self.evaluate(t_verified) # Test Inf. x[0] = np.inf with test_util.use_gpu(): with self.assertRaisesOpError(my_msg): t = constant_op.constant(x, shape=x_shape, dtype=dtypes.float32) t_verified = numerics.verify_tensor_all_finite(t, my_msg) self.evaluate(t_verified) @test_util.run_v1_only("b/120545219") class NumericsTest(test.TestCase): def testInf(self): with self.session(graph=ops.Graph()): t1 = constant_op.constant(1.0) t2 = constant_op.constant(0.0) a = math_ops.div(t1, t2) check = numerics.add_check_numerics_ops() a = control_flow_ops.with_dependencies([check], a) with self.assertRaisesOpError("Inf"): self.evaluate(a) def testNaN(self): with self.session(graph=ops.Graph()): t1 = constant_op.constant(0.0) t2 = constant_op.constant(0.0) a = math_ops.div(t1, t2) check = numerics.add_check_numerics_ops() a = control_flow_ops.with_dependencies([check], a) with self.assertRaisesOpError("NaN"): self.evaluate(a) def testBoth(self): with self.session(graph=ops.Graph()): t1 = constant_op.constant([1.0, 0.0]) t2 = constant_op.constant([0.0, 0.0]) a = math_ops.div(t1, t2) check = numerics.add_check_numerics_ops() a = control_flow_ops.with_dependencies([check], a) with self.assertRaisesOpError("Inf and NaN"): self.evaluate(a) def testPassThrough(self): with self.session(graph=ops.Graph()): t1 = constant_op.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3]) checked = array_ops.check_numerics(t1, message="pass through test") value = self.evaluate(checked) self.assertAllEqual(np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]), value) self.assertEqual([2, 3], checked.get_shape()) def testControlFlowCond(self): predicate = array_ops.placeholder(dtypes.bool, shape=[]) _ = control_flow_ops.cond(predicate, lambda: constant_op.constant([37.]), lambda: constant_op.constant([42.])) with self.assertRaisesRegexp( ValueError, r"`tf\.add_check_numerics_ops\(\) is not compatible with " r"TensorFlow control flow operations such as `tf\.cond\(\)` " r"or `tf.while_loop\(\)`\."): numerics.add_check_numerics_ops() def testControlFlowWhile(self): predicate = array_ops.placeholder(dtypes.bool, shape=[]) _ = control_flow_ops.while_loop(lambda _: predicate, lambda _: constant_op.constant([37.]), [constant_op.constant([42.])]) with self.assertRaisesRegexp( ValueError, r"`tf\.add_check_numerics_ops\(\) is not compatible with " r"TensorFlow control flow operations such as `tf\.cond\(\)` " r"or `tf.while_loop\(\)`\."): numerics.add_check_numerics_ops() if __name__ == "__main__": test.main()
from __future__ import unicode_literals import binascii import math import timeit import hashlib from django.utils import unittest from django.utils.crypto import constant_time_compare, pbkdf2 class TestUtilsCryptoMisc(unittest.TestCase): def test_constant_time_compare(self): # It's hard to test for constant time, just test the result. self.assertTrue(constant_time_compare(b'spam', b'spam')) self.assertFalse(constant_time_compare(b'spam', b'eggs')) self.assertTrue(constant_time_compare('spam', 'spam')) self.assertFalse(constant_time_compare('spam', 'eggs')) class TestUtilsCryptoPBKDF2(unittest.TestCase): # http://tools.ietf.org/html/draft-josefsson-pbkdf2-test-vectors-06 rfc_vectors = [ { "args": { "password": "password", "salt": "salt", "iterations": 1, "dklen": 20, "digest": hashlib.sha1, }, "result": "0c60c80f961f0e71f3a9b524af6012062fe037a6", }, { "args": { "password": "password", "salt": "salt", "iterations": 2, "dklen": 20, "digest": hashlib.sha1, }, "result": "ea6c014dc72d6f8ccd1ed92ace1d41f0d8de8957", }, { "args": { "password": "password", "salt": "salt", "iterations": 4096, "dklen": 20, "digest": hashlib.sha1, }, "result": "4b007901b765489abead49d926f721d065a429c1", }, # # this takes way too long :( # { # "args": { # "password": "password", # "salt": "salt", # "iterations": 16777216, # "dklen": 20, # "digest": hashlib.sha1, # }, # "result": "eefe3d61cd4da4e4e9945b3d6ba2158c2634e984", # }, { "args": { "password": "passwordPASSWORDpassword", "salt": "saltSALTsaltSALTsaltSALTsaltSALTsalt", "iterations": 4096, "dklen": 25, "digest": hashlib.sha1, }, "result": "3d2eec4fe41c849b80c8d83662c0e44a8b291a964cf2f07038", }, { "args": { "password": "pass\0word", "salt": "sa\0lt", "iterations": 4096, "dklen": 16, "digest": hashlib.sha1, }, "result": "56fa6aa75548099dcc37d7f03425e0c3", }, ] regression_vectors = [ { "args": { "password": "password", "salt": "salt", "iterations": 1, "dklen": 20, "digest": hashlib.sha256, }, "result": "120fb6cffcf8b32c43e7225256c4f837a86548c9", }, { "args": { "password": "password", "salt": "salt", "iterations": 1, "dklen": 20, "digest": hashlib.sha512, }, "result": "867f70cf1ade02cff3752599a3a53dc4af34c7a6", }, { "args": { "password": "password", "salt": "salt", "iterations": 1000, "dklen": 0, "digest": hashlib.sha512, }, "result": ("afe6c5530785b6cc6b1c6453384731bd5ee432ee" "549fd42fb6695779ad8a1c5bf59de69c48f774ef" "c4007d5298f9033c0241d5ab69305e7b64eceeb8d" "834cfec"), }, # Check leading zeros are not stripped (#17481) { "args": { "password": b'\xba', "salt": "salt", "iterations": 1, "dklen": 20, "digest": hashlib.sha1, }, "result": '0053d3b91a7f1e54effebd6d68771e8a6e0b2c5b', }, ] def test_public_vectors(self): for vector in self.rfc_vectors: result = pbkdf2(**vector['args']) self.assertEqual(binascii.hexlify(result).decode('ascii'), vector['result']) def test_regression_vectors(self): for vector in self.regression_vectors: result = pbkdf2(**vector['args']) self.assertEqual(binascii.hexlify(result).decode('ascii'), vector['result'])
# -*- coding: utf-8 -*- ############################################################################### # # GetProject # Retrieves an individual project using a project id that you specify. # # Python versions 2.6, 2.7, 3.x # # Copyright 2014, Temboo Inc. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, # software distributed under the License is distributed on an # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, # either express or implied. See the License for the specific # language governing permissions and limitations under the License. # # ############################################################################### from temboo.core.choreography import Choreography from temboo.core.choreography import InputSet from temboo.core.choreography import ResultSet from temboo.core.choreography import ChoreographyExecution import json class GetProject(Choreography): def __init__(self, temboo_session): """ Create a new instance of the GetProject Choreo. A TembooSession object, containing a valid set of Temboo credentials, must be supplied. """ super(GetProject, self).__init__(temboo_session, '/Library/Basecamp/GetProject') def new_input_set(self): return GetProjectInputSet() def _make_result_set(self, result, path): return GetProjectResultSet(result, path) def _make_execution(self, session, exec_id, path): return GetProjectChoreographyExecution(session, exec_id, path) class GetProjectInputSet(InputSet): """ An InputSet with methods appropriate for specifying the inputs to the GetProject Choreo. The InputSet object is used to specify input parameters when executing this Choreo. """ def set_AccountName(self, value): """ Set the value of the AccountName input for this Choreo. ((required, string) The Basecamp account name for you or your company. This is the first part of your account URL.) """ super(GetProjectInputSet, self)._set_input('AccountName', value) def set_Password(self, value): """ Set the value of the Password input for this Choreo. ((required, password) Your Basecamp password. You can use the value 'X' when specifying an API Key for the Username input.) """ super(GetProjectInputSet, self)._set_input('Password', value) def set_ProjectId(self, value): """ Set the value of the ProjectId input for this Choreo. ((required, integer) The ID for the project you want to retrieve.) """ super(GetProjectInputSet, self)._set_input('ProjectId', value) def set_Username(self, value): """ Set the value of the Username input for this Choreo. ((required, string) Your Basecamp username or API Key.) """ super(GetProjectInputSet, self)._set_input('Username', value) class GetProjectResultSet(ResultSet): """ A ResultSet with methods tailored to the values returned by the GetProject Choreo. The ResultSet object is used to retrieve the results of a Choreo execution. """ def getJSONFromString(self, str): return json.loads(str) def get_Response(self): """ Retrieve the value for the "Response" output from this Choreo execution. ((xml) The response from Basecamp.) """ return self._output.get('Response', None) class GetProjectChoreographyExecution(ChoreographyExecution): def _make_result_set(self, response, path): return GetProjectResultSet(response, path)
import re from urllib import robotparser from urllib.parse import urljoin from downloader import Downloader def get_robots_parser(robots_url): rp = robotparser.RobotFileParser() rp.set_url(robots_url) rp.read() return rp def get_links(html): webpage_regex = re.compile("""<a[^>]+href=["'](.*?)["']""", re.IGNORECASE) return webpage_regex.findall(html) def link_crawler(start_url, link_regex, robots_url=None, user_agent='wswp', proxies=None, delay=3, max_depth=4, num_retries=2, cache={}, scraper_callback=None): crawl_queue = [start_url] seen = {} if not robots_url: robots_url = '{}/robots.txt'.format(start_url) rp = get_robots_parser(robots_url) D = Downloader(delay=delay, user_agent=user_agent, proxies=proxies, cache=cache) while crawl_queue: url = crawl_queue.pop() if rp.can_fetch(user_agent, url): depth = seen.get(url, 0) if depth == max_depth: print('Skipping %s due to depth' % url) continue html = D(url, num_retries=num_retries) if not html: continue if scraper_callback: links = scraper_callback(url, html) or [] else: links = [] for link in get_links(html) + links: if re.match(link_regex, link): abs_link = urljoin(start_url, link) if abs_link not in seen: seen[abs_link] = depth + 1 crawl_queue.append(abs_link) else: print('Blocked by robots.txt:', url)
# Copyright (c) 2011 OpenStack, LLC. # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. """ SchedulerOptions monitors a local .json file for changes and loads it if needed. This file is converted to a data structure and passed into the filtering and weighing functions which can use it for dynamic configuration. """ import datetime import os from oslo_config import cfg from oslo_log import log from oslo_serialization import jsonutils from oslo_utils import timeutils scheduler_json_config_location_opt = cfg.StrOpt( 'scheduler_json_config_location', default='', help='Absolute path to scheduler configuration JSON file.') CONF = cfg.CONF CONF.register_opt(scheduler_json_config_location_opt) LOG = log.getLogger(__name__) class SchedulerOptions(object): """Monitor and load local .json file for filtering and weighing. SchedulerOptions monitors a local .json file for changes and loads it if needed. This file is converted to a data structure and passed into the filtering and weighing functions which can use it for dynamic configuration. """ def __init__(self): super(SchedulerOptions, self).__init__() self.data = {} self.last_modified = None self.last_checked = None def _get_file_handle(self, filename): """Get file handle. Broken out for testing.""" return open(filename) def _get_file_timestamp(self, filename): """Get the last modified datetime. Broken out for testing.""" try: return os.path.getmtime(filename) except os.error: LOG.exception("Could not stat scheduler options file " "%(filename)s.", {"filename": filename}) raise def _load_file(self, handle): """Decode the JSON file. Broken out for testing.""" try: return jsonutils.load(handle) except ValueError: LOG.exception("Could not decode scheduler options.") return {} def _get_time_now(self): """Get current UTC. Broken out for testing.""" return timeutils.utcnow() def get_configuration(self, filename=None): """Check the json file for changes and load it if needed.""" if not filename: filename = CONF.scheduler_json_config_location if not filename: return self.data if self.last_checked: now = self._get_time_now() if now - self.last_checked < datetime.timedelta(minutes=5): return self.data last_modified = self._get_file_timestamp(filename) if (not last_modified or not self.last_modified or last_modified > self.last_modified): self.data = self._load_file(self._get_file_handle(filename)) self.last_modified = last_modified if not self.data: self.data = {} return self.data
import os import re import warnings from ctypes import c_char_p from django.contrib.gis.geoip.libgeoip import GEOIP_SETTINGS from django.contrib.gis.geoip.prototypes import ( GeoIP_country_code_by_addr, GeoIP_country_code_by_name, GeoIP_country_name_by_addr, GeoIP_country_name_by_name, GeoIP_database_info, GeoIP_delete, GeoIP_lib_version, GeoIP_open, GeoIP_record_by_addr, GeoIP_record_by_name, ) from django.core.validators import ipv4_re from django.utils import six from django.utils.deprecation import RemovedInDjango20Warning from django.utils.encoding import force_bytes, force_text # Regular expressions for recognizing the GeoIP free database editions. free_regex = re.compile(r'^GEO-\d{3}FREE') lite_regex = re.compile(r'^GEO-\d{3}LITE') class GeoIPException(Exception): pass class GeoIP(object): # The flags for GeoIP memory caching. # GEOIP_STANDARD - read database from filesystem, uses least memory. # # GEOIP_MEMORY_CACHE - load database into memory, faster performance # but uses more memory # # GEOIP_CHECK_CACHE - check for updated database. If database has been # updated, reload filehandle and/or memory cache. This option # is not thread safe. # # GEOIP_INDEX_CACHE - just cache the most frequently accessed index # portion of the database, resulting in faster lookups than # GEOIP_STANDARD, but less memory usage than GEOIP_MEMORY_CACHE - # useful for larger databases such as GeoIP Organization and # GeoIP City. Note, for GeoIP Country, Region and Netspeed # databases, GEOIP_INDEX_CACHE is equivalent to GEOIP_MEMORY_CACHE # # GEOIP_MMAP_CACHE - load database into mmap shared memory ( not available # on Windows). GEOIP_STANDARD = 0 GEOIP_MEMORY_CACHE = 1 GEOIP_CHECK_CACHE = 2 GEOIP_INDEX_CACHE = 4 GEOIP_MMAP_CACHE = 8 cache_options = {opt: None for opt in (0, 1, 2, 4, 8)} # Paths to the city & country binary databases. _city_file = '' _country_file = '' # Initially, pointers to GeoIP file references are NULL. _city = None _country = None def __init__(self, path=None, cache=0, country=None, city=None): """ Initializes the GeoIP object, no parameters are required to use default settings. Keyword arguments may be passed in to customize the locations of the GeoIP data sets. * path: Base directory to where GeoIP data is located or the full path to where the city or country data files (*.dat) are located. Assumes that both the city and country data sets are located in this directory; overrides the GEOIP_PATH settings attribute. * cache: The cache settings when opening up the GeoIP datasets, and may be an integer in (0, 1, 2, 4, 8) corresponding to the GEOIP_STANDARD, GEOIP_MEMORY_CACHE, GEOIP_CHECK_CACHE, GEOIP_INDEX_CACHE, and GEOIP_MMAP_CACHE, `GeoIPOptions` C API settings, respectively. Defaults to 0, meaning that the data is read from the disk. * country: The name of the GeoIP country data file. Defaults to 'GeoIP.dat'; overrides the GEOIP_COUNTRY settings attribute. * city: The name of the GeoIP city data file. Defaults to 'GeoLiteCity.dat'; overrides the GEOIP_CITY settings attribute. """ warnings.warn( "django.contrib.gis.geoip is deprecated in favor of " "django.contrib.gis.geoip2 and the MaxMind GeoLite2 database " "format.", RemovedInDjango20Warning, 2 ) # Checking the given cache option. if cache in self.cache_options: self._cache = cache else: raise GeoIPException('Invalid GeoIP caching option: %s' % cache) # Getting the GeoIP data path. if not path: path = GEOIP_SETTINGS.get('GEOIP_PATH') if not path: raise GeoIPException('GeoIP path must be provided via parameter or the GEOIP_PATH setting.') if not isinstance(path, six.string_types): raise TypeError('Invalid path type: %s' % type(path).__name__) if os.path.isdir(path): # Constructing the GeoIP database filenames using the settings # dictionary. If the database files for the GeoLite country # and/or city datasets exist, then try and open them. country_db = os.path.join(path, country or GEOIP_SETTINGS.get('GEOIP_COUNTRY', 'GeoIP.dat')) if os.path.isfile(country_db): self._country = GeoIP_open(force_bytes(country_db), cache) self._country_file = country_db city_db = os.path.join(path, city or GEOIP_SETTINGS.get('GEOIP_CITY', 'GeoLiteCity.dat')) if os.path.isfile(city_db): self._city = GeoIP_open(force_bytes(city_db), cache) self._city_file = city_db elif os.path.isfile(path): # Otherwise, some detective work will be needed to figure # out whether the given database path is for the GeoIP country # or city databases. ptr = GeoIP_open(force_bytes(path), cache) info = GeoIP_database_info(ptr) if lite_regex.match(info): # GeoLite City database detected. self._city = ptr self._city_file = path elif free_regex.match(info): # GeoIP Country database detected. self._country = ptr self._country_file = path else: raise GeoIPException('Unable to recognize database edition: %s' % info) else: raise GeoIPException('GeoIP path must be a valid file or directory.') def __del__(self): # Cleaning any GeoIP file handles lying around. if GeoIP_delete is None: return if self._country: GeoIP_delete(self._country) if self._city: GeoIP_delete(self._city) def __repr__(self): version = '' if GeoIP_lib_version is not None: version += ' [v%s]' % force_text(GeoIP_lib_version()) return '<%(cls)s%(version)s _country_file="%(country)s", _city_file="%(city)s">' % { 'cls': self.__class__.__name__, 'version': version, 'country': self._country_file, 'city': self._city_file, } def _check_query(self, query, country=False, city=False, city_or_country=False): "Helper routine for checking the query and database availability." # Making sure a string was passed in for the query. if not isinstance(query, six.string_types): raise TypeError('GeoIP query must be a string, not type %s' % type(query).__name__) # Extra checks for the existence of country and city databases. if city_or_country and not (self._country or self._city): raise GeoIPException('Invalid GeoIP country and city data files.') elif country and not self._country: raise GeoIPException('Invalid GeoIP country data file: %s' % self._country_file) elif city and not self._city: raise GeoIPException('Invalid GeoIP city data file: %s' % self._city_file) # Return the query string back to the caller. GeoIP only takes bytestrings. return force_bytes(query) def city(self, query): """ Returns a dictionary of city information for the given IP address or Fully Qualified Domain Name (FQDN). Some information in the dictionary may be undefined (None). """ enc_query = self._check_query(query, city=True) if ipv4_re.match(query): # If an IP address was passed in return GeoIP_record_by_addr(self._city, c_char_p(enc_query)) else: # If a FQDN was passed in. return GeoIP_record_by_name(self._city, c_char_p(enc_query)) def country_code(self, query): "Returns the country code for the given IP Address or FQDN." enc_query = self._check_query(query, city_or_country=True) if self._country: if ipv4_re.match(query): return GeoIP_country_code_by_addr(self._country, enc_query) else: return GeoIP_country_code_by_name(self._country, enc_query) else: return self.city(query)['country_code'] def country_name(self, query): "Returns the country name for the given IP Address or FQDN." enc_query = self._check_query(query, city_or_country=True) if self._country: if ipv4_re.match(query): return GeoIP_country_name_by_addr(self._country, enc_query) else: return GeoIP_country_name_by_name(self._country, enc_query) else: return self.city(query)['country_name'] def country(self, query): """ Returns a dictionary with the country code and name when given an IP address or a Fully Qualified Domain Name (FQDN). For example, both '24.124.1.80' and 'djangoproject.com' are valid parameters. """ # Returning the country code and name return {'country_code': self.country_code(query), 'country_name': self.country_name(query), } # #### Coordinate retrieval routines #### def coords(self, query, ordering=('longitude', 'latitude')): cdict = self.city(query) if cdict is None: return None else: return tuple(cdict[o] for o in ordering) def lon_lat(self, query): "Returns a tuple of the (longitude, latitude) for the given query." return self.coords(query) def lat_lon(self, query): "Returns a tuple of the (latitude, longitude) for the given query." return self.coords(query, ('latitude', 'longitude')) def geos(self, query): "Returns a GEOS Point object for the given query." ll = self.lon_lat(query) if ll: from django.contrib.gis.geos import Point return Point(ll, srid=4326) else: return None # #### GeoIP Database Information Routines #### @property def country_info(self): "Returns information about the GeoIP country database." if self._country is None: ci = 'No GeoIP Country data in "%s"' % self._country_file else: ci = GeoIP_database_info(self._country) return ci @property def city_info(self): "Returns information about the GeoIP city database." if self._city is None: ci = 'No GeoIP City data in "%s"' % self._city_file else: ci = GeoIP_database_info(self._city) return ci @property def info(self): "Returns information about the GeoIP library and databases in use." info = '' if GeoIP_lib_version: info += 'GeoIP Library:\n\t%s\n' % GeoIP_lib_version() return info + 'Country:\n\t%s\nCity:\n\t%s' % (self.country_info, self.city_info) # #### Methods for compatibility w/the GeoIP-Python API. #### @classmethod def open(cls, full_path, cache): return GeoIP(full_path, cache) def _rec_by_arg(self, arg): if self._city: return self.city(arg) else: return self.country(arg) region_by_addr = city region_by_name = city record_by_addr = _rec_by_arg record_by_name = _rec_by_arg country_code_by_addr = country_code country_code_by_name = country_code country_name_by_addr = country_name country_name_by_name = country_name
from __future__ import unicode_literals import sys import unittest from datetime import datetime from django.utils import http, six from django.utils.datastructures import MultiValueDict class TestUtilsHttp(unittest.TestCase): def test_same_origin_true(self): # Identical self.assertTrue(http.same_origin('http://foo.com/', 'http://foo.com/')) # One with trailing slash - see #15617 self.assertTrue(http.same_origin('http://foo.com', 'http://foo.com/')) self.assertTrue(http.same_origin('http://foo.com/', 'http://foo.com')) # With port self.assertTrue(http.same_origin('https://foo.com:8000', 'https://foo.com:8000/')) # No port given but according to RFC6454 still the same origin self.assertTrue(http.same_origin('http://foo.com', 'http://foo.com:80/')) self.assertTrue(http.same_origin('https://foo.com', 'https://foo.com:443/')) def test_same_origin_false(self): # Different scheme self.assertFalse(http.same_origin('http://foo.com', 'https://foo.com')) # Different host self.assertFalse(http.same_origin('http://foo.com', 'http://goo.com')) # Different host again self.assertFalse(http.same_origin('http://foo.com', 'http://foo.com.evil.com')) # Different port self.assertFalse(http.same_origin('http://foo.com:8000', 'http://foo.com:8001')) # No port given self.assertFalse(http.same_origin('http://foo.com', 'http://foo.com:8000/')) self.assertFalse(http.same_origin('https://foo.com', 'https://foo.com:8000/')) def test_urlencode(self): # 2-tuples (the norm) result = http.urlencode((('a', 1), ('b', 2), ('c', 3))) self.assertEqual(result, 'a=1&b=2&c=3') # A dictionary result = http.urlencode({'a': 1, 'b': 2, 'c': 3}) acceptable_results = [ # Need to allow all of these as dictionaries have to be treated as # unordered 'a=1&b=2&c=3', 'a=1&c=3&b=2', 'b=2&a=1&c=3', 'b=2&c=3&a=1', 'c=3&a=1&b=2', 'c=3&b=2&a=1' ] self.assertIn(result, acceptable_results) result = http.urlencode({'a': [1, 2]}, doseq=False) self.assertEqual(result, 'a=%5B%271%27%2C+%272%27%5D') result = http.urlencode({'a': [1, 2]}, doseq=True) self.assertEqual(result, 'a=1&a=2') result = http.urlencode({'a': []}, doseq=True) self.assertEqual(result, '') # A MultiValueDict result = http.urlencode(MultiValueDict({ 'name': ['Adrian', 'Simon'], 'position': ['Developer'] }), doseq=True) acceptable_results = [ # MultiValueDicts are similarly unordered 'name=Adrian&name=Simon&position=Developer', 'position=Developer&name=Adrian&name=Simon' ] self.assertIn(result, acceptable_results) def test_base36(self): # reciprocity works for n in [0, 1, 1000, 1000000]: self.assertEqual(n, http.base36_to_int(http.int_to_base36(n))) if six.PY2: self.assertEqual(sys.maxint, http.base36_to_int(http.int_to_base36(sys.maxint))) # bad input self.assertRaises(ValueError, http.int_to_base36, -1) if six.PY2: self.assertRaises(ValueError, http.int_to_base36, sys.maxint + 1) for n in ['1', 'foo', {1: 2}, (1, 2, 3), 3.141]: self.assertRaises(TypeError, http.int_to_base36, n) for n in ['#', ' ']: self.assertRaises(ValueError, http.base36_to_int, n) for n in [123, {1: 2}, (1, 2, 3), 3.141]: self.assertRaises(TypeError, http.base36_to_int, n) # more explicit output testing for n, b36 in [(0, '0'), (1, '1'), (42, '16'), (818469960, 'django')]: self.assertEqual(http.int_to_base36(n), b36) self.assertEqual(http.base36_to_int(b36), n) def test_is_safe_url(self): for bad_url in ('http://example.com', 'http:///example.com', 'https://example.com', 'ftp://exampel.com', r'\\example.com', r'\\\example.com', r'/\\/example.com', r'\\\example.com', r'\\example.com', r'\\//example.com', r'/\/example.com', r'\/example.com', r'/\example.com', 'http:///example.com', 'http:/\//example.com', 'http:\/example.com', 'http:/\example.com', 'javascript:alert("XSS")', '\njavascript:alert(x)', '\x08//example.com', '\n'): self.assertFalse(http.is_safe_url(bad_url, host='testserver'), "%s should be blocked" % bad_url) for good_url in ('/view/?param=http://example.com', '/view/?param=https://example.com', '/view?param=ftp://exampel.com', 'view/?param=//example.com', 'https://testserver/', 'HTTPS://testserver/', '//testserver/', '/url%20with%20spaces/'): self.assertTrue(http.is_safe_url(good_url, host='testserver'), "%s should be allowed" % good_url) def test_urlsafe_base64_roundtrip(self): bytestring = b'foo' encoded = http.urlsafe_base64_encode(bytestring) decoded = http.urlsafe_base64_decode(encoded) self.assertEqual(bytestring, decoded) def test_urlquote(self): self.assertEqual(http.urlquote('Paris & Orl\xe9ans'), 'Paris%20%26%20Orl%C3%A9ans') self.assertEqual(http.urlquote('Paris & Orl\xe9ans', safe="&"), 'Paris%20&%20Orl%C3%A9ans') self.assertEqual( http.urlunquote('Paris%20%26%20Orl%C3%A9ans'), 'Paris & Orl\xe9ans') self.assertEqual( http.urlunquote('Paris%20&%20Orl%C3%A9ans'), 'Paris & Orl\xe9ans') self.assertEqual(http.urlquote_plus('Paris & Orl\xe9ans'), 'Paris+%26+Orl%C3%A9ans') self.assertEqual(http.urlquote_plus('Paris & Orl\xe9ans', safe="&"), 'Paris+&+Orl%C3%A9ans') self.assertEqual( http.urlunquote_plus('Paris+%26+Orl%C3%A9ans'), 'Paris & Orl\xe9ans') self.assertEqual( http.urlunquote_plus('Paris+&+Orl%C3%A9ans'), 'Paris & Orl\xe9ans') class ETagProcessingTests(unittest.TestCase): def test_parsing(self): etags = http.parse_etags(r'"", "etag", "e\"t\"ag", "e\\tag", W/"weak"') self.assertEqual(etags, ['', 'etag', 'e"t"ag', r'e\tag', 'weak']) def test_quoting(self): quoted_etag = http.quote_etag(r'e\t"ag') self.assertEqual(quoted_etag, r'"e\\t\"ag"') class HttpDateProcessingTests(unittest.TestCase): def test_http_date(self): t = 1167616461.0 self.assertEqual(http.http_date(t), 'Mon, 01 Jan 2007 01:54:21 GMT') def test_cookie_date(self): t = 1167616461.0 self.assertEqual(http.cookie_date(t), 'Mon, 01-Jan-2007 01:54:21 GMT') def test_parsing_rfc1123(self): parsed = http.parse_http_date('Sun, 06 Nov 1994 08:49:37 GMT') self.assertEqual(datetime.utcfromtimestamp(parsed), datetime(1994, 11, 6, 8, 49, 37)) def test_parsing_rfc850(self): parsed = http.parse_http_date('Sunday, 06-Nov-94 08:49:37 GMT') self.assertEqual(datetime.utcfromtimestamp(parsed), datetime(1994, 11, 6, 8, 49, 37)) def test_parsing_asctime(self): parsed = http.parse_http_date('Sun Nov 6 08:49:37 1994') self.assertEqual(datetime.utcfromtimestamp(parsed), datetime(1994, 11, 6, 8, 49, 37))
from django.conf import settings from django.test import TestCase from django.test.utils import override_settings import track.tracker as tracker from track.backends import BaseBackend SIMPLE_SETTINGS = { 'default': { 'ENGINE': 'track.tests.test_tracker.DummyBackend', 'OPTIONS': { 'flag': True } } } MULTI_SETTINGS = { 'first': { 'ENGINE': 'track.tests.test_tracker.DummyBackend', }, 'second': { 'ENGINE': 'track.tests.test_tracker.DummyBackend', } } class TestTrackerInstantiation(TestCase): """Test that a helper function can instantiate backends from their name.""" def setUp(self): # pylint: disable=protected-access super(TestTrackerInstantiation, self).setUp() self.get_backend = tracker._instantiate_backend_from_name def test_instatiate_backend(self): name = 'track.tests.test_tracker.DummyBackend' options = {'flag': True} backend = self.get_backend(name, options) self.assertIsInstance(backend, DummyBackend) self.assertTrue(backend.flag) def test_instatiate_backends_with_invalid_values(self): def get_invalid_backend(name, parameters): return self.get_backend(name, parameters) options = {} name = 'track.backends.logger' self.assertRaises(ValueError, get_invalid_backend, name, options) name = 'track.backends.logger.Foo' self.assertRaises(ValueError, get_invalid_backend, name, options) name = 'this.package.does.not.exists' self.assertRaises(ValueError, get_invalid_backend, name, options) name = 'unittest.TestCase' self.assertRaises(ValueError, get_invalid_backend, name, options) class TestTrackerDjangoInstantiation(TestCase): """Test if backends are initialized properly from Django settings.""" @override_settings(TRACKING_BACKENDS=SIMPLE_SETTINGS) def test_django_simple_settings(self): """Test configuration of a simple backend""" backends = self._reload_backends() self.assertEqual(len(backends), 1) tracker.send({}) self.assertEqual(backends.values()[0].count, 1) @override_settings(TRACKING_BACKENDS=MULTI_SETTINGS) def test_django_multi_settings(self): """Test if multiple backends can be configured properly.""" backends = self._reload_backends().values() self.assertEqual(len(backends), 2) event_count = 10 for _ in xrange(event_count): tracker.send({}) self.assertEqual(backends[0].count, event_count) self.assertEqual(backends[1].count, event_count) @override_settings(TRACKING_BACKENDS=MULTI_SETTINGS) def test_django_remove_settings(self): """Test if a backend can be remove by setting it to None.""" settings.TRACKING_BACKENDS.update({'second': None}) backends = self._reload_backends() self.assertEqual(len(backends), 1) def _reload_backends(self): # pylint: disable=protected-access # Reset backends tracker._initialize_backends_from_django_settings() return tracker.backends class DummyBackend(BaseBackend): def __init__(self, **options): super(DummyBackend, self).__init__(**options) self.flag = options.get('flag', False) self.count = 0 # pylint: disable=unused-argument def send(self, event): self.count += 1
# -*- encoding: utf-8 -*- ############################################################################## # # Copyright (C) 2009 Gbor Dukai # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## { 'name': ' - Accounting', 'version': '1.0', 'category': 'Localization/Account Charts', 'author': 'openerp-china.org', 'maintainer':'openerp-china.org', 'website':'http://openerp-china.org', 'url': 'http://code.google.com/p/openerp-china/source/browse/#svn/trunk/l10n_cn', 'description': """  \\\\\ ============================================================ """, 'depends': ['base','account'], 'demo': [], 'data': [ 'account_chart.xml', 'l10n_chart_cn_wizard.xml', 'base_data.xml', ], 'license': 'GPL-3', 'auto_install': False, 'installable': True, 'images': ['images/config_chart_l10n_cn.jpeg','images/l10n_cn_chart.jpeg'], } # vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
#! /usr/bin/env python import yaml import random import roslib class Fluent(yaml.YAMLObject): yaml_tag='!Fluent' def __setstate__(self, state): """ PyYaml does not call __init__. this is an init replacement. """ self.properties = [] self.Class_Instance = state['Class_Instance'] # fix for bug: if type(self.Class_Instance[0]) == type(False): self.Class_Instance[0] = '"On"' if self.Class_Instance[0] == 'On': self.Class_Instance[0] = '"On"' self.StartTime = state['StartTime'] self.FinishTime = state['FinishTime'] for preProp in state['Properties']: self.properties.append(Property(preProp[0], preProp[1], preProp[2])) def toYamlString(self, line_break='\n'): yString = '' yString += '!Fluent' + line_break yString += '{:<16} {:<20}'.format('Class_Instance: ', '[' + self.Class_Instance[0] + ', ' + self.Class_Instance[1] + ']') + line_break yString += '{:<16} {:<20}'.format('StartTime: ', str(self.StartTime)) + line_break yString += '{:<16} {:<20}'.format('FinishTime: ', str(self.FinishTime)) + line_break if len(self.properties) > 0: yString += 'Properties:' + line_break for prop in self.properties: yString += prop.toYamlString() else: yString += 'Properties: []' + line_break return yString def __str__(self): return self.toYamlString() class Property: def __init__(self, role_type, filler_type, role_filler): self.role_type = role_type self.filler_type = filler_type self.role_filler = role_filler for key, value in self.__dict__.items(): if type(value) == type(False): setattr(self, key, 'On') #if value == 'On': # setattr(self, key, '"On"') def toYamlString(self, line_break='\n'): return ' - [{}, {}, {}]'.format(self.role_type, '"' + self.filler_type + '"', self.role_filler) + line_break def __str__(self): return self.toYamlString() class FluentPoseModification(Fluent): yaml_tag='!FluentPoseModification' GROUP_CHOICE_MAP = {} def __setstate__(self,state): """ PyYaml does not call __init__. this is an init replacement. """ # apply group state... if not FluentPoseModification.GROUP_CHOICE_MAP.has_key(state['Group']): choice = random.randint(0, state['Choices']-1) FluentPoseModification.GROUP_CHOICE_MAP[state['Group']] = choice self.choice = FluentPoseModification.GROUP_CHOICE_MAP[state['Group']] self.Instance = state['Instance'] self.Modifications = state['Modifications'] self.Attachments = state['Attachments'] # fix for bug: #if type(self.Class_Instance[0]) == type(False): # self.Class_Instance[0] = '"On"' #if self.Class_Instance[0] == 'On': # self.Class_Instance[0] = '"On"' #self.StartTime = state['StartTime'] #self.FinishTime = state['FinishTime'] def getMod(self, propertyString): for mod in self.Modifications: if mod[0] == propertyString: return mod[1][self.choice] return 0 if __name__ == '__main__': # filepath = roslib.packages.get_pkg_dir('race_simulation_run') + '/data/test.yaml' # changedFilepath = roslib.packages.get_pkg_dir('race_simulation_run') + '/data/output.yaml' # replacementsPath = roslib.packages.get_pkg_dir('race_simulation_run') + '/data/replace.yaml' # # load fluents: # fluents = [] # with open(filepath) as f: # for fluent in yaml.load_all(f): # fluents.append(fluent) # # load modifications: # fluentPoseModifications = [] # with open(replacementsPath) as f: # for poseMod in yaml.load_all(f): # fluentPoseModifications.append(poseMod) # # modify fluents poses: # for fluent in fluents[:]: # for poseMod in fluentPoseModifications: # if (poseMod.Instance == fluent.Class_Instance[1]) \ # or (fluent.Class_Instance[1] in poseMod.Attachments): # for prop in fluent.properties: # if poseMod.getMod(prop.role_type) != 0: # prop.role_filler += poseMod.getMod(prop.role_type) # # generate new file: # with open(changedFilepath, 'w') as cf: # string = ('---\n').join(str(fluent) for fluent in fluents) # cf.write(string) # load initial knowledge and spawn objects fluents initialPath = roslib.packages.get_pkg_dir('race_static_knowledge') + '/data/race_initial_knowledge.yaml' spawnPath = roslib.packages.get_pkg_dir('race_simulation_run') + '/data/spawn_objects.yaml' replacementsPath = roslib.packages.get_pkg_dir('race_simulation_run') + '/data/replace.yaml' #replacementsPath = roslib.packages.get_pkg_dir('race_simulation_run') + '/data/replace_simple.yaml' tempFilePath = roslib.packages.get_pkg_dir('race_simulation_run') + '/data/output.yaml' # load fluents: fluents = [] with open(initialPath) as f: for fluent in yaml.load_all(f): fluents.append(fluent) with open(spawnPath) as f: for fluent in yaml.load_all(f): fluents.append(fluent) # load modifications: fluentPoseModifications = [] with open(replacementsPath) as f: for poseMod in yaml.load_all(f): fluentPoseModifications.append(poseMod) # modify fluents poses: for fluent in fluents[:]: for poseMod in fluentPoseModifications: if (poseMod.Instance == fluent.Class_Instance[1]) \ or (fluent.Class_Instance[1] in poseMod.Attachments): for prop in fluent.properties: if poseMod.getMod(prop.role_type) != 0: prop.role_filler += poseMod.getMod(prop.role_type) # generate new file: with open(tempFilePath, 'w') as cf: string = ('---\n').join(str(fluent) for fluent in fluents) cf.write(string)
#! /usr/bin/env python # -*- coding: utf-8 -*- # Copyright (c) 2011-2012, The Linux Foundation. All rights reserved. # # Redistribution and use in source and binary forms, with or without # modification, are permitted provided that the following conditions are met: # * Redistributions of source code must retain the above copyright # notice, this list of conditions and the following disclaimer. # * Redistributions in binary form must reproduce the above copyright # notice, this list of conditions and the following disclaimer in the # documentation and/or other materials provided with the distribution. # * Neither the name of The Linux Foundation nor # the names of its contributors may be used to endorse or promote # products derived from this software without specific prior written # permission. # # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE # IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND # NON-INFRINGEMENT ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR # CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, # EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, # PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; # OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, # WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR # OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF # ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. # Invoke gcc, looking for warnings, and causing a failure if there are # non-whitelisted warnings. import errno import re import os import sys import subprocess # Note that gcc uses unicode, which may depend on the locale. TODO: # force LANG to be set to en_US.UTF-8 to get consistent warnings. allowed_warnings = set([ "alignment.c:327", "mmu.c:602", "return_address.c:62", "swab.h:49", "SemaLambda.cpp:946", "CGObjCGNU.cpp:1414", "BugReporter.h:146", "RegionStore.cpp:1904", "SymbolManager.cpp:484", "RewriteObjCFoundationAPI.cpp:737", "RewriteObjCFoundationAPI.cpp:696", "CommentParser.cpp:394", "CommentParser.cpp:391", "CommentParser.cpp:356", "LegalizeDAG.cpp:3646", "IRBuilder.h:844", "DataLayout.cpp:193", "transport.c:653", "xt_socket.c:307", "xt_socket.c:161", "inet_hashtables.h:356", "xc4000.c:1049", "xc4000.c:1063", ]) # Capture the name of the object file, can find it. ofile = None warning_re = re.compile(r'''(.*/|)([^/]+\.[a-z]+:\d+):(\d+:)? warning:''') def interpret_warning(line): """Decode the message from gcc. The messages we care about have a filename, and a warning""" line = line.rstrip('\n') m = warning_re.match(line) if m and m.group(2) not in allowed_warnings: print "error, forbidden warning:", m.group(2) # If there is a warning, remove any object if it exists. if ofile: try: os.remove(ofile) except OSError: pass sys.exit(1) def run_gcc(): args = sys.argv[1:] # Look for -o try: i = args.index('-o') global ofile ofile = args[i+1] except (ValueError, IndexError): pass compiler = sys.argv[0] try: proc = subprocess.Popen(args, stderr=subprocess.PIPE) for line in proc.stderr: print line, interpret_warning(line) result = proc.wait() except OSError as e: result = e.errno if result == errno.ENOENT: print args[0] + ':',e.strerror print 'Is your PATH set correctly?' else: print ' '.join(args), str(e) return result if __name__ == '__main__': status = run_gcc() sys.exit(status)
# -*- coding: utf-8 -*- """ *************************************************************************** __init__.py --------------------- Date : July 2013 Copyright : (C) 2013 by Victor Olaya Email : volayaf at gmail dot com *************************************************************************** * * * This program is free software; you can redistribute it and/or modify * * it under the terms of the GNU General Public License as published by * * the Free Software Foundation; either version 2 of the License, or * * (at your option) any later version. * * * *************************************************************************** """ __author__ = 'Victor Olaya' __date__ = 'July 2013' __copyright__ = '(C) 2013, Victor Olaya' # This will get replaced with a git SHA1 when you do a git archive __revision__ = '$Format:%H$' import os import sys import inspect from processing.core.Processing import Processing from exampleprovider.ExampleAlgorithmProvider import ExampleAlgorithmProvider cmd_folder = os.path.split(inspect.getfile(inspect.currentframe()))[0] if cmd_folder not in sys.path: sys.path.insert(0, cmd_folder) class ProcessingExampleProviderPlugin: def __init__(self): self.provider = ExampleAlgorithmProvider() def initGui(self): Processing.addProvider(self.provider) def unload(self): Processing.removeProvider(self.provider)
# # Emulation of has_key() function for platforms that don't use ncurses # import _curses # Table mapping curses keys to the terminfo capability name _capability_names = { _curses.KEY_A1: 'ka1', _curses.KEY_A3: 'ka3', _curses.KEY_B2: 'kb2', _curses.KEY_BACKSPACE: 'kbs', _curses.KEY_BEG: 'kbeg', _curses.KEY_BTAB: 'kcbt', _curses.KEY_C1: 'kc1', _curses.KEY_C3: 'kc3', _curses.KEY_CANCEL: 'kcan', _curses.KEY_CATAB: 'ktbc', _curses.KEY_CLEAR: 'kclr', _curses.KEY_CLOSE: 'kclo', _curses.KEY_COMMAND: 'kcmd', _curses.KEY_COPY: 'kcpy', _curses.KEY_CREATE: 'kcrt', _curses.KEY_CTAB: 'kctab', _curses.KEY_DC: 'kdch1', _curses.KEY_DL: 'kdl1', _curses.KEY_DOWN: 'kcud1', _curses.KEY_EIC: 'krmir', _curses.KEY_END: 'kend', _curses.KEY_ENTER: 'kent', _curses.KEY_EOL: 'kel', _curses.KEY_EOS: 'ked', _curses.KEY_EXIT: 'kext', _curses.KEY_F0: 'kf0', _curses.KEY_F1: 'kf1', _curses.KEY_F10: 'kf10', _curses.KEY_F11: 'kf11', _curses.KEY_F12: 'kf12', _curses.KEY_F13: 'kf13', _curses.KEY_F14: 'kf14', _curses.KEY_F15: 'kf15', _curses.KEY_F16: 'kf16', _curses.KEY_F17: 'kf17', _curses.KEY_F18: 'kf18', _curses.KEY_F19: 'kf19', _curses.KEY_F2: 'kf2', _curses.KEY_F20: 'kf20', _curses.KEY_F21: 'kf21', _curses.KEY_F22: 'kf22', _curses.KEY_F23: 'kf23', _curses.KEY_F24: 'kf24', _curses.KEY_F25: 'kf25', _curses.KEY_F26: 'kf26', _curses.KEY_F27: 'kf27', _curses.KEY_F28: 'kf28', _curses.KEY_F29: 'kf29', _curses.KEY_F3: 'kf3', _curses.KEY_F30: 'kf30', _curses.KEY_F31: 'kf31', _curses.KEY_F32: 'kf32', _curses.KEY_F33: 'kf33', _curses.KEY_F34: 'kf34', _curses.KEY_F35: 'kf35', _curses.KEY_F36: 'kf36', _curses.KEY_F37: 'kf37', _curses.KEY_F38: 'kf38', _curses.KEY_F39: 'kf39', _curses.KEY_F4: 'kf4', _curses.KEY_F40: 'kf40', _curses.KEY_F41: 'kf41', _curses.KEY_F42: 'kf42', _curses.KEY_F43: 'kf43', _curses.KEY_F44: 'kf44', _curses.KEY_F45: 'kf45', _curses.KEY_F46: 'kf46', _curses.KEY_F47: 'kf47', _curses.KEY_F48: 'kf48', _curses.KEY_F49: 'kf49', _curses.KEY_F5: 'kf5', _curses.KEY_F50: 'kf50', _curses.KEY_F51: 'kf51', _curses.KEY_F52: 'kf52', _curses.KEY_F53: 'kf53', _curses.KEY_F54: 'kf54', _curses.KEY_F55: 'kf55', _curses.KEY_F56: 'kf56', _curses.KEY_F57: 'kf57', _curses.KEY_F58: 'kf58', _curses.KEY_F59: 'kf59', _curses.KEY_F6: 'kf6', _curses.KEY_F60: 'kf60', _curses.KEY_F61: 'kf61', _curses.KEY_F62: 'kf62', _curses.KEY_F63: 'kf63', _curses.KEY_F7: 'kf7', _curses.KEY_F8: 'kf8', _curses.KEY_F9: 'kf9', _curses.KEY_FIND: 'kfnd', _curses.KEY_HELP: 'khlp', _curses.KEY_HOME: 'khome', _curses.KEY_IC: 'kich1', _curses.KEY_IL: 'kil1', _curses.KEY_LEFT: 'kcub1', _curses.KEY_LL: 'kll', _curses.KEY_MARK: 'kmrk', _curses.KEY_MESSAGE: 'kmsg', _curses.KEY_MOVE: 'kmov', _curses.KEY_NEXT: 'knxt', _curses.KEY_NPAGE: 'knp', _curses.KEY_OPEN: 'kopn', _curses.KEY_OPTIONS: 'kopt', _curses.KEY_PPAGE: 'kpp', _curses.KEY_PREVIOUS: 'kprv', _curses.KEY_PRINT: 'kprt', _curses.KEY_REDO: 'krdo', _curses.KEY_REFERENCE: 'kref', _curses.KEY_REFRESH: 'krfr', _curses.KEY_REPLACE: 'krpl', _curses.KEY_RESTART: 'krst', _curses.KEY_RESUME: 'kres', _curses.KEY_RIGHT: 'kcuf1', _curses.KEY_SAVE: 'ksav', _curses.KEY_SBEG: 'kBEG', _curses.KEY_SCANCEL: 'kCAN', _curses.KEY_SCOMMAND: 'kCMD', _curses.KEY_SCOPY: 'kCPY', _curses.KEY_SCREATE: 'kCRT', _curses.KEY_SDC: 'kDC', _curses.KEY_SDL: 'kDL', _curses.KEY_SELECT: 'kslt', _curses.KEY_SEND: 'kEND', _curses.KEY_SEOL: 'kEOL', _curses.KEY_SEXIT: 'kEXT', _curses.KEY_SF: 'kind', _curses.KEY_SFIND: 'kFND', _curses.KEY_SHELP: 'kHLP', _curses.KEY_SHOME: 'kHOM', _curses.KEY_SIC: 'kIC', _curses.KEY_SLEFT: 'kLFT', _curses.KEY_SMESSAGE: 'kMSG', _curses.KEY_SMOVE: 'kMOV', _curses.KEY_SNEXT: 'kNXT', _curses.KEY_SOPTIONS: 'kOPT', _curses.KEY_SPREVIOUS: 'kPRV', _curses.KEY_SPRINT: 'kPRT', _curses.KEY_SR: 'kri', _curses.KEY_SREDO: 'kRDO', _curses.KEY_SREPLACE: 'kRPL', _curses.KEY_SRIGHT: 'kRIT', _curses.KEY_SRSUME: 'kRES', _curses.KEY_SSAVE: 'kSAV', _curses.KEY_SSUSPEND: 'kSPD', _curses.KEY_STAB: 'khts', _curses.KEY_SUNDO: 'kUND', _curses.KEY_SUSPEND: 'kspd', _curses.KEY_UNDO: 'kund', _curses.KEY_UP: 'kcuu1' } def has_key(ch): if isinstance(ch, str): ch = ord(ch) # Figure out the correct capability name for the keycode. capability_name = _capability_names.get(ch) if capability_name is None: return False #Check the current terminal description for that capability; #if present, return true, else return false. if _curses.tigetstr( capability_name ): return True else: return False if __name__ == '__main__': # Compare the output of this implementation and the ncurses has_key, # on platforms where has_key is already available try: L = [] _curses.initscr() for key in _capability_names.keys(): system = _curses.has_key(key) python = has_key(key) if system != python: L.append( 'Mismatch for key %s, system=%i, Python=%i' % (_curses.keyname( key ), system, python) ) finally: _curses.endwin() for i in L: print i
# -*- encoding: utf-8 -*- from abjad import * def test_timespantools_Timespan_is_congruent_to_timespan_01(): timespan_1 = timespantools.Timespan(0, 15) timespan_2 = timespantools.Timespan(-10, -5) assert not timespan_1.is_congruent_to_timespan(timespan_2) def test_timespantools_Timespan_is_congruent_to_timespan_02(): timespan_1 = timespantools.Timespan(0, 15) timespan_2 = timespantools.Timespan(-10, 0) assert not timespan_1.is_congruent_to_timespan(timespan_2) def test_timespantools_Timespan_is_congruent_to_timespan_03(): timespan_1 = timespantools.Timespan(0, 15) timespan_2 = timespantools.Timespan(-10, 5) assert not timespan_1.is_congruent_to_timespan(timespan_2) def test_timespantools_Timespan_is_congruent_to_timespan_04(): timespan_1 = timespantools.Timespan(0, 15) timespan_2 = timespantools.Timespan(-10, 15) assert not timespan_1.is_congruent_to_timespan(timespan_2) def test_timespantools_Timespan_is_congruent_to_timespan_05(): timespan_1 = timespantools.Timespan(0, 15) timespan_2 = timespantools.Timespan(-10, 25) assert not timespan_1.is_congruent_to_timespan(timespan_2) def test_timespantools_Timespan_is_congruent_to_timespan_06(): timespan_1 = timespantools.Timespan(0, 15) timespan_2 = timespantools.Timespan(0, 10) assert not timespan_1.is_congruent_to_timespan(timespan_2) def test_timespantools_Timespan_is_congruent_to_timespan_07(): timespan_1 = timespantools.Timespan(0, 15) timespan_2 = timespantools.Timespan(0, 15) assert timespan_1.is_congruent_to_timespan(timespan_2) def test_timespantools_Timespan_is_congruent_to_timespan_08(): timespan_1 = timespantools.Timespan(0, 15) timespan_2 = timespantools.Timespan(5, 10) assert not timespan_1.is_congruent_to_timespan(timespan_2) def test_timespantools_Timespan_is_congruent_to_timespan_09(): timespan_1 = timespantools.Timespan(0, 15) timespan_2 = timespantools.Timespan(5, 15) assert not timespan_1.is_congruent_to_timespan(timespan_2) def test_timespantools_Timespan_is_congruent_to_timespan_10(): timespan_1 = timespantools.Timespan(0, 15) timespan_2 = timespantools.Timespan(0, 25) assert not timespan_1.is_congruent_to_timespan(timespan_2) def test_timespantools_Timespan_is_congruent_to_timespan_11(): timespan_1 = timespantools.Timespan(0, 15) timespan_2 = timespantools.Timespan(5, 25) assert not timespan_1.is_congruent_to_timespan(timespan_2) def test_timespantools_Timespan_is_congruent_to_timespan_12(): timespan_1 = timespantools.Timespan(0, 15) timespan_2 = timespantools.Timespan(15, 25) assert not timespan_1.is_congruent_to_timespan(timespan_2) def test_timespantools_Timespan_is_congruent_to_timespan_13(): timespan_1 = timespantools.Timespan(0, 15) timespan_2 = timespantools.Timespan(20, 25) assert not timespan_1.is_congruent_to_timespan(timespan_2)
#!/usr/bin/env python # -*- coding: utf-8 -*- # # This file is part of the Shiboken Python Bindings Generator project. # # Copyright (C) 2013 Digia Plc and/or its subsidiary(-ies). # # Contact: PySide team <contact@pyside.org> # # This program is free software; you can redistribute it and/or # modify it under the terms of the GNU Lesser General Public License # version 2.1 as published by the Free Software Foundation. Please # review the following information to ensure the GNU Lesser General # Public License version 2.1 requirements will be met: # http://www.gnu.org/licenses/old-licenses/lgpl-2.1.html. # # # This program is distributed in the hope that it will be useful, but # WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU # Lesser General Public License for more details. # # You should have received a copy of the GNU Lesser General Public # License along with this program; if not, write to the Free Software # Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA # 02110-1301 USA '''Test cases for multiple inheritance''' import sys import unittest from sample import * class SimpleUseCase(ObjectType, Str): def __init__(self, name): ObjectType.__init__(self) Str.__init__(self, name) class SimpleUseCaseReverse(Str, ObjectType): def __init__(self, name): ObjectType.__init__(self) Str.__init__(self, name) class SimpleUseCase2(SimpleUseCase): def __init__(self, name): SimpleUseCase.__init__(self, name) class ComplexUseCase(SimpleUseCase2, Point): def __init__(self, name): SimpleUseCase2.__init__(self, name) Point.__init__(self) class ComplexUseCaseReverse(Point, SimpleUseCase2): def __init__(self, name): SimpleUseCase2.__init__(self, name) Point.__init__(self) class MultipleCppDerivedTest(unittest.TestCase): def testInstanciation(self): s = SimpleUseCase("Hi") self.assertEqual(s, "Hi") s.setObjectName(s) self.assertEqual(s.objectName(), "Hi") def testInstanciation2(self): s = SimpleUseCase2("Hi") self.assertEqual(s, "Hi") s.setObjectName(s) self.assertEqual(s.objectName(), "Hi") def testComplexInstanciation(self): c = ComplexUseCase("Hi") self.assertEqual(c, "Hi") c.setObjectName(c) self.assertEqual(c.objectName(), "Hi") c.setX(2); self.assertEqual(c.x(), 2) class MultipleCppDerivedReverseTest(unittest.TestCase): def testInstanciation(self): s = SimpleUseCaseReverse("Hi") self.assertEqual(s, "Hi") s.setObjectName(s) self.assertEqual(s.objectName(), "Hi") def testInstanciation2(self): s = SimpleUseCase2("Hi") self.assertEqual(s, "Hi") s.setObjectName(s) self.assertEqual(s.objectName(), "Hi") def testComplexInstanciation(self): c = ComplexUseCaseReverse("Hi") c.setObjectName(c) self.assertEqual(c.objectName(), "Hi") c.setX(2); self.assertEqual(c, Point(2, 0)) if __name__ == '__main__': unittest.main()
from __future__ import absolute_import from itertools import count from token import * from parso._compatibility import py_version _counter = count(N_TOKENS) # Never want to see this thing again. del N_TOKENS COMMENT = next(_counter) tok_name[COMMENT] = 'COMMENT' NL = next(_counter) tok_name[NL] = 'NL' # Sets the attributes that don't exist in these tok_name versions. if py_version >= 30: BACKQUOTE = next(_counter) tok_name[BACKQUOTE] = 'BACKQUOTE' else: RARROW = next(_counter) tok_name[RARROW] = 'RARROW' ELLIPSIS = next(_counter) tok_name[ELLIPSIS] = 'ELLIPSIS' if py_version < 35: ATEQUAL = next(_counter) tok_name[ATEQUAL] = 'ATEQUAL' ERROR_DEDENT = next(_counter) tok_name[ERROR_DEDENT] = 'ERROR_DEDENT' # Map from operator to number (since tokenize doesn't do this) opmap_raw = """\ ( LPAR ) RPAR [ LSQB ] RSQB : COLON , COMMA ; SEMI + PLUS - MINUS * STAR / SLASH | VBAR & AMPER < LESS > GREATER = EQUAL . DOT % PERCENT ` BACKQUOTE { LBRACE } RBRACE @ AT == EQEQUAL != NOTEQUAL <> NOTEQUAL <= LESSEQUAL >= GREATEREQUAL ~ TILDE ^ CIRCUMFLEX << LEFTSHIFT >> RIGHTSHIFT ** DOUBLESTAR += PLUSEQUAL -= MINEQUAL *= STAREQUAL /= SLASHEQUAL %= PERCENTEQUAL &= AMPEREQUAL |= VBAREQUAL @= ATEQUAL ^= CIRCUMFLEXEQUAL <<= LEFTSHIFTEQUAL >>= RIGHTSHIFTEQUAL **= DOUBLESTAREQUAL // DOUBLESLASH //= DOUBLESLASHEQUAL -> RARROW ... ELLIPSIS """ opmap = {} for line in opmap_raw.splitlines(): op, name = line.split() opmap[op] = globals()[name] def generate_token_id(string): """ Uses a token in the grammar (e.g. `'+'` or `'and'`returns the corresponding ID for it. The strings are part of the grammar file. """ try: return opmap[string] except KeyError: pass return globals()[string]
import fsui from fsbc.util import unused from launcher.ui.skin import Skin class SettingsHeader(fsui.Group): ICON_LEFT = 0 ICON_RIGHT = 1 def __init__( self, parent, icon, title, subtitle="", icon_position=ICON_RIGHT ): unused(subtitle) fsui.Group.__init__(self, parent) self.layout = fsui.HorizontalLayout() image = icon.image(48) self.image_view = fsui.ImageView(self, image) if icon_position == self.ICON_LEFT: self.layout.add(self.image_view) self.layout.add_spacer(20) # vert_layout = fsui.VerticalLayout() # self.layout.add( # vert_layout, expand=True, fill=False, valign=0.5) self.title_label = fsui.HeadingLabel(self, title) if Skin.fws() or True: font = fsui.Font("Roboto", 26) self.title_label.set_font(font) self.layout.add( self.title_label, expand=True, fill=False, valign=0.0 ) else: font = self.title_label.get_font() font.increase_size(3) self.title_label.set_font(font) self.layout.add( self.title_label, expand=True, fill=False, valign=0.5 ) if icon_position == self.ICON_RIGHT: self.layout.add_spacer(20) self.layout.add(self.image_view)
""" Encoding Aliases Support This module is used by the encodings package search function to map encodings names to module names. Note that the search function normalizes the encoding names before doing the lookup, so the mapping will have to map normalized encoding names to module names. Contents: The following aliases dictionary contains mappings of all IANA character set names for which the Python core library provides codecs. In addition to these, a few Python specific codec aliases have also been added. """ aliases = { # Please keep this list sorted alphabetically by value ! # ascii codec '646' : 'ascii', 'ansi_x3.4_1968' : 'ascii', 'ansi_x3_4_1968' : 'ascii', # some email headers use this non-standard name 'ansi_x3.4_1986' : 'ascii', 'cp367' : 'ascii', 'csascii' : 'ascii', 'ibm367' : 'ascii', 'iso646_us' : 'ascii', 'iso_646.irv_1991' : 'ascii', 'iso_ir_6' : 'ascii', 'us' : 'ascii', 'us_ascii' : 'ascii', # base64_codec codec 'base64' : 'base64_codec', 'base_64' : 'base64_codec', # big5 codec 'big5_tw' : 'big5', 'csbig5' : 'big5', # big5hkscs codec 'big5_hkscs' : 'big5hkscs', 'hkscs' : 'big5hkscs', # bz2_codec codec 'bz2' : 'bz2_codec', # cp037 codec '037' : 'cp037', 'csibm037' : 'cp037', 'ebcdic_cp_ca' : 'cp037', 'ebcdic_cp_nl' : 'cp037', 'ebcdic_cp_us' : 'cp037', 'ebcdic_cp_wt' : 'cp037', 'ibm037' : 'cp037', 'ibm039' : 'cp037', # cp1026 codec '1026' : 'cp1026', 'csibm1026' : 'cp1026', 'ibm1026' : 'cp1026', # cp1140 codec '1140' : 'cp1140', 'ibm1140' : 'cp1140', # cp1250 codec '1250' : 'cp1250', 'windows_1250' : 'cp1250', # cp1251 codec '1251' : 'cp1251', 'windows_1251' : 'cp1251', # cp1252 codec '1252' : 'cp1252', 'windows_1252' : 'cp1252', # cp1253 codec '1253' : 'cp1253', 'windows_1253' : 'cp1253', # cp1254 codec '1254' : 'cp1254', 'windows_1254' : 'cp1254', # cp1255 codec '1255' : 'cp1255', 'windows_1255' : 'cp1255', # cp1256 codec '1256' : 'cp1256', 'windows_1256' : 'cp1256', # cp1257 codec '1257' : 'cp1257', 'windows_1257' : 'cp1257', # cp1258 codec '1258' : 'cp1258', 'windows_1258' : 'cp1258', # cp424 codec '424' : 'cp424', 'csibm424' : 'cp424', 'ebcdic_cp_he' : 'cp424', 'ibm424' : 'cp424', # cp437 codec '437' : 'cp437', 'cspc8codepage437' : 'cp437', 'ibm437' : 'cp437', # cp500 codec '500' : 'cp500', 'csibm500' : 'cp500', 'ebcdic_cp_be' : 'cp500', 'ebcdic_cp_ch' : 'cp500', 'ibm500' : 'cp500', # cp775 codec '775' : 'cp775', 'cspc775baltic' : 'cp775', 'ibm775' : 'cp775', # cp850 codec '850' : 'cp850', 'cspc850multilingual' : 'cp850', 'ibm850' : 'cp850', # cp852 codec '852' : 'cp852', 'cspcp852' : 'cp852', 'ibm852' : 'cp852', # cp855 codec '855' : 'cp855', 'csibm855' : 'cp855', 'ibm855' : 'cp855', # cp857 codec '857' : 'cp857', 'csibm857' : 'cp857', 'ibm857' : 'cp857', # cp858 codec '858' : 'cp858', 'csibm858' : 'cp858', 'ibm858' : 'cp858', # cp860 codec '860' : 'cp860', 'csibm860' : 'cp860', 'ibm860' : 'cp860', # cp861 codec '861' : 'cp861', 'cp_is' : 'cp861', 'csibm861' : 'cp861', 'ibm861' : 'cp861', # cp862 codec '862' : 'cp862', 'cspc862latinhebrew' : 'cp862', 'ibm862' : 'cp862', # cp863 codec '863' : 'cp863', 'csibm863' : 'cp863', 'ibm863' : 'cp863', # cp864 codec '864' : 'cp864', 'csibm864' : 'cp864', 'ibm864' : 'cp864', # cp865 codec '865' : 'cp865', 'csibm865' : 'cp865', 'ibm865' : 'cp865', # cp866 codec '866' : 'cp866', 'csibm866' : 'cp866', 'ibm866' : 'cp866', # cp869 codec '869' : 'cp869', 'cp_gr' : 'cp869', 'csibm869' : 'cp869', 'ibm869' : 'cp869', # cp932 codec '932' : 'cp932', 'ms932' : 'cp932', 'mskanji' : 'cp932', 'ms_kanji' : 'cp932', # cp949 codec '949' : 'cp949', 'ms949' : 'cp949', 'uhc' : 'cp949', # cp950 codec '950' : 'cp950', 'ms950' : 'cp950', # euc_jis_2004 codec 'jisx0213' : 'euc_jis_2004', 'eucjis2004' : 'euc_jis_2004', 'euc_jis2004' : 'euc_jis_2004', # euc_jisx0213 codec 'eucjisx0213' : 'euc_jisx0213', # euc_jp codec 'eucjp' : 'euc_jp', 'ujis' : 'euc_jp', 'u_jis' : 'euc_jp', # euc_kr codec 'euckr' : 'euc_kr', 'korean' : 'euc_kr', 'ksc5601' : 'euc_kr', 'ks_c_5601' : 'euc_kr', 'ks_c_5601_1987' : 'euc_kr', 'ksx1001' : 'euc_kr', 'ks_x_1001' : 'euc_kr', # gb18030 codec 'gb18030_2000' : 'gb18030', # gb2312 codec 'chinese' : 'gb2312', 'csiso58gb231280' : 'gb2312', 'euc_cn' : 'gb2312', 'euccn' : 'gb2312', 'eucgb2312_cn' : 'gb2312', 'gb2312_1980' : 'gb2312', 'gb2312_80' : 'gb2312', 'iso_ir_58' : 'gb2312', # gbk codec '936' : 'gbk', 'cp936' : 'gbk', 'ms936' : 'gbk', # hex_codec codec 'hex' : 'hex_codec', # hp_roman8 codec 'roman8' : 'hp_roman8', 'r8' : 'hp_roman8', 'csHPRoman8' : 'hp_roman8', # hz codec 'hzgb' : 'hz', 'hz_gb' : 'hz', 'hz_gb_2312' : 'hz', # iso2022_jp codec 'csiso2022jp' : 'iso2022_jp', 'iso2022jp' : 'iso2022_jp', 'iso_2022_jp' : 'iso2022_jp', # iso2022_jp_1 codec 'iso2022jp_1' : 'iso2022_jp_1', 'iso_2022_jp_1' : 'iso2022_jp_1', # iso2022_jp_2 codec 'iso2022jp_2' : 'iso2022_jp_2', 'iso_2022_jp_2' : 'iso2022_jp_2', # iso2022_jp_2004 codec 'iso_2022_jp_2004' : 'iso2022_jp_2004', 'iso2022jp_2004' : 'iso2022_jp_2004', # iso2022_jp_3 codec 'iso2022jp_3' : 'iso2022_jp_3', 'iso_2022_jp_3' : 'iso2022_jp_3', # iso2022_jp_ext codec 'iso2022jp_ext' : 'iso2022_jp_ext', 'iso_2022_jp_ext' : 'iso2022_jp_ext', # iso2022_kr codec 'csiso2022kr' : 'iso2022_kr', 'iso2022kr' : 'iso2022_kr', 'iso_2022_kr' : 'iso2022_kr', # iso8859_10 codec 'csisolatin6' : 'iso8859_10', 'iso_8859_10' : 'iso8859_10', 'iso_8859_10_1992' : 'iso8859_10', 'iso_ir_157' : 'iso8859_10', 'l6' : 'iso8859_10', 'latin6' : 'iso8859_10', # iso8859_11 codec 'thai' : 'iso8859_11', 'iso_8859_11' : 'iso8859_11', 'iso_8859_11_2001' : 'iso8859_11', # iso8859_13 codec 'iso_8859_13' : 'iso8859_13', 'l7' : 'iso8859_13', 'latin7' : 'iso8859_13', # iso8859_14 codec 'iso_8859_14' : 'iso8859_14', 'iso_8859_14_1998' : 'iso8859_14', 'iso_celtic' : 'iso8859_14', 'iso_ir_199' : 'iso8859_14', 'l8' : 'iso8859_14', 'latin8' : 'iso8859_14', # iso8859_15 codec 'iso_8859_15' : 'iso8859_15', 'l9' : 'iso8859_15', 'latin9' : 'iso8859_15', # iso8859_16 codec 'iso_8859_16' : 'iso8859_16', 'iso_8859_16_2001' : 'iso8859_16', 'iso_ir_226' : 'iso8859_16', 'l10' : 'iso8859_16', 'latin10' : 'iso8859_16', # iso8859_2 codec 'csisolatin2' : 'iso8859_2', 'iso_8859_2' : 'iso8859_2', 'iso_8859_2_1987' : 'iso8859_2', 'iso_ir_101' : 'iso8859_2', 'l2' : 'iso8859_2', 'latin2' : 'iso8859_2', # iso8859_3 codec 'csisolatin3' : 'iso8859_3', 'iso_8859_3' : 'iso8859_3', 'iso_8859_3_1988' : 'iso8859_3', 'iso_ir_109' : 'iso8859_3', 'l3' : 'iso8859_3', 'latin3' : 'iso8859_3', # iso8859_4 codec 'csisolatin4' : 'iso8859_4', 'iso_8859_4' : 'iso8859_4', 'iso_8859_4_1988' : 'iso8859_4', 'iso_ir_110' : 'iso8859_4', 'l4' : 'iso8859_4', 'latin4' : 'iso8859_4', # iso8859_5 codec 'csisolatincyrillic' : 'iso8859_5', 'cyrillic' : 'iso8859_5', 'iso_8859_5' : 'iso8859_5', 'iso_8859_5_1988' : 'iso8859_5', 'iso_ir_144' : 'iso8859_5', # iso8859_6 codec 'arabic' : 'iso8859_6', 'asmo_708' : 'iso8859_6', 'csisolatinarabic' : 'iso8859_6', 'ecma_114' : 'iso8859_6', 'iso_8859_6' : 'iso8859_6', 'iso_8859_6_1987' : 'iso8859_6', 'iso_ir_127' : 'iso8859_6', # iso8859_7 codec 'csisolatingreek' : 'iso8859_7', 'ecma_118' : 'iso8859_7', 'elot_928' : 'iso8859_7', 'greek' : 'iso8859_7', 'greek8' : 'iso8859_7', 'iso_8859_7' : 'iso8859_7', 'iso_8859_7_1987' : 'iso8859_7', 'iso_ir_126' : 'iso8859_7', # iso8859_8 codec 'csisolatinhebrew' : 'iso8859_8', 'hebrew' : 'iso8859_8', 'iso_8859_8' : 'iso8859_8', 'iso_8859_8_1988' : 'iso8859_8', 'iso_ir_138' : 'iso8859_8', # iso8859_9 codec 'csisolatin5' : 'iso8859_9', 'iso_8859_9' : 'iso8859_9', 'iso_8859_9_1989' : 'iso8859_9', 'iso_ir_148' : 'iso8859_9', 'l5' : 'iso8859_9', 'latin5' : 'iso8859_9', # johab codec 'cp1361' : 'johab', 'ms1361' : 'johab', # koi8_r codec 'cskoi8r' : 'koi8_r', # latin_1 codec # # Note that the latin_1 codec is implemented internally in C and a # lot faster than the charmap codec iso8859_1 which uses the same # encoding. This is why we discourage the use of the iso8859_1 # codec and alias it to latin_1 instead. # '8859' : 'latin_1', 'cp819' : 'latin_1', 'csisolatin1' : 'latin_1', 'ibm819' : 'latin_1', 'iso8859' : 'latin_1', 'iso8859_1' : 'latin_1', 'iso_8859_1' : 'latin_1', 'iso_8859_1_1987' : 'latin_1', 'iso_ir_100' : 'latin_1', 'l1' : 'latin_1', 'latin' : 'latin_1', 'latin1' : 'latin_1', # mac_cyrillic codec 'maccyrillic' : 'mac_cyrillic', # mac_greek codec 'macgreek' : 'mac_greek', # mac_iceland codec 'maciceland' : 'mac_iceland', # mac_latin2 codec 'maccentraleurope' : 'mac_latin2', 'maclatin2' : 'mac_latin2', # mac_roman codec 'macroman' : 'mac_roman', # mac_turkish codec 'macturkish' : 'mac_turkish', # mbcs codec 'dbcs' : 'mbcs', # ptcp154 codec 'csptcp154' : 'ptcp154', 'pt154' : 'ptcp154', 'cp154' : 'ptcp154', 'cyrillic_asian' : 'ptcp154', # quopri_codec codec 'quopri' : 'quopri_codec', 'quoted_printable' : 'quopri_codec', 'quotedprintable' : 'quopri_codec', # rot_13 codec 'rot13' : 'rot_13', # shift_jis codec 'csshiftjis' : 'shift_jis', 'shiftjis' : 'shift_jis', 'sjis' : 'shift_jis', 's_jis' : 'shift_jis', # shift_jis_2004 codec 'shiftjis2004' : 'shift_jis_2004', 'sjis_2004' : 'shift_jis_2004', 's_jis_2004' : 'shift_jis_2004', # shift_jisx0213 codec 'shiftjisx0213' : 'shift_jisx0213', 'sjisx0213' : 'shift_jisx0213', 's_jisx0213' : 'shift_jisx0213', # tactis codec 'tis260' : 'tactis', # tis_620 codec 'tis620' : 'tis_620', 'tis_620_0' : 'tis_620', 'tis_620_2529_0' : 'tis_620', 'tis_620_2529_1' : 'tis_620', 'iso_ir_166' : 'tis_620', # utf_16 codec 'u16' : 'utf_16', 'utf16' : 'utf_16', # utf_16_be codec 'unicodebigunmarked' : 'utf_16_be', 'utf_16be' : 'utf_16_be', # utf_16_le codec 'unicodelittleunmarked' : 'utf_16_le', 'utf_16le' : 'utf_16_le', # utf_32 codec 'u32' : 'utf_32', 'utf32' : 'utf_32', # utf_32_be codec 'utf_32be' : 'utf_32_be', # utf_32_le codec 'utf_32le' : 'utf_32_le', # utf_7 codec 'u7' : 'utf_7', 'utf7' : 'utf_7', 'unicode_1_1_utf_7' : 'utf_7', # utf_8 codec 'u8' : 'utf_8', 'utf' : 'utf_8', 'utf8' : 'utf_8', 'utf8_ucs2' : 'utf_8', 'utf8_ucs4' : 'utf_8', # uu_codec codec 'uu' : 'uu_codec', # zlib_codec codec 'zip' : 'zlib_codec', 'zlib' : 'zlib_codec', }
#! /usr/bin/python # # Displays all multi-chats and allows to open them. # # (c) Copyright 2007, Vincent Oberle, vincent@oberle.org # # This software may be used and distributed according to the terms # of the GNU Public License, incorporated herein by reference. import sys import re from optparse import OptionParser from skype_api import * appname = 'chat_finder' class SkypeChat: def __init__(self, _chunk_size = 5, debug = False): self.ids = None self.chunk = 0 self.chunk_size = _chunk_size self.topics = {} self.members = {} self.friendlyname = {} self.api = SkypeAPI(appname, debug) def init_chat_ids(self): ret = self.api.send_and_block('SEARCH CHATS') r = re.search (r'CHATS (.*)', ret) if r: self.ids = r.group(1).strip().split(', ') # convert percentage to actual value self.chunk_size = len(self.ids) * self.chunk_size / 100 # Gets chat info by chunks def get_all_chats(self): if not self.ids: self.init_chat_ids() lo = self.chunk self.hi = min([self.chunk + self.chunk_size - 1, len(self.ids) - 1]) for i in self.ids[lo:self.hi]: self.api.send('GET CHAT ' + i + ' TOPIC') self.api.send('GET CHAT ' + i + ' MEMBERS') self.api.send('GET CHAT ' + i + ' FRIENDLYNAME') while True: msgs = self.api.response_wait(1) if not msgs: break for reply in msgs: r = re.search (r'CHAT (\S+) TOPIC (.+)', reply) if r: self.topics[r.group(1).strip()] = r.group(2).strip() r = re.search (r'CHAT (\S+) MEMBERS (.+)', reply) if r: self.members[r.group(1).strip()] = r.group(2).strip() r = re.search (r'CHAT (\S+) FRIENDLYNAME (.+)', reply) if r: self.friendlyname[r.group(1).strip()] = r.group(2).strip() self.chunk = min([self.chunk + self.chunk_size, len(self.ids) - 1]) def open_chat(self, name): self.api.send('OPEN CHAT ' + name) def open_chat_by_index(self, index): self.open_chat(self.ids[index]) def print_chat_list(self, filter): for i in self.ids[:self.hi]: # display only multi-chats if not self.members.has_key(i) or len(self.members[i].split()) < 3: continue # string we filter on search_str = '' if self.topics.has_key(i): search_str = search_str + self.topics[i] elif self.friendlyname.has_key(i): search_str = search_str + self.friendlyname[i] if self.members.has_key(i): search_str = search_str + self.members[i] search_str = search_str.lower() if not filter or search_str.find(filter) >= 0: t = '' if self.topics.has_key(i): t = '**' + self.topics[i] + '**' elif self.friendlyname.has_key(i): t = '"' + self.friendlyname[i] + '"' if self.members.has_key(i): t = t + ' ' + self.members[i] if t: print str(self.ids.index(i)) + ': ' + t def perc_loaded(self): return str( int(float(self.chunk) / len(self.ids) * 100) ) if __name__ == "__main__": parser = OptionParser('%prog [options]') parser.add_option('-c', '--chunk', dest='chunk', default = 5, help='Percentage of total number of chats to load each time (100 for all)') parser.add_option('-d', '--debug', action='store_true', dest='debug', default = False, help='Print debug messages') options, args = parser.parse_args() if len(args): parser.print_help() sys.exit(0) try: api = SkypeChat(int(options.chunk), options.debug) except StandardError: print 'Could not connect to Skype. Check if "' + appname + '" is authorized to connect to Skype (Options - Public API)' sys.exit(0) print 'Please patient while chats are being loaded...' api.get_all_chats() filter = None refresh = True while True: if refresh: print '' api.print_chat_list(filter) refresh = False print 'Loaded: ' + api.perc_loaded() + ' %' print 'Quick help: "/word" filter by word; "/" clean filter; "m" load more chats; number to open a chat; "q" to quit' print '> ', sys.stdout.flush() result = sys.stdin.readline().strip().lower() if not result: continue elif result == '/': filter = None refresh = True elif result[0] == '/': filter = result[1:] refresh = True elif result.isdigit(): api.open_chat_by_index(int(result)) elif result[0] == '#': api.open_chat(result) elif result == 'm': print 'Loading more chats...' api.get_all_chats() refresh = True elif result == 'h': print 'Skype Chat Finder: Finds old multi-chats and allows to open them' print 'Commands:' print ' /word Filters chat topic and member list with "word"' print ' / Show all chats (reset filter)' print ' m Loads more chats' print ' Chat number Opens the chat window in Skype' print ' q Quits' elif result == 'q': break
# Copyright (C) 2011-2012 Yaco Sistemas (http://www.yaco.es) # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. from django import template from djangosaml2.conf import config_settings_loader register = template.Library() class IdPListNode(template.Node): def __init__(self, variable_name): self.variable_name = variable_name def render(self, context): conf = config_settings_loader() context[self.variable_name] = conf.idps() return '' @register.tag def idplist(parser, token): try: tag_name, as_part, variable = token.split_contents() except ValueError: raise template.TemplateSyntaxError( '%r tag requires two arguments' % token.contents.split()[0]) if not as_part == 'as': raise template.TemplateSyntaxError( '%r tag first argument must be the literal "as"' % tag_name) return IdPListNode(variable)
# Copyright (c) 2006,2007 Mitch Garnaat http://garnaat.org/ # # Permission is hereby granted, free of charge, to any person obtaining a # copy of this software and associated documentation files (the # "Software"), to deal in the Software without restriction, including # without limitation the rights to use, copy, modify, merge, publish, dis- # tribute, sublicense, and/or sell copies of the Software, and to permit # persons to whom the Software is furnished to do so, subject to the fol- # lowing conditions: # # The above copyright notice and this permission notice shall be included # in all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS # OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL- # ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT # SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, # WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS # IN THE SOFTWARE. """ High-level abstraction of an EC2 server """ import boto import boto.utils from boto.mashups.iobject import IObject from boto.pyami.config import Config, BotoConfigPath from boto.mashups.interactive import interactive_shell from boto.sdb.db.model import Model from boto.sdb.db.property import StringProperty import os import StringIO class ServerSet(list): def __getattr__(self, name): results = [] is_callable = False for server in self: try: val = getattr(server, name) if callable(val): is_callable = True results.append(val) except: results.append(None) if is_callable: self.map_list = results return self.map return results def map(self, *args): results = [] for fn in self.map_list: results.append(fn(*args)) return results class Server(Model): @property def ec2(self): if self._ec2 is None: self._ec2 = boto.connect_ec2() return self._ec2 @classmethod def Inventory(cls): """ Returns a list of Server instances, one for each Server object persisted in the db """ l = ServerSet() rs = cls.find() for server in rs: l.append(server) return l @classmethod def Register(cls, name, instance_id, description=''): s = cls() s.name = name s.instance_id = instance_id s.description = description s.save() return s def __init__(self, id=None, **kw): Model.__init__(self, id, **kw) self._reservation = None self._instance = None self._ssh_client = None self._pkey = None self._config = None self._ec2 = None name = StringProperty(unique=True, verbose_name="Name") instance_id = StringProperty(verbose_name="Instance ID") config_uri = StringProperty() ami_id = StringProperty(verbose_name="AMI ID") zone = StringProperty(verbose_name="Availability Zone") security_group = StringProperty(verbose_name="Security Group", default="default") key_name = StringProperty(verbose_name="Key Name") elastic_ip = StringProperty(verbose_name="Elastic IP") instance_type = StringProperty(verbose_name="Instance Type") description = StringProperty(verbose_name="Description") log = StringProperty() def setReadOnly(self, value): raise AttributeError def getInstance(self): if not self._instance: if self.instance_id: try: rs = self.ec2.get_all_instances([self.instance_id]) except: return None if len(rs) > 0: self._reservation = rs[0] self._instance = self._reservation.instances[0] return self._instance instance = property(getInstance, setReadOnly, None, 'The Instance for the server') def getAMI(self): if self.instance: return self.instance.image_id ami = property(getAMI, setReadOnly, None, 'The AMI for the server') def getStatus(self): if self.instance: self.instance.update() return self.instance.state status = property(getStatus, setReadOnly, None, 'The status of the server') def getHostname(self): if self.instance: return self.instance.public_dns_name hostname = property(getHostname, setReadOnly, None, 'The public DNS name of the server') def getPrivateHostname(self): if self.instance: return self.instance.private_dns_name private_hostname = property(getPrivateHostname, setReadOnly, None, 'The private DNS name of the server') def getLaunchTime(self): if self.instance: return self.instance.launch_time launch_time = property(getLaunchTime, setReadOnly, None, 'The time the Server was started') def getConsoleOutput(self): if self.instance: return self.instance.get_console_output() console_output = property(getConsoleOutput, setReadOnly, None, 'Retrieve the console output for server') def getGroups(self): if self._reservation: return self._reservation.groups else: return None groups = property(getGroups, setReadOnly, None, 'The Security Groups controlling access to this server') def getConfig(self): if not self._config: remote_file = BotoConfigPath local_file = '%s.ini' % self.instance.id self.get_file(remote_file, local_file) self._config = Config(local_file) return self._config def setConfig(self, config): local_file = '%s.ini' % self.instance.id fp = open(local_file) config.write(fp) fp.close() self.put_file(local_file, BotoConfigPath) self._config = config config = property(getConfig, setConfig, None, 'The instance data for this server') def set_config(self, config): """ Set SDB based config """ self._config = config self._config.dump_to_sdb("botoConfigs", self.id) def load_config(self): self._config = Config(do_load=False) self._config.load_from_sdb("botoConfigs", self.id) def stop(self): if self.instance: self.instance.stop() def start(self): self.stop() ec2 = boto.connect_ec2() ami = ec2.get_all_images(image_ids = [str(self.ami_id)])[0] groups = ec2.get_all_security_groups(groupnames=[str(self.security_group)]) if not self._config: self.load_config() if not self._config.has_section("Credentials"): self._config.add_section("Credentials") self._config.set("Credentials", "aws_access_key_id", ec2.aws_access_key_id) self._config.set("Credentials", "aws_secret_access_key", ec2.aws_secret_access_key) if not self._config.has_section("Pyami"): self._config.add_section("Pyami") if self._manager.domain: self._config.set('Pyami', 'server_sdb_domain', self._manager.domain.name) self._config.set("Pyami", 'server_sdb_name', self.name) cfg = StringIO.StringIO() self._config.write(cfg) cfg = cfg.getvalue() r = ami.run(min_count=1, max_count=1, key_name=self.key_name, security_groups = groups, instance_type = self.instance_type, placement = self.zone, user_data = cfg) i = r.instances[0] self.instance_id = i.id self.put() if self.elastic_ip: ec2.associate_address(self.instance_id, self.elastic_ip) def reboot(self): if self.instance: self.instance.reboot() def get_ssh_client(self, key_file=None, host_key_file='~/.ssh/known_hosts', uname='root'): import paramiko if not self.instance: print 'No instance yet!' return if not self._ssh_client: if not key_file: iobject = IObject() key_file = iobject.get_filename('Path to OpenSSH Key file') self._pkey = paramiko.RSAKey.from_private_key_file(key_file) self._ssh_client = paramiko.SSHClient() self._ssh_client.load_system_host_keys() self._ssh_client.load_host_keys(os.path.expanduser(host_key_file)) self._ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy()) self._ssh_client.connect(self.instance.public_dns_name, username=uname, pkey=self._pkey) return self._ssh_client def get_file(self, remotepath, localpath): ssh_client = self.get_ssh_client() sftp_client = ssh_client.open_sftp() sftp_client.get(remotepath, localpath) def put_file(self, localpath, remotepath): ssh_client = self.get_ssh_client() sftp_client = ssh_client.open_sftp() sftp_client.put(localpath, remotepath) def listdir(self, remotepath): ssh_client = self.get_ssh_client() sftp_client = ssh_client.open_sftp() return sftp_client.listdir(remotepath) def shell(self, key_file=None): ssh_client = self.get_ssh_client(key_file) channel = ssh_client.invoke_shell() interactive_shell(channel) def bundle_image(self, prefix, key_file, cert_file, size): print 'bundling image...' print '\tcopying cert and pk over to /mnt directory on server' ssh_client = self.get_ssh_client() sftp_client = ssh_client.open_sftp() path, name = os.path.split(key_file) remote_key_file = '/mnt/%s' % name self.put_file(key_file, remote_key_file) path, name = os.path.split(cert_file) remote_cert_file = '/mnt/%s' % name self.put_file(cert_file, remote_cert_file) print '\tdeleting %s' % BotoConfigPath # delete the metadata.ini file if it exists try: sftp_client.remove(BotoConfigPath) except: pass command = 'sudo ec2-bundle-vol ' command += '-c %s -k %s ' % (remote_cert_file, remote_key_file) command += '-u %s ' % self._reservation.owner_id command += '-p %s ' % prefix command += '-s %d ' % size command += '-d /mnt ' if self.instance.instance_type == 'm1.small' or self.instance_type == 'c1.medium': command += '-r i386' else: command += '-r x86_64' print '\t%s' % command t = ssh_client.exec_command(command) response = t[1].read() print '\t%s' % response print '\t%s' % t[2].read() print '...complete!' def upload_bundle(self, bucket, prefix): print 'uploading bundle...' command = 'ec2-upload-bundle ' command += '-m /mnt/%s.manifest.xml ' % prefix command += '-b %s ' % bucket command += '-a %s ' % self.ec2.aws_access_key_id command += '-s %s ' % self.ec2.aws_secret_access_key print '\t%s' % command ssh_client = self.get_ssh_client() t = ssh_client.exec_command(command) response = t[1].read() print '\t%s' % response print '\t%s' % t[2].read() print '...complete!' def create_image(self, bucket=None, prefix=None, key_file=None, cert_file=None, size=None): iobject = IObject() if not bucket: bucket = iobject.get_string('Name of S3 bucket') if not prefix: prefix = iobject.get_string('Prefix for AMI file') if not key_file: key_file = iobject.get_filename('Path to RSA private key file') if not cert_file: cert_file = iobject.get_filename('Path to RSA public cert file') if not size: size = iobject.get_int('Size (in MB) of bundled image') self.bundle_image(prefix, key_file, cert_file, size) self.upload_bundle(bucket, prefix) print 'registering image...' self.image_id = self.ec2.register_image('%s/%s.manifest.xml' % (bucket, prefix)) return self.image_id def attach_volume(self, volume, device="/dev/sdp"): """ Attach an EBS volume to this server :param volume: EBS Volume to attach :type volume: boto.ec2.volume.Volume :param device: Device to attach to (default to /dev/sdp) :type device: string """ if hasattr(volume, "id"): volume_id = volume.id else: volume_id = volume return self.ec2.attach_volume(volume_id=volume_id, instance_id=self.instance_id, device=device) def detach_volume(self, volume): """ Detach an EBS volume from this server :param volume: EBS Volume to detach :type volume: boto.ec2.volume.Volume """ if hasattr(volume, "id"): volume_id = volume.id else: volume_id = volume return self.ec2.detach_volume(volume_id=volume_id, instance_id=self.instance_id) def install_package(self, package_name): print 'installing %s...' % package_name command = 'yum -y install %s' % package_name print '\t%s' % command ssh_client = self.get_ssh_client() t = ssh_client.exec_command(command) response = t[1].read() print '\t%s' % response print '\t%s' % t[2].read() print '...complete!'
# -*- coding: utf-8 -*- # # django-extensions documentation build configuration file, created by # sphinx-quickstart on Wed Apr 1 20:39:40 2009. # # This file is execfile()d with the current directory set to its containing dir. # # Note that not all possible configuration values are present in this # autogenerated file. # # All configuration values have a default; values that are commented out # serve to show the default. #import sys, os # If extensions (or modules to document with autodoc) are in another directory, # add these directories to sys.path here. If the directory is relative to the # documentation root, use os.path.abspath to make it absolute, like shown here. #sys.path.append(os.path.abspath('.')) # -- General configuration ----------------------------------------------------- # Add any Sphinx extension module names here, as strings. They can be extensions # coming with Sphinx (named 'sphinx.ext.*') or your custom ones. extensions = [] # Add any paths that contain templates here, relative to this directory. templates_path = ['_templates'] # The suffix of source filenames. source_suffix = '.rst' # The encoding of source files. #source_encoding = 'utf-8' # The master toctree document. master_doc = 'index' # General information about the project. project = u'django-extensions' copyright = u'Copyright (C) 2008-2015 Michael Trier, Bas van Oostveen and contributors' # The version info for the project you're documenting, acts as replacement for # |version| and |release|, also used in various other places throughout the # built documents. # # The short X.Y version. version = '1.5' # The full version, including alpha/beta/rc tags. release = '1.5.0' # The language for content autogenerated by Sphinx. Refer to documentation # for a list of supported languages. #language = None # There are two options for replacing |today|: either, you set today to some # non-false value, then it is used: #today = '' # Else, today_fmt is used as the format for a strftime call. #today_fmt = '%B %d, %Y' # List of documents that shouldn't be included in the build. #unused_docs = [] # List of directories, relative to source directory, that shouldn't be searched # for source files. exclude_trees = ['_build'] # The reST default role (used for this markup: `text`) to use for all documents. #default_role = None # If true, '()' will be appended to :func: etc. cross-reference text. #add_function_parentheses = True # If true, the current module name will be prepended to all description # unit titles (such as .. function::). #add_module_names = True # If true, sectionauthor and moduleauthor directives will be shown in the # output. They are ignored by default. #show_authors = False # The name of the Pygments (syntax highlighting) style to use. pygments_style = 'sphinx' # A list of ignored prefixes for module index sorting. #modindex_common_prefix = [] # -- Options for HTML output --------------------------------------------------- # The theme to use for HTML and HTML Help pages. Major themes that come with # Sphinx are currently 'default' and 'sphinxdoc'. html_theme = 'default' # Theme options are theme-specific and customize the look and feel of a theme # further. For a list of options available for each theme, see the # documentation. #html_theme_options = {} # Add any paths that contain custom themes here, relative to this directory. #html_theme_path = [] # The name for this set of Sphinx documents. If None, it defaults to # "<project> v<release> documentation". #html_title = None # A shorter title for the navigation bar. Default is the same as html_title. #html_short_title = None # The name of an image file (relative to this directory) to place at the top # of the sidebar. #html_logo = None # The name of an image file (within the static path) to use as favicon of the # docs. This file should be a Windows icon file (.ico) being 16x16 or 32x32 # pixels large. #html_favicon = None # Add any paths that contain custom static files (such as style sheets) here, # relative to this directory. They are copied after the builtin static files, # so a file named "default.css" will overwrite the builtin "default.css". html_static_path = ['_static'] # If not '', a 'Last updated on:' timestamp is inserted at every page bottom, # using the given strftime format. #html_last_updated_fmt = '%b %d, %Y' # If true, SmartyPants will be used to convert quotes and dashes to # typographically correct entities. #html_use_smartypants = True # Custom sidebar templates, maps document names to template names. #html_sidebars = {} # Additional templates that should be rendered to pages, maps page names to # template names. #html_additional_pages = {} # If false, no module index is generated. #html_use_modindex = True # If false, no index is generated. #html_use_index = True # If true, the index is split into individual pages for each letter. #html_split_index = False # If true, links to the reST sources are added to the pages. #html_show_sourcelink = True # If true, an OpenSearch description file will be output, and all pages will # contain a <link> tag referring to it. The value of this option must be the # base URL from which the finished HTML is served. #html_use_opensearch = '' # If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml"). #html_file_suffix = '' # Output file base name for HTML help builder. htmlhelp_basename = 'django-extensionsdoc' # -- Options for LaTeX output -------------------------------------------------- # The paper size ('letter' or 'a4'). #latex_paper_size = 'letter' # The font size ('10pt', '11pt' or '12pt'). #latex_font_size = '10pt' # Grouping the document tree into LaTeX files. List of tuples # (source start file, target name, title, author, documentclass [howto/manual]). latex_documents = [( 'index', 'django-extensions.tex', u'django-extensions Documentation', u'Michael Trier, Bas van Oostveen, and contributors', 'manual' ), ] # The name of an image file (relative to this directory) to place at the top of # the title page. #latex_logo = None # For "manual" documents, if this is true, then toplevel headings are parts, # not chapters. #latex_use_parts = False # Additional stuff for the LaTeX preamble. #latex_preamble = '' # Documents to append as an appendix to all manuals. #latex_appendices = [] # If false, no module index is generated. #latex_use_modindex = True
# Copyright 2015, Google Inc. # All rights reserved. # # Redistribution and use in source and binary forms, with or without # modification, are permitted provided that the following conditions are # met: # # * Redistributions of source code must retain the above copyright # notice, this list of conditions and the following disclaimer. # * Redistributions in binary form must reproduce the above # copyright notice, this list of conditions and the following disclaimer # in the documentation and/or other materials provided with the # distribution. # * Neither the name of Google Inc. nor the names of its # contributors may be used to endorse or promote products derived from # this software without specific prior written permission. # # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS # "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT # LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR # A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT # OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, # SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT # LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, # DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY # THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT # (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE # OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. """Common resources used in the gRPC route guide example.""" import json import route_guide_pb2 def read_route_guide_database(): """Reads the route guide database. Returns: The full contents of the route guide database as a sequence of route_guide_pb2.Features. """ feature_list = [] with open("route_guide_db.json") as route_guide_db_file: for item in json.load(route_guide_db_file): feature = route_guide_pb2.Feature( name=item["name"], location=route_guide_pb2.Point( latitude=item["location"]["latitude"], longitude=item["location"]["longitude"])) feature_list.append(feature) return feature_list
# -*- coding: utf-8 -*- # Copyright 2014 OpenMarket Ltd # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. """Tests REST events for /presence paths.""" from tests import unittest from twisted.internet import defer from mock import Mock from ....utils import MockHttpResource, setup_test_homeserver from synapse.api.constants import PresenceState from synapse.handlers.presence import PresenceHandler from synapse.rest.client.v1 import presence from synapse.rest.client.v1 import events from synapse.types import UserID from synapse.util.async import run_on_reactor from collections import namedtuple OFFLINE = PresenceState.OFFLINE UNAVAILABLE = PresenceState.UNAVAILABLE ONLINE = PresenceState.ONLINE myid = "@apple:test" PATH_PREFIX = "/_matrix/client/api/v1" class JustPresenceHandlers(object): def __init__(self, hs): self.presence_handler = PresenceHandler(hs) class PresenceStateTestCase(unittest.TestCase): @defer.inlineCallbacks def setUp(self): self.mock_resource = MockHttpResource(prefix=PATH_PREFIX) hs = yield setup_test_homeserver( datastore=Mock(spec=[ "get_presence_state", "set_presence_state", "insert_client_ip", ]), http_client=None, resource_for_client=self.mock_resource, resource_for_federation=self.mock_resource, ) hs.handlers = JustPresenceHandlers(hs) self.datastore = hs.get_datastore() self.datastore.get_app_service_by_token = Mock(return_value=None) def get_presence_list(*a, **kw): return defer.succeed([]) self.datastore.get_presence_list = get_presence_list def _get_user_by_token(token=None): return { "user": UserID.from_string(myid), "admin": False, "device_id": None, "token_id": 1, } hs.get_v1auth().get_user_by_token = _get_user_by_token room_member_handler = hs.handlers.room_member_handler = Mock( spec=[ "get_joined_rooms_for_user", ] ) def get_rooms_for_user(user): return defer.succeed([]) room_member_handler.get_joined_rooms_for_user = get_rooms_for_user presence.register_servlets(hs, self.mock_resource) self.u_apple = UserID.from_string(myid) @defer.inlineCallbacks def test_get_my_status(self): mocked_get = self.datastore.get_presence_state mocked_get.return_value = defer.succeed( {"state": ONLINE, "status_msg": "Available"} ) (code, response) = yield self.mock_resource.trigger("GET", "/presence/%s/status" % (myid), None) self.assertEquals(200, code) self.assertEquals( {"presence": ONLINE, "status_msg": "Available"}, response ) mocked_get.assert_called_with("apple") @defer.inlineCallbacks def test_set_my_status(self): mocked_set = self.datastore.set_presence_state mocked_set.return_value = defer.succeed({"state": OFFLINE}) (code, response) = yield self.mock_resource.trigger("PUT", "/presence/%s/status" % (myid), '{"presence": "unavailable", "status_msg": "Away"}') self.assertEquals(200, code) mocked_set.assert_called_with("apple", {"state": UNAVAILABLE, "status_msg": "Away"} ) class PresenceListTestCase(unittest.TestCase): @defer.inlineCallbacks def setUp(self): self.mock_resource = MockHttpResource(prefix=PATH_PREFIX) hs = yield setup_test_homeserver( datastore=Mock(spec=[ "has_presence_state", "get_presence_state", "allow_presence_visible", "is_presence_visible", "add_presence_list_pending", "set_presence_list_accepted", "del_presence_list", "get_presence_list", "insert_client_ip", ]), http_client=None, resource_for_client=self.mock_resource, resource_for_federation=self.mock_resource, ) hs.handlers = JustPresenceHandlers(hs) self.datastore = hs.get_datastore() self.datastore.get_app_service_by_token = Mock(return_value=None) def has_presence_state(user_localpart): return defer.succeed( user_localpart in ("apple", "banana",) ) self.datastore.has_presence_state = has_presence_state def _get_user_by_token(token=None): return { "user": UserID.from_string(myid), "admin": False, "device_id": None, "token_id": 1, } hs.handlers.room_member_handler = Mock( spec=[ "get_joined_rooms_for_user", ] ) hs.get_v1auth().get_user_by_token = _get_user_by_token presence.register_servlets(hs, self.mock_resource) self.u_apple = UserID.from_string("@apple:test") self.u_banana = UserID.from_string("@banana:test") @defer.inlineCallbacks def test_get_my_list(self): self.datastore.get_presence_list.return_value = defer.succeed( [{"observed_user_id": "@banana:test", "accepted": True}], ) (code, response) = yield self.mock_resource.trigger("GET", "/presence/list/%s" % (myid), None) self.assertEquals(200, code) self.assertEquals([ {"user_id": "@banana:test", "presence": OFFLINE, "accepted": True}, ], response) self.datastore.get_presence_list.assert_called_with( "apple", accepted=True ) @defer.inlineCallbacks def test_invite(self): self.datastore.add_presence_list_pending.return_value = ( defer.succeed(()) ) self.datastore.is_presence_visible.return_value = defer.succeed( True ) (code, response) = yield self.mock_resource.trigger("POST", "/presence/list/%s" % (myid), """{"invite": ["@banana:test"]}""" ) self.assertEquals(200, code) self.datastore.add_presence_list_pending.assert_called_with( "apple", "@banana:test" ) self.datastore.set_presence_list_accepted.assert_called_with( "apple", "@banana:test" ) @defer.inlineCallbacks def test_drop(self): self.datastore.del_presence_list.return_value = ( defer.succeed(()) ) (code, response) = yield self.mock_resource.trigger("POST", "/presence/list/%s" % (myid), """{"drop": ["@banana:test"]}""" ) self.assertEquals(200, code) self.datastore.del_presence_list.assert_called_with( "apple", "@banana:test" ) class PresenceEventStreamTestCase(unittest.TestCase): @defer.inlineCallbacks def setUp(self): self.mock_resource = MockHttpResource(prefix=PATH_PREFIX) # HIDEOUS HACKERY # TODO(paul): This should be injected in via the HomeServer DI system from synapse.streams.events import ( PresenceEventSource, NullSource, EventSources ) old_SOURCE_TYPES = EventSources.SOURCE_TYPES def tearDown(): EventSources.SOURCE_TYPES = old_SOURCE_TYPES self.tearDown = tearDown EventSources.SOURCE_TYPES = { k: NullSource for k in old_SOURCE_TYPES.keys() } EventSources.SOURCE_TYPES["presence"] = PresenceEventSource hs = yield setup_test_homeserver( http_client=None, resource_for_client=self.mock_resource, resource_for_federation=self.mock_resource, datastore=Mock(spec=[ "set_presence_state", "get_presence_list", "get_rooms_for_user", ]), clock=Mock(spec=[ "call_later", "cancel_call_later", "time_msec", "looping_call", ]), ) hs.get_clock().time_msec.return_value = 1000000 def _get_user_by_req(req=None): return (UserID.from_string(myid), "") hs.get_v1auth().get_user_by_req = _get_user_by_req presence.register_servlets(hs, self.mock_resource) events.register_servlets(hs, self.mock_resource) hs.handlers.room_member_handler = Mock(spec=[]) self.room_members = [] def get_rooms_for_user(user): if user in self.room_members: return ["a-room"] else: return [] hs.handlers.room_member_handler.get_joined_rooms_for_user = get_rooms_for_user hs.handlers.room_member_handler.get_room_members = ( lambda r: self.room_members if r == "a-room" else [] ) self.mock_datastore = hs.get_datastore() self.mock_datastore.get_app_service_by_token = Mock(return_value=None) self.mock_datastore.get_app_service_by_user_id = Mock( return_value=defer.succeed(None) ) self.mock_datastore.get_rooms_for_user = ( lambda u: [ namedtuple("Room", "room_id")(r) for r in get_rooms_for_user(UserID.from_string(u)) ] ) def get_profile_displayname(user_id): return defer.succeed("Frank") self.mock_datastore.get_profile_displayname = get_profile_displayname def get_profile_avatar_url(user_id): return defer.succeed(None) self.mock_datastore.get_profile_avatar_url = get_profile_avatar_url def user_rooms_intersect(user_list): room_member_ids = map(lambda u: u.to_string(), self.room_members) shared = all(map(lambda i: i in room_member_ids, user_list)) return defer.succeed(shared) self.mock_datastore.user_rooms_intersect = user_rooms_intersect def get_joined_hosts_for_room(room_id): return [] self.mock_datastore.get_joined_hosts_for_room = get_joined_hosts_for_room self.presence = hs.get_handlers().presence_handler self.u_apple = UserID.from_string("@apple:test") self.u_banana = UserID.from_string("@banana:test") @defer.inlineCallbacks def test_shortpoll(self): self.room_members = [self.u_apple, self.u_banana] self.mock_datastore.set_presence_state.return_value = defer.succeed( {"state": ONLINE} ) self.mock_datastore.get_presence_list.return_value = defer.succeed( [] ) (code, response) = yield self.mock_resource.trigger("GET", "/events?timeout=0", None) self.assertEquals(200, code) # We've forced there to be only one data stream so the tokens will # all be ours # I'll already get my own presence state change self.assertEquals({"start": "0_1_0", "end": "0_1_0", "chunk": []}, response ) self.mock_datastore.set_presence_state.return_value = defer.succeed( {"state": ONLINE} ) self.mock_datastore.get_presence_list.return_value = defer.succeed([]) yield self.presence.set_state(self.u_banana, self.u_banana, state={"presence": ONLINE} ) yield run_on_reactor() (code, response) = yield self.mock_resource.trigger("GET", "/events?from=s0_1_0&timeout=0", None) self.assertEquals(200, code) self.assertEquals({"start": "s0_1_0", "end": "s0_2_0", "chunk": [ {"type": "m.presence", "content": { "user_id": "@banana:test", "presence": ONLINE, "displayname": "Frank", "last_active_ago": 0, }}, ]}, response)
#!/usr/bin/env python # $Id: basic_ftpd.py 977 2012-01-22 23:05:09Z g.rodola $ # pyftpdlib is released under the MIT license, reproduced below: # ====================================================================== # Copyright (C) 2007-2012 Giampaolo Rodola' <g.rodola@gmail.com> # # All Rights Reserved # # Permission is hereby granted, free of charge, to any person # obtaining a copy of this software and associated documentation # files (the "Software"), to deal in the Software without # restriction, including without limitation the rights to use, # copy, modify, merge, publish, distribute, sublicense, and/or sell # copies of the Software, and to permit persons to whom the # Software is furnished to do so, subject to the following # conditions: # # The above copyright notice and this permission notice shall be # included in all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, # EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES # OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND # NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT # HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, # WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING # FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR # OTHER DEALINGS IN THE SOFTWARE. # # ====================================================================== """A basic FTP server which uses a DummyAuthorizer for managing 'virtual users', setting a limit for incoming connections. """ import os from pyftpdlib import ftpserver def main(): # Instantiate a dummy authorizer for managing 'virtual' users authorizer = ftpserver.DummyAuthorizer() # Define a new user having full r/w permissions and a read-only # anonymous user authorizer.add_user('user', '12345', os.getcwd(), perm='elradfmwM') authorizer.add_anonymous(os.getcwd()) # Instantiate FTP handler class ftp_handler = ftpserver.FTPHandler ftp_handler.authorizer = authorizer # Define a customized banner (string returned when client connects) ftp_handler.banner = "pyftpdlib %s based ftpd ready." %ftpserver.__ver__ # Specify a masquerade address and the range of ports to use for # passive connections. Decomment in case you're behind a NAT. #ftp_handler.masquerade_address = '151.25.42.11' #ftp_handler.passive_ports = range(60000, 65535) # Instantiate FTP server class and listen to 0.0.0.0:21 address = ('', 21) ftpd = ftpserver.FTPServer(address, ftp_handler) # set a limit for connections ftpd.max_cons = 256 ftpd.max_cons_per_ip = 5 # start ftp server ftpd.serve_forever() if __name__ == '__main__': main()
# -*- coding: utf-8 -*- """The Apple System Log (ASL) file parser.""" import os from dfdatetime import posix_time as dfdatetime_posix_time from dfdatetime import semantic_time as dfdatetime_semantic_time from plaso.containers import events from plaso.containers import time_events from plaso.lib import definitions from plaso.lib import dtfabric_helper from plaso.lib import errors from plaso.lib import specification from plaso.parsers import interface from plaso.parsers import manager class ASLEventData(events.EventData): """Apple System Log (ASL) event data. Attributes: computer_name (str): name of the host. extra_information (str): extra fields associated to the event. facility (str): facility. group_id (int): group identifier (GID). level (str): level of criticality of the event. message_id (int): message identifier. message (str): message of the event. pid (int): process identifier (PID). read_uid (int): user identifier that can read this file, where -1 represents all. read_gid (int): the group identifier that can read this file, where -1 represents all. record_position (int): position of the event record. sender (str): sender or process that created the event. user_sid (str): user identifier (UID). """ DATA_TYPE = 'mac:asl:event' def __init__(self): """Initializes event data.""" super(ASLEventData, self).__init__(data_type=self.DATA_TYPE) self.computer_name = None self.extra_information = None self.facility = None self.group_id = None self.level = None self.message_id = None self.message = None self.pid = None self.read_gid = None self.read_uid = None self.record_position = None self.sender = None self.user_sid = None class ASLFileEventData(events.EventData): """Apple System Log (ASL) file event data. Attributes: format_version (int): ASL file format version. is_dirty (bool): True if the last log entry offset does not match value in file header and the file is considered dirty. """ DATA_TYPE = 'mac:asl:file' def __init__(self): """Initializes event data.""" super(ASLFileEventData, self).__init__(data_type=self.DATA_TYPE) self.format_version = None self.is_dirty = None class ASLParser(interface.FileObjectParser, dtfabric_helper.DtFabricHelper): """Parser for Apple System Log (ASL) files.""" NAME = 'asl_log' DATA_FORMAT = 'Apple System Log (ASL) file' _DEFINITION_FILE = os.path.join( os.path.dirname(__file__), 'asl.yaml') # Most significant bit of a 64-bit string offset. _STRING_OFFSET_MSB = 1 << 63 def _ParseRecord(self, parser_mediator, file_object, record_offset): """Parses a record and produces events. Args: parser_mediator (ParserMediator): mediates interactions between parsers and other components, such as storage and dfvfs. file_object (file): file-like object. record_offset (int): offset of the record relative to the start of the file. Returns: int: next record offset. Raises: ParseError: if the record cannot be parsed. """ record_map = self._GetDataTypeMap('asl_record') try: record, record_data_size = self._ReadStructureFromFileObject( file_object, record_offset, record_map) except (ValueError, errors.ParseError) as exception: raise errors.ParseError(( 'Unable to parse record at offset: 0x{0:08x} with error: ' '{1!s}').format(record_offset, exception)) hostname = self._ParseRecordString( file_object, record.hostname_string_offset) sender = self._ParseRecordString( file_object, record.sender_string_offset) facility = self._ParseRecordString( file_object, record.facility_string_offset) message = self._ParseRecordString( file_object, record.message_string_offset) file_offset = record_offset + record_data_size additional_data_size = record.data_size + 6 - record_data_size if additional_data_size % 8 != 0: raise errors.ParseError( 'Invalid record additional data size: {0:d}.'.format( additional_data_size)) additional_data = self._ReadData( file_object, file_offset, additional_data_size) extra_fields = {} for additional_data_offset in range(0, additional_data_size - 8, 16): record_extra_field = self._ParseRecordExtraField( additional_data[additional_data_offset:], file_offset) file_offset += 16 name = self._ParseRecordString( file_object, record_extra_field.name_string_offset) value = self._ParseRecordString( file_object, record_extra_field.value_string_offset) if name is not None: extra_fields[name] = value # TODO: implement determine previous record offset event_data = ASLEventData() event_data.computer_name = hostname event_data.extra_information = ', '.join([ '{0:s}: {1!s}'.format(name, value) for name, value in sorted(extra_fields.items())]) event_data.facility = facility event_data.group_id = record.group_identifier event_data.level = record.alert_level event_data.message_id = record.message_identifier event_data.message = message event_data.pid = record.process_identifier event_data.read_gid = record.real_group_identifier event_data.read_uid = record.real_user_identifier event_data.record_position = record_offset event_data.sender = sender # Note that the user_sid value is expected to be a string. event_data.user_sid = '{0:d}'.format(record.user_identifier) timestamp = ( (record.written_time * 1000000000) + record.written_time_nanoseconds) date_time = dfdatetime_posix_time.PosixTimeInNanoseconds( timestamp=timestamp) event = time_events.DateTimeValuesEvent( date_time, definitions.TIME_DESCRIPTION_WRITTEN) parser_mediator.ProduceEventWithEventData(event, event_data) return record.next_record_offset def _ParseRecordExtraField(self, byte_stream, file_offset): """Parses a record extra field. Args: byte_stream (bytes): byte stream. file_offset (int): offset of the record extra field relative to the start of the file. Returns: asl_record_extra_field: record extra field. Raises: ParseError: if the record extra field cannot be parsed. """ extra_field_map = self._GetDataTypeMap('asl_record_extra_field') try: record_extra_field = self._ReadStructureFromByteStream( byte_stream, file_offset, extra_field_map) except (ValueError, errors.ParseError) as exception: raise errors.ParseError(( 'Unable to parse record extra field at offset: 0x{0:08x} with error: ' '{1!s}').format(file_offset, exception)) return record_extra_field def _ParseRecordString(self, file_object, string_offset): """Parses a record string. Args: file_object (file): file-like object. string_offset (int): offset of the string relative to the start of the file. Returns: str: record string or None if string offset is 0. Raises: ParseError: if the record string cannot be parsed. """ if string_offset == 0: return None if string_offset & self._STRING_OFFSET_MSB: if (string_offset >> 60) != 8: raise errors.ParseError('Invalid inline record string flag.') string_size = (string_offset >> 56) & 0x0f if string_size >= 8: raise errors.ParseError('Invalid inline record string size.') string_data = bytes(bytearray([ string_offset >> (8 * byte_index) & 0xff for byte_index in range(6, -1, -1)])) try: return string_data[:string_size].decode('utf-8') except UnicodeDecodeError as exception: raise errors.ParseError( 'Unable to decode inline record string with error: {0!s}.'.format( exception)) record_string_map = self._GetDataTypeMap('asl_record_string') try: record_string, _ = self._ReadStructureFromFileObject( file_object, string_offset, record_string_map) except (ValueError, errors.ParseError) as exception: raise errors.ParseError(( 'Unable to parse record string at offset: 0x{0:08x} with error: ' '{1!s}').format(string_offset, exception)) return record_string.string.rstrip('\x00') @classmethod def GetFormatSpecification(cls): """Retrieves the format specification. Returns: FormatSpecification: format specification. """ format_specification = specification.FormatSpecification(cls.NAME) format_specification.AddNewSignature( b'ASL DB\x00\x00\x00\x00\x00\x00', offset=0) return format_specification def ParseFileObject(self, parser_mediator, file_object): """Parses an ASL file-like object. Args: parser_mediator (ParserMediator): mediates interactions between parsers and other components, such as storage and dfvfs. file_object (dfvfs.FileIO): file-like object. Raises: UnableToParseFile: when the file cannot be parsed. """ file_header_map = self._GetDataTypeMap('asl_file_header') try: file_header, _ = self._ReadStructureFromFileObject( file_object, 0, file_header_map) except (ValueError, errors.ParseError) as exception: raise errors.UnableToParseFile( 'Unable to parse file header with error: {0!s}'.format( exception)) is_dirty = False file_size = file_object.get_size() if file_header.first_log_entry_offset > 0: last_log_entry_offset = 0 file_offset = file_header.first_log_entry_offset while file_offset < file_size: last_log_entry_offset = file_offset try: file_offset = self._ParseRecord( parser_mediator, file_object, file_offset) except errors.ParseError as exception: parser_mediator.ProduceExtractionWarning( 'unable to parse record with error: {0!s}'.format(exception)) return if file_offset == 0: break if last_log_entry_offset != file_header.last_log_entry_offset: is_dirty = True parser_mediator.ProduceRecoveryWarning( 'last log entry offset does not match value in file header.') event_data = ASLFileEventData() event_data.format_version = file_header.format_version event_data.is_dirty = is_dirty if file_header.creation_time: date_time = dfdatetime_posix_time.PosixTime( timestamp=file_header.creation_time) else: date_time = dfdatetime_semantic_time.NotSet() event = time_events.DateTimeValuesEvent( date_time, definitions.TIME_DESCRIPTION_CREATION) parser_mediator.ProduceEventWithEventData(event, event_data) manager.ParsersManager.RegisterParser(ASLParser)
import unittest, sys from ctypes.test import need_symbol class SimpleTypesTestCase(unittest.TestCase): def setUp(self): import ctypes try: from _ctypes import set_conversion_mode except ImportError: pass else: self.prev_conv_mode = set_conversion_mode("ascii", "strict") def tearDown(self): try: from _ctypes import set_conversion_mode except ImportError: pass else: set_conversion_mode(*self.prev_conv_mode) def test_subclasses(self): from ctypes import c_void_p, c_char_p # ctypes 0.9.5 and before did overwrite from_param in SimpleType_new class CVOIDP(c_void_p): def from_param(cls, value): return value * 2 from_param = classmethod(from_param) class CCHARP(c_char_p): def from_param(cls, value): return value * 4 from_param = classmethod(from_param) self.assertEqual(CVOIDP.from_param("abc"), "abcabc") self.assertEqual(CCHARP.from_param("abc"), "abcabcabcabc") @need_symbol('c_wchar_p') def test_subclasses_c_wchar_p(self): from ctypes import c_wchar_p class CWCHARP(c_wchar_p): def from_param(cls, value): return value * 3 from_param = classmethod(from_param) self.assertEqual(CWCHARP.from_param("abc"), "abcabcabc") # XXX Replace by c_char_p tests def test_cstrings(self): from ctypes import c_char_p, byref # c_char_p.from_param on a Python String packs the string # into a cparam object s = b"123" self.assertIs(c_char_p.from_param(s)._obj, s) # new in 0.9.1: convert (encode) unicode to ascii self.assertEqual(c_char_p.from_param(b"123")._obj, b"123") self.assertRaises(TypeError, c_char_p.from_param, "123\377") self.assertRaises(TypeError, c_char_p.from_param, 42) # calling c_char_p.from_param with a c_char_p instance # returns the argument itself: a = c_char_p(b"123") self.assertIs(c_char_p.from_param(a), a) @need_symbol('c_wchar_p') def test_cw_strings(self): from ctypes import byref, c_wchar_p c_wchar_p.from_param("123") self.assertRaises(TypeError, c_wchar_p.from_param, 42) self.assertRaises(TypeError, c_wchar_p.from_param, b"123\377") pa = c_wchar_p.from_param(c_wchar_p("123")) self.assertEqual(type(pa), c_wchar_p) def test_int_pointers(self): from ctypes import c_short, c_uint, c_int, c_long, POINTER, pointer LPINT = POINTER(c_int) ## p = pointer(c_int(42)) ## x = LPINT.from_param(p) x = LPINT.from_param(pointer(c_int(42))) self.assertEqual(x.contents.value, 42) self.assertEqual(LPINT(c_int(42)).contents.value, 42) self.assertEqual(LPINT.from_param(None), None) if c_int != c_long: self.assertRaises(TypeError, LPINT.from_param, pointer(c_long(42))) self.assertRaises(TypeError, LPINT.from_param, pointer(c_uint(42))) self.assertRaises(TypeError, LPINT.from_param, pointer(c_short(42))) def test_byref_pointer(self): # The from_param class method of POINTER(typ) classes accepts what is # returned by byref(obj), it type(obj) == typ from ctypes import c_short, c_uint, c_int, c_long, pointer, POINTER, byref LPINT = POINTER(c_int) LPINT.from_param(byref(c_int(42))) self.assertRaises(TypeError, LPINT.from_param, byref(c_short(22))) if c_int != c_long: self.assertRaises(TypeError, LPINT.from_param, byref(c_long(22))) self.assertRaises(TypeError, LPINT.from_param, byref(c_uint(22))) def test_byref_pointerpointer(self): # See above from ctypes import c_short, c_uint, c_int, c_long, pointer, POINTER, byref LPLPINT = POINTER(POINTER(c_int)) LPLPINT.from_param(byref(pointer(c_int(42)))) self.assertRaises(TypeError, LPLPINT.from_param, byref(pointer(c_short(22)))) if c_int != c_long: self.assertRaises(TypeError, LPLPINT.from_param, byref(pointer(c_long(22)))) self.assertRaises(TypeError, LPLPINT.from_param, byref(pointer(c_uint(22)))) def test_array_pointers(self): from ctypes import c_short, c_uint, c_int, c_long, POINTER INTARRAY = c_int * 3 ia = INTARRAY() self.assertEqual(len(ia), 3) self.assertEqual([ia[i] for i in range(3)], [0, 0, 0]) # Pointers are only compatible with arrays containing items of # the same type! LPINT = POINTER(c_int) LPINT.from_param((c_int*3)()) self.assertRaises(TypeError, LPINT.from_param, c_short*3) self.assertRaises(TypeError, LPINT.from_param, c_long*3) self.assertRaises(TypeError, LPINT.from_param, c_uint*3) def test_noctypes_argtype(self): import _ctypes_test from ctypes import CDLL, c_void_p, ArgumentError func = CDLL(_ctypes_test.__file__)._testfunc_p_p func.restype = c_void_p # TypeError: has no from_param method self.assertRaises(TypeError, setattr, func, "argtypes", (object,)) class Adapter(object): def from_param(cls, obj): return None func.argtypes = (Adapter(),) self.assertEqual(func(None), None) self.assertEqual(func(object()), None) class Adapter(object): def from_param(cls, obj): return obj func.argtypes = (Adapter(),) # don't know how to convert parameter 1 self.assertRaises(ArgumentError, func, object()) self.assertEqual(func(c_void_p(42)), 42) class Adapter(object): def from_param(cls, obj): raise ValueError(obj) func.argtypes = (Adapter(),) # ArgumentError: argument 1: ValueError: 99 self.assertRaises(ArgumentError, func, 99) ################################################################ if __name__ == '__main__': unittest.main()
# -*- coding: utf-8 -*- ############################################################################## # # OpenERP, Open Source Management Solution # Copyright (C) 2009-today OpenERP SA (<http://www.openerp.com>) # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/> # ############################################################################## { 'name': 'Signup', 'description': """ Allow users to sign up and reset their password =============================================== """, 'author': 'OpenERP SA', 'version': '1.0', 'category': 'Authentication', 'website': 'https://www.odoo.com', 'installable': True, 'auto_install': True, 'depends': [ 'base_setup', 'email_template', 'web', ], 'data': [ 'auth_signup_data.xml', 'res_config.xml', 'res_users_view.xml', 'views/auth_signup_login.xml', ], 'bootstrap': True, }
# Based on local.py (c) 2012, Michael DeHaan <michael.dehaan@gmail.com> # Based on chroot.py (c) 2013, Maykel Moya <mmoya@speedyrails.com> # Copyright (c) 2013, Michael Scherer <misc@zarb.org> # Copyright (c) 2017 Ansible Project # GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt) from __future__ import (absolute_import, division, print_function) __metaclass__ = type DOCUMENTATION = """ author: Michael Scherer (@msherer) <misc@zarb.org> connection: funcd short_description: Use funcd to connect to target description: - This transport permits you to use Ansible over Func. - For people who have already setup func and that wish to play with ansible, this permit to move gradually to ansible without having to redo completely the setup of the network. version_added: "1.1" options: remote_addr: description: - The path of the chroot you want to access. default: inventory_hostname vars: - name: ansible_host - name: ansible_func_host """ HAVE_FUNC = False try: import func.overlord.client as fc HAVE_FUNC = True except ImportError: pass import os import tempfile import shutil from ansible.errors import AnsibleError from ansible.utils.display import Display display = Display() class Connection(object): ''' Func-based connections ''' has_pipelining = False def __init__(self, runner, host, port, *args, **kwargs): self.runner = runner self.host = host # port is unused, this go on func self.port = port def connect(self, port=None): if not HAVE_FUNC: raise AnsibleError("func is not installed") self.client = fc.Client(self.host) return self def exec_command(self, cmd, become_user=None, sudoable=False, executable='/bin/sh', in_data=None): ''' run a command on the remote minion ''' if in_data: raise AnsibleError("Internal Error: this module does not support optimized module pipelining") # totally ignores privlege escalation display.vvv("EXEC %s" % (cmd), host=self.host) p = self.client.command.run(cmd)[self.host] return (p[0], p[1], p[2]) def _normalize_path(self, path, prefix): if not path.startswith(os.path.sep): path = os.path.join(os.path.sep, path) normpath = os.path.normpath(path) return os.path.join(prefix, normpath[1:]) def put_file(self, in_path, out_path): ''' transfer a file from local to remote ''' out_path = self._normalize_path(out_path, '/') display.vvv("PUT %s TO %s" % (in_path, out_path), host=self.host) self.client.local.copyfile.send(in_path, out_path) def fetch_file(self, in_path, out_path): ''' fetch a file from remote to local ''' in_path = self._normalize_path(in_path, '/') display.vvv("FETCH %s TO %s" % (in_path, out_path), host=self.host) # need to use a tmp dir due to difference of semantic for getfile # ( who take a # directory as destination) and fetch_file, who # take a file directly tmpdir = tempfile.mkdtemp(prefix="func_ansible") self.client.local.getfile.get(in_path, tmpdir) shutil.move(os.path.join(tmpdir, self.host, os.path.basename(in_path)), out_path) shutil.rmtree(tmpdir) def close(self): ''' terminate the connection; nothing to do here ''' pass
# Copyright (C) 2003-2007, 2009, 2010 Nominum, Inc. # # Permission to use, copy, modify, and distribute this software and its # documentation for any purpose with or without fee is hereby granted, # provided that the above copyright notice and this permission notice # appear in all copies. # # THE SOFTWARE IS PROVIDED "AS IS" AND NOMINUM DISCLAIMS ALL WARRANTIES # WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF # MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL NOMINUM BE LIABLE FOR # ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES # WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN # ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT # OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE. import dns.rdtypes.nsbase class CNAME(dns.rdtypes.nsbase.NSBase): """CNAME record Note: although CNAME is officially a singleton type, dnspython allows non-singleton CNAME rdatasets because such sets have been commonly used by BIND and other nameservers for load balancing.""" pass
# -*- encoding: utf-8 -*- ############################################################################## # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program. If not, see http://www.gnu.org/licenses/. # ############################################################################## { "name": "Sale - Product variants", "version": "1.0", "depends": [ "product", "sale", "product_variants_no_automatic_creation", ], "author": "OdooMRP team," "AvanzOSC," "Serv. Tecnol. Avanzados - Pedro M. Baeza", "contributors": [ "Mikel Arregi <mikelarregi@avanzosc.es>", "Oihane Crucelaegui <oihanecrucelaegi@avanzosc.es>", "Pedro M. Baeza <pedro.baeza@serviciosbaeza.com>", "Ana Juaristi <ajuaristio@gmail.com>", ], "category": "Sales Management", "website": "http://www.odoomrp.com", "summary": "Product variants in sale management", "data": [ "security/ir.model.access.csv", "security/sale_product_variants_security.xml", 'views/res_config_view.xml', "views/sale_view.xml", ], "installable": True, "post_init_hook": "assign_product_template", }
from django.db import models class Poll(models.Model): question = models.CharField(max_length=200) def __unicode__(self): return u"Q: %s " % self.question class Choice(models.Model): poll = models.ForeignKey(Poll) choice = models.CharField(max_length=200) def __unicode__(self): return u"Choice: %s in poll %s" % (self.choice, self.poll) __test__ = {'API_TESTS':""" # Regression test for the use of None as a query value. None is interpreted as # an SQL NULL, but only in __exact queries. # Set up some initial polls and choices >>> p1 = Poll(question='Why?') >>> p1.save() >>> c1 = Choice(poll=p1, choice='Because.') >>> c1.save() >>> c2 = Choice(poll=p1, choice='Why Not?') >>> c2.save() # Exact query with value None returns nothing ("is NULL" in sql, but every 'id' # field has a value). >>> Choice.objects.filter(choice__exact=None) [] Excluding the previous result returns everything. >>> Choice.objects.exclude(choice=None).order_by('id') [<Choice: Choice: Because. in poll Q: Why? >, <Choice: Choice: Why Not? in poll Q: Why? >] # Valid query, but fails because foo isn't a keyword >>> Choice.objects.filter(foo__exact=None) Traceback (most recent call last): ... FieldError: Cannot resolve keyword 'foo' into field. Choices are: choice, id, poll # Can't use None on anything other than __exact >>> Choice.objects.filter(id__gt=None) Traceback (most recent call last): ... ValueError: Cannot use None as a query value # Can't use None on anything other than __exact >>> Choice.objects.filter(foo__gt=None) Traceback (most recent call last): ... ValueError: Cannot use None as a query value # Related managers use __exact=None implicitly if the object hasn't been saved. >>> p2 = Poll(question="How?") >>> p2.choice_set.all() [] """}
#!/usr/bin/env python # # Copyright 2012 Free Software Foundation, Inc. # # This file is part of GNU Radio # # GNU Radio is free software; you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation; either version 3, or (at your option) # any later version. # # GNU Radio is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with GNU Radio; see the file COPYING. If not, write to # the Free Software Foundation, Inc., 51 Franklin Street, # Boston, MA 02110-1301, USA. # from gnuradio import gr, gr_unittest import digital_swig as digital class test_probe_density(gr_unittest.TestCase): def setUp(self): self.tb = gr.top_block() def tearDown(self): self.tb = None def test_001(self): src_data = [0, 1, 0, 1] expected_data = 1 src = gr.vector_source_b (src_data) op = digital.probe_density_b(1) self.tb.connect (src, op) self.tb.run () result_data = op.density() self.assertEqual (expected_data, result_data) def test_002(self): src_data = [1, 1, 1, 1] expected_data = 1 src = gr.vector_source_b (src_data) op = digital.probe_density_b(0.01) self.tb.connect (src, op) self.tb.run () result_data = op.density() self.assertEqual (expected_data, result_data) def test_003(self): src_data = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1] expected_data = 0.95243 src = gr.vector_source_b (src_data) op = digital.probe_density_b(0.01) self.tb.connect (src, op) self.tb.run () result_data = op.density() print result_data self.assertAlmostEqual (expected_data, result_data, 5) if __name__ == '__main__': gr_unittest.run(test_probe_density, "test_probe_density.xml")
#!/usr/bin/env python # Copyright (c) 2016 The Khronos Group Inc. # # Permission is hereby granted, free of charge, to any person obtaining a # copy of this software and/or associated documentation files (the # "Materials"), to deal in the Materials without restriction, including # without limitation the rights to use, copy, modify, merge, publish, # distribute, sublicense, and/or sell copies of the Materials, and to # permit persons to whom the Materials are furnished to do so, subject to # the following conditions: # # The above copyright notice and this permission notice shall be included # in all copies or substantial portions of the Materials. # # THE MATERIALS ARE PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, # EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF # MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. # IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY # CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, # TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE # MATERIALS OR THE USE OR OTHER DEALINGS IN THE MATERIALS. """ Generator for uniformbuffers* tests. This file needs to be run in its folder. """ import sys _DO_NOT_EDIT_WARNING = """<!-- This file is auto-generated from uniformbuffers_test_generator.py DO NOT EDIT! --> """ _HTML_TEMPLATE = """<html> <head> <meta http-equiv="Content-Type" content="text/html; charset=utf-8"> <title>WebGL Uniform Block Conformance Tests</title> <link rel="stylesheet" href="../../../../resources/js-test-style.css"/> <script src="../../../../js/js-test-pre.js"></script> <script src="../../../../js/webgl-test-utils.js"></script> <script src="../../../../closure-library/closure/goog/base.js"></script> <script src="../../../deqp-deps.js"></script> <script>goog.require('functional.gles3.es3fUniformBlockTests');</script> </head> <body> <div id="description"></div> <div id="console"></div> <canvas id="canvas" width="200" height="100"> </canvas> <script> var wtu = WebGLTestUtils; var gl = wtu.create3DContext('canvas', null, 2); functional.gles3.es3fUniformBlockTests.run([%(start)s, %(end)s]); </script> </body> </html> """ _GROUPS = [ 'single_basic_type', 'single_basic_array', 'single_struct', 'single_struct_array', 'single_nested_struct', 'single_nested_struct_array', 'instance_array_basic_type', 'multi_basic_types', 'multi_nested_struct', 'random', ] def GenerateFilename(group): """Generate test filename.""" filename = group filename += ".html" return filename def WriteTest(filename, start, end): """Write one test.""" file = open(filename, "wb") file.write(_DO_NOT_EDIT_WARNING) file.write(_HTML_TEMPLATE % { 'start': start, 'end': end }) file.close def GenerateTests(): """Generate all tests.""" filelist = [] for ii in range(len(_GROUPS)): filename = GenerateFilename(_GROUPS[ii]) filelist.append(filename) WriteTest(filename, ii, ii + 1) return filelist def GenerateTestList(filelist): file = open("00_test_list.txt", "wb") file.write('\n'.join(filelist)) file.close def main(argv): """This is the main function.""" filelist = GenerateTests() GenerateTestList(filelist) if __name__ == '__main__': sys.exit(main(sys.argv[1:]))
""" Tests for credit courses on the student dashboard. """ import unittest import datetime from mock import patch import pytz from django.conf import settings from django.core.urlresolvers import reverse from django.test.utils import override_settings from xmodule.modulestore.tests.django_utils import ModuleStoreTestCase from xmodule.modulestore.tests.factories import CourseFactory from student.models import CourseEnrollmentAttribute from student.tests.factories import UserFactory, CourseEnrollmentFactory from openedx.core.djangoapps.credit.models import CreditCourse, CreditProvider, CreditEligibility from openedx.core.djangoapps.credit import api as credit_api TEST_CREDIT_PROVIDER_SECRET_KEY = "931433d583c84ca7ba41784bad3232e6" @unittest.skipUnless(settings.ROOT_URLCONF == 'lms.urls', 'Test only valid in lms') @override_settings(CREDIT_PROVIDER_SECRET_KEYS={ "hogwarts": TEST_CREDIT_PROVIDER_SECRET_KEY, }) @patch.dict(settings.FEATURES, {"ENABLE_CREDIT_ELIGIBILITY": True}) class CreditCourseDashboardTest(ModuleStoreTestCase): """ Tests for credit courses on the student dashboard. """ USERNAME = "ron" PASSWORD = "mobiliarbus" PROVIDER_ID = "hogwarts" PROVIDER_NAME = "Hogwarts School of Witchcraft and Wizardry" PROVIDER_STATUS_URL = "http://credit.example.com/status" def setUp(self): """Create a course and an enrollment. """ super(CreditCourseDashboardTest, self).setUp() # Create a user and log in self.user = UserFactory.create(username=self.USERNAME, password=self.PASSWORD) result = self.client.login(username=self.USERNAME, password=self.PASSWORD) self.assertTrue(result, msg="Could not log in") # Create a course and configure it as a credit course self.course = CourseFactory() CreditCourse.objects.create(course_key=self.course.id, enabled=True) # pylint: disable=no-member # Configure a credit provider CreditProvider.objects.create( provider_id=self.PROVIDER_ID, display_name=self.PROVIDER_NAME, provider_status_url=self.PROVIDER_STATUS_URL, enable_integration=True, ) # Configure a single credit requirement (minimum passing grade) credit_api.set_credit_requirements( self.course.id, # pylint: disable=no-member [ { "namespace": "grade", "name": "grade", "display_name": "Final Grade", "criteria": { "min_grade": 0.8 } } ] ) # Enroll the user in the course as "verified" self.enrollment = CourseEnrollmentFactory( user=self.user, course_id=self.course.id, # pylint: disable=no-member mode="verified" ) def test_not_eligible_for_credit(self): # The user is not yet eligible for credit, so no additional information should be displayed on the dashboard. response = self._load_dashboard() self.assertNotContains(response, "credit-eligibility-msg") self.assertNotContains(response, "purchase-credit-btn") def test_eligible_for_credit(self): # Simulate that the user has completed the only requirement in the course # so the user is eligible for credit. self._make_eligible() # The user should have the option to purchase credit response = self._load_dashboard() self.assertContains(response, "credit-eligibility-msg") self.assertContains(response, "purchase-credit-btn") # Move the eligibility deadline so it's within 30 days eligibility = CreditEligibility.objects.get(username=self.USERNAME) eligibility.deadline = datetime.datetime.now(pytz.UTC) + datetime.timedelta(days=29) eligibility.save() # The user should still have the option to purchase credit, # but there should also be a message urging the user to purchase soon. response = self._load_dashboard() self.assertContains(response, "credit-eligibility-msg") self.assertContains(response, "purchase-credit-btn") def test_purchased_credit(self): # Simulate that the user has purchased credit, but has not # yet initiated a request to the credit provider self._make_eligible() self._purchase_credit() response = self._load_dashboard() self.assertContains(response, "credit-request-not-started-msg") def test_purchased_credit_and_request_pending(self): # Simulate that the user has purchased credit and initiated a request, # but we haven't yet heard back from the credit provider. self._make_eligible() self._purchase_credit() self._initiate_request() # Expect that the user's status is "pending" response = self._load_dashboard() self.assertContains(response, "credit-request-pending-msg") def test_purchased_credit_and_request_approved(self): # Simulate that the user has purchased credit and initiated a request, # and had that request approved by the credit provider self._make_eligible() self._purchase_credit() request_uuid = self._initiate_request() self._set_request_status(request_uuid, "approved") # Expect that the user's status is "approved" response = self._load_dashboard() self.assertContains(response, "credit-request-approved-msg") def test_purchased_credit_and_request_rejected(self): # Simulate that the user has purchased credit and initiated a request, # and had that request rejected by the credit provider self._make_eligible() self._purchase_credit() request_uuid = self._initiate_request() self._set_request_status(request_uuid, "rejected") # Expect that the user's status is "approved" response = self._load_dashboard() self.assertContains(response, "credit-request-rejected-msg") def test_credit_status_error(self): # Simulate an error condition: the user has a credit enrollment # but no enrollment attribute indicating which provider the user # purchased credit from. self._make_eligible() self._purchase_credit() CourseEnrollmentAttribute.objects.all().delete() # Expect an error message response = self._load_dashboard() self.assertContains(response, "credit-error-msg") def _load_dashboard(self): """Load the student dashboard and return the HttpResponse. """ return self.client.get(reverse("dashboard")) def _make_eligible(self): """Make the user eligible for credit in the course. """ credit_api.set_credit_requirement_status( self.USERNAME, self.course.id, # pylint: disable=no-member "grade", "grade", status="satisfied", reason={ "final_grade": 0.95 } ) def _purchase_credit(self): """Purchase credit from a provider in the course. """ self.enrollment.mode = "credit" self.enrollment.save() # pylint: disable=no-member CourseEnrollmentAttribute.objects.create( enrollment=self.enrollment, namespace="credit", name="provider_id", value=self.PROVIDER_ID, ) def _initiate_request(self): """Initiate a request for credit from a provider. """ request = credit_api.create_credit_request( self.course.id, # pylint: disable=no-member self.PROVIDER_ID, self.USERNAME ) return request["parameters"]["request_uuid"] def _set_request_status(self, uuid, status): """Set the status of a request for credit, simulating the notification from the provider. """ credit_api.update_credit_request_status(uuid, self.PROVIDER_ID, status)
# # (c) 2015 Peter Sprygada, <psprygada@ansible.com> # # Copyright (c) 2016 Dell Inc. # # This code is part of Ansible, but is an independent component. # This particular file snippet, and this file snippet only, is BSD licensed. # Modules you write using this snippet, which is embedded dynamically by Ansible # still belong to the author of the module, and may assign their own license # to the complete work. # # Redistribution and use in source and binary forms, with or without modification, # are permitted provided that the following conditions are met: # # * Redistributions of source code must retain the above copyright # notice, this list of conditions and the following disclaimer. # * Redistributions in binary form must reproduce the above copyright notice, # this list of conditions and the following disclaimer in the documentation # and/or other materials provided with the distribution. # # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND # ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED # WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. # IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, # INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, # PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT # LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE # USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. # import re from ansible.module_utils.shell import CliBase from ansible.module_utils.network import register_transport, to_list, Command from ansible.module_utils.netcfg import NetworkConfig, ConfigLine def get_config(module): contents = module.params['config'] if not contents: contents = module.config.get_config() module.params['config'] = contents return NetworkConfig(indent=1, contents=contents[0]) else: return NetworkConfig(indent=1, contents=contents) def get_sublevel_config(running_config, module): contents = list() current_config_contents = list() obj = running_config.get_object(module.params['parents']) if obj: contents = obj.children contents[:0] = module.params['parents'] indent = 0 for c in contents: if isinstance(c, str): current_config_contents.append(c.rjust(len(c) + indent, ' ')) if isinstance(c, ConfigLine): current_config_contents.append(c.raw) indent = indent + 1 sublevel_config = '\n'.join(current_config_contents) return sublevel_config class Cli(CliBase): NET_PASSWD_RE = re.compile(r"[\r\n]?password:\s?$", re.I) WARNING_PROMPTS_RE = [ re.compile(r"[\r\n]?\[confirm yes/no\]:\s?$"), re.compile(r"[\r\n]?\[y/n\]:\s?$"), re.compile(r"[\r\n]?\[yes/no\]:\s?$") ] CLI_PROMPTS_RE = [ re.compile(r"[\r\n]?[\w+\-\.:\/\[\]]+(?:\([^\)]+\)){,3}(?:>|#) ?$"), re.compile(r"\[\w+\@[\w\-\.]+(?: [^\]])\] ?[>#\$] ?$") ] CLI_ERRORS_RE = [ re.compile(r"% ?Error: (?:(?!\bdoes not exist\b)(?!\balready exists\b)(?!\bHost not found\b)(?!\bnot active\b).)*$"), re.compile(r"% ?Bad secret"), re.compile(r"invalid input", re.I), re.compile(r"(?:incomplete|ambiguous) command", re.I), re.compile(r"connection timed out", re.I), re.compile(r"'[^']' +returned error code: ?\d+"), ] def connect(self, params, **kwargs): super(Cli, self).connect(params, kickstart=False, **kwargs) self.shell.send('terminal length 0') def authorize(self, params, **kwargs): passwd = params['auth_pass'] self.run_commands( Command('enable', prompt=self.NET_PASSWD_RE, response=passwd) ) def configure(self, commands, **kwargs): cmds = ['configure terminal'] cmdlist = list() for c in to_list(commands): cmd = Command(c, prompt=self.WARNING_PROMPTS_RE, response='yes') cmdlist.append(cmd) cmds.extend(cmdlist) cmds.append('end') responses = self.execute(cmds) responses.pop(0) return responses def get_config(self, **kwargs): return self.execute(['show running-config']) def load_config(self, commands, **kwargs): return self.configure(commands) def save_config(self): cmdlist = list() cmd = 'copy running-config startup-config' cmdlist.append(Command(cmd, prompt=self.WARNING_PROMPTS_RE, response='yes')) self.execute(cmdlist) Cli = register_transport('cli', default=True)(Cli)
""" Finds and returns the latest bitcoin price Usage Examples: - "What is the price of bitcoin?" - "How much is a bitcoin worth?" """ from athena.classes.module import Module from athena.classes.task import ActiveTask from athena import brain class QuitTask(ActiveTask): def __init__(self): super().__init__(patterns=[r'\b(athena )?(quit|stop)\b.*']) def action(self, text): brain.inst.quit() class ListModulesTask(ActiveTask): def __init__(self): super().__init__(words=['list modules', 'list mods']) def action(self, text): brain.inst.list_mods() class ToggleModuleTask(ActiveTask): def __init__(self): super().__init__(patterns=[r'.*\b(enable|add|disable|remove) (.*)']) self.groups = {1: 'enable', 2: 'module'} def match(self, text): return self.match_and_save_groups(text, self.groups) def action(self, text): mod_name = self.module.lower().strip().replace(' ', '_') if 'disable' in self.enable.lower() or 'remove' in self.enable.lower(): brain.inst.disable_mod(mod_name) else: brain.inst.enable_mod(mod_name) class AthenaControl(Module): def __init__(self): tasks = [QuitTask(), ListModulesTask(), ToggleModuleTask()] super().__init__('athena_control', tasks, priority=3)
# Copyright (c) 2012 OpenStack Foundation. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or # implied. # See the License for the specific language governing permissions and # limitations under the License. import mock from neutron import manager from neutron.tests.unit.ryu import fake_ryu from neutron.tests.unit import test_db_plugin as test_plugin class RyuPluginV2TestCase(test_plugin.NeutronDbPluginV2TestCase): _plugin_name = 'neutron.plugins.ryu.ryu_neutron_plugin.RyuNeutronPluginV2' def setUp(self): self.ryu_patcher = fake_ryu.patch_fake_ryu_client() self.ryu_patcher.start() super(RyuPluginV2TestCase, self).setUp(self._plugin_name) self.addCleanup(self.ryu_patcher.stop) plugin = manager.NeutronManager.get_plugin() plugin.notifier = mock.Mock() class TestRyuBasicGet(test_plugin.TestBasicGet, RyuPluginV2TestCase): pass class TestRyuV2HTTPResponse(test_plugin.TestV2HTTPResponse, RyuPluginV2TestCase): pass class TestRyuPortsV2(test_plugin.TestPortsV2, RyuPluginV2TestCase): pass class TestRyuNetworksV2(test_plugin.TestNetworksV2, RyuPluginV2TestCase): pass
import random import string from django.db import models from django.utils import six from django.utils.encoding import python_2_unicode_compatible @python_2_unicode_compatible class MyWrapper(object): def __init__(self, value): self.value = value def __repr__(self): return "<%s: %s>" % (self.__class__.__name__, self.value) def __str__(self): return self.value def __eq__(self, other): if isinstance(other, self.__class__): return self.value == other.value return self.value == other class MyAutoField(six.with_metaclass(models.SubfieldBase, models.CharField)): def __init__(self, *args, **kwargs): kwargs['max_length'] = 10 super(MyAutoField, self).__init__(*args, **kwargs) def pre_save(self, instance, add): value = getattr(instance, self.attname, None) if not value: value = MyWrapper(''.join(random.sample(string.ascii_lowercase, 10))) setattr(instance, self.attname, value) return value def to_python(self, value): if not value: return if not isinstance(value, MyWrapper): value = MyWrapper(value) return value def get_db_prep_save(self, value, connection): if not value: return if isinstance(value, MyWrapper): return six.text_type(value) return value def get_db_prep_value(self, value, connection, prepared=False): if not value: return if isinstance(value, MyWrapper): return six.text_type(value) return value
"""Wrapper for mod_fcgid, for use as a CherryPy HTTP server when testing. To autostart fcgid, the "apache" executable or script must be on your system path, or you must override the global APACHE_PATH. On some platforms, "apache" may be called "apachectl", "apache2ctl", or "httpd"--create a symlink to them if needed. You'll also need the WSGIServer from flup.servers. See http://projects.amor.org/misc/wiki/ModPythonGateway KNOWN BUGS ========== 1. Apache processes Range headers automatically; CherryPy's truncated output is then truncated again by Apache. See test_core.testRanges. This was worked around in http://www.cherrypy.org/changeset/1319. 2. Apache does not allow custom HTTP methods like CONNECT as per the spec. See test_core.testHTTPMethods. 3. Max request header and body settings do not work with Apache. 4. Apache replaces status "reason phrases" automatically. For example, CherryPy may set "304 Not modified" but Apache will write out "304 Not Modified" (capital "M"). 5. Apache does not allow custom error codes as per the spec. 6. Apache (or perhaps modpython, or modpython_gateway) unquotes %xx in the Request-URI too early. 7. mod_python will not read request bodies which use the "chunked" transfer-coding (it passes REQUEST_CHUNKED_ERROR to ap_setup_client_block instead of REQUEST_CHUNKED_DECHUNK, see Apache2's http_protocol.c and mod_python's requestobject.c). 8. Apache will output a "Content-Length: 0" response header even if there's no response entity body. This isn't really a bug; it just differs from the CherryPy default. """ import os curdir = os.path.join(os.getcwd(), os.path.dirname(__file__)) import re import sys import time import cherrypy from cherrypy.process import plugins, servers from cherrypy.test import test def read_process(cmd, args=""): pipein, pipeout = os.popen4("%s %s" % (cmd, args)) try: firstline = pipeout.readline() if (re.search(r"(not recognized|No such file|not found)", firstline, re.IGNORECASE)): raise IOError('%s must be on your system path.' % cmd) output = firstline + pipeout.read() finally: pipeout.close() return output APACHE_PATH = "httpd" CONF_PATH = "fcgi.conf" conf_fcgid = """ # Apache2 server conf file for testing CherryPy with mod_fcgid. DocumentRoot "%(root)s" ServerName 127.0.0.1 Listen %(port)s LoadModule fastcgi_module modules/mod_fastcgi.dll LoadModule rewrite_module modules/mod_rewrite.so Options ExecCGI SetHandler fastcgi-script RewriteEngine On RewriteRule ^(.*)$ /fastcgi.pyc [L] FastCgiExternalServer "%(server)s" -host 127.0.0.1:4000 """ class ModFCGISupervisor(test.LocalSupervisor): using_apache = True using_wsgi = True template = conf_fcgid def __str__(self): return "FCGI Server on %s:%s" % (self.host, self.port) def start(self, modulename): cherrypy.server.httpserver = servers.FlupFCGIServer( application=cherrypy.tree, bindAddress=('127.0.0.1', 4000)) cherrypy.server.httpserver.bind_addr = ('127.0.0.1', 4000) # For FCGI, we both start apache... self.start_apache() # ...and our local server test.LocalServer.start(self, modulename) def start_apache(self): fcgiconf = CONF_PATH if not os.path.isabs(fcgiconf): fcgiconf = os.path.join(curdir, fcgiconf) # Write the Apache conf file. f = open(fcgiconf, 'wb') try: server = repr(os.path.join(curdir, 'fastcgi.pyc'))[1:-1] output = self.template % {'port': self.port, 'root': curdir, 'server': server} output = output.replace('\r\n', '\n') f.write(output) finally: f.close() result = read_process(APACHE_PATH, "-k start -f %s" % fcgiconf) if result: print(result) def stop(self): """Gracefully shutdown a server that is serving forever.""" read_process(APACHE_PATH, "-k stop") test.LocalServer.stop(self) def sync_apps(self): cherrypy.server.httpserver.fcgiserver.application = self.get_app()
# -*- coding: utf-8 -*- # Form implementation generated from reading ui file 'new_dev.ui' # # Created: Sun Jun 8 13:50:08 2014 # by: pyside-uic 0.2.15 running on PySide 1.2.1 # # WARNING! All changes made in this file will be lost! from PySide import QtCore, QtGui class Ui_Dialog(object): def setupUi(self, Dialog): Dialog.setObjectName("Dialog") Dialog.resize(619, 391) self.horizontalLayout_2 = QtGui.QHBoxLayout(Dialog) self.horizontalLayout_2.setObjectName("horizontalLayout_2") self.horizontalLayout = QtGui.QHBoxLayout() self.horizontalLayout.setObjectName("horizontalLayout") self.listWidget = QtGui.QListWidget(Dialog) self.listWidget.setObjectName("listWidget") self.horizontalLayout.addWidget(self.listWidget) self.horizontalLayout_2.addLayout(self.horizontalLayout) self.buttonBox = QtGui.QDialogButtonBox(Dialog) self.buttonBox.setOrientation(QtCore.Qt.Vertical) self.buttonBox.setStandardButtons(QtGui.QDialogButtonBox.Cancel|QtGui.QDialogButtonBox.Ok) self.buttonBox.setObjectName("buttonBox") self.horizontalLayout_2.addWidget(self.buttonBox) self.retranslateUi(Dialog) QtCore.QObject.connect(self.buttonBox, QtCore.SIGNAL("accepted()"), Dialog.accept) QtCore.QObject.connect(self.buttonBox, QtCore.SIGNAL("rejected()"), Dialog.reject) QtCore.QMetaObject.connectSlotsByName(Dialog) def retranslateUi(self, Dialog): Dialog.setWindowTitle(QtGui.QApplication.translate("Dialog", "Dialog", None, QtGui.QApplication.UnicodeUTF8))
import numpy as np import matplotlib.pyplot as plt def plot_decision_function(classifier, fea, gnd, title): ''' plot the decision function in 2-d plane classifiers: the svm models fea: array like, shape = (smp_num, fea_num) gnd: array like, shape = (smp_num,) title: title of plot ''' fea_min = fea.min(axis = 0) fea_max = fea.max(axis = 0) mesh_num = 100 # meshgrid xx, yy = np.meshgrid(np.linspace(fea_min[0], fea_max[0], mesh_num), \ np.linspace(fea_min[1], fea_max[1], mesh_num)) Z = classifier.decision_function(np.c_[xx.ravel(), yy.ravel()], last_model_flag = False) Z_first = Z[:, 0].copy() Z_last = Z[:, -1].copy() Z_first = Z_first.reshape(xx.shape) Z_last = Z_last.reshape(xx.shape) del Z # plot the line, the points leg_svm = plt.contour(xx, yy, Z_first, levels = [0.0], colors = 'k') leg_rsvm = plt.contour(xx, yy, Z_last, levels = [0.0], colors = 'r') posi_index = gnd == 1 nega_index = gnd == -1 marker_size = 70 plt.scatter(fea[:, 0], fea[:, 1], marker = 'o', \ s = classifier.smp_weights_mat[:, -1] * marker_size * 4, c = 'w', alpha = 1.0, edgecolors = 'm', label = 'weights') plt.scatter(fea[posi_index, 0], fea[posi_index, 1], marker = '^', s = marker_size, c = 'g', alpha = 0.8, label = 'posi') plt.scatter(fea[nega_index, 0], fea[nega_index, 1], marker = 'x', s = marker_size, c = 'b', label = 'nega') leg_svm.collections[0].set_label('svm') leg_rsvm.collections[0].set_label('rsvm') plt.legend(loc = 'upper left') plt.axis('on') plt.title(title)
from siskin.openurl import openurl_parameters_from_intermediateschema def test_openurl_from_intermediateschema(): cases = ( ('empty doc', {}, {}), ( 'title only', { 'rft.atitle': 'empty doc' }, { 'ctx_enc': 'info:ofi/enc:UTF-8', 'ctx_ver': 'Z39.88-2004', 'rfr_id': 'info:sid/www.ub.uni-leipzig.de:generator', 'rft.atitle': 'empty doc', 'url_ver': 'Z39.88-2004', }, ), ( 'title and date', { 'rft.atitle': 'title and date', 'rft.date': '2018-10-10', }, { 'ctx_enc': 'info:ofi/enc:UTF-8', 'ctx_ver': 'Z39.88-2004', 'rfr_id': 'info:sid/www.ub.uni-leipzig.de:generator', 'rft.date': '2018-10-10', 'rft.atitle': 'title and date', 'url_ver': 'Z39.88-2004', }, ), ( 'title and date, language', { 'languages': ['eng', 'fra'], 'rft.atitle': 'title and date, language', 'rft.date': '2018-10-10', }, { 'ctx_enc': 'info:ofi/enc:UTF-8', 'ctx_ver': 'Z39.88-2004', 'rfr_id': 'info:sid/www.ub.uni-leipzig.de:generator', 'rft.date': '2018-10-10', 'rft.language': 'eng', 'rft.atitle': 'title and date, language', 'url_ver': 'Z39.88-2004', }, ), ( 'title and date, language, book', { 'languages': ['eng', 'fra'], 'rft.atitle': 'Hello', 'rft.date': '2018-10-10', 'rft.genre': 'book', }, { 'ctx_enc': 'info:ofi/enc:UTF-8', 'ctx_ver': 'Z39.88-2004', 'rfr_id': 'info:sid/www.ub.uni-leipzig.de:generator', 'rft.atitle': 'Hello', 'rft.btitle': 'Hello', 'rft.date': '2018-10-10', 'rft.genre': 'book', 'rft.language': 'eng', 'rft_val_fmt': 'info:ofi/fmt:kev:mtx:book', 'url_ver': 'Z39.88-2004', }, ), ( 'crossref-1', { "finc.format": "ElectronicArticle", "finc.mega_collection": ["Springer Science + Business Media (CrossRef)"], "finc.id": "ai-49-aHR0cDovL2R4LmRvaS5vcmcvMTAuMTAxNi9qLm51cnguMjAwNi4wNS4wMjU", "finc.source_id": "49", "ris.type": "EJOUR", "rft.atitle": "An Analysis of Correlations Among 4 Outcome Scales Employed in Clinical Trials of Patients With Major Depressive Disorder", "rft.epage": "412", "rft.genre": "article", "rft.issn": ["1545-5343"], "rft.issue": "3", "rft.jtitle": "NeuroRX", "rft.tpages": "2", "rft.pages": "411-412", "rft.pub": ["Springer Science + Business Media"], "rft.date": "2006-07-01", "x.date": "2006-07-01T00:00:00Z", "rft.spage": "411", "rft.volume": "3", "authors": [{ "rft.aulast": "JIANG", "rft.aufirst": "Q" }, { "rft.aulast": "AHMED", "rft.aufirst": "S" }, { "rft.aulast": "PEDERSEN", "rft.aufirst": "R" }, { "rft.aulast": "MUSGNUNG", "rft.aufirst": "J" }, { "rft.aulast": "ENTSUAH", "rft.aufirst": "R" }], "doi": "10.1016/j.nurx.2006.05.025", "languages": ["eng"], "url": ["http://dx.doi.org/10.1016/j.nurx.2006.05.025"], "version": "0.9", "x.subjects": ["Pharmacology (medical)"], "x.type": "journal-article" }, { 'ctx_enc': 'info:ofi/enc:UTF-8', 'ctx_ver': 'Z39.88-2004', 'rfr_id': 'info:sid/www.ub.uni-leipzig.de:generator', 'rft.atitle': 'An Analysis of Correlations Among 4 Outcome Scales Employed in Clinical Trials of Patients With Major Depressive Disorder', 'rft.aufirst': 'Q', 'rft.aulast': 'JIANG', 'rft.date': '2006-07-01', 'rft.epage': '412', 'rft.genre': 'article', 'rft.issn': '1545-5343', 'rft.issue': '3', 'rft.jtitle': 'NeuroRX', 'rft.language': 'eng', 'rft.pages': '411-412', 'rft.spage': '411', 'rft.volume': '3', 'rft_id': 'info:doi/10.1016/j.nurx.2006.05.025', 'url_ver': 'Z39.88-2004', }, ), ) for _, doc, want in cases: result = openurl_parameters_from_intermediateschema(doc) assert result == want
# Copyright 2017 Google Inc. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. """Tests the Bigquery client.""" import mock import httplib2 from googleapiclient.errors import HttpError from google.apputils import basetest from google.cloud.security.common.gcp_api import bigquery as bq from google.cloud.security.common.gcp_api import _base_client as _base_client from google.cloud.security.common.gcp_api import errors as api_errors from tests.common.gcp_api.test_data import fake_bigquery as fbq class BigqueryTestCase(basetest.TestCase): """Test the Bigquery API Client.""" MAX_BIGQUERY_API_CALLS_PER_100_SECONDS = 88888 @mock.patch.object(bq, 'FLAGS') @mock.patch.object(_base_client.BaseClient, '__init__', autospec=True) def setUp(self, mock_base_client, mock_flags): """Set up.""" mock_flags.max_bigquery_api_calls_per_100_seconds = ( self.MAX_BIGQUERY_API_CALLS_PER_100_SECONDS) self.bq_api_client = bq.BigQueryClient() self.http_response = httplib2.Response( {'status': '400', 'content-type': 'application/json'} ) def test_api_client_is_initialized(self): """Test that the api client is initialized.""" self.assertEquals( self.MAX_BIGQUERY_API_CALLS_PER_100_SECONDS, self.bq_api_client.rate_limiter.max_calls) self.assertEquals( bq.BigQueryClient.DEFAULT_QUOTA_TIMESPAN_PER_SECONDS, self.bq_api_client.rate_limiter.period) def test_get_bigquery_projectids_raises(self): mock_bq_stub = mock.MagicMock() self.bq_api_client.service = mock.MagicMock() self.bq_api_client.service.projects.return_value = mock_bq_stub self.bq_api_client._execute = mock.MagicMock( side_effect=HttpError(self.http_response, '{}') ) with self.assertRaises(api_errors.ApiExecutionError): self.bq_api_client.get_bigquery_projectids() def test_get_bigquery_projectids(self): mock_bq_stub = mock.MagicMock() self.bq_api_client.service = mock.MagicMock() self.bq_api_client.service.projects.return_value = mock_bq_stub self.bq_api_client._build_paged_result = mock.MagicMock( return_value=fbq.PROJECTS_LIST_REQUEST_RESPONSE ) return_value = self.bq_api_client.get_bigquery_projectids() self.assertListEqual(return_value, fbq.PROJECTS_LIST_EXPECTED) def test_get_datasets_for_projectid_raises(self): mock_bq_stub = mock.MagicMock() self.bq_api_client.service = mock.MagicMock() self.bq_api_client.service.datasets.return_value = mock_bq_stub self.bq_api_client._execute = mock.MagicMock( side_effect=HttpError(self.http_response, '{}') ) with self.assertRaises(api_errors.ApiExecutionError): self.bq_api_client.get_datasets_for_projectid(fbq.PROJECT_IDS[0]) def test_getdatasets_for_projectid(self): mock_bq_stub = mock.MagicMock() self.bq_api_client.service = mock.MagicMock() self.bq_api_client.service.datasets.return_value = mock_bq_stub self.bq_api_client._build_paged_result = mock.MagicMock( return_value=fbq.DATASETS_LIST_REQUEST_RESPONSE ) return_value = self.bq_api_client.get_datasets_for_projectid('') self.assertListEqual(return_value, fbq.DATASETS_LIST_EXPECTED) def test_get_dataset_access_raises(self): mock_bq_stub = mock.MagicMock() self.bq_api_client.service = mock.MagicMock() self.bq_api_client.service.datasets.return_value = mock_bq_stub self.bq_api_client._execute = mock.MagicMock( side_effect=HttpError(self.http_response, '{}') ) with self.assertRaises(api_errors.ApiExecutionError): self.bq_api_client.get_dataset_access(fbq.PROJECT_IDS[0], fbq.DATASET_ID) def test_get_dataset_access(self): mock_bq_stub = mock.MagicMock() self.bq_api_client.service = mock.MagicMock() self.bq_api_client.service.datasets.return_value = mock_bq_stub self.bq_api_client._build_paged_result = mock.MagicMock( return_value=fbq.DATASETS_GET_REQUEST_RESPONSE ) return_value = self.bq_api_client.get_dataset_access('','') self.assertListEqual(return_value, fbq.DATASETS_GET_EXPECTED) if __name__ == '__main__': basetest.main()
from __future__ import print_function import json import os import os.path import re import sys import warnings from collections import defaultdict from distutils.command.build_scripts import build_scripts as BuildScripts from distutils.command.sdist import sdist as SDist try: from setuptools import setup, find_packages from setuptools.command.build_py import build_py as BuildPy from setuptools.command.install_lib import install_lib as InstallLib from setuptools.command.install_scripts import install_scripts as InstallScripts except ImportError: print("Ansible now needs setuptools in order to build. Install it using" " your package manager (usually python-setuptools) or via pip (pip" " install setuptools).", file=sys.stderr) sys.exit(1) sys.path.insert(0, os.path.abspath('lib')) from ansible.release import __version__, __author__ SYMLINK_CACHE = 'SYMLINK_CACHE.json' def _find_symlinks(topdir, extension=''): """Find symlinks that should be maintained Maintained symlinks exist in the bin dir or are modules which have aliases. Our heuristic is that they are a link in a certain path which point to a file in the same directory. """ symlinks = defaultdict(list) for base_path, dirs, files in os.walk(topdir): for filename in files: filepath = os.path.join(base_path, filename) if os.path.islink(filepath) and filename.endswith(extension): target = os.readlink(filepath) if os.path.dirname(target) == '': link = filepath[len(topdir):] if link.startswith('/'): link = link[1:] symlinks[os.path.basename(target)].append(link) return symlinks def _cache_symlinks(symlink_data): with open(SYMLINK_CACHE, 'w') as f: json.dump(symlink_data, f) def _maintain_symlinks(symlink_type, base_path): """Switch a real file into a symlink""" try: # Try the cache first because going from git checkout to sdist is the # only time we know that we're going to cache correctly with open(SYMLINK_CACHE, 'r') as f: symlink_data = json.load(f) except (IOError, OSError) as e: # IOError on py2, OSError on py3. Both have errno if e.errno == 2: # SYMLINKS_CACHE doesn't exist. Fallback to trying to create the # cache now. Will work if we're running directly from a git # checkout or from an sdist created earlier. symlink_data = {'script': _find_symlinks('bin'), 'library': _find_symlinks('lib', '.py'), } # Sanity check that something we know should be a symlink was # found. We'll take that to mean that the current directory # structure properly reflects symlinks in the git repo if 'ansible-playbook' in symlink_data['script']['ansible']: _cache_symlinks(symlink_data) else: raise RuntimeError( "Pregenerated symlink list was not present and expected " "symlinks in ./bin were missing or broken. " "Perhaps this isn't a git checkout?" ) else: raise symlinks = symlink_data[symlink_type] for source in symlinks: for dest in symlinks[source]: dest_path = os.path.join(base_path, dest) if not os.path.islink(dest_path): try: os.unlink(dest_path) except OSError as e: if e.errno == 2: # File does not exist which is all we wanted pass os.symlink(source, dest_path) class BuildPyCommand(BuildPy): def run(self): BuildPy.run(self) _maintain_symlinks('library', self.build_lib) class BuildScriptsCommand(BuildScripts): def run(self): BuildScripts.run(self) _maintain_symlinks('script', self.build_dir) class InstallLibCommand(InstallLib): def run(self): InstallLib.run(self) _maintain_symlinks('library', self.install_dir) class InstallScriptsCommand(InstallScripts): def run(self): InstallScripts.run(self) _maintain_symlinks('script', self.install_dir) class SDistCommand(SDist): def run(self): # have to generate the cache of symlinks for release as sdist is the # only command that has access to symlinks from the git repo symlinks = {'script': _find_symlinks('bin'), 'library': _find_symlinks('lib', '.py'), } _cache_symlinks(symlinks) SDist.run(self) def read_file(file_name): """Read file and return its contents.""" with open(file_name, 'r') as f: return f.read() def read_requirements(file_name): """Read requirements file as a list.""" reqs = read_file(file_name).splitlines() if not reqs: raise RuntimeError( "Unable to read requirements from the %s file" "That indicates this copy of the source code is incomplete." % file_name ) return reqs PYCRYPTO_DIST = 'pycrypto' def get_crypto_req(): """Detect custom crypto from ANSIBLE_CRYPTO_BACKEND env var. pycrypto or cryptography. We choose a default but allow the user to override it. This translates into pip install of the sdist deciding what package to install and also the runtime dependencies that pkg_resources knows about. """ crypto_backend = os.environ.get('ANSIBLE_CRYPTO_BACKEND', '').strip() if crypto_backend == PYCRYPTO_DIST: # Attempt to set version requirements return '%s >= 2.6' % PYCRYPTO_DIST return crypto_backend or None def substitute_crypto_to_req(req): """Replace crypto requirements if customized.""" crypto_backend = get_crypto_req() if crypto_backend is None: return req def is_not_crypto(r): CRYPTO_LIBS = PYCRYPTO_DIST, 'cryptography' return not any(r.lower().startswith(c) for c in CRYPTO_LIBS) return [r for r in req if is_not_crypto(r)] + [crypto_backend] def read_extras(): """Specify any extra requirements for installation.""" extras = dict() extra_requirements_dir = 'packaging/requirements' for extra_requirements_filename in os.listdir(extra_requirements_dir): filename_match = re.search(r'^requirements-(\w*).txt$', extra_requirements_filename) if not filename_match: continue extra_req_file_path = os.path.join(extra_requirements_dir, extra_requirements_filename) try: extras[filename_match.group(1)] = read_file(extra_req_file_path).splitlines() except RuntimeError: pass return extras def get_dynamic_setup_params(): """Add dynamically calculated setup params to static ones.""" return { # Retrieve the long description from the README 'long_description': read_file('README.rst'), 'install_requires': substitute_crypto_to_req( read_requirements('requirements.txt'), ), 'extras_require': read_extras(), } static_setup_params = dict( # Use the distutils SDist so that symlinks are not expanded # Use a custom Build for the same reason cmdclass={ 'build_py': BuildPyCommand, 'build_scripts': BuildScriptsCommand, 'install_lib': InstallLibCommand, 'install_scripts': InstallScriptsCommand, 'sdist': SDistCommand, }, name='ansible', version=__version__, description='Radically simple IT automation', author=__author__, author_email='info@ansible.com', url='https://ansible.com/', project_urls={ 'Bug Tracker': 'https://github.com/ansible/ansible/issues', 'CI: Shippable': 'https://app.shippable.com/github/ansible/ansible', 'Code of Conduct': 'https://docs.ansible.com/ansible/latest/community/code_of_conduct.html', 'Documentation': 'https://docs.ansible.com/ansible/', 'Mailing lists': 'https://docs.ansible.com/ansible/latest/community/communication.html#mailing-list-information', 'Source Code': 'https://github.com/ansible/ansible', }, license='GPLv3+', # Ansible will also make use of a system copy of python-six and # python-selectors2 if installed but use a Bundled copy if it's not. python_requires='>=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*', package_dir={'': 'lib'}, packages=find_packages('lib'), package_data={ '': [ 'executor/powershell/*.ps1', 'module_utils/csharp/*.cs', 'module_utils/csharp/*/*.cs', 'module_utils/powershell/*.psm1', 'module_utils/powershell/*/*.psm1', 'modules/windows/*.ps1', 'modules/windows/*/*.ps1', 'galaxy/data/*/*.*', 'galaxy/data/*/*/.*', 'galaxy/data/*/*/*.*', 'galaxy/data/*/tests/inventory', 'config/base.yml', 'config/module_defaults.yml', ], }, classifiers=[ 'Development Status :: 5 - Production/Stable', 'Environment :: Console', 'Intended Audience :: Developers', 'Intended Audience :: Information Technology', 'Intended Audience :: System Administrators', 'License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)', 'Natural Language :: English', 'Operating System :: POSIX', 'Programming Language :: Python :: 2', 'Programming Language :: Python :: 2.7', 'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.5', 'Programming Language :: Python :: 3.6', 'Programming Language :: Python :: 3.7', 'Topic :: System :: Installation/Setup', 'Topic :: System :: Systems Administration', 'Topic :: Utilities', ], scripts=[ 'bin/ansible', 'bin/ansible-playbook', 'bin/ansible-pull', 'bin/ansible-doc', 'bin/ansible-galaxy', 'bin/ansible-console', 'bin/ansible-connection', 'bin/ansible-vault', 'bin/ansible-config', 'bin/ansible-inventory', ], data_files=[], # Installing as zip files would break due to references to __file__ zip_safe=False ) def main(): """Invoke installation process using setuptools.""" setup_params = dict(static_setup_params, **get_dynamic_setup_params()) ignore_warning_regex = ( r"Unknown distribution option: '(project_urls|python_requires)'" ) warnings.filterwarnings( 'ignore', message=ignore_warning_regex, category=UserWarning, module='distutils.dist', ) setup(**setup_params) warnings.resetwarnings() if __name__ == '__main__': main()
# # Licensed to the Apache Software Foundation (ASF) under one # or more contributor license agreements. See the NOTICE file # distributed with this work for additional information # regarding copyright ownership. The ASF licenses this file # to you under the Apache License, Version 2.0 (the # "License"); you may not use this file except in compliance # with the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, # software distributed under the License is distributed on an # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY # KIND, either express or implied. See the License for the # specific language governing permissions and limitations # under the License. """This module is deprecated. Please use `airflow.providers.amazon.aws.hooks.athena`.""" import warnings # pylint: disable=unused-import from airflow.providers.amazon.aws.hooks.athena import AWSAthenaHook # noqa warnings.warn( "This module is deprecated. Please use `airflow.providers.amazon.aws.hooks.athena`.", DeprecationWarning, stacklevel=2, )
from mpi4py import MPI import mpiunittest as unittest from sys import getrefcount as getrc class BaseTestCommAttr(object): keyval = MPI.KEYVAL_INVALID def tearDown(self): self.comm.Free() if self.keyval != MPI.KEYVAL_INVALID: self.keyval = MPI.Comm.Free_keyval(self.keyval) self.assertEqual(self.keyval, MPI.KEYVAL_INVALID) def testAttr(self, copy_fn=None, delete_fn=None): self.keyval = MPI.Comm.Create_keyval(copy_fn, delete_fn) self.assertNotEqual(self.keyval, MPI.KEYVAL_INVALID) attrval = [1,2,3] rc = getrc(attrval) self.comm.Set_attr(self.keyval, attrval) self.assertEqual(getrc(attrval), rc+1) o = self.comm.Get_attr(self.keyval) self.assertTrue(o is attrval) self.assertEqual(getrc(attrval), rc+2) o = None dupcomm = self.comm.Clone() if copy_fn is True: self.assertEqual(getrc(attrval), rc+2) o = dupcomm.Get_attr(self.keyval) if copy_fn is True: self.assertTrue(o is attrval) self.assertEqual(getrc(attrval), rc+3) elif not copy_fn: self.assertTrue(o is None) self.assertEqual(getrc(attrval), rc+1) dupcomm.Free() o = None self.assertEqual(getrc(attrval), rc+1) self.comm.Delete_attr(self.keyval) self.assertEqual(getrc(attrval), rc) o = self.comm.Get_attr(self.keyval) self.assertTrue(o is None) def testAttrCopyFalse(self): self.testAttr(False) def testAttrCopyTrue(self): self.testAttr(True) def testAttrCopyDelete(self): self.keyval = MPI.Comm.Create_keyval( copy_fn=MPI.Comm.Clone, delete_fn=MPI.Comm.Free) self.assertNotEqual(self.keyval, MPI.KEYVAL_INVALID) comm1 = self.comm dupcomm1 = comm1.Clone() rc = getrc(dupcomm1) comm1.Set_attr(self.keyval, dupcomm1) self.assertTrue(dupcomm1 != MPI.COMM_NULL) self.assertTrue(getrc(dupcomm1), rc+1) comm2 = comm1.Clone() dupcomm2 = comm2.Get_attr(self.keyval) self.assertTrue(dupcomm1 != dupcomm2) self.assertTrue(getrc(dupcomm1), rc+1) self.assertTrue(getrc(dupcomm2), 3) comm2.Free() self.assertTrue(dupcomm2 == MPI.COMM_NULL) self.assertTrue(getrc(dupcomm1), rc+1) self.assertTrue(getrc(dupcomm2), 2) self.comm.Delete_attr(self.keyval) self.assertTrue(dupcomm1 == MPI.COMM_NULL) self.assertTrue(getrc(dupcomm1), rc) class TestCommAttrWorld(BaseTestCommAttr, unittest.TestCase): def setUp(self): self.comm = MPI.COMM_WORLD.Dup() class TestCommAttrSelf(BaseTestCommAttr, unittest.TestCase): def setUp(self): self.comm = MPI.COMM_SELF.Dup() class BaseTestDatatypeAttr(object): keyval = MPI.KEYVAL_INVALID def tearDown(self): self.datatype.Free() if self.keyval != MPI.KEYVAL_INVALID: self.keyval = MPI.Datatype.Free_keyval(self.keyval) self.assertEqual(self.keyval, MPI.KEYVAL_INVALID) def testAttr(self, copy_fn=None, delete_fn=None): self.keyval = MPI.Datatype.Create_keyval(copy_fn, delete_fn) self.assertNotEqual(self.keyval, MPI.KEYVAL_INVALID) attrval = [1,2,3] rc = getrc(attrval) self.datatype.Set_attr(self.keyval, attrval) self.assertEqual(getrc(attrval), rc+1) o = self.datatype.Get_attr(self.keyval) self.assertTrue(o is attrval) self.assertEqual(getrc(attrval), rc+2) o = None dupdatatype = self.datatype.Dup() if copy_fn is True: self.assertEqual(getrc(attrval), rc+2) o = dupdatatype.Get_attr(self.keyval) if copy_fn is True: self.assertTrue(o is attrval) self.assertEqual(getrc(attrval), rc+3) elif not copy_fn: self.assertTrue(o is None) self.assertEqual(getrc(attrval), rc+1) dupdatatype.Free() o = None self.assertEqual(getrc(attrval), rc+1) self.datatype.Delete_attr(self.keyval) self.assertEqual(getrc(attrval), rc) o = self.datatype.Get_attr(self.keyval) self.assertTrue(o is None) def testAttrCopyFalse(self): self.testAttr(False) def testAttrCopyTrue(self): self.testAttr(True) def testAttrCopyDelete(self): self.keyval = MPI.Datatype.Create_keyval( copy_fn=MPI.Datatype.Dup, delete_fn=MPI.Datatype.Free) self.assertNotEqual(self.keyval, MPI.KEYVAL_INVALID) datatype1 = self.datatype dupdatatype1 = datatype1.Dup() rc = getrc(dupdatatype1) datatype1.Set_attr(self.keyval, dupdatatype1) self.assertTrue(dupdatatype1 != MPI.DATATYPE_NULL) self.assertTrue(getrc(dupdatatype1), rc+1) datatype2 = datatype1.Dup() dupdatatype2 = datatype2.Get_attr(self.keyval) self.assertTrue(dupdatatype1 != dupdatatype2) self.assertTrue(getrc(dupdatatype1), rc+1) self.assertTrue(getrc(dupdatatype2), 3) datatype2.Free() self.assertTrue(dupdatatype2 == MPI.DATATYPE_NULL) self.assertTrue(getrc(dupdatatype1), rc+1) self.assertTrue(getrc(dupdatatype2), 2) self.datatype.Delete_attr(self.keyval) self.assertTrue(dupdatatype1 == MPI.DATATYPE_NULL) self.assertTrue(getrc(dupdatatype1), rc) class TestDatatypeAttrBYTE(BaseTestDatatypeAttr, unittest.TestCase): def setUp(self): self.datatype = MPI.BYTE.Dup() class TestDatatypeAttrINT(BaseTestDatatypeAttr, unittest.TestCase): def setUp(self): self.datatype = MPI.INT.Dup() class TestDatatypeAttrFLOAT(BaseTestDatatypeAttr, unittest.TestCase): def setUp(self): self.datatype = MPI.FLOAT.Dup() class TestWinAttr(unittest.TestCase): keyval = MPI.KEYVAL_INVALID def setUp(self): self.win = MPI.Win.Create(MPI.BOTTOM, 1, MPI.INFO_NULL, MPI.COMM_SELF) def tearDown(self): self.win.Free() if self.keyval != MPI.KEYVAL_INVALID: self.keyval = MPI.Win.Free_keyval(self.keyval) self.assertEqual(self.keyval, MPI.KEYVAL_INVALID) def testAttr(self, copy_fn=None, delete_fn=None): self.keyval = MPI.Win.Create_keyval(copy_fn, delete_fn) self.assertNotEqual(self.keyval, MPI.KEYVAL_INVALID) attrval = [1,2,3] rc = getrc(attrval) self.win.Set_attr(self.keyval, attrval) self.assertEqual(getrc(attrval), rc+1) o = self.win.Get_attr(self.keyval) self.assertTrue(o is attrval) self.assertEqual(getrc(attrval), rc+2) o = None self.assertEqual(getrc(attrval), rc+1) self.win.Delete_attr(self.keyval) self.assertEqual(getrc(attrval), rc) o = self.win.Get_attr(self.keyval) self.assertTrue(o is None) def testAttrCopyDelete(self): self.keyval = MPI.Win.Create_keyval(delete_fn=MPI.Win.Free) self.assertNotEqual(self.keyval, MPI.KEYVAL_INVALID) newwin = MPI.Win.Create(MPI.BOTTOM, 1, MPI.INFO_NULL, MPI.COMM_SELF) rc = getrc(newwin) # self.win.Set_attr(self.keyval, newwin) self.assertTrue(newwin != MPI.WIN_NULL) self.assertTrue(getrc(newwin), rc+1) # self.win.Delete_attr(self.keyval) self.assertTrue(newwin == MPI.WIN_NULL) self.assertTrue(getrc(newwin), rc) try: k = MPI.Datatype.Create_keyval() k = MPI.Datatype.Free_keyval(k) except NotImplementedError: del TestDatatypeAttrBYTE del TestDatatypeAttrINT del TestDatatypeAttrFLOAT try: k = MPI.Win.Create_keyval() k = MPI.Win.Free_keyval(k) except NotImplementedError: del TestWinAttr _name, _version = MPI.get_vendor() if (_name == 'Open MPI' and _version <= (1, 5, 1)): if MPI.Query_thread() > MPI.THREAD_SINGLE: del BaseTestCommAttr.testAttrCopyDelete del TestWinAttr.testAttrCopyDelete if __name__ == '__main__': unittest.main()
# Copyright 2007 Google, Inc. All Rights Reserved. # Licensed to PSF under a Contributor Agreement. """Abstract Base Classes (ABCs) for numbers, according to PEP 3141. TODO: Fill out more detailed documentation on the operators.""" from __future__ import division from abc import ABCMeta, abstractmethod, abstractproperty __all__ = ["Number", "Complex", "Real", "Rational", "Integral"] class Number(object): """All numbers inherit from this class. If you just want to check if an argument x is a number, without caring what kind, use isinstance(x, Number). """ __metaclass__ = ABCMeta __slots__ = () # Concrete numeric types must provide their own hash implementation __hash__ = None ## Notes on Decimal ## ---------------- ## Decimal has all of the methods specified by the Real abc, but it should ## not be registered as a Real because decimals do not interoperate with ## binary floats (i.e. Decimal('3.14') + 2.71828 is undefined). But, ## abstract reals are expected to interoperate (i.e. R1 + R2 should be ## expected to work if R1 and R2 are both Reals). class Complex(Number): """Complex defines the operations that work on the builtin complex type. In short, those are: a conversion to complex, .real, .imag, +, -, *, /, abs(), .conjugate, ==, and !=. If it is given heterogenous arguments, and doesn't have special knowledge about them, it should fall back to the builtin complex type as described below. """ __slots__ = () @abstractmethod def __complex__(self): """Return a builtin complex instance. Called for complex(self).""" # Will be __bool__ in 3.0. def __nonzero__(self): """True if self != 0. Called for bool(self).""" return self != 0 @abstractproperty def real(self): """Retrieve the real component of this number. This should subclass Real. """ raise NotImplementedError @abstractproperty def imag(self): """Retrieve the real component of this number. This should subclass Real. """ raise NotImplementedError @abstractmethod def __add__(self, other): """self + other""" raise NotImplementedError @abstractmethod def __radd__(self, other): """other + self""" raise NotImplementedError @abstractmethod def __neg__(self): """-self""" raise NotImplementedError @abstractmethod def __pos__(self): """+self""" raise NotImplementedError def __sub__(self, other): """self - other""" return self + -other def __rsub__(self, other): """other - self""" return -self + other @abstractmethod def __mul__(self, other): """self * other""" raise NotImplementedError @abstractmethod def __rmul__(self, other): """other * self""" raise NotImplementedError @abstractmethod def __div__(self, other): """self / other without __future__ division May promote to float. """ raise NotImplementedError @abstractmethod def __rdiv__(self, other): """other / self without __future__ division""" raise NotImplementedError @abstractmethod def __truediv__(self, other): """self / other with __future__ division. Should promote to float when necessary. """ raise NotImplementedError @abstractmethod def __rtruediv__(self, other): """other / self with __future__ division""" raise NotImplementedError @abstractmethod def __pow__(self, exponent): """self**exponent; should promote to float or complex when necessary.""" raise NotImplementedError @abstractmethod def __rpow__(self, base): """base ** self""" raise NotImplementedError @abstractmethod def __abs__(self): """Returns the Real distance from 0. Called for abs(self).""" raise NotImplementedError @abstractmethod def conjugate(self): """(x+y*i).conjugate() returns (x-y*i).""" raise NotImplementedError @abstractmethod def __eq__(self, other): """self == other""" raise NotImplementedError def __ne__(self, other): """self != other""" # The default __ne__ doesn't negate __eq__ until 3.0. return not (self == other) Complex.register(complex) class Real(Complex): """To Complex, Real adds the operations that work on real numbers. In short, those are: a conversion to float, trunc(), divmod, %, <, <=, >, and >=. Real also provides defaults for the derived operations. """ __slots__ = () @abstractmethod def __float__(self): """Any Real can be converted to a native float object. Called for float(self).""" raise NotImplementedError @abstractmethod def __trunc__(self): """trunc(self): Truncates self to an Integral. Returns an Integral i such that: * i>0 iff self>0; * abs(i) <= abs(self); * for any Integral j satisfying the first two conditions, abs(i) >= abs(j) [i.e. i has "maximal" abs among those]. i.e. "truncate towards 0". """ raise NotImplementedError def __divmod__(self, other): """divmod(self, other): The pair (self // other, self % other). Sometimes this can be computed faster than the pair of operations. """ return (self // other, self % other) def __rdivmod__(self, other): """divmod(other, self): The pair (self // other, self % other). Sometimes this can be computed faster than the pair of operations. """ return (other // self, other % self) @abstractmethod def __floordiv__(self, other): """self // other: The floor() of self/other.""" raise NotImplementedError @abstractmethod def __rfloordiv__(self, other): """other // self: The floor() of other/self.""" raise NotImplementedError @abstractmethod def __mod__(self, other): """self % other""" raise NotImplementedError @abstractmethod def __rmod__(self, other): """other % self""" raise NotImplementedError @abstractmethod def __lt__(self, other): """self < other < on Reals defines a total ordering, except perhaps for NaN.""" raise NotImplementedError @abstractmethod def __le__(self, other): """self <= other""" raise NotImplementedError # Concrete implementations of Complex abstract methods. def __complex__(self): """complex(self) == complex(float(self), 0)""" return complex(float(self)) @property def real(self): """Real numbers are their real component.""" return +self @property def imag(self): """Real numbers have no imaginary component.""" return 0 def conjugate(self): """Conjugate is a no-op for Reals.""" return +self Real.register(float) class Rational(Real): """.numerator and .denominator should be in lowest terms.""" __slots__ = () @abstractproperty def numerator(self): raise NotImplementedError @abstractproperty def denominator(self): raise NotImplementedError # Concrete implementation of Real's conversion to float. def __float__(self): """float(self) = self.numerator / self.denominator It's important that this conversion use the integer's "true" division rather than casting one side to float before dividing so that ratios of huge integers convert without overflowing. """ return self.numerator / self.denominator class Integral(Rational): """Integral adds a conversion to long and the bit-string operations.""" __slots__ = () @abstractmethod def __long__(self): """long(self)""" raise NotImplementedError def __index__(self): """index(self)""" return long(self) @abstractmethod def __pow__(self, exponent, modulus=None): """self ** exponent % modulus, but maybe faster. Accept the modulus argument if you want to support the 3-argument version of pow(). Raise a TypeError if exponent < 0 or any argument isn't Integral. Otherwise, just implement the 2-argument version described in Complex. """ raise NotImplementedError @abstractmethod def __lshift__(self, other): """self << other""" raise NotImplementedError @abstractmethod def __rlshift__(self, other): """other << self""" raise NotImplementedError @abstractmethod def __rshift__(self, other): """self >> other""" raise NotImplementedError @abstractmethod def __rrshift__(self, other): """other >> self""" raise NotImplementedError @abstractmethod def __and__(self, other): """self & other""" raise NotImplementedError @abstractmethod def __rand__(self, other): """other & self""" raise NotImplementedError @abstractmethod def __xor__(self, other): """self ^ other""" raise NotImplementedError @abstractmethod def __rxor__(self, other): """other ^ self""" raise NotImplementedError @abstractmethod def __or__(self, other): """self | other""" raise NotImplementedError @abstractmethod def __ror__(self, other): """other | self""" raise NotImplementedError @abstractmethod def __invert__(self): """~self""" raise NotImplementedError # Concrete implementations of Rational and Real abstract methods. def __float__(self): """float(self) == float(long(self))""" return float(long(self)) @property def numerator(self): """Integers are their own numerators.""" return +self @property def denominator(self): """Integers have a denominator of 1.""" return 1 Integral.register(int) Integral.register(long)
"""MsgTypes module: contains distributed object message types""" from direct.showbase.PythonUtil import invertDictLossless MsgName2Id = { # 2 new params: passwd, char bool 0/1 1 = new account # 2 new return values: 129 = not found, 12 = bad passwd, 'CLIENT_LOGIN': 1, 'CLIENT_LOGIN_RESP': 2, 'CLIENT_GET_AVATARS': 3, # Sent by the server when it is dropping the connection deliberately. 'CLIENT_GO_GET_LOST': 4, 'CLIENT_GET_AVATARS_RESP': 5, 'CLIENT_CREATE_AVATAR': 6, 'CLIENT_CREATE_AVATAR_RESP': 7, 'CLIENT_GET_FRIEND_LIST': 10, 'CLIENT_GET_FRIEND_LIST_RESP': 11, 'CLIENT_GET_AVATAR_DETAILS': 14, 'CLIENT_GET_AVATAR_DETAILS_RESP': 15, 'CLIENT_LOGIN_2': 16, 'CLIENT_LOGIN_2_RESP': 17, 'CLIENT_OBJECT_UPDATE_FIELD': 24, 'CLIENT_OBJECT_UPDATE_FIELD_RESP': 24, 'CLIENT_OBJECT_DISABLE': 25, 'CLIENT_OBJECT_DISABLE_RESP': 25, 'CLIENT_OBJECT_DISABLE_OWNER': 26, 'CLIENT_OBJECT_DISABLE_OWNER_RESP': 26, 'CLIENT_OBJECT_DELETE': 27, 'CLIENT_OBJECT_DELETE_RESP': 27, 'CLIENT_SET_ZONE_CMU': 29, 'CLIENT_REMOVE_ZONE': 30, 'CLIENT_SET_AVATAR': 32, 'CLIENT_CREATE_OBJECT_REQUIRED': 34, 'CLIENT_CREATE_OBJECT_REQUIRED_RESP': 34, 'CLIENT_CREATE_OBJECT_REQUIRED_OTHER': 35, 'CLIENT_CREATE_OBJECT_REQUIRED_OTHER_RESP': 35, 'CLIENT_CREATE_OBJECT_REQUIRED_OTHER_OWNER': 36, 'CLIENT_CREATE_OBJECT_REQUIRED_OTHER_OWNER_RESP':36, 'CLIENT_REQUEST_GENERATES': 36, 'CLIENT_DISCONNECT': 37, 'CLIENT_GET_STATE_RESP': 47, 'CLIENT_DONE_INTEREST_RESP': 48, 'CLIENT_DELETE_AVATAR': 49, 'CLIENT_DELETE_AVATAR_RESP': 5, 'CLIENT_HEARTBEAT': 52, 'CLIENT_FRIEND_ONLINE': 53, 'CLIENT_FRIEND_OFFLINE': 54, 'CLIENT_REMOVE_FRIEND': 56, 'CLIENT_CHANGE_PASSWORD': 65, 'CLIENT_SET_NAME_PATTERN': 67, 'CLIENT_SET_NAME_PATTERN_ANSWER': 68, 'CLIENT_SET_WISHNAME': 70, 'CLIENT_SET_WISHNAME_RESP': 71, 'CLIENT_SET_WISHNAME_CLEAR': 72, 'CLIENT_SET_SECURITY': 73, 'CLIENT_SET_DOID_RANGE': 74, 'CLIENT_GET_AVATARS_RESP2': 75, 'CLIENT_CREATE_AVATAR2': 76, 'CLIENT_SYSTEM_MESSAGE': 78, 'CLIENT_SET_AVTYPE': 80, 'CLIENT_GET_PET_DETAILS': 81, 'CLIENT_GET_PET_DETAILS_RESP': 82, 'CLIENT_ADD_INTEREST': 97, 'CLIENT_REMOVE_INTEREST': 99, 'CLIENT_OBJECT_LOCATION': 102, 'CLIENT_LOGIN_3': 111, 'CLIENT_LOGIN_3_RESP': 110, 'CLIENT_GET_FRIEND_LIST_EXTENDED': 115, 'CLIENT_GET_FRIEND_LIST_EXTENDED_RESP': 116, 'CLIENT_SET_FIELD_SENDABLE': 120, 'CLIENT_SYSTEMMESSAGE_AKNOWLEDGE': 123, 'CLIENT_CHANGE_GENERATE_ORDER': 124, # new toontown specific login message, adds last logged in, and if child account has parent acount 'CLIENT_LOGIN_TOONTOWN': 125, 'CLIENT_LOGIN_TOONTOWN_RESP': 126, 'STATESERVER_OBJECT_GENERATE_WITH_REQUIRED': 2001, 'STATESERVER_OBJECT_GENERATE_WITH_REQUIRED_OTHER': 2003, 'STATESERVER_OBJECT_UPDATE_FIELD': 2004, 'STATESERVER_OBJECT_CREATE_WITH_REQUIRED_CONTEXT': 2050, 'STATESERVER_OBJECT_CREATE_WITH_REQUIR_OTHER_CONTEXT': 2051, 'STATESERVER_BOUNCE_MESSAGE': 2086, } # create id->name table for debugging MsgId2Names = invertDictLossless(MsgName2Id) # put msg names in module scope, assigned to msg value globals().update(MsgName2Id) # These messages are ignored when the client is headed to the quiet zone QUIET_ZONE_IGNORED_LIST = [ # We mustn't ignore updates, because some updates for localToon # are always important. #CLIENT_OBJECT_UPDATE_FIELD, # These are now handled. If it is a create for a class that is in the # uber zone, we should create it. #CLIENT_CREATE_OBJECT_REQUIRED, #CLIENT_CREATE_OBJECT_REQUIRED_OTHER, ] # The following is a different set of numbers from above. # These are the sub-message types for CLIENT_LOGIN_2. CLIENT_LOGIN_2_GREEN = 1 # Disney's GoReg subscription token, not used. CLIENT_LOGIN_2_PLAY_TOKEN = 2 # VR Studio PlayToken. CLIENT_LOGIN_2_BLUE = 3 # The international GoReg token. CLIENT_LOGIN_3_DISL_TOKEN = 4 # SSL encoded blob from DISL system.
# Copyright 2016 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== from __future__ import absolute_import from __future__ import division from __future__ import print_function import numpy as np from tensorflow.python.framework import dtypes from tensorflow.python.framework import tensor_shape from tensorflow.python.ops import array_ops from tensorflow.python.ops import math_ops from tensorflow.python.ops.distributions import multinomial from tensorflow.python.platform import test class MultinomialTest(test.TestCase): def setUp(self): self._rng = np.random.RandomState(42) def testSimpleShapes(self): with self.test_session(): p = [.1, .3, .6] dist = multinomial.Multinomial(total_count=1., probs=p) self.assertEqual(3, dist.event_shape_tensor().eval()) self.assertAllEqual([], dist.batch_shape_tensor().eval()) self.assertEqual(tensor_shape.TensorShape([3]), dist.event_shape) self.assertEqual(tensor_shape.TensorShape([]), dist.batch_shape) def testComplexShapes(self): with self.test_session(): p = 0.5 * np.ones([3, 2, 2], dtype=np.float32) n = [[3., 2], [4, 5], [6, 7]] dist = multinomial.Multinomial(total_count=n, probs=p) self.assertEqual(2, dist.event_shape_tensor().eval()) self.assertAllEqual([3, 2], dist.batch_shape_tensor().eval()) self.assertEqual(tensor_shape.TensorShape([2]), dist.event_shape) self.assertEqual(tensor_shape.TensorShape([3, 2]), dist.batch_shape) def testN(self): p = [[0.1, 0.2, 0.7], [0.2, 0.3, 0.5]] n = [[3.], [4]] with self.test_session(): dist = multinomial.Multinomial(total_count=n, probs=p) self.assertEqual((2, 1), dist.total_count.get_shape()) self.assertAllClose(n, dist.total_count.eval()) def testP(self): p = [[0.1, 0.2, 0.7]] with self.test_session(): dist = multinomial.Multinomial(total_count=3., probs=p) self.assertEqual((1, 3), dist.probs.get_shape()) self.assertEqual((1, 3), dist.logits.get_shape()) self.assertAllClose(p, dist.probs.eval()) def testLogits(self): p = np.array([[0.1, 0.2, 0.7]], dtype=np.float32) logits = np.log(p) - 50. with self.test_session(): multinom = multinomial.Multinomial(total_count=3., logits=logits) self.assertEqual((1, 3), multinom.probs.get_shape()) self.assertEqual((1, 3), multinom.logits.get_shape()) self.assertAllClose(p, multinom.probs.eval()) self.assertAllClose(logits, multinom.logits.eval()) def testPmfandCountsAgree(self): p = [[0.1, 0.2, 0.7]] n = [[5.]] with self.test_session(): dist = multinomial.Multinomial(total_count=n, probs=p, validate_args=True) dist.prob([2., 3, 0]).eval() dist.prob([3., 0, 2]).eval() with self.assertRaisesOpError("must be non-negative"): dist.prob([-1., 4, 2]).eval() with self.assertRaisesOpError("counts must sum to `self.total_count`"): dist.prob([3., 3, 0]).eval() def testPmfNonIntegerCounts(self): p = [[0.1, 0.2, 0.7]] n = [[5.]] with self.test_session(): # No errors with integer n. multinom = multinomial.Multinomial( total_count=n, probs=p, validate_args=True) multinom.prob([2., 1, 2]).eval() multinom.prob([3., 0, 2]).eval() # Counts don't sum to n. with self.assertRaisesOpError("counts must sum to `self.total_count`"): multinom.prob([2., 3, 2]).eval() # Counts are non-integers. x = array_ops.placeholder(dtypes.float32) with self.assertRaisesOpError( "cannot contain fractional components."): multinom.prob(x).eval(feed_dict={x: [1.0, 2.5, 1.5]}) multinom = multinomial.Multinomial( total_count=n, probs=p, validate_args=False) multinom.prob([1., 2., 2.]).eval() # Non-integer arguments work. multinom.prob([1.0, 2.5, 1.5]).eval() def testPmfBothZeroBatches(self): with self.test_session(): # Both zero-batches. No broadcast p = [0.5, 0.5] counts = [1., 0] pmf = multinomial.Multinomial(total_count=1., probs=p).prob(counts) self.assertAllClose(0.5, pmf.eval()) self.assertEqual((), pmf.get_shape()) def testPmfBothZeroBatchesNontrivialN(self): with self.test_session(): # Both zero-batches. No broadcast p = [0.1, 0.9] counts = [3., 2] dist = multinomial.Multinomial(total_count=5., probs=p) pmf = dist.prob(counts) # 5 choose 3 = 5 choose 2 = 10. 10 * (.9)^2 * (.1)^3 = 81/10000. self.assertAllClose(81. / 10000, pmf.eval()) self.assertEqual((), pmf.get_shape()) def testPmfPStretchedInBroadcastWhenSameRank(self): with self.test_session(): p = [[0.1, 0.9]] counts = [[1., 0], [0, 1]] pmf = multinomial.Multinomial(total_count=1., probs=p).prob(counts) self.assertAllClose([0.1, 0.9], pmf.eval()) self.assertEqual((2), pmf.get_shape()) def testPmfPStretchedInBroadcastWhenLowerRank(self): with self.test_session(): p = [0.1, 0.9] counts = [[1., 0], [0, 1]] pmf = multinomial.Multinomial(total_count=1., probs=p).prob(counts) self.assertAllClose([0.1, 0.9], pmf.eval()) self.assertEqual((2), pmf.get_shape()) def testPmfCountsStretchedInBroadcastWhenSameRank(self): with self.test_session(): p = [[0.1, 0.9], [0.7, 0.3]] counts = [[1., 0]] pmf = multinomial.Multinomial(total_count=1., probs=p).prob(counts) self.assertAllClose(pmf.eval(), [0.1, 0.7]) self.assertEqual((2), pmf.get_shape()) def testPmfCountsStretchedInBroadcastWhenLowerRank(self): with self.test_session(): p = [[0.1, 0.9], [0.7, 0.3]] counts = [1., 0] pmf = multinomial.Multinomial(total_count=1., probs=p).prob(counts) self.assertAllClose(pmf.eval(), [0.1, 0.7]) self.assertEqual(pmf.get_shape(), (2)) def testPmfShapeCountsStretchedN(self): with self.test_session(): # [2, 2, 2] p = [[[0.1, 0.9], [0.1, 0.9]], [[0.7, 0.3], [0.7, 0.3]]] # [2, 2] n = [[3., 3], [3, 3]] # [2] counts = [2., 1] pmf = multinomial.Multinomial(total_count=n, probs=p).prob(counts) pmf.eval() self.assertEqual(pmf.get_shape(), (2, 2)) def testPmfShapeCountsPStretchedN(self): with self.test_session(): p = [0.1, 0.9] counts = [3., 2] n = np.full([4, 3], 5., dtype=np.float32) pmf = multinomial.Multinomial(total_count=n, probs=p).prob(counts) pmf.eval() self.assertEqual((4, 3), pmf.get_shape()) def testMultinomialMean(self): with self.test_session(): n = 5. p = [0.1, 0.2, 0.7] dist = multinomial.Multinomial(total_count=n, probs=p) expected_means = 5 * np.array(p, dtype=np.float32) self.assertEqual((3,), dist.mean().get_shape()) self.assertAllClose(expected_means, dist.mean().eval()) def testMultinomialCovariance(self): with self.test_session(): n = 5. p = [0.1, 0.2, 0.7] dist = multinomial.Multinomial(total_count=n, probs=p) expected_covariances = [[9. / 20, -1 / 10, -7 / 20], [-1 / 10, 4 / 5, -7 / 10], [-7 / 20, -7 / 10, 21 / 20]] self.assertEqual((3, 3), dist.covariance().get_shape()) self.assertAllClose(expected_covariances, dist.covariance().eval()) def testMultinomialCovarianceBatch(self): with self.test_session(): # Shape [2] n = [5.] * 2 # Shape [4, 1, 2] p = [[[0.1, 0.9]], [[0.1, 0.9]]] * 2 dist = multinomial.Multinomial(total_count=n, probs=p) # Shape [2, 2] inner_var = [[9. / 20, -9 / 20], [-9 / 20, 9 / 20]] # Shape [4, 2, 2, 2] expected_covariances = [[inner_var, inner_var]] * 4 self.assertEqual((4, 2, 2, 2), dist.covariance().get_shape()) self.assertAllClose(expected_covariances, dist.covariance().eval()) def testCovarianceMultidimensional(self): # Shape [3, 5, 4] p = np.random.dirichlet([.25, .25, .25, .25], [3, 5]).astype(np.float32) # Shape [6, 3, 3] p2 = np.random.dirichlet([.3, .3, .4], [6, 3]).astype(np.float32) ns = np.random.randint(low=1, high=11, size=[3, 5]).astype(np.float32) ns2 = np.random.randint(low=1, high=11, size=[6, 1]).astype(np.float32) with self.test_session(): dist = multinomial.Multinomial(ns, p) dist2 = multinomial.Multinomial(ns2, p2) covariance = dist.covariance() covariance2 = dist2.covariance() self.assertEqual((3, 5, 4, 4), covariance.get_shape()) self.assertEqual((6, 3, 3, 3), covariance2.get_shape()) def testCovarianceFromSampling(self): # We will test mean, cov, var, stddev on a DirichletMultinomial constructed # via broadcast between alpha, n. theta = np.array([[1., 2, 3], [2.5, 4, 0.01]], dtype=np.float32) theta /= np.sum(theta, 1)[..., array_ops.newaxis] # Ideally we'd be able to test broadcasting but, the multinomial sampler # doesn't support different total counts. n = np.float32(5) with self.test_session() as sess: # batch_shape=[2], event_shape=[3] dist = multinomial.Multinomial(n, theta) x = dist.sample(int(250e3), seed=1) sample_mean = math_ops.reduce_mean(x, 0) x_centered = x - sample_mean[array_ops.newaxis, ...] sample_cov = math_ops.reduce_mean(math_ops.matmul( x_centered[..., array_ops.newaxis], x_centered[..., array_ops.newaxis, :]), 0) sample_var = array_ops.matrix_diag_part(sample_cov) sample_stddev = math_ops.sqrt(sample_var) [ sample_mean_, sample_cov_, sample_var_, sample_stddev_, analytic_mean, analytic_cov, analytic_var, analytic_stddev, ] = sess.run([ sample_mean, sample_cov, sample_var, sample_stddev, dist.mean(), dist.covariance(), dist.variance(), dist.stddev(), ]) self.assertAllClose(sample_mean_, analytic_mean, atol=0., rtol=0.01) self.assertAllClose(sample_cov_, analytic_cov, atol=0., rtol=0.01) self.assertAllClose(sample_var_, analytic_var, atol=0., rtol=0.01) self.assertAllClose(sample_stddev_, analytic_stddev, atol=0., rtol=0.01) def testSampleUnbiasedNonScalarBatch(self): with self.test_session() as sess: dist = multinomial.Multinomial( total_count=5., logits=math_ops.log(2. * self._rng.rand(4, 3, 2).astype(np.float32))) n = int(3e3) x = dist.sample(n, seed=0) sample_mean = math_ops.reduce_mean(x, 0) # Cyclically rotate event dims left. x_centered = array_ops.transpose(x - sample_mean, [1, 2, 3, 0]) sample_covariance = math_ops.matmul( x_centered, x_centered, adjoint_b=True) / n [ sample_mean_, sample_covariance_, actual_mean_, actual_covariance_, ] = sess.run([ sample_mean, sample_covariance, dist.mean(), dist.covariance(), ]) self.assertAllEqual([4, 3, 2], sample_mean.get_shape()) self.assertAllClose(actual_mean_, sample_mean_, atol=0., rtol=0.07) self.assertAllEqual([4, 3, 2, 2], sample_covariance.get_shape()) self.assertAllClose( actual_covariance_, sample_covariance_, atol=0., rtol=0.10) def testSampleUnbiasedScalarBatch(self): with self.test_session() as sess: dist = multinomial.Multinomial( total_count=5., logits=math_ops.log(2. * self._rng.rand(4).astype(np.float32))) n = int(5e3) x = dist.sample(n, seed=0) sample_mean = math_ops.reduce_mean(x, 0) x_centered = x - sample_mean # Already transposed to [n, 2]. sample_covariance = math_ops.matmul( x_centered, x_centered, adjoint_a=True) / n [ sample_mean_, sample_covariance_, actual_mean_, actual_covariance_, ] = sess.run([ sample_mean, sample_covariance, dist.mean(), dist.covariance(), ]) self.assertAllEqual([4], sample_mean.get_shape()) self.assertAllClose(actual_mean_, sample_mean_, atol=0., rtol=0.07) self.assertAllEqual([4, 4], sample_covariance.get_shape()) self.assertAllClose( actual_covariance_, sample_covariance_, atol=0., rtol=0.10) if __name__ == "__main__": test.main()
import re import os import sys import warnings import platform import tempfile from subprocess import Popen, PIPE, STDOUT from numpy.distutils.cpuinfo import cpu from numpy.distutils.fcompiler import FCompiler from numpy.distutils.exec_command import exec_command from numpy.distutils.misc_util import msvc_runtime_library from numpy.distutils.compat import get_exception compilers = ['GnuFCompiler', 'Gnu95FCompiler'] TARGET_R = re.compile("Target: ([a-zA-Z0-9_\-]*)") # XXX: handle cross compilation def is_win64(): return sys.platform == "win32" and platform.architecture()[0] == "64bit" if is_win64(): #_EXTRAFLAGS = ["-fno-leading-underscore"] _EXTRAFLAGS = [] else: _EXTRAFLAGS = [] class GnuFCompiler(FCompiler): compiler_type = 'gnu' compiler_aliases = ('g77',) description = 'GNU Fortran 77 compiler' def gnu_version_match(self, version_string): """Handle the different versions of GNU fortran compilers""" m = re.match(r'GNU Fortran', version_string) if not m: return None m = re.match(r'GNU Fortran\s+95.*?([0-9-.]+)', version_string) if m: return ('gfortran', m.group(1)) m = re.match(r'GNU Fortran.*?([0-9-.]+)', version_string) if m: v = m.group(1) if v.startswith('0') or v.startswith('2') or v.startswith('3'): # the '0' is for early g77's return ('g77', v) else: # at some point in the 4.x series, the ' 95' was dropped # from the version string return ('gfortran', v) def version_match(self, version_string): v = self.gnu_version_match(version_string) if not v or v[0] != 'g77': return None return v[1] # 'g77 --version' results # SunOS: GNU Fortran (GCC 3.2) 3.2 20020814 (release) # Debian: GNU Fortran (GCC) 3.3.3 20040110 (prerelease) (Debian) # GNU Fortran (GCC) 3.3.3 (Debian 20040401) # GNU Fortran 0.5.25 20010319 (prerelease) # Redhat: GNU Fortran (GCC 3.2.2 20030222 (Red Hat Linux 3.2.2-5)) 3.2.2 20030222 (Red Hat Linux 3.2.2-5) # GNU Fortran (GCC) 3.4.2 (mingw-special) possible_executables = ['g77', 'f77'] executables = { 'version_cmd' : [None, "--version"], 'compiler_f77' : [None, "-g", "-Wall", "-fno-second-underscore"], 'compiler_f90' : None, # Use --fcompiler=gnu95 for f90 codes 'compiler_fix' : None, 'linker_so' : [None, "-g", "-Wall"], 'archiver' : ["ar", "-cr"], 'ranlib' : ["ranlib"], 'linker_exe' : [None, "-g", "-Wall"] } module_dir_switch = None module_include_switch = None # Cygwin: f771: warning: -fPIC ignored for target (all code is # position independent) if os.name != 'nt' and sys.platform != 'cygwin': pic_flags = ['-fPIC'] # use -mno-cygwin for g77 when Python is not Cygwin-Python if sys.platform == 'win32': for key in ['version_cmd', 'compiler_f77', 'linker_so', 'linker_exe']: executables[key].append('-mno-cygwin') g2c = 'g2c' suggested_f90_compiler = 'gnu95' #def get_linker_so(self): # # win32 linking should be handled by standard linker # # Darwin g77 cannot be used as a linker. # #if re.match(r'(darwin)', sys.platform): # # return # return FCompiler.get_linker_so(self) def get_flags_linker_so(self): opt = self.linker_so[1:] if sys.platform=='darwin': target = os.environ.get('MACOSX_DEPLOYMENT_TARGET', None) # If MACOSX_DEPLOYMENT_TARGET is set, we simply trust the value # and leave it alone. But, distutils will complain if the # environment's value is different from the one in the Python # Makefile used to build Python. We let disutils handle this # error checking. if not target: # If MACOSX_DEPLOYMENT_TARGET is not set in the environment, # we try to get it first from the Python Makefile and then we # fall back to setting it to 10.3 to maximize the set of # versions we can work with. This is a reasonable default # even when using the official Python dist and those derived # from it. import distutils.sysconfig as sc g = {} filename = sc.get_makefile_filename() sc.parse_makefile(filename, g) target = g.get('MACOSX_DEPLOYMENT_TARGET', '10.3') os.environ['MACOSX_DEPLOYMENT_TARGET'] = target if target == '10.3': s = 'Env. variable MACOSX_DEPLOYMENT_TARGET set to 10.3' warnings.warn(s) opt.extend(['-undefined', 'dynamic_lookup', '-bundle']) else: opt.append("-shared") if sys.platform.startswith('sunos'): # SunOS often has dynamically loaded symbols defined in the # static library libg2c.a The linker doesn't like this. To # ignore the problem, use the -mimpure-text flag. It isn't # the safest thing, but seems to work. 'man gcc' says: # ".. Instead of using -mimpure-text, you should compile all # source code with -fpic or -fPIC." opt.append('-mimpure-text') return opt def get_libgcc_dir(self): status, output = exec_command(self.compiler_f77 + ['-print-libgcc-file-name'], use_tee=0) if not status: return os.path.dirname(output) return None def get_library_dirs(self): opt = [] if sys.platform[:5] != 'linux': d = self.get_libgcc_dir() if d: # if windows and not cygwin, libg2c lies in a different folder if sys.platform == 'win32' and not d.startswith('/usr/lib'): d = os.path.normpath(d) if not os.path.exists(os.path.join(d, "lib%s.a" % self.g2c)): d2 = os.path.abspath(os.path.join(d, '../../../../lib')) if os.path.exists(os.path.join(d2, "lib%s.a" % self.g2c)): opt.append(d2) opt.append(d) return opt def get_libraries(self): opt = [] d = self.get_libgcc_dir() if d is not None: g2c = self.g2c + '-pic' f = self.static_lib_format % (g2c, self.static_lib_extension) if not os.path.isfile(os.path.join(d,f)): g2c = self.g2c else: g2c = self.g2c if g2c is not None: opt.append(g2c) c_compiler = self.c_compiler if sys.platform == 'win32' and c_compiler and \ c_compiler.compiler_type=='msvc': # the following code is not needed (read: breaks) when using MinGW # in case want to link F77 compiled code with MSVC opt.append('gcc') runtime_lib = msvc_runtime_library() if runtime_lib: opt.append(runtime_lib) if sys.platform == 'darwin': opt.append('cc_dynamic') return opt def get_flags_debug(self): return ['-g'] def get_flags_opt(self): v = self.get_version() if v and v<='3.3.3': # With this compiler version building Fortran BLAS/LAPACK # with -O3 caused failures in lib.lapack heevr,syevr tests. opt = ['-O2'] else: opt = ['-O3'] opt.append('-funroll-loops') return opt def _c_arch_flags(self): """ Return detected arch flags from CFLAGS """ from distutils import sysconfig try: cflags = sysconfig.get_config_vars()['CFLAGS'] except KeyError: return [] arch_re = re.compile(r"-arch\s+(\w+)") arch_flags = [] for arch in arch_re.findall(cflags): arch_flags += ['-arch', arch] return arch_flags def get_flags_arch(self): return [] class Gnu95FCompiler(GnuFCompiler): compiler_type = 'gnu95' compiler_aliases = ('gfortran',) description = 'GNU Fortran 95 compiler' def version_match(self, version_string): v = self.gnu_version_match(version_string) if not v or v[0] != 'gfortran': return None v = v[1] if v>='4.': # gcc-4 series releases do not support -mno-cygwin option pass else: # use -mno-cygwin flag for gfortran when Python is not Cygwin-Python if sys.platform == 'win32': for key in ['version_cmd', 'compiler_f77', 'compiler_f90', 'compiler_fix', 'linker_so', 'linker_exe']: self.executables[key].append('-mno-cygwin') return v # 'gfortran --version' results: # XXX is the below right? # Debian: GNU Fortran 95 (GCC 4.0.3 20051023 (prerelease) (Debian 4.0.2-3)) # GNU Fortran 95 (GCC) 4.1.2 20061115 (prerelease) (Debian 4.1.1-21) # OS X: GNU Fortran 95 (GCC) 4.1.0 # GNU Fortran 95 (GCC) 4.2.0 20060218 (experimental) # GNU Fortran (GCC) 4.3.0 20070316 (experimental) possible_executables = ['gfortran', 'f95'] executables = { 'version_cmd' : ["<F90>", "--version"], 'compiler_f77' : [None, "-Wall", "-ffixed-form", "-fno-second-underscore"] + _EXTRAFLAGS, 'compiler_f90' : [None, "-Wall", "-fno-second-underscore"] + _EXTRAFLAGS, 'compiler_fix' : [None, "-Wall", "-ffixed-form", "-fno-second-underscore"] + _EXTRAFLAGS, 'linker_so' : ["<F90>", "-Wall"], 'archiver' : ["ar", "-cr"], 'ranlib' : ["ranlib"], 'linker_exe' : [None, "-Wall"] } module_dir_switch = '-J' module_include_switch = '-I' g2c = 'gfortran' def _universal_flags(self, cmd): """Return a list of -arch flags for every supported architecture.""" if not sys.platform == 'darwin': return [] arch_flags = [] # get arches the C compiler gets. c_archs = self._c_arch_flags() if "i386" in c_archs: c_archs[c_archs.index("i386")] = "i686" # check the arches the Fortran compiler supports, and compare with # arch flags from C compiler for arch in ["ppc", "i686", "x86_64", "ppc64"]: if _can_target(cmd, arch) and arch in c_archs: arch_flags.extend(["-arch", arch]) return arch_flags def get_flags(self): flags = GnuFCompiler.get_flags(self) arch_flags = self._universal_flags(self.compiler_f90) if arch_flags: flags[:0] = arch_flags return flags def get_flags_linker_so(self): flags = GnuFCompiler.get_flags_linker_so(self) arch_flags = self._universal_flags(self.linker_so) if arch_flags: flags[:0] = arch_flags return flags def get_library_dirs(self): opt = GnuFCompiler.get_library_dirs(self) if sys.platform == 'win32': c_compiler = self.c_compiler if c_compiler and c_compiler.compiler_type == "msvc": target = self.get_target() if target: d = os.path.normpath(self.get_libgcc_dir()) root = os.path.join(d, os.pardir, os.pardir, os.pardir, os.pardir) mingwdir = os.path.normpath(os.path.join(root, target, "lib")) full = os.path.join(mingwdir, "libmingwex.a") if os.path.exists(full): opt.append(mingwdir) return opt def get_libraries(self): opt = GnuFCompiler.get_libraries(self) if sys.platform == 'darwin': opt.remove('cc_dynamic') if sys.platform == 'win32': c_compiler = self.c_compiler if c_compiler and c_compiler.compiler_type == "msvc": if "gcc" in opt: i = opt.index("gcc") opt.insert(i+1, "mingwex") opt.insert(i+1, "mingw32") # XXX: fix this mess, does not work for mingw if is_win64(): c_compiler = self.c_compiler if c_compiler and c_compiler.compiler_type == "msvc": return [] else: raise NotImplementedError("Only MS compiler supported with gfortran on win64") return opt def get_target(self): status, output = exec_command(self.compiler_f77 + ['-v'], use_tee=0) if not status: m = TARGET_R.search(output) if m: return m.group(1) return "" def get_flags_opt(self): if is_win64(): return ['-O0'] else: return GnuFCompiler.get_flags_opt(self) def _can_target(cmd, arch): """Return true is the command supports the -arch flag for the given architecture.""" newcmd = cmd[:] fid, filename = tempfile.mkstemp(suffix=".f") try: d = os.path.dirname(filename) output = os.path.splitext(filename)[0] + ".o" try: newcmd.extend(["-arch", arch, "-c", filename]) p = Popen(newcmd, stderr=STDOUT, stdout=PIPE, cwd=d) p.communicate() return p.returncode == 0 finally: if os.path.exists(output): os.remove(output) finally: os.remove(filename) return False if __name__ == '__main__': from distutils import log log.set_verbosity(2) compiler = GnuFCompiler() compiler.customize() print(compiler.get_version()) raw_input('Press ENTER to continue...') try: compiler = Gnu95FCompiler() compiler.customize() print(compiler.get_version()) except Exception: msg = get_exception() print(msg) raw_input('Press ENTER to continue...')
# Copyright 2017 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== """Inception Resnet v2 Faster R-CNN implementation. See "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning" by Szegedy et al. (https://arxiv.org/abs/1602.07261) as well as "Speed/accuracy trade-offs for modern convolutional object detectors" by Huang et al. (https://arxiv.org/abs/1611.10012) """ import tensorflow as tf from object_detection.meta_architectures import faster_rcnn_meta_arch from nets import inception_resnet_v2 slim = tf.contrib.slim class FasterRCNNInceptionResnetV2FeatureExtractor( faster_rcnn_meta_arch.FasterRCNNFeatureExtractor): """Faster R-CNN with Inception Resnet v2 feature extractor implementation.""" def __init__(self, is_training, first_stage_features_stride, reuse_weights=None, weight_decay=0.0): """Constructor. Args: is_training: See base class. first_stage_features_stride: See base class. reuse_weights: See base class. weight_decay: See base class. Raises: ValueError: If `first_stage_features_stride` is not 8 or 16. """ if first_stage_features_stride != 8 and first_stage_features_stride != 16: raise ValueError('`first_stage_features_stride` must be 8 or 16.') super(FasterRCNNInceptionResnetV2FeatureExtractor, self).__init__( is_training, first_stage_features_stride, reuse_weights, weight_decay) def preprocess(self, resized_inputs): """Faster R-CNN with Inception Resnet v2 preprocessing. Maps pixel values to the range [-1, 1]. Args: resized_inputs: A [batch, height_in, width_in, channels] float32 tensor representing a batch of images with values between 0 and 255.0. Returns: preprocessed_inputs: A [batch, height_out, width_out, channels] float32 tensor representing a batch of images. """ return (2.0 / 255.0) * resized_inputs - 1.0 def _extract_proposal_features(self, preprocessed_inputs, scope): """Extracts first stage RPN features. Extracts features using the first half of the Inception Resnet v2 network. We construct the network in `align_feature_maps=True` mode, which means that all VALID paddings in the network are changed to SAME padding so that the feature maps are aligned. Args: preprocessed_inputs: A [batch, height, width, channels] float32 tensor representing a batch of images. scope: A scope name. Returns: rpn_feature_map: A tensor with shape [batch, height, width, depth] Raises: InvalidArgumentError: If the spatial size of `preprocessed_inputs` (height or width) is less than 33. ValueError: If the created network is missing the required activation. """ if len(preprocessed_inputs.get_shape().as_list()) != 4: raise ValueError('`preprocessed_inputs` must be 4 dimensional, got a ' 'tensor of shape %s' % preprocessed_inputs.get_shape()) with slim.arg_scope(inception_resnet_v2.inception_resnet_v2_arg_scope( weight_decay=self._weight_decay)): # Forces is_training to False to disable batch norm update. with slim.arg_scope([slim.batch_norm], is_training=False): with tf.variable_scope('InceptionResnetV2', reuse=self._reuse_weights) as scope: rpn_feature_map, _ = ( inception_resnet_v2.inception_resnet_v2_base( preprocessed_inputs, final_endpoint='PreAuxLogits', scope=scope, output_stride=self._first_stage_features_stride, align_feature_maps=True)) return rpn_feature_map def _extract_box_classifier_features(self, proposal_feature_maps, scope): """Extracts second stage box classifier features. This function reconstructs the "second half" of the Inception ResNet v2 network after the part defined in `_extract_proposal_features`. Args: proposal_feature_maps: A 4-D float tensor with shape [batch_size * self.max_num_proposals, crop_height, crop_width, depth] representing the feature map cropped to each proposal. scope: A scope name. Returns: proposal_classifier_features: A 4-D float tensor with shape [batch_size * self.max_num_proposals, height, width, depth] representing box classifier features for each proposal. """ with tf.variable_scope('InceptionResnetV2', reuse=self._reuse_weights): with slim.arg_scope(inception_resnet_v2.inception_resnet_v2_arg_scope( weight_decay=self._weight_decay)): # Forces is_training to False to disable batch norm update. with slim.arg_scope([slim.batch_norm], is_training=False): with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], stride=1, padding='SAME'): with tf.variable_scope('Mixed_7a'): with tf.variable_scope('Branch_0'): tower_conv = slim.conv2d(proposal_feature_maps, 256, 1, scope='Conv2d_0a_1x1') tower_conv_1 = slim.conv2d( tower_conv, 384, 3, stride=2, padding='VALID', scope='Conv2d_1a_3x3') with tf.variable_scope('Branch_1'): tower_conv1 = slim.conv2d( proposal_feature_maps, 256, 1, scope='Conv2d_0a_1x1') tower_conv1_1 = slim.conv2d( tower_conv1, 288, 3, stride=2, padding='VALID', scope='Conv2d_1a_3x3') with tf.variable_scope('Branch_2'): tower_conv2 = slim.conv2d( proposal_feature_maps, 256, 1, scope='Conv2d_0a_1x1') tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3, scope='Conv2d_0b_3x3') tower_conv2_2 = slim.conv2d( tower_conv2_1, 320, 3, stride=2, padding='VALID', scope='Conv2d_1a_3x3') with tf.variable_scope('Branch_3'): tower_pool = slim.max_pool2d( proposal_feature_maps, 3, stride=2, padding='VALID', scope='MaxPool_1a_3x3') net = tf.concat( [tower_conv_1, tower_conv1_1, tower_conv2_2, tower_pool], 3) net = slim.repeat(net, 9, inception_resnet_v2.block8, scale=0.20) net = inception_resnet_v2.block8(net, activation_fn=None) proposal_classifier_features = slim.conv2d( net, 1536, 1, scope='Conv2d_7b_1x1') return proposal_classifier_features def restore_from_classification_checkpoint_fn( self, first_stage_feature_extractor_scope, second_stage_feature_extractor_scope): """Returns a map of variables to load from a foreign checkpoint. Note that this overrides the default implementation in faster_rcnn_meta_arch.FasterRCNNFeatureExtractor which does not work for InceptionResnetV2 checkpoints. TODO: revisit whether it's possible to force the `Repeat` namescope as created in `_extract_box_classifier_features` to start counting at 2 (e.g. `Repeat_2`) so that the default restore_fn can be used. Args: first_stage_feature_extractor_scope: A scope name for the first stage feature extractor. second_stage_feature_extractor_scope: A scope name for the second stage feature extractor. Returns: A dict mapping variable names (to load from a checkpoint) to variables in the model graph. """ variables_to_restore = {} for variable in tf.global_variables(): if variable.op.name.startswith( first_stage_feature_extractor_scope): var_name = variable.op.name.replace( first_stage_feature_extractor_scope + '/', '') variables_to_restore[var_name] = variable if variable.op.name.startswith( second_stage_feature_extractor_scope): var_name = variable.op.name.replace( second_stage_feature_extractor_scope + '/InceptionResnetV2/Repeat', 'InceptionResnetV2/Repeat_2') var_name = var_name.replace( second_stage_feature_extractor_scope + '/', '') variables_to_restore[var_name] = variable return variables_to_restore
############################################################################### # # Tests for XlsxWriter. # # Copyright (c), 2013-2016, John McNamara, jmcnamara@cpan.org # from ..excel_comparsion_test import ExcelComparisonTest from ...workbook import Workbook class TestCompareXLSXFiles(ExcelComparisonTest): """ Test file created by XlsxWriter against a file created by Excel. """ def setUp(self): self.maxDiff = None filename = 'fit_to_pages01.xlsx' test_dir = 'xlsxwriter/test/comparison/' self.got_filename = test_dir + '_test_' + filename self.exp_filename = test_dir + 'xlsx_files/' + filename self.ignore_files = ['xl/printerSettings/printerSettings1.bin', 'xl/worksheets/_rels/sheet1.xml.rels'] self.ignore_elements = {'[Content_Types].xml': ['<Default Extension="bin"'], 'xl/worksheets/sheet1.xml': ['<pageMargins', '<pageSetup']} def test_create_file(self): """Test the creation of a simple XlsxWriter file with fit to print.""" workbook = Workbook(self.got_filename) worksheet = workbook.add_worksheet() worksheet.fit_to_pages(1, 1) worksheet.set_paper(9) worksheet.write('A1', 'Foo') workbook.close() self.assertExcelEqual()
#!/bin/python import fnmatch import os import shutil import subprocess import sys line_nb = False for arg in sys.argv[1:]: if (arg == "--with-line-nb"): print("Enabling line numbers in the context locations.") line_nb = True else: os.sys.exit("Non supported argument '" + arg + "'. Aborting.") if (not os.path.exists("tools")): os.sys.exit("ERROR: This script should be started from the root of the git repo.") matches = [] for root, dirnames, filenames in os.walk('.'): for filename in fnmatch.filter(filenames, '*.cpp'): if (filename.find("collada") != -1): continue matches.append(os.path.join(root, filename)) for filename in fnmatch.filter(filenames, '*.h'): if (filename.find("collada") != -1): continue matches.append(os.path.join(root, filename)) matches.sort() unique_str = [] unique_loc = {} main_po = """ # LANGUAGE translation of the Godot Engine editor # Copyright (C) 2016 Juan Linietsky, Ariel Manzur and the Godot community # This file is distributed under the same license as the Godot source code. # FIRST AUTHOR <EMAIL@ADDRESS>, YEAR. # #, fuzzy msgid "" msgstr "" "Project-Id-Version: Godot Engine editor\\n" "Content-Type: text/plain; charset=UTF-8\\n" "Content-Transfer-Encoding: 8-bit\\n" """ print("Updating the tools.pot template...") for fname in matches: f = open(fname, "rb") l = f.readline() lc = 1 while (l): patterns = ['RTR(\"', 'TTR(\"'] idx = 0 pos = 0 while (pos >= 0): pos = l.find(patterns[idx], pos) if (pos == -1): if (idx < len(patterns) - 1): idx += 1 pos = 0 continue pos += 5 msg = "" while (pos < len(l) and (l[pos] != '"' or l[pos - 1] == '\\')): msg += l[pos] pos += 1 location = os.path.relpath(fname).replace('\\','/') if (line_nb): location += ":" + str(lc) if (not msg in unique_str): main_po += "\n#: " + location + "\n" main_po += 'msgid "' + msg + '"\n' main_po += 'msgstr ""\n' unique_str.append(msg) unique_loc[msg] = [location] elif (not location in unique_loc[msg]): # Add additional location to previous occurence too msg_pos = main_po.find('\nmsgid "' + msg + '"') if (msg_pos == -1): print("Someone apparently thought writing Python was as easy as GDScript. Ping Akien.") main_po = main_po[:msg_pos] + ' ' + location + main_po[msg_pos:] unique_loc[msg].append(location) l = f.readline() lc += 1 f.close() f = open("tools.pot", "wb") f.write(main_po) f.close() if (os.name == "posix"): os.system("msgmerge -w80 tools.pot tools.pot > tools.pot.wrap") shutil.move("tools.pot.wrap", "tools.pot") shutil.move("tools.pot", "tools/translations/tools.pot") # TODO: Make that in a portable way, if we care; if not, kudos to Unix users if (os.name == "posix"): added = subprocess.check_output("git diff tools/translations/tools.pot | grep \+msgid | wc -l", shell = True) removed = subprocess.check_output("git diff tools/translations/tools.pot | grep \\\-msgid | wc -l", shell = True) print("\n# Template changes compared to the staged status:") print("# Additions: %s msgids.\n# Deletions: %s msgids." % (int(added), int(removed)))
#!/usr/bin/env python3 # -*- coding: utf-8 -*- # # kerrpy documentation build configuration file, created by # sphinx-quickstart on Wed Aug 10 14:00:40 2016. # # This file is execfile()d with the current directory set to its # containing dir. # # Note that not all possible configuration values are present in this # autogenerated file. # # All configuration values have a default; values that are commented out # serve to show the default. # If extensions (or modules to document with autodoc) are in another directory, # add these directories to sys.path here. If the directory is relative to the # documentation root, use os.path.abspath to make it absolute, like shown here. # import sphinx_bootstrap_theme import os import sys sys.path.insert(0, os.path.abspath('../../kerrpy')) print(sys.path) # -- General configuration ------------------------------------------------ # If your documentation needs a minimal Sphinx version, state it here. # # needs_sphinx = '1.0' # Add any Sphinx extension module names here, as strings. They can be # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom # ones. extensions = ['sphinx.ext.autodoc', 'sphinx.ext.napoleon', 'sphinx.ext.todo', 'sphinx.ext.mathjax', 'sphinx.ext.viewcode', 'sphinx.ext.githubpages', 'sphinxcontrib.bibtex', 'breathe'] # Add any paths that contain templates here, relative to this directory. templates_path = ['_templates'] # The suffix(es) of source filenames. # You can specify multiple suffix as a list of string: # # source_suffix = ['.rst', '.md'] source_suffix = '.rst' # The encoding of source files. # # source_encoding = 'utf-8-sig' # The master toctree document. master_doc = 'index' # General information about the project. project = 'kerrpy' copyright = '2017, Pablo Galindo and Alejandro Garca' author = 'Pablo Galindo and Alejandro Garca' # The version info for the project you're documenting, acts as replacement for # |version| and |release|, also used in various other places throughout the # built documents. # # The short X.Y version. version = '0.1' # The full version, including alpha/beta/rc tags. release = '0.1' # The language for content autogenerated by Sphinx. Refer to documentation # for a list of supported languages. # # This is also used if you do content translation via gettext catalogs. # Usually you set "language" from the command line for these cases. language = None # There are two options for replacing |today|: either, you set today to some # non-false value, then it is used: # # today = '' # # Else, today_fmt is used as the format for a strftime call. # # today_fmt = '%B %d, %Y' # List of patterns, relative to source directory, that match files and # directories to ignore when looking for source files. # This patterns also effect to html_static_path and html_extra_path exclude_patterns = [] # The reST default role (used for this markup: `text`) to use for all # documents. # # default_role = None # If true, '()' will be appended to :func: etc. cross-reference text. # # add_function_parentheses = True # If true, the current module name will be prepended to all description # unit titles (such as .. function::). # # add_module_names = True # If true, sectionauthor and moduleauthor directives will be shown in the # output. They are ignored by default. # # show_authors = False # The name of the Pygments (syntax highlighting) style to use. pygments_style = 'sphinx' # A list of ignored prefixes for module index sorting. # modindex_common_prefix = [] # If true, keep warnings as "system message" paragraphs in the built documents. # keep_warnings = False # If true, `todo` and `todoList` produce output, else they produce nothing. todo_include_todos = True # -- Options for HTML output ---------------------------------------------- # The theme to use for HTML and HTML Help pages. See the documentation for # a list of builtin themes. # html_theme = 'bootstrap' # Theme options are theme-specific and customize the look and feel of a theme # further. For a list of options available for each theme, see the # documentation. # html_theme_options = {'bootswatch_theme': "paper"} # Add any paths that contain custom themes here, relative to this directory. html_theme_path = sphinx_bootstrap_theme.get_html_theme_path() # The name for this set of Sphinx documents. # "<project> v<release> documentation" by default. # # html_title = 'kerrpy v0.1' # A shorter title for the navigation bar. Default is the same as html_title. # # html_short_title = None # The name of an image file (relative to this directory) to place at the top # of the sidebar. # # html_logo = None # The name of an image file (relative to this directory) to use as a favicon of # the docs. This file should be a Windows icon file (.ico) being 16x16 or 32x32 # pixels large. # # html_favicon = None # Add any paths that contain custom static files (such as style sheets) here, # relative to this directory. They are copied after the builtin static files, # so a file named "default.css" will overwrite the builtin "default.css". html_static_path = ['_static'] # Add any extra paths that contain custom files (such as robots.txt or # .htaccess) here, relative to this directory. These files are copied # directly to the root of the documentation. # # html_extra_path = [] # If not None, a 'Last updated on:' timestamp is inserted at every page # bottom, using the given strftime format. # The empty string is equivalent to '%b %d, %Y'. # # html_last_updated_fmt = None # If true, SmartyPants will be used to convert quotes and dashes to # typographically correct entities. # # html_use_smartypants = True # Custom sidebar templates, maps document names to template names. # # html_sidebars = {} # Additional templates that should be rendered to pages, maps page names to # template names. # # html_additional_pages = {} # If false, no module index is generated. # # html_domain_indices = True # If false, no index is generated. # # html_use_index = True # If true, the index is split into individual pages for each letter. # # html_split_index = False # If true, links to the reST sources are added to the pages. # # html_show_sourcelink = True # If true, "Created using Sphinx" is shown in the HTML footer. Default is True. # # html_show_sphinx = True # If true, "(C) Copyright ..." is shown in the HTML footer. Default is True. # # html_show_copyright = True # If true, an OpenSearch description file will be output, and all pages will # contain a <link> tag referring to it. The value of this option must be the # base URL from which the finished HTML is served. # # html_use_opensearch = '' # This is the file name suffix for HTML files (e.g. ".xhtml"). # html_file_suffix = None # Language to be used for generating the HTML full-text search index. # Sphinx supports the following languages: # 'da', 'de', 'en', 'es', 'fi', 'fr', 'h', 'it', 'ja' # 'nl', 'no', 'pt', 'ro', 'r', 'sv', 'tr', 'zh' # # html_search_language = 'en' # A dictionary with options for the search language support, empty by default. # 'ja' uses this config value. # 'zh' user can custom change `jieba` dictionary path. # # html_search_options = {'type': 'default'} # The name of a javascript file (relative to the configuration directory) that # implements a search results scorer. If empty, the default will be used. # # html_search_scorer = 'scorer.js' # Output file base name for HTML help builder. htmlhelp_basename = 'kerrpydoc' # -- Options for LaTeX output --------------------------------------------- latex_elements = { # The paper size ('letterpaper' or 'a4paper'). # # 'papersize': 'letterpaper', # The font size ('10pt', '11pt' or '12pt'). # # 'pointsize': '10pt', # Additional stuff for the LaTeX preamble. # # 'preamble': '', # Latex figure (float) alignment # # 'figure_align': 'htbp', } # Grouping the document tree into LaTeX files. List of tuples # (source start file, target name, title, # author, documentclass [howto, manual, or own class]). # latex_documents = [(master_doc, 'kerrpy.tex', 'kerrpy Documentation', 'Alejandro Garca Montoro', 'manual'), ] # The name of an image file (relative to this directory) to place at the top of # the title page. # # latex_logo = None # For "manual" documents, if this is true, then toplevel headings are parts, # not chapters. # # latex_use_parts = False # If true, show page references after internal links. # # latex_show_pagerefs = False # If true, show URL addresses after external links. # # latex_show_urls = False # Documents to append as an appendix to all manuals. # # latex_appendices = [] # It false, will not define \strong, \code, itleref, \crossref ... but only # \sphinxstrong, ..., \sphinxtitleref, ... To help avoid clash with user added # packages. # # latex_keep_old_macro_names = True # If false, no module index is generated. # # latex_domain_indices = True # -- Options for manual page output --------------------------------------- # One entry per manual page. List of tuples # (source start file, name, description, authors, manual section). man_pages = [(master_doc, 'kerrpy', 'kerrpy Documentation', [author], 1)] # If true, show URL addresses after external links. # # man_show_urls = False # -- Options for Texinfo output ------------------------------------------- # Grouping the document tree into Texinfo files. List of tuples # (source start file, target name, title, author, # dir menu entry, description, category) texinfo_documents = [(master_doc, 'kerrpy', 'kerrpy Documentation', author, 'kerrpy', 'One line description of project.', 'Miscellaneous'), ] # Documents to append as an appendix to all manuals. # # texinfo_appendices = [] # If false, no module index is generated. # # texinfo_domain_indices = True # How to display URL addresses: 'footnote', 'no', or 'inline'. # # texinfo_show_urls = 'footnote' # If true, do not generate a @detailmenu in the "Top" node's menu. # # texinfo_no_detailmenu = False # Number the figures numfig = True # # Taken from http://stackoverflow.com/a/5599712 # def skip(app, what, name, obj, skip, options): # if name == "__init__": # return False # return skip # # # def setup(app): # app.connect("autodoc-skip-member", skip) autodoc_default_flags = ['members', 'special-members'] # -- Doxygen configuration ------------------------------------------------ breathe_projects = {"kerrpy": "../build/doxygen/xml/", } breathe_default_project = "kerrpy" # Sphinx goes nuts with cuda macros, so we need to add them (see http://www.sphinx-doc.org/en/1.5.1/config.html#confval-cpp_id_attributes) cpp_id_attributes = ['__device__', '__global__']
"""Exceptions used throughout package""" class PipError(Exception): """Base pip exception""" class InstallationError(PipError): """General exception during installation""" class UninstallationError(PipError): """General exception during uninstallation""" class DistributionNotFound(InstallationError): """Raised when a distribution cannot be found to satisfy a requirement""" class BestVersionAlreadyInstalled(PipError): """Raised when the most up-to-date version of a package is already installed. """ class BadCommand(PipError): """Raised when virtualenv or a command is not found""" class CommandError(PipError): """Raised when there is an error in command-line arguments""" class PreviousBuildDirError(PipError): """Raised when there's a previous conflicting build directory""" class HashMismatch(InstallationError): """Distribution file hash values don't match.""" class InvalidWheelFilename(InstallationError): """Invalid wheel filename.""" class UnsupportedWheel(InstallationError): """Unsupported wheel."""
import json from django.contrib.postgres import forms, lookups from django.contrib.postgres.fields.array import ArrayField from django.core import exceptions from django.db.models import Field, TextField, Transform from django.utils import six from django.utils.translation import ugettext_lazy as _ __all__ = ['HStoreField'] class HStoreField(Field): empty_strings_allowed = False description = _('Map of strings to strings') default_error_messages = { 'not_a_string': _('The value of "%(key)s" is not a string.'), } def db_type(self, connection): return 'hstore' def get_transform(self, name): transform = super(HStoreField, self).get_transform(name) if transform: return transform return KeyTransformFactory(name) def validate(self, value, model_instance): super(HStoreField, self).validate(value, model_instance) for key, val in value.items(): if not isinstance(val, six.string_types): raise exceptions.ValidationError( self.error_messages['not_a_string'], code='not_a_string', params={'key': key}, ) def to_python(self, value): if isinstance(value, six.string_types): value = json.loads(value) return value def value_to_string(self, obj): value = self._get_val_from_obj(obj) return json.dumps(value) def formfield(self, **kwargs): defaults = { 'form_class': forms.HStoreField, } defaults.update(kwargs) return super(HStoreField, self).formfield(**defaults) HStoreField.register_lookup(lookups.DataContains) HStoreField.register_lookup(lookups.ContainedBy) @HStoreField.register_lookup class HasKeyLookup(lookups.PostgresSimpleLookup): lookup_name = 'has_key' operator = '?' @HStoreField.register_lookup class HasKeysLookup(lookups.PostgresSimpleLookup): lookup_name = 'has_keys' operator = '?&' class KeyTransform(Transform): output_field = TextField() def __init__(self, key_name, *args, **kwargs): super(KeyTransform, self).__init__(*args, **kwargs) self.key_name = key_name def as_sql(self, compiler, connection): lhs, params = compiler.compile(self.lhs) return "%s -> '%s'" % (lhs, self.key_name), params class KeyTransformFactory(object): def __init__(self, key_name): self.key_name = key_name def __call__(self, *args, **kwargs): return KeyTransform(self.key_name, *args, **kwargs) @HStoreField.register_lookup class KeysTransform(lookups.FunctionTransform): lookup_name = 'keys' function = 'akeys' output_field = ArrayField(TextField()) @HStoreField.register_lookup class ValuesTransform(lookups.FunctionTransform): lookup_name = 'values' function = 'avals' output_field = ArrayField(TextField())
# All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from tempest.lib.common import http from tempest.tests import base class TestClosingHttp(base.TestCase): def setUp(self): super(TestClosingHttp, self).setUp() self.cert_none = "CERT_NONE" self.cert_location = "/etc/ssl/certs/ca-certificates.crt" def test_constructor_invalid_ca_certs_and_timeout(self): connection = http.ClosingHttp( disable_ssl_certificate_validation=False, ca_certs=None, timeout=None) for attr in ('cert_reqs', 'ca_certs', 'timeout'): self.assertNotIn(attr, connection.connection_pool_kw) def test_constructor_valid_ca_certs(self): cert_required = 'CERT_REQUIRED' connection = http.ClosingHttp( disable_ssl_certificate_validation=False, ca_certs=self.cert_location, timeout=None) self.assertEqual(cert_required, connection.connection_pool_kw['cert_reqs']) self.assertEqual(self.cert_location, connection.connection_pool_kw['ca_certs']) self.assertNotIn('timeout', connection.connection_pool_kw) def test_constructor_ssl_cert_validation_disabled(self): connection = http.ClosingHttp( disable_ssl_certificate_validation=True, ca_certs=None, timeout=30) self.assertEqual(self.cert_none, connection.connection_pool_kw['cert_reqs']) self.assertEqual(30, connection.connection_pool_kw['timeout']) self.assertNotIn('ca_certs', connection.connection_pool_kw) def test_constructor_ssl_cert_validation_disabled_and_ca_certs(self): connection = http.ClosingHttp( disable_ssl_certificate_validation=True, ca_certs=self.cert_location, timeout=None) self.assertNotIn('timeout', connection.connection_pool_kw) self.assertEqual(self.cert_none, connection.connection_pool_kw['cert_reqs']) self.assertNotIn('ca_certs', connection.connection_pool_kw)
from functools import wraps from django.utils.decorators import decorator_from_middleware_with_args, available_attrs from django.utils.cache import patch_cache_control, add_never_cache_headers from django.middleware.cache import CacheMiddleware def cache_page(*args, **kwargs): """ Decorator for views that tries getting the page from the cache and populates the cache if the page isn't in the cache yet. The cache is keyed by the URL and some data from the headers. Additionally there is the key prefix that is used to distinguish different cache areas in a multi-site setup. You could use the sites.get_current_site().domain, for example, as that is unique across a Django project. Additionally, all headers from the response's Vary header will be taken into account on caching -- just like the middleware does. """ # We also add some asserts to give better error messages in case people are # using other ways to call cache_page that no longer work. if len(args) != 1 or callable(args[0]): raise TypeError("cache_page has a single mandatory positional argument: timeout") cache_timeout = args[0] cache_alias = kwargs.pop('cache', None) key_prefix = kwargs.pop('key_prefix', None) if kwargs: raise TypeError("cache_page has two optional keyword arguments: cache and key_prefix") return decorator_from_middleware_with_args(CacheMiddleware)(cache_timeout=cache_timeout, cache_alias=cache_alias, key_prefix=key_prefix) def cache_control(**kwargs): def _cache_controller(viewfunc): @wraps(viewfunc, assigned=available_attrs(viewfunc)) def _cache_controlled(request, *args, **kw): response = viewfunc(request, *args, **kw) patch_cache_control(response, **kwargs) return response return _cache_controlled return _cache_controller def never_cache(view_func): """ Decorator that adds headers to a response so that it will never be cached. """ @wraps(view_func, assigned=available_attrs(view_func)) def _wrapped_view_func(request, *args, **kwargs): response = view_func(request, *args, **kwargs) add_never_cache_headers(response) return response return _wrapped_view_func
# Copyright (c) 2016-present, Facebook, Inc. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. ############################################################################## from __future__ import absolute_import from __future__ import division from __future__ import print_function from __future__ import unicode_literals from caffe2.python.schema import ( Struct, FetchRecord, NewRecord, FeedRecord, InitEmptyRecord) from caffe2.python import core, workspace from caffe2.python.session import LocalSession from caffe2.python.dataset import Dataset from caffe2.python.pipeline import pipe from caffe2.python.queue_util import Queue from caffe2.python.task import TaskGroup from caffe2.python.test_util import TestCase from caffe2.python.net_builder import ops import numpy as np import math class TestPipeline(TestCase): def test_dequeue_many(self): init_net = core.Net('init') N = 17 NUM_DEQUEUE_RECORDS = 3 src_values = Struct( ('uid', np.array(range(N))), ('value', 0.1 * np.array(range(N)))) expected_dst = Struct( ('uid', 2 * np.array(range(N))), ('value', np.array(N * [0.0]))) with core.NameScope('init'): src_blobs = NewRecord(init_net, src_values) dst_blobs = InitEmptyRecord(init_net, src_values.clone_schema()) counter = init_net.Const(0) ONE = init_net.Const(1) def proc1(rec): with core.NameScope('proc1'): out = NewRecord(ops, rec) ops.Add([rec.uid(), rec.uid()], [out.uid()]) out.value.set(blob=rec.value(), unsafe=True) return out def proc2(rec): with core.NameScope('proc2'): out = NewRecord(ops, rec) out.uid.set(blob=rec.uid(), unsafe=True) ops.Sub([rec.value(), rec.value()], [out.value()]) ops.Add([counter, ONE], [counter]) return out src_ds = Dataset(src_blobs) dst_ds = Dataset(dst_blobs) with TaskGroup() as tg: out1 = pipe( src_ds.reader(), output=Queue( capacity=11, num_dequeue_records=NUM_DEQUEUE_RECORDS), processor=proc1) out2 = pipe(out1, processor=proc2) pipe(out2, dst_ds.writer()) ws = workspace.C.Workspace() FeedRecord(src_blobs, src_values, ws) session = LocalSession(ws) session.run(init_net) session.run(tg) output = FetchRecord(dst_blobs, ws=ws) num_dequeues = ws.blobs[str(counter)].fetch() self.assertEquals( num_dequeues, int(math.ceil(float(N) / NUM_DEQUEUE_RECORDS))) for a, b in zip(output.field_blobs(), expected_dst.field_blobs()): np.testing.assert_array_equal(a, b)
# -*- coding: utf-8 -*- ## @package ivf.cmds.save_depth # # ivf.cmds.save_depth utility package. # @author tody # @date 2016/02/02 from PyQt4.QtGui import * from PyQt4.QtCore import * import os from ivf.cmds.base_cmds import BaseCommand from ivf.scene.gl3d.image_plane import ImagePlane from ivf.io_util.obj_model import saveOBJ class SaveDepthCommand(BaseCommand): def __init__(self, scene, file_path="", parent=None): super(SaveDepthCommand, self).__init__(scene, "Save Depth Mesh", parent) self._file_path = file_path self._show_ui = file_path is "" self._root_dir = os.path.expanduser('~') def _runImp(self): if self._show_ui: self._file_path = str(QFileDialog.getSaveFileName(None, "Save Depth Mesh", self._root_dir, "Obj File (*.obj)" )) if self._file_path is "": return RGBA_8U = self._scene.image() D_32F = self._scene.depth() if D_32F is None: return model = ImagePlane(RGBA_8U) model.setDepth(D_32F) vertices = model.mesh().positions() index_array = model.mesh().indexArray() vertex_colors = model.mesh().vertexColors() saveOBJ(self._file_path, vertices, index_array, vertex_colors)
# -*- encoding: utf-8 -*- ############################################################################## # # Account Payment Blocking module for Odoo # Copyright (C) 2014-2015 ACSONE SA/NV (http://acsone.eu) # @author Stphane Bidoul <stephane.bidoul@acsone.eu> # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## from . import model from . import tests
from trex_astf_lib.api import * # IPV6 tunable example # # ipv6.src_msb # ipv6.dst_msb # ipv6.enable # class Prof1(): def __init__(self): pass def get_profile(self, **kwargs): # ip generator ip_gen_c = ASTFIPGenDist(ip_range=["16.0.0.0", "16.0.0.255"], distribution="seq") ip_gen_s = ASTFIPGenDist(ip_range=["48.0.0.0", "48.0.255.255"], distribution="seq") ip_gen = ASTFIPGen(glob=ASTFIPGenGlobal(ip_offset="1.0.0.0"), dist_client=ip_gen_c, dist_server=ip_gen_s) c_glob_info = ASTFGlobalInfo() c_glob_info.tcp.mss = 1 return ASTFProfile(default_ip_gen=ip_gen, # Defaults affects all files default_c_glob_info=c_glob_info, cap_list=[ ASTFCapInfo(file="../avl/delay_10_http_browsing_0.pcap", cps=1) ] ) def register(): return Prof1()
# Copyright 2014 Pants project contributors (see CONTRIBUTORS.md). # Licensed under the Apache License, Version 2.0 (see LICENSE). """pex support for interacting with interpreters.""" from __future__ import absolute_import import os import re import subprocess import sys from collections import defaultdict from pkg_resources import Distribution, Requirement, find_distributions from .base import maybe_requirement from .compatibility import string from .tracer import TRACER try: from numbers import Integral except ImportError: Integral = (int, long) # Determine in the most platform-compatible way possible the identity of the interpreter # and its known packages. ID_PY = b""" import sys if hasattr(sys, 'pypy_version_info'): subversion = 'PyPy' elif sys.platform.startswith('java'): subversion = 'Jython' else: subversion = 'CPython' print("%s %s %s %s" % ( subversion, sys.version_info[0], sys.version_info[1], sys.version_info[2])) setuptools_path = None try: import pkg_resources except ImportError: sys.exit(0) requirements = {} for item in sys.path: for dist in pkg_resources.find_distributions(item): requirements[str(dist.as_requirement())] = dist.location for requirement_str, location in requirements.items(): rs = requirement_str.split('==', 2) if len(rs) == 2: print('%s %s %s' % (rs[0], rs[1], location)) """ class PythonIdentity(object): class Error(Exception): pass class InvalidError(Error): pass class UnknownRequirement(Error): pass # TODO(wickman) Support interpreter-specific versions, e.g. PyPy-2.2.1 HASHBANGS = { 'CPython': 'python%(major)d.%(minor)d', 'Jython': 'jython', 'PyPy': 'pypy', } @classmethod def get_subversion(cls): if hasattr(sys, 'pypy_version_info'): subversion = 'PyPy' elif sys.platform.startswith('java'): subversion = 'Jython' else: subversion = 'CPython' return subversion @classmethod def get(cls): return cls(cls.get_subversion(), sys.version_info[0], sys.version_info[1], sys.version_info[2]) @classmethod def from_id_string(cls, id_string): values = id_string.split() if len(values) != 4: raise cls.InvalidError("Invalid id string: %s" % id_string) return cls(str(values[0]), int(values[1]), int(values[2]), int(values[3])) @classmethod def from_path(cls, dirname): interp, version = dirname.split('-') major, minor, patch = version.split('.') return cls(str(interp), int(major), int(minor), int(patch)) def __init__(self, interpreter, major, minor, patch): for var in (major, minor, patch): assert isinstance(var, Integral) self._interpreter = interpreter self._version = (major, minor, patch) @property def interpreter(self): return self._interpreter @property def version(self): return self._version @property def requirement(self): return self.distribution.as_requirement() @property def distribution(self): return Distribution(project_name=self._interpreter, version='.'.join(map(str, self._version))) @classmethod def parse_requirement(cls, requirement, default_interpreter='CPython'): if isinstance(requirement, Requirement): return requirement elif isinstance(requirement, string): try: requirement = Requirement.parse(requirement) except ValueError: try: requirement = Requirement.parse('%s%s' % (default_interpreter, requirement)) except ValueError: raise ValueError('Unknown requirement string: %s' % requirement) return requirement else: raise ValueError('Unknown requirement type: %r' % (requirement,)) def matches(self, requirement): """Given a Requirement, check if this interpreter matches.""" try: requirement = self.parse_requirement(requirement, self._interpreter) except ValueError as e: raise self.UnknownRequirement(str(e)) return self.distribution in requirement def hashbang(self): hashbang_string = self.HASHBANGS.get(self.interpreter, 'CPython') % { 'major': self._version[0], 'minor': self._version[1], 'patch': self._version[2], } return '#!/usr/bin/env %s' % hashbang_string @property def python(self): # return the python version in the format of the 'python' key for distributions # specifically, '2.6', '2.7', '3.2', etc. return '%d.%d' % (self.version[0:2]) def __str__(self): return '%s-%s.%s.%s' % (self._interpreter, self._version[0], self._version[1], self._version[2]) def __repr__(self): return 'PythonIdentity(%r, %s, %s, %s)' % ( self._interpreter, self._version[0], self._version[1], self._version[2]) def __eq__(self, other): return all([isinstance(other, PythonIdentity), self.interpreter == other.interpreter, self.version == other.version]) def __hash__(self): return hash((self._interpreter, self._version)) class PythonInterpreter(object): REGEXEN = ( re.compile(r'jython$'), # NB: OSX ships python binaries named Python so we allow for capital-P. re.compile(r'[Pp]ython$'), re.compile(r'python[23].[0-9]$'), re.compile(r'pypy$'), re.compile(r'pypy-1.[0-9]$'), ) CACHE = {} # memoize executable => PythonInterpreter try: # Versions of distribute prior to the setuptools merge would automatically replace # 'setuptools' requirements with 'distribute'. It provided the 'replacement' kwarg # to toggle this, but it was removed post-merge. COMPATIBLE_SETUPTOOLS = Requirement.parse('setuptools>=1.0', replacement=False) except TypeError: COMPATIBLE_SETUPTOOLS = Requirement.parse('setuptools>=1.0') class Error(Exception): pass class IdentificationError(Error): pass class InterpreterNotFound(Error): pass @classmethod def get(cls): return cls.from_binary(sys.executable) @classmethod def all(cls, paths=None): if paths is None: paths = os.getenv('PATH', '').split(':') return cls.filter(cls.find(paths)) @classmethod def _parse_extras(cls, output_lines): def iter_lines(): for line in output_lines: try: dist_name, dist_version, location = line.split() except ValueError: raise cls.IdentificationError('Could not identify requirement: %s' % line) yield ((dist_name, dist_version), location) return dict(iter_lines()) @classmethod def _from_binary_internal(cls, path_extras): def iter_extras(): for item in sys.path + list(path_extras): for dist in find_distributions(item): if dist.version: yield ((dist.key, dist.version), dist.location) return cls(sys.executable, PythonIdentity.get(), dict(iter_extras())) @classmethod def _from_binary_external(cls, binary, path_extras): environ = cls.sanitized_environment() environ['PYTHONPATH'] = ':'.join(path_extras) po = subprocess.Popen( [binary], stdin=subprocess.PIPE, stdout=subprocess.PIPE, env=environ) so, _ = po.communicate(ID_PY) output = so.decode('utf8').splitlines() if len(output) == 0: raise cls.IdentificationError('Could not establish identity of %s' % binary) identity, extras = output[0], output[1:] return cls( binary, PythonIdentity.from_id_string(identity), extras=cls._parse_extras(extras)) @classmethod def expand_path(cls, path): if os.path.isfile(path): return [path] elif os.path.isdir(path): return [os.path.join(path, fn) for fn in os.listdir(path)] return [] @classmethod def from_env(cls, hashbang): """Resolve a PythonInterpreter as /usr/bin/env would. :param hashbang: A string, e.g. "python3.3" representing some binary on the $PATH. """ paths = os.getenv('PATH', '').split(':') for path in paths: for fn in cls.expand_path(path): basefile = os.path.basename(fn) if hashbang == basefile: try: return cls.from_binary(fn) except Exception as e: TRACER.log('Could not identify %s: %s' % (fn, e)) @classmethod def from_binary(cls, binary, path_extras=None): path_extras = path_extras or () if binary not in cls.CACHE: if binary == sys.executable: cls.CACHE[binary] = cls._from_binary_internal(path_extras) else: cls.CACHE[binary] = cls._from_binary_external(binary, path_extras) return cls.CACHE[binary] @classmethod def find(cls, paths): """ Given a list of files or directories, try to detect python interpreters amongst them. Returns a list of PythonInterpreter objects. """ pythons = [] for path in paths: for fn in cls.expand_path(path): basefile = os.path.basename(fn) if any(matcher.match(basefile) is not None for matcher in cls.REGEXEN): try: pythons.append(cls.from_binary(fn)) except Exception as e: TRACER.log('Could not identify %s: %s' % (fn, e)) continue return pythons @classmethod def filter(cls, pythons): """ Given a map of python interpreters in the format provided by PythonInterpreter.find(), filter out duplicate versions and versions we would prefer not to use. Returns a map in the same format as find. """ good = [] MAJOR, MINOR, SUBMINOR = range(3) def version_filter(version): return (version[MAJOR] == 2 and version[MINOR] >= 6 or version[MAJOR] == 3 and version[MINOR] >= 2) all_versions = set(interpreter.identity.version for interpreter in pythons) good_versions = filter(version_filter, all_versions) for version in good_versions: # For each candidate, use the latest version we find on the filesystem. candidates = defaultdict(list) for interp in pythons: if interp.identity.version == version: candidates[interp.identity.interpreter].append(interp) for interp_class in candidates: candidates[interp_class].sort( key=lambda interp: os.path.getmtime(interp.binary), reverse=True) good.append(candidates[interp_class].pop(0)) return good @classmethod def sanitized_environment(cls): # N.B. This is merely a hack because sysconfig.py on the default OS X # installation of 2.6/2.7 breaks. env_copy = os.environ.copy() env_copy.pop('MACOSX_DEPLOYMENT_TARGET', None) return env_copy @classmethod def replace(cls, requirement): self = cls.get() if self.identity.matches(requirement): return False for pi in cls.all(): if pi.identity.matches(requirement): break else: raise cls.InterpreterNotFound('Could not find interpreter matching filter!') os.execve(pi.binary, [pi.binary] + sys.argv, cls.sanitized_environment()) def __init__(self, binary, identity, extras=None): """Construct a PythonInterpreter. You should probably PythonInterpreter.from_binary instead. :param binary: The full path of the python binary. :param identity: The :class:`PythonIdentity` of the PythonInterpreter. :param extras: A mapping from (dist.key, dist.version) to dist.location of the extras associated with this interpreter. """ self._binary = os.path.realpath(binary) self._extras = extras or {} self._identity = identity def with_extra(self, key, version, location): extras = self._extras.copy() extras[(key, version)] = location return self.__class__(self._binary, self._identity, extras) @property def extras(self): return self._extras.copy() @property def binary(self): return self._binary @property def identity(self): return self._identity @property def python(self): return self._identity.python @property def version(self): return self._identity.version @property def version_string(self): return str(self._identity) def satisfies(self, capability): if not isinstance(capability, list): raise TypeError('Capability must be a list, got %s' % type(capability)) return not any(self.get_location(req) is None for req in capability) def get_location(self, req): req = maybe_requirement(req) for dist, location in self.extras.items(): dist_name, dist_version = dist if req.key == dist_name and dist_version in req: return location def __hash__(self): return hash((self._binary, self._identity)) def __eq__(self, other): if not isinstance(other, PythonInterpreter): return False return (self._binary, self._identity) == (other._binary, other._identity) def __lt__(self, other): if not isinstance(other, PythonInterpreter): return False return self.version < other.version def __repr__(self): return '%s(%r, %r, %r)' % (self.__class__.__name__, self._binary, self._identity, self._extras)
#!/usr/bin/env python # Copyright (c) 2011 The Chromium Authors. All rights reserved. # Use of this source code is governed by a BSD-style license that can be # found in the LICENSE file. """Issue a series of GetHash requests to the SafeBrowsing servers and measure the response times. Usage: $ ./gethash_timer.py --period=600 --samples=20 --output=resp.csv --period (or -p): The amount of time (in seconds) to wait between GetHash requests. Using a value of more than 300 (5 minutes) to include the effect of DNS. --samples (or -s): The number of requests to issue. If this parameter is not specified, the test will run indefinitely. --output (or -o): The path to a file where the output will be written in CSV format: sample_number,response_code,elapsed_time_ms """ import getopt import httplib import sys import time _GETHASH_HOST = 'safebrowsing.clients.google.com' _GETHASH_REQUEST = ( '/safebrowsing/gethash?client=googleclient&appver=1.0&pver=2.1') # Global logging file handle. g_file_handle = None def IssueGetHash(prefix): '''Issue one GetHash request to the safebrowsing servers. Args: prefix: A 4 byte value to look up on the server. Returns: The HTTP response code for the GetHash request. ''' body = '4:4\n' + prefix h = httplib.HTTPConnection(_GETHASH_HOST) h.putrequest('POST', _GETHASH_REQUEST) h.putheader('content-length', str(len(body))) h.endheaders() h.send(body) response_code = h.getresponse().status h.close() return response_code def TimedGetHash(prefix): '''Measure the amount of time it takes to receive a GetHash response. Args: prefix: A 4 byte value to look up on the the server. Returns: A tuple of HTTP resonse code and the response time (in milliseconds). ''' start = time.time() response_code = IssueGetHash(prefix) return response_code, (time.time() - start) * 1000 def RunTimedGetHash(period, samples=None): '''Runs an experiment to measure the amount of time it takes to receive multiple responses from the GetHash servers. Args: period: A floating point value that indicates (in seconds) the delay between requests. samples: An integer value indicating the number of requests to make. If 'None', the test continues indefinitely. Returns: None. ''' global g_file_handle prefix = '\x50\x61\x75\x6c' sample_count = 1 while True: response_code, elapsed_time = TimedGetHash(prefix) LogResponse(sample_count, response_code, elapsed_time) sample_count += 1 if samples is not None and sample_count == samples: break time.sleep(period) def LogResponse(sample_count, response_code, elapsed_time): '''Output the response for one GetHash query. Args: sample_count: The current sample number. response_code: The HTTP response code for the GetHash request. elapsed_time: The round-trip time (in milliseconds) for the GetHash request. Returns: None. ''' global g_file_handle output_list = (sample_count, response_code, elapsed_time) print 'Request: %d, status: %d, elapsed time: %f ms' % output_list if g_file_handle is not None: g_file_handle.write(('%d,%d,%f' % output_list) + '\n') g_file_handle.flush() def SetupOutputFile(file_name): '''Open a file for logging results. Args: file_name: A path to a file to store the output. Returns: None. ''' global g_file_handle g_file_handle = open(file_name, 'w') def main(): period = 10 samples = None options, args = getopt.getopt(sys.argv[1:], 's:p:o:', ['samples=', 'period=', 'output=']) for option, value in options: if option == '-s' or option == '--samples': samples = int(value) elif option == '-p' or option == '--period': period = float(value) elif option == '-o' or option == '--output': file_name = value else: print 'Bad option: %s' % option return 1 try: print 'Starting Timed GetHash ----------' SetupOutputFile(file_name) RunTimedGetHash(period, samples) except KeyboardInterrupt: pass print 'Timed GetHash complete ----------' g_file_handle.close() if __name__ == '__main__': sys.exit(main())
# -*- coding: utf-8 -*- # Copyright (c) 2015, Vispy Development Team. # Distributed under the (new) BSD License. See LICENSE.txt for more info. from __future__ import division import numpy as np from .line import LineVisual from ..color import ColorArray from ..color.colormap import _normalize, get_colormap from ..geometry.isocurve import isocurve from ..testing import has_matplotlib # checking for matplotlib _HAS_MPL = has_matplotlib() if _HAS_MPL: from matplotlib import _cntr as cntr class IsocurveVisual(LineVisual): """Displays an isocurve of a 2D scalar array. Parameters ---------- data : ndarray | None 2D scalar array. levels : ndarray, shape (Nlev,) | None The levels at which the isocurve is constructed from "*data*". color_lev : Color, colormap name, tuple, list or array The color to use when drawing the line. If a list is given, it must be of shape (Nlev), if an array is given, it must be of shape (Nlev, ...). and provide one color per level (rgba, colorname). clim : tuple (min, max) limits to apply when mapping level values through a colormap. **kwargs : dict Keyword arguments to pass to `LineVisual`. Notes ----- """ def __init__(self, data=None, levels=None, color_lev=None, clim=None, **kwargs): self._data = None self._levels = levels self._color_lev = color_lev self._clim = clim self._need_color_update = True self._need_level_update = True self._need_recompute = True self._X = None self._Y = None self._iso = None self._level_min = None self._data_is_uniform = False self._lc = None self._cl = None self._li = None self._connect = None self._verts = None kwargs['method'] = 'gl' kwargs['antialias'] = False LineVisual.__init__(self, **kwargs) if data is not None: self.set_data(data) @property def levels(self): """ The threshold at which the isocurve is constructed from the 2D data. """ return self._levels @levels.setter def levels(self, levels): self._levels = levels self._need_level_update = True self._need_recompute = True self.update() @property def color(self): return self._color_lev @color.setter def color(self, color): self._color_lev = color self._need_level_update = True self._need_color_update = True self.update() def set_data(self, data): """ Set the scalar array data Parameters ---------- data : ndarray A 2D array of scalar values. The isocurve is constructed to show all locations in the scalar field equal to ``self.levels``. """ self._data = data # if using matplotlib isoline algorithm we have to check for meshgrid # and we can setup the tracer object here if _HAS_MPL: if self._X is None or self._X.T.shape != data.shape: self._X, self._Y = np.meshgrid(np.arange(data.shape[0]), np.arange(data.shape[1])) self._iso = cntr.Cntr(self._X, self._Y, self._data.astype(float)) if self._clim is None: self._clim = (data.min(), data.max()) # sanity check, # should we raise an error here, since no isolines can be drawn? # for now, _prepare_draw returns False if no isoline can be drawn if self._data.min() != self._data.max(): self._data_is_uniform = False else: self._data_is_uniform = True self._need_recompute = True self.update() def _get_verts_and_connect(self, paths): """ retrieve vertices and connects from given paths-list """ verts = np.vstack(paths) gaps = np.add.accumulate(np.array([len(x) for x in paths])) - 1 connect = np.ones(gaps[-1], dtype=bool) connect[gaps[:-1]] = False return verts, connect def _compute_iso_line(self): """ compute LineVisual vertices, connects and color-index """ level_index = [] connects = [] verts = [] # calculate which level are within data range # this works for now and the existing examples, but should be tested # thoroughly also with the data-sanity check in set_data-function choice = np.nonzero((self.levels > self._data.min()) & (self._levels < self._data.max())) levels_to_calc = np.array(self.levels)[choice] # save minimum level index self._level_min = choice[0][0] for level in levels_to_calc: # if we use matplotlib isoline algorithm we need to add half a # pixel in both (x,y) dimensions because isolines are aligned to # pixel centers if _HAS_MPL: nlist = self._iso.trace(level, level, 0) paths = nlist[:len(nlist)//2] v, c = self._get_verts_and_connect(paths) v += np.array([0.5, 0.5]) else: paths = isocurve(self._data.astype(float).T, level, extend_to_edge=True, connected=True) v, c = self._get_verts_and_connect(paths) level_index.append(v.shape[0]) connects.append(np.hstack((c, [False]))) verts.append(v) self._li = np.hstack(level_index) self._connect = np.hstack(connects) self._verts = np.vstack(verts) def _compute_iso_color(self): """ compute LineVisual color from level index and corresponding color """ level_color = [] colors = self._lc for i, index in enumerate(self._li): level_color.append(np.zeros((index, 4)) + colors[i+self._level_min]) self._cl = np.vstack(level_color) def _levels_to_colors(self): # computes ColorArrays for given levels # try _color_lev as colormap, except as everything else try: f_color_levs = get_colormap(self._color_lev) except: colors = ColorArray(self._color_lev).rgba else: lev = _normalize(self._levels, self._clim[0], self._clim[1]) # map function expects (Nlev,1)! colors = f_color_levs.map(lev[:, np.newaxis]) # broadcast to (nlev, 4) array if len(colors) == 1: colors = colors * np.ones((len(self._levels), 1)) # detect color_lev/levels mismatch and raise error if (len(colors) != len(self._levels)): raise TypeError("Color/level mismatch. Color must be of shape " "(Nlev, ...) and provide one color per level") self._lc = colors def _prepare_draw(self, view): if (self._data is None or self._levels is None or self._color_lev is None or self._data_is_uniform): return False if self._need_level_update: self._levels_to_colors() self._need_level_update = False if self._need_recompute: self._compute_iso_line() self._compute_iso_color() LineVisual.set_data(self, pos=self._verts, connect=self._connect, color=self._cl) self._need_recompute = False if self._need_color_update: self._compute_iso_color() LineVisual.set_data(self, color=self._cl) self._need_color_update = False return LineVisual._prepare_draw(self, view)
# -*- coding: utf-8 -*- """ *************************************************************************** SagaAlgorithmsTests.py --------------------- Date : September 2017 Copyright : (C) 2017 by Alexander Bruy Email : alexander dot bruy at gmail dot com *************************************************************************** * * * This program is free software; you can redistribute it and/or modify * * it under the terms of the GNU General Public License as published by * * the Free Software Foundation; either version 2 of the License, or * * (at your option) any later version. * * * *************************************************************************** """ __author__ = 'Alexander Bruy' __date__ = 'September 2017' __copyright__ = '(C) 2017, Alexander Bruy' import os import nose2 import shutil import tempfile from qgis.core import (QgsProcessingParameterNumber, QgsProcessingParameterDefinition, QgsVectorLayer, QgsApplication, QgsFeature, QgsGeometry, QgsPointXY, QgsProcessingContext, QgsProject, QgsProcessingFeedback, QgsProcessingFeatureSourceDefinition) from qgis.testing import start_app, unittest from processing.algs.saga.SagaParameters import Parameters, SagaImageOutputParam import AlgorithmsTestBase class TestSagaAlgorithms(unittest.TestCase, AlgorithmsTestBase.AlgorithmsTest): @classmethod def setUpClass(cls): start_app() from processing.core.Processing import Processing Processing.initialize() cls.cleanup_paths = [] cls.temp_dir = tempfile.mkdtemp() cls.cleanup_paths.append(cls.temp_dir) @classmethod def tearDownClass(cls): from processing.core.Processing import Processing Processing.deinitialize() for path in cls.cleanup_paths: shutil.rmtree(path) def test_definition_file(self): return 'saga_algorithm_tests.yaml' def test_is_parameter_line(self): # Test determining whether a line is a parameter line self.assertFalse(Parameters.is_parameter_line('')) self.assertFalse(Parameters.is_parameter_line('xxxxxxxxx')) self.assertTrue(Parameters.is_parameter_line('QgsProcessingParameterNumber|R_PERCTL_MIN|Percentiles Range for RED max|QgsProcessingParameterNumber.Integer|1|False|1|99')) self.assertTrue(Parameters.is_parameter_line('*QgsProcessingParameterNumber|R_PERCTL_MIN|Percentiles Range for RED max|QgsProcessingParameterNumber.Integer|1|False|1|99')) self.assertTrue(Parameters.is_parameter_line('SagaImageOutput|RGB|Output RGB')) def test_param_line(self): # Test creating a parameter from a description line param = Parameters.create_parameter_from_line('QgsProcessingParameterNumber|R_PERCTL_MIN|Percentiles Range for RED max|QgsProcessingParameterNumber.Integer|1|False|1|99') self.assertIsInstance(param, QgsProcessingParameterNumber) self.assertEqual(param.name(), 'R_PERCTL_MIN') self.assertEqual(param.description(), 'Percentiles Range for RED max') self.assertEqual(param.dataType(), QgsProcessingParameterNumber.Integer) self.assertFalse(param.flags() & QgsProcessingParameterDefinition.FlagOptional) self.assertEqual(param.minimum(), 1) self.assertEqual(param.maximum(), 99) # Test SagaImageOutputParam line param = Parameters.create_parameter_from_line('SagaImageOutput|RGB|Output RGB') self.assertIsInstance(param, SagaImageOutputParam) self.assertEqual(param.name(), 'RGB') self.assertEqual(param.description(), 'Output RGB') self.assertEqual(param.defaultFileExtension(), 'tif') self.assertEqual(param.supportedOutputRasterLayerExtensions(), ['tif']) def test_non_ascii_output(self): # create a memory layer and add to project and context layer = QgsVectorLayer("Point?crs=epsg:3857&field=fldtxt:string&field=fldint:integer", "testmem", "memory") self.assertTrue(layer.isValid()) pr = layer.dataProvider() f = QgsFeature() f.setAttributes(["test", 123]) f.setGeometry(QgsGeometry.fromPointXY(QgsPointXY(100, 200))) f2 = QgsFeature() f2.setAttributes(["test2", 457]) f2.setGeometry(QgsGeometry.fromPointXY(QgsPointXY(110, 200))) self.assertTrue(pr.addFeatures([f, f2])) self.assertEqual(layer.featureCount(), 2) QgsProject.instance().addMapLayer(layer) context = QgsProcessingContext() context.setProject(QgsProject.instance()) alg = QgsApplication.processingRegistry().createAlgorithmById('saga:fixeddistancebuffer') self.assertIsNotNone(alg) temp_file = os.path.join(self.temp_dir, 'non_ascii_.shp') parameters = {'SHAPES': 'testmem', 'DIST_FIELD_DEFAULT': 5, 'NZONES': 1, 'DARC': 5, 'DISSOLVE': False, 'POLY_INNER': False, 'BUFFER': temp_file} feedback = QgsProcessingFeedback() results, ok = alg.run(parameters, context, feedback) self.assertTrue(ok) self.assertTrue(os.path.exists(temp_file)) # make sure that layer has correct features res = QgsVectorLayer(temp_file, 'res') self.assertTrue(res.isValid()) self.assertEqual(res.featureCount(), 2) QgsProject.instance().removeMapLayer(layer) if __name__ == '__main__': nose2.main()
""" String algorithms """ def balanced_parens(s: str) -> bool: open = 0 for c in s: if c=='(': open += 1 if c==')': if open > 0: open -= 1 else: return False return open==0 assert balanced_parens('') assert balanced_parens('()') assert balanced_parens('((()))') assert balanced_parens('((()()()))') assert balanced_parens('((()()()))()(())(()())') assert not balanced_parens('(()') assert not balanced_parens('((())))') assert not balanced_parens('((()())') assert not balanced_parens('())(()') def longest_valid_parens(s: str) -> int: """ return the length of the longest run of valid nested parens. Given a string containing just the characters '(' and ')', find the length of the longest well-formed substring. """ seeds = [(i,i+1) for i in range(len(s)-1) if s[i:i+2]=='()'] grew = True while grew or merged: grew = 0 merged = 0 # grow for i in range(len(seeds)): a,b = seeds[i] if a>0 and b+1<len(s) and s[a-1]=='(' and s[b+1]==')': grew += 1 seeds[i] = (a-1, b+1) # merge new_seeds = [] s0 = seeds[0] for s1 in seeds[1:]: if s0[1]+1==s1[0]: merged += 1 s0 = (s0[0], s1[1]) else: new_seeds.append(s0) s0 = s1 new_seeds.append(s0) seeds = new_seeds return max(b-a+1 for a,b in seeds)
# Authors: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr> # Denis Engemann <denis.engemann@gmail.com> # Eric Larson <larson.eric.d@gmail.com> # # License: BSD (3-clause) import numpy as np from .peak_finder import peak_finder from .. import pick_types, pick_channels from ..utils import logger, verbose from ..filter import band_pass_filter from ..epochs import Epochs @verbose def find_eog_events(raw, event_id=998, l_freq=1, h_freq=10, filter_length='10s', ch_name=None, tstart=0, verbose=None): """Locate EOG artifacts Parameters ---------- raw : instance of Raw The raw data. event_id : int The index to assign to found events. l_freq : float Low cut-off frequency in Hz. h_freq : float High cut-off frequency in Hz. filter_length : str | int | None Number of taps to use for filtering. ch_name: str | None If not None, use specified channel(s) for EOG tstart : float Start detection after tstart seconds. verbose : bool, str, int, or None If not None, override default verbose level (see mne.verbose). Returns ------- eog_events : array Events. """ # Getting EOG Channel eog_inds = _get_eog_channel_index(ch_name, raw) logger.info('EOG channel index for this subject is: %s' % eog_inds) eog, _ = raw[eog_inds, :] eog_events = _find_eog_events(eog, event_id=event_id, l_freq=l_freq, h_freq=h_freq, sampling_rate=raw.info['sfreq'], first_samp=raw.first_samp, filter_length=filter_length, tstart=tstart) return eog_events def _find_eog_events(eog, event_id, l_freq, h_freq, sampling_rate, first_samp, filter_length='10s', tstart=0.): """Helper function""" logger.info('Filtering the data to remove DC offset to help ' 'distinguish blinks from saccades') # filtering to remove dc offset so that we know which is blink and saccades fmax = np.minimum(45, sampling_rate / 2.0 - 0.75) # protect Nyquist filteog = np.array([band_pass_filter(x, sampling_rate, 2, fmax, filter_length=filter_length) for x in eog]) temp = np.sqrt(np.sum(filteog ** 2, axis=1)) indexmax = np.argmax(temp) # easier to detect peaks with filtering. filteog = band_pass_filter(eog[indexmax], sampling_rate, l_freq, h_freq, filter_length=filter_length) # detecting eog blinks and generating event file logger.info('Now detecting blinks and generating corresponding events') temp = filteog - np.mean(filteog) n_samples_start = int(sampling_rate * tstart) if np.abs(np.max(temp)) > np.abs(np.min(temp)): eog_events, _ = peak_finder(filteog[n_samples_start:], extrema=1) else: eog_events, _ = peak_finder(filteog[n_samples_start:], extrema=-1) eog_events += n_samples_start n_events = len(eog_events) logger.info("Number of EOG events detected : %d" % n_events) eog_events = np.array([eog_events + first_samp, np.zeros(n_events, int), event_id * np.ones(n_events, int)]).T return eog_events def _get_eog_channel_index(ch_name, inst): if isinstance(ch_name, str): # Check if multiple EOG Channels if ',' in ch_name: ch_name = ch_name.split(',') else: ch_name = [ch_name] eog_inds = pick_channels(inst.ch_names, include=ch_name) if len(eog_inds) == 0: raise ValueError('%s not in channel list' % ch_name) else: logger.info('Using channel %s as EOG channel%s' % ( " and ".join(ch_name), '' if len(eog_inds) < 2 else 's')) elif ch_name is None: eog_inds = pick_types(inst.info, meg=False, eeg=False, stim=False, eog=True, ecg=False, emg=False, ref_meg=False, exclude='bads') if len(eog_inds) == 0: logger.info('No EOG channels found') logger.info('Trying with EEG 061 and EEG 062') eog_inds = pick_channels(inst.ch_names, include=['EEG 061', 'EEG 062']) if len(eog_inds) != 2: raise RuntimeError('EEG 61 or EEG 62 channel not found !!') else: raise ValueError('Could not find EOG channel.') return eog_inds @verbose def create_eog_epochs(raw, ch_name=None, event_id=998, picks=None, tmin=-0.5, tmax=0.5, l_freq=1, h_freq=10, reject=None, flat=None, baseline=None, preload=True, verbose=None): """Conveniently generate epochs around EOG artifact events Parameters ---------- raw : instance of Raw The raw data ch_name : str The name of the channel to use for EOG peak detection. The argument is mandatory if the dataset contains no EOG channels. event_id : int The index to assign to found events picks : array-like of int | None (default) Indices of channels to include (if None, all channels are used). tmin : float Start time before event. tmax : float End time after event. l_freq : float Low pass frequency. h_freq : float High pass frequency. reject : dict | None Rejection parameters based on peak-to-peak amplitude. Valid keys are 'grad' | 'mag' | 'eeg' | 'eog' | 'ecg'. If reject is None then no rejection is done. Example:: reject = dict(grad=4000e-13, # T / m (gradiometers) mag=4e-12, # T (magnetometers) eeg=40e-6, # uV (EEG channels) eog=250e-6 # uV (EOG channels) ) flat : dict | None Rejection parameters based on flatness of signal. Valid keys are 'grad' | 'mag' | 'eeg' | 'eog' | 'ecg', and values are floats that set the minimum acceptable peak-to-peak amplitude. If flat is None then no rejection is done. baseline : tuple or list of length 2, or None The time interval to apply rescaling / baseline correction. If None do not apply it. If baseline is (a, b) the interval is between "a (s)" and "b (s)". If a is None the beginning of the data is used and if b is None then b is set to the end of the interval. If baseline is equal ot (None, None) all the time interval is used. If None, no correction is applied. preload : bool Preload epochs or not. verbose : bool, str, int, or None If not None, override default verbose level (see mne.verbose). Returns ------- eog_epochs : instance of Epochs Data epoched around EOG events. """ events = find_eog_events(raw, ch_name=ch_name, event_id=event_id, l_freq=l_freq, h_freq=h_freq) # create epochs around EOG events eog_epochs = Epochs(raw, events=events, event_id=event_id, tmin=tmin, tmax=tmax, proj=False, reject=reject, flat=flat, picks=picks, baseline=baseline, preload=preload) return eog_epochs
#!/usr/bin/env python # file: launcher.py import os import sys import logging from optparse import OptionParser, OptionGroup from eventbrain.util.daemon import Daemon FORMAT = '%(asctime)-15s:%(name)s:%(process)d:%(levelname)s === %(message)s' logging.basicConfig(format=FORMAT, level=logging.INFO, stream=sys.stdout) usage = "Usage: %prog [options] start|stop|restart" def set_daemonize(option, opt_str, value, parser): parser.values.daemonize = True parser = OptionParser(usage=usage) parser.add_option("-t", "--type", dest="type", help="type of object to process('actor', 'a' " "or 'decision', 'd')") parser.add_option("-i", "--id", dest="Id", help="Id of the object to process") parser.add_option("-p", "--pid-dir", dest="pid_dir", default="/var/run/eventbrain/", help="Directory to store pid files for daemonized objects. " "Default path is %default") parser.add_option("-l", "--log-file", dest="logfile", default='/dev/null', help="File to write logs. Default is %default") parser.add_option("-d", "--daemonize", dest="daemonize", action="callback", callback=set_daemonize, default=False, help="Start in daemon mode") parser.add_option("-o", "--options", dest="opts", default=None, help="Additional options to send to the class constructor") parser.add_option("-c", "--config", dest="config", default=None, help="Config file with initial settings. " "If a config file is provided, " "other parameters are ignored.") server_opts = OptionGroup(parser, "RabbitMQ options") server_opts.add_option("-s", "--server", dest="host", default='localhost', help="RabbitMQ server. Default is %default") server_opts.add_option("-u", "--user", dest="user", help="RabbitMQ credentials: username") server_opts.add_option("-v", "--vhost", dest="vhost", help="RabbitMQ credentials: virtual host") server_opts.add_option("-w", "--password", dest="password", help="RabbitMQ credentials: password") parser.add_option_group(server_opts) (options, args) = parser.parse_args() commands = ('start', 'stop', 'restart') types = ('actor', 'a', 'decision', 'd') command = args[0] class DaemonRunner(Daemon): def run(self): print "Run" if hasattr(options, "kwargs"): kwargs = options.kwargs else: kwargs = {} if options.opts: for opt in options.opts.split(";"): (k, v) = opt.split("=") kwargs[k] = v print "kwargs", kwargs if options.user and options.password: kwargs['user'] = options.user kwargs['vhost'] = options.vhost kwargs['password'] = options.password if options.host: kwargs['host'] = options.host inst = self.klass(**kwargs) try: inst.connect() except KeyboardInterrupt: inst.disconnect(reason="keyboard interruption") def run_actor(obj_id): print "Starting actor %s" % obj_id klass = _import('actors', obj_id) print "Found actor with exchange %s" % klass.id if options.daemonize: daemon = DaemonRunner(pid_file('a', klass), stdout=options.logfile, stderr=options.logfile) daemon.klass = klass daemon.start() else: kwargs = {} if options.user and options.password: kwargs['user'] = options.user kwargs['vhost'] = options.vhost kwargs['password'] = options.password if options.host: kwargs['host'] = options.host if options.opts: for opt in options.opts.split(";"): (k, v) = opt.split("=") kwargs[k] = v print "kwargs", kwargs inst = klass(**kwargs) try: inst.connect() except KeyboardInterrupt: inst.disconnect(reason="keyboard interruption") print "Done" def stop_actor(obj_id): print "Stopping actor %s" % obj_id klass = _import('actors', obj_id) daemon = DaemonRunner(pid_file('a', klass)) daemon.stop() print "Done" def run_decision(obj_id): print "Starting decision %s" % obj_id klass = _import('decisions', obj_id) print "Found decision with exchange %s" % klass.id if options.daemonize: daemon = DaemonRunner(pid_file('d', klass), stdout=options.logfile, stderr=options.logfile) daemon.klass = klass daemon.start() else: kwargs = {} if options.user and options.password: kwargs['user'] = options.user kwargs['vhost'] = options.vhost kwargs['password'] = options.password if options.host: kwargs['host'] = options.host if options.opts: for opt in options.opts.split(";"): (k, v) = opt.split("=") kwargs[k] = v print "kwargs", kwargs inst = klass(**kwargs) try: inst.connect() except KeyboardInterrupt: inst.disconnect(reason="keyboard interruption") print "Done" def stop_decision(obj_id): print "Stopping decision %s" % obj_id klass = _import('decisions', obj_id) daemon = DaemonRunner(pid_file('d', klass)) daemon.stop() print "Done" def pid_file(prefix, klass): pidfile = os.path.join(options.pid_dir, "".join([prefix, '-', klass.id, ".pid"])) pidfile = os.path.abspath(pidfile) print "PID file: %s" % pidfile return pidfile def _import(scope, obj_id): try: (_mod, _klass) = obj_id.split('.') module = __import__('eventbrain.contrib.%s.%s' % (scope, _mod), fromlist=[_klass]) klass = getattr(module, _klass) except Exception, ex: print "Cannot import class %s\n%r" % (obj_id, ex) exit(1) return klass def from_config(): import ConfigParser config = ConfigParser.RawConfigParser() config.readfp(open(options.config)) sections = config.sections() if config.has_section("Main"): if config.has_option("Main", "host"): parser.values.host = config.get("Main", "host") if config.has_option("Main", "user"): parser.values.user = config.get("Main", "user") if config.has_option("Main", "password"): parser.values.password = config.get("Main", "password") if config.has_option("Main", "vhost"): parser.values.vhost = config.get("Main", "vhost") for section in sections: print ">>> Found section ", section if section == "Main": continue else: # Fork to avoid exiting from main thread after daemonizing fpid = os.fork() if fpid != 0: process_section(config, section) exit(0) else: continue return True def process_section(config, section): if config.has_option(section, "type"): _type = config.get(section, "type") if _type not in types: print "Unrecognized type: %s" % _type return False kwargs = {} for item in config.items(section): if item[0] == "daemonize": parser.values.daemonize = config.getboolean(section, "daemonize") elif item[0] == "pid_dir": parser.values.pid_dir = item[1] elif item[0] == "log_file": parser.values.logfile = item[1] else: kwargs[item[0]] = item[1] print "kwargs", kwargs parser.values.kwargs = kwargs if _type in ('actor', 'a'): if command == "start": run_actor(section) elif command == "stop": stop_actor(section) elif command == "restart": stop_actor(section) run_actor(section) elif _type in ('decision', 'd'): if command == "start": run_decision(section) elif command == "stop": stop_decision(section) elif command == "restart": stop_decision(section) run_decision(section) if __name__ == "__main__": if options.config: if from_config(): exit(0) else: exit(1) if not options.type: print "Type not specified" exit(1) if options.type not in types: print "Unrecognized type: %s" % options.type exit(1) if not options.Id: print "Id not specified" exit(1) if not args or args[0] not in commands: print "Unknown command %s" % ",".join(args) exit(1) if options.type in ('actor', 'a'): # Actor if command == "start": run_actor(options.Id) elif command == "stop": stop_actor(options.Id) elif command == "restart": stop_actor(options.Id) run_actor(options.Id) if options.type in ('decision', 'd'): # Decision if command == "start": run_decision(options.Id) elif command == "stop": stop_decision(options.Id) elif command == "restart": stop_decision(options.Id) run_decision(options.Id)
# Copyright 2016 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== """Tests for initializers.""" from __future__ import absolute_import from __future__ import division from __future__ import print_function import numpy as np from scipy import stats import tensorflow as tf class ExponentialTest(tf.test.TestCase): def testExponentialLogPDF(self): with tf.Session(): batch_size = 6 lam = tf.constant([2.0] * batch_size) lam_v = 2.0 x = np.array([2.5, 2.5, 4.0, 0.1, 1.0, 2.0], dtype=np.float32) exponential = tf.contrib.distributions.Exponential(lam=lam) expected_log_pdf = stats.expon.logpdf(x, scale=1 / lam_v) log_pdf = exponential.log_pdf(x) self.assertEqual(log_pdf.get_shape(), (6,)) self.assertAllClose(log_pdf.eval(), expected_log_pdf) pdf = exponential.pdf(x) self.assertEqual(pdf.get_shape(), (6,)) self.assertAllClose(pdf.eval(), np.exp(expected_log_pdf)) def testExponentialCDF(self): with tf.Session(): batch_size = 6 lam = tf.constant([2.0] * batch_size) lam_v = 2.0 x = np.array([2.5, 2.5, 4.0, 0.1, 1.0, 2.0], dtype=np.float32) exponential = tf.contrib.distributions.Exponential(lam=lam) expected_cdf = stats.expon.cdf(x, scale=1 / lam_v) cdf = exponential.cdf(x) self.assertEqual(cdf.get_shape(), (6,)) self.assertAllClose(cdf.eval(), expected_cdf) def testExponentialMean(self): with tf.Session(): lam_v = np.array([1.0, 4.0, 2.5]) expected_mean = stats.expon.mean(scale=1 / lam_v) exponential = tf.contrib.distributions.Exponential(lam=lam_v) self.assertEqual(exponential.mean().get_shape(), (3,)) self.assertAllClose(exponential.mean().eval(), expected_mean) def testExponentialVariance(self): with tf.Session(): lam_v = np.array([1.0, 4.0, 2.5]) expected_variance = stats.expon.var(scale=1 / lam_v) exponential = tf.contrib.distributions.Exponential(lam=lam_v) self.assertEqual(exponential.variance().get_shape(), (3,)) self.assertAllClose(exponential.variance().eval(), expected_variance) def testExponentialEntropy(self): with tf.Session(): lam_v = np.array([1.0, 4.0, 2.5]) expected_entropy = stats.expon.entropy(scale=1 / lam_v) exponential = tf.contrib.distributions.Exponential(lam=lam_v) self.assertEqual(exponential.entropy().get_shape(), (3,)) self.assertAllClose(exponential.entropy().eval(), expected_entropy) def testExponentialSample(self): with self.test_session(): lam = tf.constant([3.0, 4.0]) lam_v = [3.0, 4.0] n = tf.constant(100000) exponential = tf.contrib.distributions.Exponential(lam=lam) samples = exponential.sample(n, seed=137) sample_values = samples.eval() self.assertEqual(sample_values.shape, (100000, 2)) self.assertFalse(np.any(sample_values < 0.0)) for i in range(2): self.assertLess( stats.kstest( sample_values[:, i], stats.expon(scale=1.0/lam_v[i]).cdf)[0], 0.01) def testExponentialSampleMultiDimensional(self): with self.test_session(): batch_size = 2 lam_v = [3.0, 22.0] lam = tf.constant([lam_v] * batch_size) exponential = tf.contrib.distributions.Exponential(lam=lam) n = 100000 samples = exponential.sample(n, seed=138) self.assertEqual(samples.get_shape(), (n, batch_size, 2)) sample_values = samples.eval() self.assertFalse(np.any(sample_values < 0.0)) for i in range(2): self.assertLess( stats.kstest( sample_values[:, 0, i], stats.expon(scale=1.0/lam_v[i]).cdf)[0], 0.01) self.assertLess( stats.kstest( sample_values[:, 1, i], stats.expon(scale=1.0/lam_v[i]).cdf)[0], 0.01) if __name__ == '__main__': tf.test.main()
import unittest from bs4 import BeautifulSoup import re def justOne(ls): assert(len(ls) == 1) return ls[0] def scrapePage(html_doc): soup = BeautifulSoup(html_doc, 'html.parser') ratings = [s.get_text() for s in soup.find_all("span",attrs={ "class": re.compile(r"game_review_summary .*")})] assert(len(ratings) != 1) reviewCounts = [x.attrs["content"] for x in soup.find_all("meta",attrs={"itemprop":"reviewCount"})] aList = [t.get_text() for t in soup.find_all("div",class_="game_area_details_specs")] def tagChecker(*things): for thing in things: if thing in aList: return True return False return { "title": justOne(soup.find_all("div",class_="apphub_AppName")).get_text() , "overall_rating" : ratings[1] if len(ratings) > 0 else None , "num_reviews" : reviewCounts[0] if len(reviewCounts) > 0 else None , "release_year" : justOne(soup.find_all("span",class_="date")).get_text()[-4:] , "user_tags" : [x.get_text().strip() for x in justOne(soup.find_all("div",class_="glance_tags popular_tags")).find_all("a")] , "multiplayer" : tagChecker("Multi-player") , "co-op" : tagChecker("Co-op") , "local_multiplayer" : tagChecker("Shared/Split Screen") , "steam_cloud" : tagChecker("Steam Cloud") , "controller_supported" : tagChecker("Full controller support", "Partial Controller Support") } class ScraperTests(unittest.TestCase): def assertKeyValue(self, d, key, value): self.assertIn(key, d) self.assertEqual(d[key], value) def test_example_page(self): with open("examples/Age of Wonders III on Steam.html", "r") as f: page_text = "".join(f.readlines()) res = scrapePage(page_text) self.assertKeyValue(res, "title", "Age of Wonders III") self.assertKeyValue(res, "overall_rating", "Very Positive") self.assertKeyValue(res, "num_reviews", "3504") self.assertKeyValue(res, "release_year","2014") #from class "release_date" self.assertKeyValue(res, "user_tags", ['Strategy', 'Turn-Based Strategy', 'Fantasy', 'RPG', '4X', 'Turn-Based', 'Multiplayer', 'Singleplayer', 'Tactical', 'Co-op', 'Adventure', 'Hex Grid', 'Great Soundtrack', 'Grand Strategy', 'Classic', 'Atmospheric', 'Moddable', 'Action', 'Female Protagonist', 'Indie']) #from class "glance_tags popular_tags" self.assertKeyValue(res, "multiplayer", True) #"Multi-Player" from class "game_area_details_specs" self.assertKeyValue(res, "co-op", True) #"Co-op" from class "game_area_details_specs" self.assertKeyValue(res, "local_multiplayer", True) #"Shared/Split Screen" from class "game_area_details_specs" self.assertKeyValue(res, "steam_cloud", True) #Cross-Platform Multiplayer from class "game_area_details_specs" self.assertKeyValue(res, "controller_supported", False) #Full OR Partial Controller Support from class "game_area_details_specs" def test_no_recent_reviews(self): with open("examples/No Recent Reviews.html") as f: page_text = "".join(f.readlines()) res = scrapePage(page_text) self.assertKeyValue(res,"overall_rating", "Very Positive") def test_no_reviews(self): with open("examples/No Reviews.html") as f: page_text = "".join(f.readlines()) res = scrapePage(page_text) self.assertKeyValue(res,"overall_rating", None) self.assertKeyValue(res,"num_reviews", None) # TODO: Real implementation def filterGames(ls,q): return [ls[0]] # TODO: This is just a silly example class FilterTests(unittest.TestCase): def test_basic_filter(self): examples = [ {"title" : "blah", "overall_rating" : "Very Positive"} , {"title" : "bad", "overall_rating" : "Very Negative"} ] # TODO: Will - Do you have ideas about the form of the query input to the filter q = "overall_rating > Ok" self.assertEqual(filterGames(examples, q), [ {"title" : "blah", "overall_rating" : "Very Positive"} ]) if __name__ =="__main__": unittest.main()
import unittest from bet_calculator.bet_calculator import Bet_Calculator from decimal import * class Bet_Test_Case(unittest.TestCase): """Test the Bet_Calculator class""" def setUp(self): self.bet_calculator = Bet_Calculator() def test_if_is_calculating_that_odds_will_profit(self): """ Just remember that to be profit te quotation in houses must follow the equation: d1 => decimal 1 in one bet house, d2 => decimal 2 in other bet house 1 ________ < (d2-1) (d1 - 1) """ # Testing edge cases # Just remember 1.33333...4 is not a the reapeating decimal # 1.33333.... and is bigger than it so it's an edge case self.bet_calculator.decimal_team1_house1 = '1.333333333333333334' self.bet_calculator.decimal_team2_house2 = '4.0' # put in the edge to be more sure that is calculating # correctly self.bet_calculator.decimal_team1_house2 = '3.0000000001' self.bet_calculator.decimal_team2_house1 = '1.5' self.assertTrue(self.bet_calculator.can_bet_team1_house1) self.assertTrue(self.bet_calculator.can_bet_team1_house2) # Testing normal cases self.bet_calculator.decimal_team1_house1 = '1.5' self.bet_calculator.decimal_team2_house2 = '4.0' self.bet_calculator.decimal_team1_house2 = '3.0' self.bet_calculator.decimal_team2_house1 = '1.8' self.assertTrue(self.bet_calculator.can_bet_team1_house1) self.assertTrue(self.bet_calculator.can_bet_team1_house2) def test_if_is_calculating_that_odds_will_not_profit(self): """ Just remember that to not be profit te quotation in houses must follow the equation: d1 => decimal 1 in one bet house, d2 => decimal 2 in other bet house 1 ________ >= (d2-1) (d1 - 1) """ # Testing Edge Cases # Just remember 1.333333333333333333 is not a the reapeating decimal # 1.33333.... and is smaller than it so it's an edge case self.bet_calculator.decimal_team1_house1 = '1.333333333333333333' self.bet_calculator.decimal_team2_house2 = '4.0' # put in the edge to be more sure that is calculating # correctly self.bet_calculator.decimal_team1_house2 = '2.999999999999999999' self.bet_calculator.decimal_team2_house1 = '1.5' self.assertFalse(self.bet_calculator.can_bet_team1_house1) self.assertFalse(self.bet_calculator.can_bet_team1_house2) # Testing Normal Cases self.bet_calculator.decimal_team1_house1 = '1.3' self.bet_calculator.decimal_team2_house2 = '1.5' # put in the edge to be more sure that is calculating # correctly self.bet_calculator.decimal_team1_house2 = '1.01' self.bet_calculator.decimal_team2_house1 = '3' self.assertFalse(self.bet_calculator.can_bet_team1_house1) self.assertFalse(self.bet_calculator.can_bet_team1_house2) def test_cash_made_when_you_profit(self): """ Will be inserted some odds, and they don't need to profit the Profit equation is simple: d1 => decimal in team1 d2 => decimal in team 2, tm => total money spent in your gamble m1 => (money spent on team1) part of the tm you spent in team1 (It's obvious you spent tm-m1 in team2) m2 => (money spent on team2) part of the tm you spent in team2 = tm-m1 cashMade => ? you will win the following cash if team1 wins: cashMade = m1 x d1 - tm and if team2 wins the cashMade will be: cashMade = m2 x d2 - tm """ self.bet_calculator.cash_to_bet = '200' # tm in the equation self.bet_calculator.decimal_team1_house1 = '1.2' self.bet_calculator.decimal_team2_house2 = '6.5' self.bet_calculator.decimal_team1_house2 = '1.4' self.bet_calculator.decimal_team2_house1 = '4.5' # 180.43 x 1.2 = 216.516 =...> 216.516 - 200 = 16.516 self.assertEqual(self.bet_calculator.profit_if_team1_wins('180.43', bet_team1_house1 = True) , Decimal('16.516')) # 150.75 x 1.4 = 211.05 =...> 211.05 - 200 = 11.05 self.assertEqual(self.bet_calculator.profit_if_team1_wins('150.75', bet_team1_house1 = False) , Decimal('11.05')) # 35 x 6.5 = 227.5 =...> 227.5 - 200 = 27.05 # If you bet team1 in house 1 you bet team2 in house 2 self.assertEqual(self.bet_calculator.profit_if_team2_wins('35', bet_team1_house1 = True) , Decimal('27.5')) # 64.41 x 4.5 = 289.845 =...> 289.845 - 200 = 89.845 # If you don't bet team1 in house 1 you bet team2 in house 1 self.assertEqual(self.bet_calculator.profit_if_team2_wins('64.41', bet_team1_house1 = False) , Decimal('89.845')) def test_cash_made_when_you_lose(self): """ Will be inserted some odds, and they don't need to profit the Profit equation is simple: d1 => decimal in team1 d2 => decimal in team 2, tm => total money spent in your gamble m1 => (money spent on team1) part of the tm you spent in team1 (It's obvious you spent tm-m1 in team2) m2 => (money spent on team2) part of the tm you spent in team2 = tm-m1 cashMade => ? you will win the following cash if team1 wins: cashMade = m1 x d1 - tm and if team2 wins the cashMade will be: cashMade = m2 x d2 - tm So, if tm is bigger than m1 x d1, or m2 x d2 you will lose money if team1 or team2 win. So the result will be negative """ self.bet_calculator.cash_to_bet = '200' # tm in the equation self.bet_calculator.decimal_team1_house1 = '1.5' self.bet_calculator.decimal_team2_house2 = '3.2' self.bet_calculator.decimal_team1_house2 = '1.7' self.bet_calculator.decimal_team2_house1 = '2.2' # 50.54 x 1.5 = 75.81 =...> 75.81 - 200 = -124.19 self.assertEqual(self.bet_calculator.profit_if_team1_wins('50.54', bet_team1_house1 = True) , Decimal('-124.19')) # 81.56 x 1.7 = 138.652 =...> 138.652 - 200 = -61.348 self.assertEqual(self.bet_calculator.profit_if_team1_wins('81.56', bet_team1_house1 = False) , Decimal('-61.348')) # 55.17 x 3.2 = 176.544 =...> 176.544 - 200 = -23.456 (it was coincidence :) ) # If you bet team1 in house 1 you bet team2 in house 2 self.assertEqual(self.bet_calculator.profit_if_team2_wins('55.17', bet_team1_house1 = True) , Decimal('-23.456')) # 85.42 x 2.2 = 187.924 =...> 187.924 - 200 = -12.076 # If you don't bet team1 in house 1 you bet team2 in house 1 self.assertEqual(self.bet_calculator.profit_if_team2_wins('85.42', bet_team1_house1 = False) , Decimal('-12.076')) def test_cash_made_when_you_bet_more_than_total_cash(self): """ We don't have to see the all equation. But if you bet in one team more than total cash you said it was supposed to have an exception """ self.bet_calculator.cash_to_bet = '200' # tm in the equation # these informations don't matter -------------- self.bet_calculator.decimal_team1_house1 = '1.5' self.bet_calculator.decimal_team2_house2 = '3.2' self.bet_calculator.decimal_team1_house2 = '1.7' self.bet_calculator.decimal_team2_house1 = '2.2' # ---------------------------------------------- # Edge Case self.assertRaises(Exception, self.bet_calculator.profit_if_team1_wins, '200.000000000000001', bet_team1_house1 = True) self.assertRaises(Exception, self.bet_calculator.profit_if_team1_wins, '200.000000000000001', bet_team1_house1 = False) self.assertRaises(Exception, self.bet_calculator.profit_if_team2_wins, '200.000000000000001', bet_team1_house1 = True) self.assertRaises(Exception, self.bet_calculator.profit_if_team2_wins, '200.000000000000001', bet_team1_house1 = False) # Normal Case self.assertRaises(Exception, self.bet_calculator.profit_if_team1_wins, '205', bet_team1_house1 = True) self.assertRaises(Exception, self.bet_calculator.profit_if_team1_wins, '301', bet_team1_house1 = False) self.assertRaises(Exception, self.bet_calculator.profit_if_team2_wins, '400', bet_team1_house1 = True) self.assertRaises(Exception, self.bet_calculator.profit_if_team2_wins, '405', bet_team1_house1 = False) def test_calc_of_least_guaranteed_profit_when_team1_wins(self): """ Test if team 1 wins, I will get the least possible profit (it will be 0, because we want to assure any loses). The least possible profit that you ensure no loses if team 1 wins, if that occurs you will maintain the same mone you bet at the principle Ps: Always remember that if the result of the match is draw you will lose everything You have to have in mind that you have to bet at least to guarantee the return of your total money back. The following equation must be followed to get your money back without loses: d1 = decimal in team1 tm = total money invested (in team1 and team2) m1 = How much I should spend in team1 without lose the tm I Invested => ? tm if d1 x m1 = tm => m1 = _________ d1 """ self.bet_calculator.cash_to_bet = '200' # tm in the equation self.bet_calculator.decimal_team1_house1 = '1.5' self.bet_calculator.decimal_team2_house2 = '3.2' self.bet_calculator.decimal_team1_house2 = '1.7' self.bet_calculator.decimal_team2_house1 = '2.5' # Because the real result is 133.3333333..., we need to quantize to 5 (don't have to be exactly 5) # decimal points to ensure the result will be followed self.assertEqual( self.bet_calculator.least_possible_value_team1(bet_team1_house1 = True).quantize(Decimal('0.00001')), Decimal('133.33333') ) # Because the real result is 117.6470588235294..., we need to quantize to 5 (don't have to be exactly 5) # decimal points to ensure the result will be followed self.assertEqual( self.bet_calculator.least_possible_value_team1(bet_team1_house1 = False).quantize(Decimal('0.00001')), Decimal('117.64706') ) def test_calc_of_least_guaranteed_profit_when_team1_wins_when_the_decimals_dont_profit(self): """ In this situation, when the decimals don't profit it will raise Exception, because there is no guaranteed pair of Bets Ps: If the result of the match is draw you will lose everything """ self.bet_calculator.cash_to_bet = '200' # tm in the equation self.bet_calculator.decimal_team1_house1 = '1.5' self.bet_calculator.decimal_team2_house2 = '1.8' self.bet_calculator.decimal_team1_house2 = '1.01' self.bet_calculator.decimal_team2_house1 = '10.999999999' self.assertRaises(Exception, self.bet_calculator.least_possible_value_team1, bet_team1_house1 = True) self.assertRaises(Exception, self.bet_calculator.least_possible_value_team1, bet_team1_house1 = False) def test_calc_of_biggest_guaranteed_profit_if_team1_wins(self): """ Test if team1 wins I will win the most profitable bet in this situation without having lost if team2 wins. Ps: Always remember that if the result of the match is draw you will lose everything With this situation in mind, the equation is... d2 = decimal in team2 tm = total money invested (in team1 and team2) m2 = How much I shoud spend in team2 without lose the tm I Invested => ? tm m2 = __________ (The same as I did in the previous test with d1 and m1) d2 m1 = How much I should spend in team1 having in mind that if team1 wins I Will have that most profitable bet => ? Be sure, that when I ensure that I will have no lost if team2 wins, we will use the max money I could spent in team1, so this bet ensure that I will have the most profitable return if team1 wins so: tm m1 = tm - _________ OR tm - m2 d2 """ self.bet_calculator.cash_to_bet = '200' # tm in the equation self.bet_calculator.decimal_team1_house1 = '1.3' self.bet_calculator.decimal_team2_house2 = '4.5' self.bet_calculator.decimal_team1_house2 = '1.9' self.bet_calculator.decimal_team2_house1 = '2.6' # Because the real result is 155.55555..., we need to quantize to 5 (don't have to be exactly 5) # decimal points to ensure the result will be followed self.assertEqual( self.bet_calculator.biggest_possible_value_team1(bet_team1_house1 = True).quantize(Decimal('0.00001')), Decimal('155.55556') ) # Because the real result is 123.0769230769..., we need to quantize to 5 (don't have to be exactly 5) # decimal points to ensure the result will be followed self.assertEqual( self.bet_calculator.biggest_possible_value_team1(bet_team1_house1 = False).quantize(Decimal('0.00001')), Decimal('123.07692') ) if __name__ == '__main__': unittest.main()
# -*- coding: utf-8 -*- """ tests.templating ~~~~~~~~~~~~~~~~ Template functionality :copyright: (c) 2015 by Armin Ronacher. :license: BSD, see LICENSE for more details. """ import pytest import flask import logging from jinja2 import TemplateNotFound def test_context_processing(): app = flask.Flask(__name__) @app.context_processor def context_processor(): return {'injected_value': 42} @app.route('/') def index(): return flask.render_template('context_template.html', value=23) rv = app.test_client().get('/') assert rv.data == b'<p>23|42' def test_original_win(): app = flask.Flask(__name__) @app.route('/') def index(): return flask.render_template_string('{{ config }}', config=42) rv = app.test_client().get('/') assert rv.data == b'42' def test_request_less_rendering(): app = flask.Flask(__name__) app.config['WORLD_NAME'] = 'Special World' @app.context_processor def context_processor(): return dict(foo=42) with app.app_context(): rv = flask.render_template_string('Hello {{ config.WORLD_NAME }} ' '{{ foo }}') assert rv == 'Hello Special World 42' def test_standard_context(): app = flask.Flask(__name__) app.secret_key = 'development key' @app.route('/') def index(): flask.g.foo = 23 flask.session['test'] = 'aha' return flask.render_template_string(''' {{ request.args.foo }} {{ g.foo }} {{ config.DEBUG }} {{ session.test }} ''') rv = app.test_client().get('/?foo=42') assert rv.data.split() == [b'42', b'23', b'False', b'aha'] def test_escaping(): text = '<p>Hello World!' app = flask.Flask(__name__) @app.route('/') def index(): return flask.render_template('escaping_template.html', text=text, html=flask.Markup(text)) lines = app.test_client().get('/').data.splitlines() assert lines == [ b'&lt;p&gt;Hello World!', b'<p>Hello World!', b'<p>Hello World!', b'<p>Hello World!', b'&lt;p&gt;Hello World!', b'<p>Hello World!' ] def test_no_escaping(): text = '<p>Hello World!' app = flask.Flask(__name__) @app.route('/') def index(): return flask.render_template('non_escaping_template.txt', text=text, html=flask.Markup(text)) lines = app.test_client().get('/').data.splitlines() assert lines == [ b'<p>Hello World!', b'<p>Hello World!', b'<p>Hello World!', b'<p>Hello World!', b'&lt;p&gt;Hello World!', b'<p>Hello World!', b'<p>Hello World!', b'<p>Hello World!' ] def test_escaping_without_template_filename(): app = flask.Flask(__name__) with app.test_request_context(): assert flask.render_template_string( '{{ foo }}', foo='<test>') == '&lt;test&gt;' assert flask.render_template('mail.txt', foo='<test>') == \ '<test> Mail' def test_macros(): app = flask.Flask(__name__) with app.test_request_context(): macro = flask.get_template_attribute('_macro.html', 'hello') assert macro('World') == 'Hello World!' def test_template_filter(): app = flask.Flask(__name__) @app.template_filter() def my_reverse(s): return s[::-1] assert 'my_reverse' in app.jinja_env.filters.keys() assert app.jinja_env.filters['my_reverse'] == my_reverse assert app.jinja_env.filters['my_reverse']('abcd') == 'dcba' def test_add_template_filter(): app = flask.Flask(__name__) def my_reverse(s): return s[::-1] app.add_template_filter(my_reverse) assert 'my_reverse' in app.jinja_env.filters.keys() assert app.jinja_env.filters['my_reverse'] == my_reverse assert app.jinja_env.filters['my_reverse']('abcd') == 'dcba' def test_template_filter_with_name(): app = flask.Flask(__name__) @app.template_filter('strrev') def my_reverse(s): return s[::-1] assert 'strrev' in app.jinja_env.filters.keys() assert app.jinja_env.filters['strrev'] == my_reverse assert app.jinja_env.filters['strrev']('abcd') == 'dcba' def test_add_template_filter_with_name(): app = flask.Flask(__name__) def my_reverse(s): return s[::-1] app.add_template_filter(my_reverse, 'strrev') assert 'strrev' in app.jinja_env.filters.keys() assert app.jinja_env.filters['strrev'] == my_reverse assert app.jinja_env.filters['strrev']('abcd') == 'dcba' def test_template_filter_with_template(): app = flask.Flask(__name__) @app.template_filter() def super_reverse(s): return s[::-1] @app.route('/') def index(): return flask.render_template('template_filter.html', value='abcd') rv = app.test_client().get('/') assert rv.data == b'dcba' def test_add_template_filter_with_template(): app = flask.Flask(__name__) def super_reverse(s): return s[::-1] app.add_template_filter(super_reverse) @app.route('/') def index(): return flask.render_template('template_filter.html', value='abcd') rv = app.test_client().get('/') assert rv.data == b'dcba' def test_template_filter_with_name_and_template(): app = flask.Flask(__name__) @app.template_filter('super_reverse') def my_reverse(s): return s[::-1] @app.route('/') def index(): return flask.render_template('template_filter.html', value='abcd') rv = app.test_client().get('/') assert rv.data == b'dcba' def test_add_template_filter_with_name_and_template(): app = flask.Flask(__name__) def my_reverse(s): return s[::-1] app.add_template_filter(my_reverse, 'super_reverse') @app.route('/') def index(): return flask.render_template('template_filter.html', value='abcd') rv = app.test_client().get('/') assert rv.data == b'dcba' def test_template_test(): app = flask.Flask(__name__) @app.template_test() def boolean(value): return isinstance(value, bool) assert 'boolean' in app.jinja_env.tests.keys() assert app.jinja_env.tests['boolean'] == boolean assert app.jinja_env.tests['boolean'](False) def test_add_template_test(): app = flask.Flask(__name__) def boolean(value): return isinstance(value, bool) app.add_template_test(boolean) assert 'boolean' in app.jinja_env.tests.keys() assert app.jinja_env.tests['boolean'] == boolean assert app.jinja_env.tests['boolean'](False) def test_template_test_with_name(): app = flask.Flask(__name__) @app.template_test('boolean') def is_boolean(value): return isinstance(value, bool) assert 'boolean' in app.jinja_env.tests.keys() assert app.jinja_env.tests['boolean'] == is_boolean assert app.jinja_env.tests['boolean'](False) def test_add_template_test_with_name(): app = flask.Flask(__name__) def is_boolean(value): return isinstance(value, bool) app.add_template_test(is_boolean, 'boolean') assert 'boolean' in app.jinja_env.tests.keys() assert app.jinja_env.tests['boolean'] == is_boolean assert app.jinja_env.tests['boolean'](False) def test_template_test_with_template(): app = flask.Flask(__name__) @app.template_test() def boolean(value): return isinstance(value, bool) @app.route('/') def index(): return flask.render_template('template_test.html', value=False) rv = app.test_client().get('/') assert b'Success!' in rv.data def test_add_template_test_with_template(): app = flask.Flask(__name__) def boolean(value): return isinstance(value, bool) app.add_template_test(boolean) @app.route('/') def index(): return flask.render_template('template_test.html', value=False) rv = app.test_client().get('/') assert b'Success!' in rv.data def test_template_test_with_name_and_template(): app = flask.Flask(__name__) @app.template_test('boolean') def is_boolean(value): return isinstance(value, bool) @app.route('/') def index(): return flask.render_template('template_test.html', value=False) rv = app.test_client().get('/') assert b'Success!' in rv.data def test_add_template_test_with_name_and_template(): app = flask.Flask(__name__) def is_boolean(value): return isinstance(value, bool) app.add_template_test(is_boolean, 'boolean') @app.route('/') def index(): return flask.render_template('template_test.html', value=False) rv = app.test_client().get('/') assert b'Success!' in rv.data def test_add_template_global(): app = flask.Flask(__name__) @app.template_global() def get_stuff(): return 42 assert 'get_stuff' in app.jinja_env.globals.keys() assert app.jinja_env.globals['get_stuff'] == get_stuff assert app.jinja_env.globals['get_stuff'](), 42 with app.app_context(): rv = flask.render_template_string('{{ get_stuff() }}') assert rv == '42' def test_custom_template_loader(): class MyFlask(flask.Flask): def create_global_jinja_loader(self): from jinja2 import DictLoader return DictLoader({'index.html': 'Hello Custom World!'}) app = MyFlask(__name__) @app.route('/') def index(): return flask.render_template('index.html') c = app.test_client() rv = c.get('/') assert rv.data == b'Hello Custom World!' def test_iterable_loader(): app = flask.Flask(__name__) @app.context_processor def context_processor(): return {'whiskey': 'Jameson'} @app.route('/') def index(): return flask.render_template( ['no_template.xml', # should skip this one 'simple_template.html', # should render this 'context_template.html'], value=23) rv = app.test_client().get('/') assert rv.data == b'<h1>Jameson</h1>' def test_templates_auto_reload(): # debug is False, config option is None app = flask.Flask(__name__) assert app.debug is False assert app.config['TEMPLATES_AUTO_RELOAD'] is None assert app.jinja_env.auto_reload is False # debug is False, config option is False app = flask.Flask(__name__) app.config['TEMPLATES_AUTO_RELOAD'] = False assert app.debug is False assert app.jinja_env.auto_reload is False # debug is False, config option is True app = flask.Flask(__name__) app.config['TEMPLATES_AUTO_RELOAD'] = True assert app.debug is False assert app.jinja_env.auto_reload is True # debug is True, config option is None app = flask.Flask(__name__) app.config['DEBUG'] = True assert app.config['TEMPLATES_AUTO_RELOAD'] is None assert app.jinja_env.auto_reload is True # debug is True, config option is False app = flask.Flask(__name__) app.config['DEBUG'] = True app.config['TEMPLATES_AUTO_RELOAD'] = False assert app.jinja_env.auto_reload is False # debug is True, config option is True app = flask.Flask(__name__) app.config['DEBUG'] = True app.config['TEMPLATES_AUTO_RELOAD'] = True assert app.jinja_env.auto_reload is True def test_template_loader_debugging(test_apps): from blueprintapp import app called = [] class _TestHandler(logging.Handler): def handle(x, record): called.append(True) text = str(record.msg) assert '1: trying loader of application "blueprintapp"' in text assert ('2: trying loader of blueprint "admin" ' '(blueprintapp.apps.admin)') in text assert ('trying loader of blueprint "frontend" ' '(blueprintapp.apps.frontend)') in text assert 'Error: the template could not be found' in text assert ('looked up from an endpoint that belongs to ' 'the blueprint "frontend"') in text assert 'See http://flask.pocoo.org/docs/blueprints/#templates' in text with app.test_client() as c: try: old_load_setting = app.config['EXPLAIN_TEMPLATE_LOADING'] old_handlers = app.logger.handlers[:] app.logger.handlers = [_TestHandler()] app.config['EXPLAIN_TEMPLATE_LOADING'] = True with pytest.raises(TemplateNotFound) as excinfo: c.get('/missing') assert 'missing_template.html' in str(excinfo.value) finally: app.logger.handlers[:] = old_handlers app.config['EXPLAIN_TEMPLATE_LOADING'] = old_load_setting assert len(called) == 1 def test_custom_jinja_env(): class CustomEnvironment(flask.templating.Environment): pass class CustomFlask(flask.Flask): jinja_environment = CustomEnvironment app = CustomFlask(__name__) assert isinstance(app.jinja_env, CustomEnvironment)
from matplotlib.patches import Circle import matplotlib.pyplot as plt import numpy as np import pytest from pysisyphus.calculators.AnaPot import AnaPot from pysisyphus.dynamics.velocity_verlet import md def test_velocity_verlet(): geom = AnaPot.get_geom((0.52, 1.80, 0)) x0 = geom.coords.copy() v0 = .1 * np.random.rand(*geom.coords.shape) t = 3 dts = (.005, .01, .02, .04, .08) all_xs = list() for dt in dts: geom.coords = x0.copy() md_kwargs = { "v0": v0.copy(), "t": t, "dt": dt, } md_result = md(geom, **md_kwargs) all_xs.append(md_result.coords) calc = geom.calculator calc.plot() ax = calc.ax for dt, xs in zip(dts, all_xs): ax.plot(*xs.T[:2], "o-", label=f"dt={dt:.3f}") # ax.plot(*xs.T[:2], "-", label=f"dt={dt:.3f}") ax.legend() plt.show() def ase_md_playground(): geom = AnaPot.get_geom((0.52, 1.80, 0), atoms=("H", )) atoms = geom.as_ase_atoms() # ase_calc = FakeASE(geom.calculator) # from ase.optimize import BFGS # dyn = BFGS(atoms) # dyn.run(fmax=0.05) import ase from ase import units from ase.io.trajectory import Trajectory from ase.md.velocitydistribution import MaxwellBoltzmannDistribution from ase.md.verlet import VelocityVerlet MaxwellBoltzmannDistribution(atoms, 300 * units.kB) momenta = atoms.get_momenta() momenta[0, 2] = 0. # Zero 3rd dimension atoms.set_momenta(momenta) dyn = VelocityVerlet(atoms, .005 * units.fs) # 5 fs time step. def printenergy(a): """Function to print the potential, kinetic and total energy""" epot = a.get_potential_energy() / len(a) ekin = a.get_kinetic_energy() / len(a) print('Energy per atom: Epot = %.3feV Ekin = %.3feV (T=%3.0fK) ' 'Etot = %.3feV' % (epot, ekin, ekin / (1.5 * units.kB), epot + ekin)) # Now run the dynamics printenergy(atoms) traj_fn = 'asemd.traj' traj = Trajectory(traj_fn, 'w', atoms) dyn.attach(traj.write, interval=5) # dyn.attach(bumms().bimms, interval=1) dyn.run(10000) printenergy(atoms) traj.close() traj = ase.io.read(traj_fn+"@:")#, "r") pos = [a.get_positions() for a in traj] from pysisyphus.constants import BOHR2ANG pos = np.array(pos) / BOHR2ANG calc = geom.calculator calc.plot() ax = calc.ax ax.plot(*pos[:,0,:2].T) plt.show() if __name__ == "__main__": ase_md_playground()
""" This module implements a method to find Euler-Lagrange Equations for given Lagrangian. """ from itertools import combinations_with_replacement from sympy import Function, sympify, diff, Eq, S, Symbol, Derivative from sympy.core.compatibility import (iterable, range) def euler_equations(L, funcs=(), vars=()): r""" Find the Euler-Lagrange equations [1]_ for a given Lagrangian. Parameters ========== L : Expr The Lagrangian that should be a function of the functions listed in the second argument and their derivatives. For example, in the case of two functions `f(x,y)`, `g(x,y)` and two independent variables `x`, `y` the Lagrangian would have the form: .. math:: L\left(f(x,y),g(x,y),\frac{\partial f(x,y)}{\partial x}, \frac{\partial f(x,y)}{\partial y}, \frac{\partial g(x,y)}{\partial x}, \frac{\partial g(x,y)}{\partial y},x,y\right) In many cases it is not necessary to provide anything, except the Lagrangian, it will be auto-detected (and an error raised if this couldn't be done). funcs : Function or an iterable of Functions The functions that the Lagrangian depends on. The Euler equations are differential equations for each of these functions. vars : Symbol or an iterable of Symbols The Symbols that are the independent variables of the functions. Returns ======= eqns : list of Eq The list of differential equations, one for each function. Examples ======== >>> from sympy import Symbol, Function >>> from sympy.calculus.euler import euler_equations >>> x = Function('x') >>> t = Symbol('t') >>> L = (x(t).diff(t))**2/2 - x(t)**2/2 >>> euler_equations(L, x(t), t) [Eq(-x(t) - Derivative(x(t), t, t), 0)] >>> u = Function('u') >>> x = Symbol('x') >>> L = (u(t, x).diff(t))**2/2 - (u(t, x).diff(x))**2/2 >>> euler_equations(L, u(t, x), [t, x]) [Eq(-Derivative(u(t, x), t, t) + Derivative(u(t, x), x, x), 0)] References ========== .. [1] http://en.wikipedia.org/wiki/Euler%E2%80%93Lagrange_equation """ funcs = tuple(funcs) if iterable(funcs) else (funcs,) if not funcs: funcs = tuple(L.atoms(Function)) else: for f in funcs: if not isinstance(f, Function): raise TypeError('Function expected, got: %s' % f) vars = tuple(vars) if iterable(vars) else (vars,) if not vars: vars = funcs[0].args else: vars = tuple(sympify(var) for var in vars) if not all(isinstance(v, Symbol) for v in vars): raise TypeError('Variables are not symbols, got %s' % vars) for f in funcs: if not vars == f.args: raise ValueError("Variables %s don't match args: %s" % (vars, f)) order = max(len(d.variables) for d in L.atoms(Derivative) if d.expr in funcs) eqns = [] for f in funcs: eq = diff(L, f) for i in range(1, order + 1): for p in combinations_with_replacement(vars, i): eq = eq + S.NegativeOne**i*diff(L, diff(f, *p), *p) eqns.append(Eq(eq)) return eqns
# -*- encoding: utf-8 -*- ############################################################################## # # OpenERP, Open Source Management Solution # Copyright (C) 2004-2009 Tiny SPRL (<http://tiny.be>). All Rights Reserved # $Id$ # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## import datetime import time from openerp.osv import osv from openerp.tools.translate import _ from openerp.report import report_sxw from openerp.tools.safe_eval import safe_eval as eval # # Use period and Journal for selection or resources # class report_assert_account(report_sxw.rml_parse): def __init__(self, cr, uid, name, context): super(report_assert_account, self).__init__(cr, uid, name, context=context) self.localcontext.update( { 'time': time, 'datetime': datetime, 'execute_code': self.execute_code, }) def execute_code(self, code_exec): def reconciled_inv(): """ returns the list of invoices that are set as reconciled = True """ return self.pool.get('account.invoice').search(self.cr, self.uid, [('reconciled','=',True)]) def order_columns(item, cols=None): """ This function is used to display a dictionary as a string, with its columns in the order chosen. :param item: dict :param cols: list of field names :returns: a list of tuples (fieldname: value) in a similar way that would dict.items() do except that the returned values are following the order given by cols :rtype: [(key, value)] """ if cols is None: cols = item.keys() return [(col, item.get(col)) for col in cols if col in item.keys()] localdict = { 'cr': self.cr, 'uid': self.uid, 'reconciled_inv': reconciled_inv, #specific function used in different tests 'result': None, #used to store the result of the test 'column_order': None, #used to choose the display order of columns (in case you are returning a list of dict) } eval(code_exec, localdict, mode="exec", nocopy=True) result = localdict['result'] column_order = localdict.get('column_order', None) if not isinstance(result, (tuple, list, set)): result = [result] if not result: result = [_('The test was passed successfully')] else: def _format(item): if isinstance(item, dict): return ', '.join(["%s: %s" % (tup[0], tup[1]) for tup in order_columns(item, column_order)]) else: return item result = [_(_format(rec)) for rec in result] return result class report_accounttest(osv.AbstractModel): _name = 'report.account_test.report_accounttest' _inherit = 'report.abstract_report' _template = 'account_test.report_accounttest' _wrapped_report_class = report_assert_account # vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
# Copyright 2015 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== """Utilities for ImageNet data preprocessing & prediction decoding. """ from __future__ import absolute_import from __future__ import division from __future__ import print_function from keras_applications import imagenet_utils from tensorflow.python.keras.applications import keras_modules_injection from tensorflow.python.util.tf_export import keras_export @keras_export('keras.applications.imagenet_utils.decode_predictions') @keras_modules_injection def decode_predictions(*args, **kwargs): return imagenet_utils.decode_predictions(*args, **kwargs) @keras_export('keras.applications.imagenet_utils.preprocess_input') @keras_modules_injection def preprocess_input(*args, **kwargs): return imagenet_utils.preprocess_input(*args, **kwargs)
#!/usr/bin/env python ######################################################################## # $HeadURL$ # File : dirac-wms-job-attributes # Author : Stuart Paterson ######################################################################## """ Retrieve attributes associated with the given DIRAC job """ __RCSID__ = "$Id$" import DIRAC from DIRAC.Core.Base import Script Script.setUsageMessage( '\n'.join( [ __doc__.split( '\n' )[1], 'Usage:', ' %s [option|cfgfile] ... JobID ...' % Script.scriptName, 'Arguments:', ' JobID: DIRAC Job ID' ] ) ) Script.parseCommandLine( ignoreErrors = True ) args = Script.getPositionalArgs() if len( args ) < 1: Script.showHelp() from DIRAC.Interfaces.API.Dirac import Dirac dirac = Dirac() exitCode = 0 errorList = [] for job in args: result = dirac.attributes( int(job), printOutput = True ) if not result['OK']: errorList.append( ( job, result['Message'] ) ) exitCode = 2 for error in errorList: print "ERROR %s: %s" % error DIRAC.exit( exitCode )
#!/usr/bin/env python # -*- coding: utf-8 -*- """ Simple built-in backend. """ __author__ = "Llus Vilanova <vilanova@ac.upc.edu>" __copyright__ = "Copyright 2012-2014, Llus Vilanova <vilanova@ac.upc.edu>" __license__ = "GPL version 2 or (at your option) any later version" __maintainer__ = "Stefan Hajnoczi" __email__ = "stefanha@linux.vnet.ibm.com" from tracetool import out PUBLIC = True def is_string(arg): strtype = ('const char*', 'char*', 'const char *', 'char *') if arg.lstrip().startswith(strtype): return True else: return False def generate_h_begin(events): for event in events: out('void _simple_%(api)s(%(args)s);', api=event.api(), args=event.args) out('') def generate_h(event): out(' _simple_%(api)s(%(args)s);', api=event.api(), args=", ".join(event.args.names())) def generate_c_begin(events): out('#include "trace.h"', '#include "trace/control.h"', '#include "trace/simple.h"', '') def generate_c(event): out('void _simple_%(api)s(%(args)s)', '{', ' TraceBufferRecord rec;', api=event.api(), args=event.args) sizes = [] for type_, name in event.args: if is_string(type_): out(' size_t arg%(name)s_len = %(name)s ? MIN(strlen(%(name)s), MAX_TRACE_STRLEN) : 0;', name=name) strsizeinfo = "4 + arg%s_len" % name sizes.append(strsizeinfo) else: sizes.append("8") sizestr = " + ".join(sizes) if len(event.args) == 0: sizestr = '0' out('', ' if (!trace_event_get_state(%(event_id)s)) {', ' return;', ' }', '', ' if (trace_record_start(&rec, %(event_id)s, %(size_str)s)) {', ' return; /* Trace Buffer Full, Event Dropped ! */', ' }', event_id='TRACE_' + event.name.upper(), size_str=sizestr) if len(event.args) > 0: for type_, name in event.args: # string if is_string(type_): out(' trace_record_write_str(&rec, %(name)s, arg%(name)s_len);', name=name) # pointer var (not string) elif type_.endswith('*'): out(' trace_record_write_u64(&rec, (uintptr_t)(uint64_t *)%(name)s);', name=name) # primitive data type else: out(' trace_record_write_u64(&rec, (uint64_t)%(name)s);', name=name) out(' trace_record_finish(&rec);', '}', '')
# -*- coding: utf-8 -*- import re from module.plugins.internal.MultiHoster import MultiHoster from module.plugins.internal.misc import json class RPNetBiz(MultiHoster): __name__ = "RPNetBiz" __type__ = "hoster" __version__ = "0.20" __status__ = "testing" __pattern__ = r'https?://.+rpnet\.biz' __config__ = [("activated" , "bool", "Activated" , True ), ("use_premium" , "bool", "Use premium account if available" , True ), ("fallback" , "bool", "Fallback to free download if premium fails" , False), ("chk_filesize", "bool", "Check file size" , True ), ("max_wait" , "int" , "Reconnect if waiting time is greater than minutes", 10 ), ("revertfailed", "bool", "Revert to standard download if fails" , True )] __description__ = """RPNet.biz multi-hoster plugin""" __license__ = "GPLv3" __authors__ = [("Dman", "dmanugm@gmail.com")] def setup(self): self.chunk_limit = -1 def handle_premium(self, pyfile): user, info = self.account.select() res = self.load("https://premium.rpnet.biz/client_api.php", get={'username': user, 'password': info['login']['password'], 'action' : "generate", 'links' : pyfile.url}) self.log_debug("JSON data: %s" % res) link_status = json.loads(res)['links'][0] #: Get the first link... since we only queried one #: Check if we only have an id as a HDD link if 'id' in link_status: self.log_debug("Need to wait at least 30 seconds before requery") self.wait(30) #: Wait for 30 seconds #: Lets query the server again asking for the status on the link, #: We need to keep doing this until we reach 100 attemps = 30 my_try = 0 while (my_try <= attemps): self.log_debug("Try: %d ; Max Tries: %d" % (my_try, attemps)) res = self.load("https://premium.rpnet.biz/client_api.php", get={'username': user, 'password': info['login']['password'], 'action' : "downloadInformation", 'id' : link_status['id']}) self.log_debug("JSON data hdd query: %s" % res) download_status = json.loads(res)['download'] if download_status['status'] == "100": link_status['generated'] = download_status['rpnet_link'] self.log_debug("Successfully downloaded to rpnet HDD: %s" % link_status['generated']) break else: self.log_debug("At %s%% for the file download" % download_status['status']) self.wait(30) my_try += 1 if my_try > attemps: #: We went over the limit! self.fail(_("Waited for about 15 minutes for download to finish but failed")) if 'generated' in link_status: self.link = link_status['generated'] return elif 'error' in link_status: self.fail(link_status['error']) else: self.fail(_("Something went wrong, not supposed to enter here"))
import unittest import theano from theano import tensor, function, Variable, Generic import numpy import os class T_load_tensor(unittest.TestCase): def setUp(self): self.data = numpy.arange(5, dtype=numpy.int32) self.filename = os.path.join( theano.config.compiledir, "_test.npy") numpy.save(self.filename, self.data) def test0(self): path = Variable(Generic()) # Not specifying mmap_mode defaults to None, and the data is # copied into main memory x = tensor.load(path, 'int32', (False,)) y = x * 2 fn = function([path], y) assert (fn(self.filename) == (self.data * 2)).all() def test_invalid_modes(self): # Modes 'r+', 'r', and 'w+' cannot work with Theano, becausei # the output array may be modified inplace, and that should not # modify the original file. path = Variable(Generic()) for mmap_mode in ('r+', 'r', 'w+', 'toto'): self.assertRaises(ValueError, tensor.load, path, 'int32', (False,), mmap_mode) def test1(self): path = Variable(Generic()) # 'c' means "copy-on-write", which allow the array to be overwritten # by an inplace Op in the graph, without modifying the underlying # file. x = tensor.load(path, 'int32', (False,), 'c') # x ** 2 has been chosen because it will work inplace. y = (x ** 2).sum() fn = function([path], y) # Call fn() twice, to check that inplace ops do not cause trouble assert (fn(self.filename) == (self.data ** 2).sum()).all() assert (fn(self.filename) == (self.data ** 2).sum()).all() def test_memmap(self): path = Variable(Generic()) x = tensor.load(path, 'int32', (False,), mmap_mode='c') fn = function([path], x) assert type(fn(self.filename)) == numpy.core.memmap def tearDown(self): os.remove(os.path.join( theano.config.compiledir, "_test.npy"))
import sys import unittest from test.support import run_unittest L = [ ('0', 0), ('1', 1), ('9', 9), ('10', 10), ('99', 99), ('100', 100), ('314', 314), (' 314', 314), ('314 ', 314), (' \t\t 314 \t\t ', 314), (repr(sys.maxsize), sys.maxsize), (' 1x', ValueError), (' 1 ', 1), (' 1\02 ', ValueError), ('', ValueError), (' ', ValueError), (' \t\t ', ValueError), ("\u0200", ValueError) ] class IntTestCases(unittest.TestCase): def test_basic(self): self.assertEqual(int(314), 314) self.assertEqual(int(3.14), 3) # Check that conversion from float truncates towards zero self.assertEqual(int(-3.14), -3) self.assertEqual(int(3.9), 3) self.assertEqual(int(-3.9), -3) self.assertEqual(int(3.5), 3) self.assertEqual(int(-3.5), -3) self.assertEqual(int("-3"), -3) self.assertEqual(int(" -3 "), -3) self.assertEqual(int("\N{EM SPACE}-3\N{EN SPACE}"), -3) # Different base: self.assertEqual(int("10",16), 16) # Test conversion from strings and various anomalies for s, v in L: for sign in "", "+", "-": for prefix in "", " ", "\t", " \t\t ": ss = prefix + sign + s vv = v if sign == "-" and v is not ValueError: vv = -v try: self.assertEqual(int(ss), vv) except ValueError: pass s = repr(-1-sys.maxsize) x = int(s) self.assertEqual(x+1, -sys.maxsize) self.assertIsInstance(x, int) # should return int self.assertEqual(int(s[1:]), sys.maxsize+1) # should return int x = int(1e100) self.assertIsInstance(x, int) x = int(-1e100) self.assertIsInstance(x, int) # SF bug 434186: 0x80000000/2 != 0x80000000>>1. # Worked by accident in Windows release build, but failed in debug build. # Failed in all Linux builds. x = -1-sys.maxsize self.assertEqual(x >> 1, x//2) self.assertRaises(ValueError, int, '123\0') self.assertRaises(ValueError, int, '53', 40) # SF bug 1545497: embedded NULs were not detected with # explicit base self.assertRaises(ValueError, int, '123\0', 10) self.assertRaises(ValueError, int, '123\x00 245', 20) x = int('1' * 600) self.assertIsInstance(x, int) self.assertRaises(TypeError, int, 1, 12) self.assertEqual(int('0o123', 0), 83) self.assertEqual(int('0x123', 16), 291) # Bug 1679: "0x" is not a valid hex literal self.assertRaises(ValueError, int, "0x", 16) self.assertRaises(ValueError, int, "0x", 0) self.assertRaises(ValueError, int, "0o", 8) self.assertRaises(ValueError, int, "0o", 0) self.assertRaises(ValueError, int, "0b", 2) self.assertRaises(ValueError, int, "0b", 0) # Bug #3236: Return small longs from PyLong_FromString self.assertTrue(int("10") is 10) self.assertTrue(int("-1") is -1) # SF bug 1334662: int(string, base) wrong answers # Various representations of 2**32 evaluated to 0 # rather than 2**32 in previous versions self.assertEqual(int('100000000000000000000000000000000', 2), 4294967296) self.assertEqual(int('102002022201221111211', 3), 4294967296) self.assertEqual(int('10000000000000000', 4), 4294967296) self.assertEqual(int('32244002423141', 5), 4294967296) self.assertEqual(int('1550104015504', 6), 4294967296) self.assertEqual(int('211301422354', 7), 4294967296) self.assertEqual(int('40000000000', 8), 4294967296) self.assertEqual(int('12068657454', 9), 4294967296) self.assertEqual(int('4294967296', 10), 4294967296) self.assertEqual(int('1904440554', 11), 4294967296) self.assertEqual(int('9ba461594', 12), 4294967296) self.assertEqual(int('535a79889', 13), 4294967296) self.assertEqual(int('2ca5b7464', 14), 4294967296) self.assertEqual(int('1a20dcd81', 15), 4294967296) self.assertEqual(int('100000000', 16), 4294967296) self.assertEqual(int('a7ffda91', 17), 4294967296) self.assertEqual(int('704he7g4', 18), 4294967296) self.assertEqual(int('4f5aff66', 19), 4294967296) self.assertEqual(int('3723ai4g', 20), 4294967296) self.assertEqual(int('281d55i4', 21), 4294967296) self.assertEqual(int('1fj8b184', 22), 4294967296) self.assertEqual(int('1606k7ic', 23), 4294967296) self.assertEqual(int('mb994ag', 24), 4294967296) self.assertEqual(int('hek2mgl', 25), 4294967296) self.assertEqual(int('dnchbnm', 26), 4294967296) self.assertEqual(int('b28jpdm', 27), 4294967296) self.assertEqual(int('8pfgih4', 28), 4294967296) self.assertEqual(int('76beigg', 29), 4294967296) self.assertEqual(int('5qmcpqg', 30), 4294967296) self.assertEqual(int('4q0jto4', 31), 4294967296) self.assertEqual(int('4000000', 32), 4294967296) self.assertEqual(int('3aokq94', 33), 4294967296) self.assertEqual(int('2qhxjli', 34), 4294967296) self.assertEqual(int('2br45qb', 35), 4294967296) self.assertEqual(int('1z141z4', 36), 4294967296) # tests with base 0 # this fails on 3.0, but in 2.x the old octal syntax is allowed self.assertEqual(int(' 0o123 ', 0), 83) self.assertEqual(int(' 0o123 ', 0), 83) self.assertEqual(int('000', 0), 0) self.assertEqual(int('0o123', 0), 83) self.assertEqual(int('0x123', 0), 291) self.assertEqual(int('0b100', 0), 4) self.assertEqual(int(' 0O123 ', 0), 83) self.assertEqual(int(' 0X123 ', 0), 291) self.assertEqual(int(' 0B100 ', 0), 4) # without base still base 10 self.assertEqual(int('0123'), 123) self.assertEqual(int('0123', 10), 123) # tests with prefix and base != 0 self.assertEqual(int('0x123', 16), 291) self.assertEqual(int('0o123', 8), 83) self.assertEqual(int('0b100', 2), 4) self.assertEqual(int('0X123', 16), 291) self.assertEqual(int('0O123', 8), 83) self.assertEqual(int('0B100', 2), 4) # the code has special checks for the first character after the # type prefix self.assertRaises(ValueError, int, '0b2', 2) self.assertRaises(ValueError, int, '0b02', 2) self.assertRaises(ValueError, int, '0B2', 2) self.assertRaises(ValueError, int, '0B02', 2) self.assertRaises(ValueError, int, '0o8', 8) self.assertRaises(ValueError, int, '0o08', 8) self.assertRaises(ValueError, int, '0O8', 8) self.assertRaises(ValueError, int, '0O08', 8) self.assertRaises(ValueError, int, '0xg', 16) self.assertRaises(ValueError, int, '0x0g', 16) self.assertRaises(ValueError, int, '0Xg', 16) self.assertRaises(ValueError, int, '0X0g', 16) # SF bug 1334662: int(string, base) wrong answers # Checks for proper evaluation of 2**32 + 1 self.assertEqual(int('100000000000000000000000000000001', 2), 4294967297) self.assertEqual(int('102002022201221111212', 3), 4294967297) self.assertEqual(int('10000000000000001', 4), 4294967297) self.assertEqual(int('32244002423142', 5), 4294967297) self.assertEqual(int('1550104015505', 6), 4294967297) self.assertEqual(int('211301422355', 7), 4294967297) self.assertEqual(int('40000000001', 8), 4294967297) self.assertEqual(int('12068657455', 9), 4294967297) self.assertEqual(int('4294967297', 10), 4294967297) self.assertEqual(int('1904440555', 11), 4294967297) self.assertEqual(int('9ba461595', 12), 4294967297) self.assertEqual(int('535a7988a', 13), 4294967297) self.assertEqual(int('2ca5b7465', 14), 4294967297) self.assertEqual(int('1a20dcd82', 15), 4294967297) self.assertEqual(int('100000001', 16), 4294967297) self.assertEqual(int('a7ffda92', 17), 4294967297) self.assertEqual(int('704he7g5', 18), 4294967297) self.assertEqual(int('4f5aff67', 19), 4294967297) self.assertEqual(int('3723ai4h', 20), 4294967297) self.assertEqual(int('281d55i5', 21), 4294967297) self.assertEqual(int('1fj8b185', 22), 4294967297) self.assertEqual(int('1606k7id', 23), 4294967297) self.assertEqual(int('mb994ah', 24), 4294967297) self.assertEqual(int('hek2mgm', 25), 4294967297) self.assertEqual(int('dnchbnn', 26), 4294967297) self.assertEqual(int('b28jpdn', 27), 4294967297) self.assertEqual(int('8pfgih5', 28), 4294967297) self.assertEqual(int('76beigh', 29), 4294967297) self.assertEqual(int('5qmcpqh', 30), 4294967297) self.assertEqual(int('4q0jto5', 31), 4294967297) self.assertEqual(int('4000001', 32), 4294967297) self.assertEqual(int('3aokq95', 33), 4294967297) self.assertEqual(int('2qhxjlj', 34), 4294967297) self.assertEqual(int('2br45qc', 35), 4294967297) self.assertEqual(int('1z141z5', 36), 4294967297) def test_intconversion(self): # Test __int__() class ClassicMissingMethods: pass self.assertRaises(TypeError, int, ClassicMissingMethods()) class MissingMethods(object): pass self.assertRaises(TypeError, int, MissingMethods()) class Foo0: def __int__(self): return 42 class Foo1(object): def __int__(self): return 42 class Foo2(int): def __int__(self): return 42 class Foo3(int): def __int__(self): return self class Foo4(int): def __int__(self): return 42 class Foo5(int): def __int__(self): return 42. self.assertEqual(int(Foo0()), 42) self.assertEqual(int(Foo1()), 42) self.assertEqual(int(Foo2()), 42) self.assertEqual(int(Foo3()), 0) self.assertEqual(int(Foo4()), 42) self.assertRaises(TypeError, int, Foo5()) class Classic: pass for base in (object, Classic): class IntOverridesTrunc(base): def __int__(self): return 42 def __trunc__(self): return -12 self.assertEqual(int(IntOverridesTrunc()), 42) class JustTrunc(base): def __trunc__(self): return 42 self.assertEqual(int(JustTrunc()), 42) for trunc_result_base in (object, Classic): class Integral(trunc_result_base): def __int__(self): return 42 class TruncReturnsNonInt(base): def __trunc__(self): return Integral() self.assertEqual(int(TruncReturnsNonInt()), 42) class NonIntegral(trunc_result_base): def __trunc__(self): # Check that we avoid infinite recursion. return NonIntegral() class TruncReturnsNonIntegral(base): def __trunc__(self): return NonIntegral() try: int(TruncReturnsNonIntegral()) except TypeError as e: self.assertEqual(str(e), "__trunc__ returned non-Integral" " (type NonIntegral)") else: self.fail("Failed to raise TypeError with %s" % ((base, trunc_result_base),)) def test_error_message(self): testlist = ('\xbd', '123\xbd', ' 123 456 ') for s in testlist: try: int(s) except ValueError as e: self.assertIn(s.strip(), e.args[0]) else: self.fail("Expected int(%r) to raise a ValueError", s) def test_main(): run_unittest(IntTestCases) if __name__ == "__main__": test_main()
import struct class BaseHash: def __init__(self, name): self.name = name def prepare(self, hash): if type(hash) is long: newhash = '' for i in range(56, -8, -8): newhash += chr((hash>>i)%256) hash = newhash return hash class HashMD5(BaseHash): def __init__(self, name): BaseHash.__init__(self, name) import md5 self._hash = md5.md5 self.name = name def hash(self, hash): hash = self.prepare(hash) return self.fold(self._hash(hash).digest()) def fold(self, hash): result = map(ord, hash) n = 0L for i in range(8): result[i] ^= result[i+8] n <<= 8 n |= result[i] return n class HashSHA1(BaseHash): def __init__(self, name): BaseHash.__init__(self, name) import sha self._hash = sha.sha self.name = name def hash(self, hash): hash = self.prepare(hash) return self.fold(self._hash(hash).digest()) def fold(self, hash): hash = map(ord, hash) n = 0L n |= hash[3]^hash[11]^hash[19] n <<= 8 n |= hash[2]^hash[10]^hash[18] n <<= 8 n |= hash[1]^hash[9]^hash[17] n <<= 8 n |= hash[0]^hash[8]^hash[16] n <<= 8 n |= hash[7]^hash[15] n <<= 8 n |= hash[6]^hash[14] n <<= 8 n |= hash[5]^hash[13] n <<= 8 n |= hash[4]^hash[12] return n class HashMD4MHASH(HashMD5): def __init__(self, name): BaseHash.__init__(self, name) import mhash self._hash = mhash.MHASH self._md4 = mhash.MHASH_MD4 self.name = name def hash(self, hash): hash = self.prepare(hash) return self.fold(self._hash(self._md4, hash).digest()) def test(): try: import mhash except ImportError: return 0 return 1 test = staticmethod(test) class HashMD4Crypto(HashMD5): def __init__(self, name): BaseHash.__init__(self, name) from Crypto.Hash import MD4 self._hash = MD4.new self.name = name def test(): try: from Crypto.Hash import MD4 except ImportError: return 0 return 1 test = staticmethod(test)
# -*- coding: utf-8 -*- from __future__ import absolute_import from sentry.models import Project, User from sentry.services.udp import SentryUDPServer from sentry.testutils import TestCase, get_auth_header class SentryUDPTest(TestCase): def setUp(self): self.address = (('0.0.0.0', 0)) self.server = SentryUDPServer(*self.address) self.user = User.objects.create(username='coreapi') self.project = Project.objects.create(owner=self.user, name='Foo', slug='bar') self.pm = self.project.team.member_set.get_or_create(user=self.user)[0] self.pk = self.project.key_set.get_or_create(user=self.user)[0] def test_failure(self): self.assertNotEquals(None, self.server.handle('deadbeef', self.address)) def test_success(self): data = {'message': 'hello', 'server_name': 'not_dcramer.local', 'level': 40, 'site': 'not_a_real_site'} message = self._makeMessage(data) header = get_auth_header('udpTest', api_key=self.pk.public_key, secret_key=self.pk.secret_key) packet = header + '\n\n' + message self.assertEquals(None, self.server.handle(packet, self.address))
from unittest import TestCase from parsers.http_parser import * __author__ = 'kimothy' class TestHttpParser(TestCase): def test_parse_url(self): testList = parse_url("//foo/bar//") self.assertEqual(testList[0], "foo") self.assertEqual(testList[1], "bar") self.assertEqual(len(testList), 2) def test_parse_body(self): jsonExample = '{\ "error": {\ "message": "(#803) Cannot query users by their username (kimothykr)",\ "type": "OAuthException",\ "code": 803,\ "fbtrace_id": "DtEEUjyuC6h"\ }\ }' with self.assertRaises(Exception) : testDictionary = parse_body('{error:te') testDictionary = parse_body(jsonExample) self.assertEqual(testDictionary["error"]["code"], 803) self.assertEqual(testDictionary["error"]["fbtrace_id"], "DtEEUjyuC6h") self.assertEqual(len(testDictionary), 1) self.assertEqual(len(testDictionary["error"]), 4)
#!/usr/bin/env python3 # -*- coding: utf-8 -*- # kashev.rocks # Kashev Dalmia - kashev.dalmia@gmail.com from flask.ext.script import Manager from flask.ext.assets import ManageAssets from src.kashevrocks import app from src.assets import register_assets manager = Manager(app) assets_env = register_assets(app) manager.add_command("assets", ManageAssets(assets_env)) @manager.command def liveserver(debug=True): """ Runs a live reloading server which watches non-python code as well. """ import livereload app.debug = debug assets_env.debug = debug server = livereload.Server(app.wsgi_app) server.watch('src/') server.serve() @manager.command def clean(): """ Cleans up all generated and cache files from the project. """ import shutil import os paths_to_clean = ['src/static/.webassets-cache', 'src/static/generated', 'debug.log'] for path in paths_to_clean: try: shutil.rmtree(path) except NotADirectoryError: os.remove(path) # It's a file, not a directory except FileNotFoundError: pass # They're not there, that's fine. if __name__ == "__main__": manager.run()
# -*- coding:ascii -*- from mako import runtime, filters, cache UNDEFINED = runtime.UNDEFINED __M_dict_builtin = dict __M_locals_builtin = locals _magic_number = 9 _modified_time = 1397087641.387625 _enable_loop = True _template_filename = 'C:\\app\\account\\templates/password_reset.html' _template_uri = 'password_reset.html' _source_encoding = 'ascii' import os, os.path, re _exports = ['content'] def _mako_get_namespace(context, name): try: return context.namespaces[(__name__, name)] except KeyError: _mako_generate_namespaces(context) return context.namespaces[(__name__, name)] def _mako_generate_namespaces(context): pass def _mako_inherit(template, context): _mako_generate_namespaces(context) return runtime._inherit_from(context, 'base_template.htm', _template_uri) def render_body(context,**pageargs): __M_caller = context.caller_stack._push_frame() try: __M_locals = __M_dict_builtin(pageargs=pageargs) def content(): return render_content(context._locals(__M_locals)) __M_writer = context.writer() # SOURCE LINE 2 __M_writer('\n\n\n') if 'parent' not in context._data or not hasattr(context._data['parent'], 'content'): context['self'].content(**pageargs) # SOURCE LINE 12 __M_writer(' \n\n\n') return '' finally: context.caller_stack._pop_frame() def render_content(context,**pageargs): __M_caller = context.caller_stack._push_frame() try: def content(): return render_content(context) __M_writer = context.writer() # SOURCE LINE 5 __M_writer('\n \n<h3>Password Reset</h3>\n\n<p>Your password has been reset. Please log in again with your new password.</p>\n\t\n\n') return '' finally: context.caller_stack._pop_frame()
""" XX. Model inheritance Model inheritance exists in two varieties: - abstract base classes which are a way of specifying common information inherited by the subclasses. They don't exist as a separate model. - non-abstract base classes (the default), which are models in their own right with their own database tables and everything. Their subclasses have references back to them, created automatically. Both styles are demonstrated here. """ from django.db import models # # Abstract base classes # class CommonInfo(models.Model): name = models.CharField(max_length=50) age = models.PositiveIntegerField() class Meta: abstract = True ordering = ['name'] def __unicode__(self): return u'%s %s' % (self.__class__.__name__, self.name) class Worker(CommonInfo): job = models.CharField(max_length=50) class Student(CommonInfo): school_class = models.CharField(max_length=10) class Meta: pass class StudentWorker(Student, Worker): pass # # Abstract base classes with related models # class Post(models.Model): title = models.CharField(max_length=50) class Attachment(models.Model): post = models.ForeignKey(Post, related_name='attached_%(class)s_set') content = models.TextField() class Meta: abstract = True def __unicode__(self): return self.content class Comment(Attachment): is_spam = models.BooleanField() class Link(Attachment): url = models.URLField() # # Multi-table inheritance # class Chef(models.Model): name = models.CharField(max_length=50) def __unicode__(self): return u"%s the chef" % self.name class Place(models.Model): name = models.CharField(max_length=50) address = models.CharField(max_length=80) def __unicode__(self): return u"%s the place" % self.name class Rating(models.Model): rating = models.IntegerField(null=True, blank=True) class Meta: abstract = True ordering = ['-rating'] class Restaurant(Place, Rating): serves_hot_dogs = models.BooleanField() serves_pizza = models.BooleanField() chef = models.ForeignKey(Chef, null=True, blank=True) class Meta(Rating.Meta): db_table = 'my_restaurant' def __unicode__(self): return u"%s the restaurant" % self.name class ItalianRestaurant(Restaurant): serves_gnocchi = models.BooleanField() def __unicode__(self): return u"%s the italian restaurant" % self.name class Supplier(Place): customers = models.ManyToManyField(Restaurant, related_name='provider') def __unicode__(self): return u"%s the supplier" % self.name class ParkingLot(Place): # An explicit link to the parent (we can control the attribute name). parent = models.OneToOneField(Place, primary_key=True, parent_link=True) main_site = models.ForeignKey(Place, related_name='lot') def __unicode__(self): return u"%s the parking lot" % self.name # # Abstract base classes with related models where the sub-class has the # same name in a different app and inherits from the same abstract base # class. # NOTE: The actual API tests for the following classes are in # model_inheritance_same_model_name/models.py - They are defined # here in order to have the name conflict between apps # class Title(models.Model): title = models.CharField(max_length=50) class NamedURL(models.Model): title = models.ForeignKey(Title, related_name='attached_%(app_label)s_%(class)s_set') url = models.URLField() class Meta: abstract = True class Copy(NamedURL): content = models.TextField() def __unicode__(self): return self.content class Mixin(object): def __init__(self): self.other_attr = 1 super(Mixin, self).__init__() class MixinModel(models.Model, Mixin): pass
""" eta^3 polynomials planner author: Joe Dinius, Ph.D (https://jwdinius.github.io) Atsushi Sakai (@Atsushi_twi) Ref: - [eta^3-Splines for the Smooth Path Generation of Wheeled Mobile Robots] (https://ieeexplore.ieee.org/document/4339545/) """ import numpy as np import matplotlib.pyplot as plt from scipy.integrate import quad # NOTE: *_pose is a 3-array: # 0 - x coord, 1 - y coord, 2 - orientation angle \theta show_animation = True class Eta3Path(object): """ Eta3Path input segments: a list of `Eta3PathSegment` instances defining a continuous path """ def __init__(self, segments): # ensure input has the correct form assert(isinstance(segments, list) and isinstance( segments[0], Eta3PathSegment)) # ensure that each segment begins from the previous segment's end (continuity) for r, s in zip(segments[:-1], segments[1:]): assert(np.array_equal(r.end_pose, s.start_pose)) self.segments = segments def calc_path_point(self, u): """ Eta3Path::calc_path_point input normalized interpolation point along path object, 0 <= u <= len(self.segments) returns 2d (x,y) position vector """ assert(0 <= u <= len(self.segments)) if np.isclose(u, len(self.segments)): segment_idx = len(self.segments) - 1 u = 1. else: segment_idx = int(np.floor(u)) u -= segment_idx return self.segments[segment_idx].calc_point(u) class Eta3PathSegment(object): """ Eta3PathSegment - constructs an eta^3 path segment based on desired shaping, eta, and curvature vector, kappa. If either, or both, of eta and kappa are not set during initialization, they will default to zeros. input start_pose - starting pose array (x, y, \theta) end_pose - ending pose array (x, y, \theta) eta - shaping parameters, default=None kappa - curvature parameters, default=None """ def __init__(self, start_pose, end_pose, eta=None, kappa=None): # make sure inputs are of the correct size assert(len(start_pose) == 3 and len(start_pose) == len(end_pose)) self.start_pose = start_pose self.end_pose = end_pose # if no eta is passed, initialize it to array of zeros if not eta: eta = np.zeros((6,)) else: # make sure that eta has correct size assert(len(eta) == 6) # if no kappa is passed, initialize to array of zeros if not kappa: kappa = np.zeros((4,)) else: assert(len(kappa) == 4) # set up angle cosines and sines for simpler computations below ca = np.cos(start_pose[2]) sa = np.sin(start_pose[2]) cb = np.cos(end_pose[2]) sb = np.sin(end_pose[2]) # 2 dimensions (x,y) x 8 coefficients per dimension self.coeffs = np.empty((2, 8)) # constant terms (u^0) self.coeffs[0, 0] = start_pose[0] self.coeffs[1, 0] = start_pose[1] # linear (u^1) self.coeffs[0, 1] = eta[0] * ca self.coeffs[1, 1] = eta[0] * sa # quadratic (u^2) self.coeffs[0, 2] = 1. / 2 * eta[2] * \ ca - 1. / 2 * eta[0]**2 * kappa[0] * sa self.coeffs[1, 2] = 1. / 2 * eta[2] * \ sa + 1. / 2 * eta[0]**2 * kappa[0] * ca # cubic (u^3) self.coeffs[0, 3] = 1. / 6 * eta[4] * ca - 1. / 6 * \ (eta[0]**3 * kappa[1] + 3. * eta[0] * eta[2] * kappa[0]) * sa self.coeffs[1, 3] = 1. / 6 * eta[4] * sa + 1. / 6 * \ (eta[0]**3 * kappa[1] + 3. * eta[0] * eta[2] * kappa[0]) * ca # quartic (u^4) tmp1 = 35. * (end_pose[0] - start_pose[0]) tmp2 = (20. * eta[0] + 5 * eta[2] + 2. / 3 * eta[4]) * ca tmp3 = (5. * eta[0] ** 2 * kappa[0] + 2. / 3 * eta[0] ** 3 * kappa[1] + 2. * eta[0] * eta[2] * kappa[0]) * sa tmp4 = (15. * eta[1] - 5. / 2 * eta[3] + 1. / 6 * eta[5]) * cb tmp5 = (5. / 2 * eta[1] ** 2 * kappa[2] - 1. / 6 * eta[1] ** 3 * kappa[3] - 1. / 2 * eta[1] * eta[3] * kappa[2]) * sb self.coeffs[0, 4] = tmp1 - tmp2 + tmp3 - tmp4 - tmp5 tmp1 = 35. * (end_pose[1] - start_pose[1]) tmp2 = (20. * eta[0] + 5. * eta[2] + 2. / 3 * eta[4]) * sa tmp3 = (5. * eta[0] ** 2 * kappa[0] + 2. / 3 * eta[0] ** 3 * kappa[1] + 2. * eta[0] * eta[2] * kappa[0]) * ca tmp4 = (15. * eta[1] - 5. / 2 * eta[3] + 1. / 6 * eta[5]) * sb tmp5 = (5. / 2 * eta[1] ** 2 * kappa[2] - 1. / 6 * eta[1] ** 3 * kappa[3] - 1. / 2 * eta[1] * eta[3] * kappa[2]) * cb self.coeffs[1, 4] = tmp1 - tmp2 - tmp3 - tmp4 + tmp5 # quintic (u^5) tmp1 = -84. * (end_pose[0] - start_pose[0]) tmp2 = (45. * eta[0] + 10. * eta[2] + eta[4]) * ca tmp3 = (10. * eta[0] ** 2 * kappa[0] + eta[0] ** 3 * kappa[1] + 3. * eta[0] * eta[2] * kappa[0]) * sa tmp4 = (39. * eta[1] - 7. * eta[3] + 1. / 2 * eta[5]) * cb tmp5 = + (7. * eta[1] ** 2 * kappa[2] - 1. / 2 * eta[1] ** 3 * kappa[3] - 3. / 2 * eta[1] * eta[3] * kappa[2]) * sb self.coeffs[0, 5] = tmp1 + tmp2 - tmp3 + tmp4 + tmp5 tmp1 = -84. * (end_pose[1] - start_pose[1]) tmp2 = (45. * eta[0] + 10. * eta[2] + eta[4]) * sa tmp3 = (10. * eta[0] ** 2 * kappa[0] + eta[0] ** 3 * kappa[1] + 3. * eta[0] * eta[2] * kappa[0]) * ca tmp4 = (39. * eta[1] - 7. * eta[3] + 1. / 2 * eta[5]) * sb tmp5 = - (7. * eta[1] ** 2 * kappa[2] - 1. / 2 * eta[1] ** 3 * kappa[3] - 3. / 2 * eta[1] * eta[3] * kappa[2]) * cb self.coeffs[1, 5] = tmp1 + tmp2 + tmp3 + tmp4 + tmp5 # sextic (u^6) tmp1 = 70. * (end_pose[0] - start_pose[0]) tmp2 = (36. * eta[0] + 15. / 2 * eta[2] + 2. / 3 * eta[4]) * ca tmp3 = + (15. / 2 * eta[0] ** 2 * kappa[0] + 2. / 3 * eta[0] ** 3 * kappa[1] + 2. * eta[0] * eta[2] * kappa[0]) * sa tmp4 = (34. * eta[1] - 13. / 2 * eta[3] + 1. / 2 * eta[5]) * cb tmp5 = - (13. / 2 * eta[1] ** 2 * kappa[2] - 1. / 2 * eta[1] ** 3 * kappa[3] - 3. / 2 * eta[1] * eta[3] * kappa[2]) * sb self.coeffs[0, 6] = tmp1 - tmp2 + tmp3 - tmp4 + tmp5 tmp1 = 70. * (end_pose[1] - start_pose[1]) tmp2 = - (36. * eta[0] + 15. / 2 * eta[2] + 2. / 3 * eta[4]) * sa tmp3 = - (15. / 2 * eta[0] ** 2 * kappa[0] + 2. / 3 * eta[0] ** 3 * kappa[1] + 2. * eta[0] * eta[2] * kappa[0]) * ca tmp4 = - (34. * eta[1] - 13. / 2 * eta[3] + 1. / 2 * eta[5]) * sb tmp5 = + (13. / 2 * eta[1] ** 2 * kappa[2] - 1. / 2 * eta[1] ** 3 * kappa[3] - 3. / 2 * eta[1] * eta[3] * kappa[2]) * cb self.coeffs[1, 6] = tmp1 + tmp2 + tmp3 + tmp4 + tmp5 # septic (u^7) tmp1 = -20. * (end_pose[0] - start_pose[0]) tmp2 = (10. * eta[0] + 2. * eta[2] + 1. / 6 * eta[4]) * ca tmp3 = - (2. * eta[0] ** 2 * kappa[0] + 1. / 6 * eta[0] ** 3 * kappa[1] + 1. / 2 * eta[0] * eta[2] * kappa[0]) * sa tmp4 = (10. * eta[1] - 2. * eta[3] + 1. / 6 * eta[5]) * cb tmp5 = (2. * eta[1] ** 2 * kappa[2] - 1. / 6 * eta[1] ** 3 * kappa[3] - 1. / 2 * eta[1] * eta[3] * kappa[2]) * sb self.coeffs[0, 7] = tmp1 + tmp2 + tmp3 + tmp4 + tmp5 tmp1 = -20. * (end_pose[1] - start_pose[1]) tmp2 = (10. * eta[0] + 2. * eta[2] + 1. / 6 * eta[4]) * sa tmp3 = (2. * eta[0] ** 2 * kappa[0] + 1. / 6 * eta[0] ** 3 * kappa[1] + 1. / 2 * eta[0] * eta[2] * kappa[0]) * ca tmp4 = (10. * eta[1] - 2. * eta[3] + 1. / 6 * eta[5]) * sb tmp5 = - (2. * eta[1] ** 2 * kappa[2] - 1. / 6 * eta[1] ** 3 * kappa[3] - 1. / 2 * eta[1] * eta[3] * kappa[2]) * cb self.coeffs[1, 7] = tmp1 + tmp2 + tmp3 + tmp4 + tmp5 self.s_dot = lambda u: max(np.linalg.norm( self.coeffs[:, 1:].dot(np.array( [1, 2. * u, 3. * u**2, 4. * u**3, 5. * u**4, 6. * u**5, 7. * u**6]))), 1e-6) self.f_length = lambda ue: quad(lambda u: self.s_dot(u), 0, ue) self.segment_length = self.f_length(1)[0] def calc_point(self, u): """ Eta3PathSegment::calc_point input u - parametric representation of a point along the segment, 0 <= u <= 1 returns (x,y) of point along the segment """ assert(0 <= u <= 1) return self.coeffs.dot(np.array([1, u, u**2, u**3, u**4, u**5, u**6, u**7])) def calc_deriv(self, u, order=1): """ Eta3PathSegment::calc_deriv input u - parametric representation of a point along the segment, 0 <= u <= 1 returns (d^nx/du^n,d^ny/du^n) of point along the segment, for 0 < n <= 2 """ assert(0 <= u <= 1) assert(0 < order <= 2) if order == 1: return self.coeffs[:, 1:].dot(np.array([1, 2. * u, 3. * u**2, 4. * u**3, 5. * u**4, 6. * u**5, 7. * u**6])) return self.coeffs[:, 2:].dot(np.array([2, 6. * u, 12. * u**2, 20. * u**3, 30. * u**4, 42. * u**5])) def test1(): for i in range(10): path_segments = [] # segment 1: lane-change curve start_pose = [0, 0, 0] end_pose = [4, 3.0, 0] # NOTE: The ordering on kappa is [kappa_A, kappad_A, kappa_B, kappad_B], with kappad_* being the curvature derivative kappa = [0, 0, 0, 0] eta = [i, i, 0, 0, 0, 0] path_segments.append(Eta3PathSegment( start_pose=start_pose, end_pose=end_pose, eta=eta, kappa=kappa)) path = Eta3Path(path_segments) # interpolate at several points along the path ui = np.linspace(0, len(path_segments), 1001) pos = np.empty((2, ui.size)) for j, u in enumerate(ui): pos[:, j] = path.calc_path_point(u) if show_animation: # plot the path plt.plot(pos[0, :], pos[1, :]) # for stopping simulation with the esc key. plt.gcf().canvas.mpl_connect( 'key_release_event', lambda event: [exit(0) if event.key == 'escape' else None]) plt.pause(1.0) if show_animation: plt.close("all") def test2(): for i in range(10): path_segments = [] # segment 1: lane-change curve start_pose = [0, 0, 0] end_pose = [4, 3.0, 0] # NOTE: The ordering on kappa is [kappa_A, kappad_A, kappa_B, kappad_B], with kappad_* being the curvature derivative kappa = [0, 0, 0, 0] eta = [0, 0, (i - 5) * 20, (5 - i) * 20, 0, 0] path_segments.append(Eta3PathSegment( start_pose=start_pose, end_pose=end_pose, eta=eta, kappa=kappa)) path = Eta3Path(path_segments) # interpolate at several points along the path ui = np.linspace(0, len(path_segments), 1001) pos = np.empty((2, ui.size)) for j, u in enumerate(ui): pos[:, j] = path.calc_path_point(u) if show_animation: # plot the path plt.plot(pos[0, :], pos[1, :]) plt.pause(1.0) if show_animation: plt.close("all") def test3(): path_segments = [] # segment 1: lane-change curve start_pose = [0, 0, 0] end_pose = [4, 1.5, 0] # NOTE: The ordering on kappa is [kappa_A, kappad_A, kappa_B, kappad_B], with kappad_* being the curvature derivative kappa = [0, 0, 0, 0] eta = [4.27, 4.27, 0, 0, 0, 0] path_segments.append(Eta3PathSegment( start_pose=start_pose, end_pose=end_pose, eta=eta, kappa=kappa)) # segment 2: line segment start_pose = [4, 1.5, 0] end_pose = [5.5, 1.5, 0] kappa = [0, 0, 0, 0] eta = [0, 0, 0, 0, 0, 0] path_segments.append(Eta3PathSegment( start_pose=start_pose, end_pose=end_pose, eta=eta, kappa=kappa)) # segment 3: cubic spiral start_pose = [5.5, 1.5, 0] end_pose = [7.4377, 1.8235, 0.6667] kappa = [0, 0, 1, 1] eta = [1.88, 1.88, 0, 0, 0, 0] path_segments.append(Eta3PathSegment( start_pose=start_pose, end_pose=end_pose, eta=eta, kappa=kappa)) # segment 4: generic twirl arc start_pose = [7.4377, 1.8235, 0.6667] end_pose = [7.8, 4.3, 1.8] kappa = [1, 1, 0.5, 0] eta = [7, 10, 10, -10, 4, 4] path_segments.append(Eta3PathSegment( start_pose=start_pose, end_pose=end_pose, eta=eta, kappa=kappa)) # segment 5: circular arc start_pose = [7.8, 4.3, 1.8] end_pose = [5.4581, 5.8064, 3.3416] kappa = [0.5, 0, 0.5, 0] eta = [2.98, 2.98, 0, 0, 0, 0] path_segments.append(Eta3PathSegment( start_pose=start_pose, end_pose=end_pose, eta=eta, kappa=kappa)) # construct the whole path path = Eta3Path(path_segments) # interpolate at several points along the path ui = np.linspace(0, len(path_segments), 1001) pos = np.empty((2, ui.size)) for i, u in enumerate(ui): pos[:, i] = path.calc_path_point(u) # plot the path if show_animation: plt.figure('Path from Reference') plt.plot(pos[0, :], pos[1, :]) plt.xlabel('x') plt.ylabel('y') plt.title('Path') plt.pause(1.0) plt.show() def main(): """ recreate path from reference (see Table 1) """ test1() test2() test3() if __name__ == '__main__': main()
# -*- coding: utf-8 -*- # # Copyright 2010-2013, Thorsten Weimann; 2014, Alexander Shorin; 2016 Pedro Salgado # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # """ Sphinx documentation build configuration file. All configuration values have a default; values that are commented out serve to show the default. If extensions (or modules to document with autodoc) are in another directory, add these directories to sys.path here. If the directory is relative to the documentation root, use os.path.abspath to make it absolute, like shown here. """ import sys import os sys.path.insert(0, os.path.abspath('..')) import semantic_version from recommonmark.parser import CommonMarkParser _package = 'steenzout.barcode' _version = semantic_version.Version('1.0.0-beta3') # -- General configuration ------------------------------------------------ # If your documentation needs a minimal Sphinx version, state it here. # # needs_sphinx = '1.0' # Add any Sphinx extension module names here, as strings. They can be # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom # ones. extensions = [ 'sphinx.ext.autodoc', 'sphinx.ext.coverage', 'sphinx.ext.napoleon', 'sphinx.ext.viewcode', ] # Add any paths that contain templates here, relative to this directory. templates_path = ['_templates'] # The suffix(es) of source filenames. # You can specify multiple suffix as a list of string: # # source_suffix = '.rst' source_parsers = { '.md': CommonMarkParser } source_suffix = [ '.md', '.rst' ] # The encoding of source files. # # source_encoding = 'utf-8-sig' # The master toctree document. master_doc = 'index' # General information about the project. project = _package author = 'Thorsten Weimann, Alexander Shorin, Pedro Salgado' copyright = '2010-2013, Thorsten Weimann; 2014, Alexander Shorin; 2016 Pedro Salgado' # The version info for the project you're documenting, acts as replacement for # |version| and |release|, also used in various other places throughout the # built documents. # # The short X.Y version. version = u'%d.%d' % (_version.major, _version.minor) # The full version, including alpha/beta/rc tags. release = version # The language for content autogenerated by Sphinx. Refer to documentation # for a list of supported languages. # # This is also used if you do content translation via gettext catalogs. # Usually you set "language" from the command line for these cases. language = 'en' # There are two options for replacing |today|: either, you set today to some # non-false value, then it is used: # # today = '' # # Else, today_fmt is used as the format for a strftime call. # # today_fmt = '%B %d, %Y' # List of patterns, relative to source directory, that match files and # directories to ignore when looking for source files. # This patterns also effect to html_static_path and html_extra_path exclude_patterns = ['_build', '_templates', 'Thumbs.db', '.DS_Store'] # The reST default role (used for this markup: `text`) to use for all # documents. # # default_role = None # If true, '()' will be appended to :func: etc. cross-reference text. # # add_function_parentheses = True # If true, the current module name will be prepended to all description # unit titles (such as .. function::). # add_module_names = False # If true, sectionauthor and moduleauthor directives will be shown in the # output. They are ignored by default. # # show_authors = False # The name of the Pygments (syntax highlighting) style to use. pygments_style = 'sphinx' # A list of ignored prefixes for module index sorting. # modindex_common_prefix = [] # If true, keep warnings as "system message" paragraphs in the built documents. # keep_warnings = False # If true, `todo` and `todoList` produce output, else they produce nothing. todo_include_todos = False # -- Options for HTML output ---------------------------------------------- # The theme to use for HTML and HTML Help pages. See the documentation for # a list of builtin themes. # html_theme = 'sphinx_rtd_theme' # Theme options are theme-specific and customize the look and feel of a theme # further. For a list of options available for each theme, see the # documentation. # html_theme_options = { } # Add any paths that contain custom themes here, relative to this directory. # html_theme_path = [] # The name for this set of Sphinx documents. # "<project> v<release> documentation" by default. # # html_title = u'' # A shorter title for the navigation bar. Default is the same as html_title. # # html_short_title = None # The name of an image file (relative to this directory) to place at the top # of the sidebar. # # html_logo = None # The name of an image file (relative to this directory) to use as a favicon of # the docs. This file should be a Windows icon file (.ico) being 16x16 or 32x32 # pixels large. # # html_favicon = None # Add any paths that contain custom static files (such as style sheets) here, # relative to this directory. They are copied after the builtin static files, # so a file named "default.css" will overwrite the builtin "default.css". html_static_path = ['_static'] # Add any extra paths that contain custom files (such as robots.txt or # .htaccess) here, relative to this directory. These files are copied # directly to the root of the documentation. # # html_extra_path = [] # If not None, a 'Last updated on:' timestamp is inserted at every page # bottom, using the given strftime format. # The empty string is equivalent to '%b %d, %Y'. # # html_last_updated_fmt = None # If true, SmartyPants will be used to convert quotes and dashes to # typographically correct entities. # # html_use_smartypants = True # Custom sidebar templates, maps document names to template names. # # html_sidebars = {} # Additional templates that should be rendered to pages, maps page names to # template names. # # html_additional_pages = {} # If false, no module index is generated. # # html_domain_indices = True # If false, no index is generated. # # html_use_index = True # If true, the index is split into individual pages for each letter. # # html_split_index = False # If true, links to the reST sources are added to the pages. # html_show_sourcelink = False # If true, "Created using Sphinx" is shown in the HTML footer. Default is True. # # html_show_sphinx = True # If true, "(C) Copyright ..." is shown in the HTML footer. Default is True. # # html_show_copyright = True # If true, an OpenSearch description file will be output, and all pages will # contain a <link> tag referring to it. The value of this option must be the # base URL from which the finished HTML is served. # # html_use_opensearch = '' # This is the file name suffix for HTML files (e.g. ".xhtml"). # html_file_suffix = None # Language to be used for generating the HTML full-text search index. # Sphinx supports the following languages: # 'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja' # 'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr', 'zh' # # html_search_language = 'en' # A dictionary with options for the search language support, empty by default. # 'ja' uses this config value. # 'zh' user can custom change `jieba` dictionary path. # # html_search_options = {'type': 'default'} # The name of a javascript file (relative to the configuration directory) that # implements a search results scorer. If empty, the default will be used. # # html_search_scorer = 'scorer.js' # Output file base name for HTML help builder. htmlhelp_basename = u'py_%s' % _package.replace('.', '_') # -- Options for LaTeX output --------------------------------------------- # latex_elements = { # The paper size ('letterpaper' or 'a4paper'). # # 'papersize': 'letterpaper', # # The font size ('10pt', '11pt' or '12pt'). # # 'pointsize': '10pt', # # Additional stuff for the LaTeX preamble. # # 'preamble': '', # # Latex figure (float) alignment # # 'figure_align': 'htbp', # } latex_elements = {} # Grouping the document tree into LaTeX files. # List of tuples ( # source start file, # target name, # title, # author, # documentclass [howto, manual, or own class] # ). latex_documents = [( master_doc, u'py_%s.tex' % _package.replace('.', '_'), u'%s documentation' % _package, 'Thorsten Weimann, Alexander Shorin, Pedro Salgado', u'manual' )] # The name of an image file (relative to this directory) to place at the top of # the title page. # # latex_logo = None # For "manual" documents, if this is true, then toplevel headings are parts, # not chapters. # # latex_use_parts = False # If true, show page references after internal links. # # latex_show_pagerefs = False # If true, show URL addresses after external links. # # latex_show_urls = False # Documents to append as an appendix to all manuals. # # latex_appendices = [] # It false, will not define \strong, \code, itleref, \crossref ... but only # \sphinxstrong, ..., \sphinxtitleref, ... To help avoid clash with user added # sphinxs. # # latex_keep_old_macro_names = True # If false, no module index is generated. # # latex_domain_indices = True # -- Options for manual page output --------------------------------------- # One entry per manual page. # List of tuples ( # source start file, # name, # description, # authors, # manual section # ). man_pages = [( master_doc, 'py_%s' % _package.replace('.', '_'), '%s documentation' % _package, ['Thorsten Weimann, Alexander Shorin, Pedro Salgado'], 1 )] # If true, show URL addresses after external links. # # man_show_urls = False # -- Options for Texinfo output ------------------------------------------- # Grouping the document tree into Texinfo files. # List of tuples ( # source start file, # target name, # title, # author, # dir menu entry, # description, # category # ). texinfo_documents = [( master_doc, u'py_%s' % _package.replace('.', '_'), u'%s documentation' % _package, 'Thorsten Weimann, Alexander Shorin, Pedro Salgado', u'', u'%s documentation.' % _package, u'Miscellaneous' )] # Documents to append as an appendix to all manuals. # # texinfo_appendices = [] # If false, no module index is generated. # # texinfo_domain_indices = True # How to display URL addresses: 'footnote', 'no', or 'inline'. # # texinfo_show_urls = 'footnote' # If true, do not generate a @detailmenu in the "Top" node's menu. # # texinfo_no_detailmenu = False # Generate API doc from sphinx import apidoc apidoc.main(['-f', '-T', '--separate', '-o', 'apidoc', '../steenzout'])
# # Unpacker for Dean Edward's p.a.c.k.e.r, a part of javascript beautifier # by Einar Lielmanis <einar@jsbeautifier.org> # # written by Stefano Sanfilippo <a.little.coder@gmail.com> # # usage: # # if detect(some_string): # unpacked = unpack(some_string) # """Unpacker for Dean Edward's p.a.c.k.e.r""" import re import string from jsbeautifier.unpackers import UnpackingError PRIORITY = 1 def detect(source): """Detects whether `source` is P.A.C.K.E.R. coded.""" return source.replace(' ', '').startswith('eval(function(p,a,c,k,e,r') def unpack(source): """Unpacks P.A.C.K.E.R. packed js code.""" payload, symtab, radix, count = _filterargs(source) if count != len(symtab): raise UnpackingError('Malformed p.a.c.k.e.r. symtab.') try: unbase = Unbaser(radix) except TypeError: raise UnpackingError('Unknown p.a.c.k.e.r. encoding.') def lookup(match): """Look up symbols in the synthetic symtab.""" word = match.group(0) return symtab[unbase(word)] or word source = re.sub(r'\b\w+\b', lookup, payload) return _replacestrings(source) def _filterargs(source): """Juice from a source file the four args needed by decoder.""" argsregex = (r"}\('(.*)', *(\d+), *(\d+), *'(.*)'\." r"split\('\|'\), *(\d+), *(.*)\)\)") args = re.search(argsregex, source, re.DOTALL).groups() try: return args[0], args[3].split('|'), int(args[1]), int(args[2]) except ValueError: raise UnpackingError('Corrupted p.a.c.k.e.r. data.') def _replacestrings(source): """Strip string lookup table (list) and replace values in source.""" match = re.search(r'var *(_\w+)\=\["(.*?)"\];', source, re.DOTALL) if match: varname, strings = match.groups() startpoint = len(match.group(0)) lookup = strings.split('","') variable = '%s[%%d]' % varname for index, value in enumerate(lookup): source = source.replace(variable % index, '"%s"' % value) return source[startpoint:] return source class Unbaser(object): """Functor for a given base. Will efficiently convert strings to natural numbers.""" ALPHABET = { 62 : '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ', 95 : (' !"#$%&\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ' '[\]^_`abcdefghijklmnopqrstuvwxyz{|}~') } def __init__(self, base): self.base = base # If base can be handled by int() builtin, let it do it for us if 2 <= base <= 36: self.unbase = lambda string: int(string, base) else: # Build conversion dictionary cache try: self.dictionary = dict((cipher, index) for index, cipher in enumerate(self.ALPHABET[base])) except KeyError: raise TypeError('Unsupported base encoding.') self.unbase = self._dictunbaser def __call__(self, string): return self.unbase(string) def _dictunbaser(self, string): """Decodes a value to an integer.""" ret = 0 for index, cipher in enumerate(string[::-1]): ret += (self.base ** index) * self.dictionary[cipher] return ret
# -*- coding: utf-8 -*- """ markupsafe ~~~~~~~~~~ Implements a Markup string. :copyright: (c) 2010 by Armin Ronacher. :license: BSD, see LICENSE for more details. """ import re from markupsafe._compat import text_type, string_types, int_types, \ unichr, PY2 __all__ = ['Markup', 'soft_unicode', 'escape', 'escape_silent'] _striptags_re = re.compile(r'(<!--.*?-->|<[^>]*>)') _entity_re = re.compile(r'&([^;]+);') class Markup(text_type): r"""Marks a string as being safe for inclusion in HTML/XML output without needing to be escaped. This implements the `__html__` interface a couple of frameworks and web applications use. :class:`Markup` is a direct subclass of `unicode` and provides all the methods of `unicode` just that it escapes arguments passed and always returns `Markup`. The `escape` function returns markup objects so that double escaping can't happen. The constructor of the :class:`Markup` class can be used for three different things: When passed an unicode object it's assumed to be safe, when passed an object with an HTML representation (has an `__html__` method) that representation is used, otherwise the object passed is converted into a unicode string and then assumed to be safe: >>> Markup("Hello <em>World</em>!") Markup(u'Hello <em>World</em>!') >>> class Foo(object): ... def __html__(self): ... return '<a href="#">foo</a>' ... >>> Markup(Foo()) Markup(u'<a href="#">foo</a>') If you want object passed being always treated as unsafe you can use the :meth:`escape` classmethod to create a :class:`Markup` object: >>> Markup.escape("Hello <em>World</em>!") Markup(u'Hello &lt;em&gt;World&lt;/em&gt;!') Operations on a markup string are markup aware which means that all arguments are passed through the :func:`escape` function: >>> em = Markup("<em>%s</em>") >>> em % "foo & bar" Markup(u'<em>foo &amp; bar</em>') >>> strong = Markup("<strong>%(text)s</strong>") >>> strong % {'text': '<blink>hacker here</blink>'} Markup(u'<strong>&lt;blink&gt;hacker here&lt;/blink&gt;</strong>') >>> Markup("<em>Hello</em> ") + "<foo>" Markup(u'<em>Hello</em> &lt;foo&gt;') """ __slots__ = () def __new__(cls, base=u'', encoding=None, errors='strict'): if hasattr(base, '__html__'): base = base.__html__() if encoding is None: return text_type.__new__(cls, base) return text_type.__new__(cls, base, encoding, errors) def __html__(self): return self def __add__(self, other): if isinstance(other, string_types) or hasattr(other, '__html__'): return self.__class__(super(Markup, self).__add__(self.escape(other))) return NotImplemented def __radd__(self, other): if hasattr(other, '__html__') or isinstance(other, string_types): return self.escape(other).__add__(self) return NotImplemented def __mul__(self, num): if isinstance(num, int_types): return self.__class__(text_type.__mul__(self, num)) return NotImplemented __rmul__ = __mul__ def __mod__(self, arg): if isinstance(arg, tuple): arg = tuple(_MarkupEscapeHelper(x, self.escape) for x in arg) else: arg = _MarkupEscapeHelper(arg, self.escape) return self.__class__(text_type.__mod__(self, arg)) def __repr__(self): return '%s(%s)' % ( self.__class__.__name__, text_type.__repr__(self) ) def join(self, seq): return self.__class__(text_type.join(self, map(self.escape, seq))) join.__doc__ = text_type.join.__doc__ def split(self, *args, **kwargs): return list(map(self.__class__, text_type.split(self, *args, **kwargs))) split.__doc__ = text_type.split.__doc__ def rsplit(self, *args, **kwargs): return list(map(self.__class__, text_type.rsplit(self, *args, **kwargs))) rsplit.__doc__ = text_type.rsplit.__doc__ def splitlines(self, *args, **kwargs): return list(map(self.__class__, text_type.splitlines(self, *args, **kwargs))) splitlines.__doc__ = text_type.splitlines.__doc__ def unescape(self): r"""Unescape markup again into an text_type string. This also resolves known HTML4 and XHTML entities: >>> Markup("Main &raquo; <em>About</em>").unescape() u'Main \xbb <em>About</em>' """ from markupsafe._constants import HTML_ENTITIES def handle_match(m): name = m.group(1) if name in HTML_ENTITIES: return unichr(HTML_ENTITIES[name]) try: if name[:2] in ('#x', '#X'): return unichr(int(name[2:], 16)) elif name.startswith('#'): return unichr(int(name[1:])) except ValueError: pass return u'' return _entity_re.sub(handle_match, text_type(self)) def striptags(self): r"""Unescape markup into an text_type string and strip all tags. This also resolves known HTML4 and XHTML entities. Whitespace is normalized to one: >>> Markup("Main &raquo; <em>About</em>").striptags() u'Main \xbb About' """ stripped = u' '.join(_striptags_re.sub('', self).split()) return Markup(stripped).unescape() @classmethod def escape(cls, s): """Escape the string. Works like :func:`escape` with the difference that for subclasses of :class:`Markup` this function would return the correct subclass. """ rv = escape(s) if rv.__class__ is not cls: return cls(rv) return rv def make_wrapper(name): orig = getattr(text_type, name) def func(self, *args, **kwargs): args = _escape_argspec(list(args), enumerate(args), self.escape) #_escape_argspec(kwargs, kwargs.iteritems(), None) return self.__class__(orig(self, *args, **kwargs)) func.__name__ = orig.__name__ func.__doc__ = orig.__doc__ return func for method in '__getitem__', 'capitalize', \ 'title', 'lower', 'upper', 'replace', 'ljust', \ 'rjust', 'lstrip', 'rstrip', 'center', 'strip', \ 'translate', 'expandtabs', 'swapcase', 'zfill': locals()[method] = make_wrapper(method) # new in python 2.5 if hasattr(text_type, 'partition'): def partition(self, sep): return tuple(map(self.__class__, text_type.partition(self, self.escape(sep)))) def rpartition(self, sep): return tuple(map(self.__class__, text_type.rpartition(self, self.escape(sep)))) # new in python 2.6 if hasattr(text_type, 'format'): format = make_wrapper('format') # not in python 3 if hasattr(text_type, '__getslice__'): __getslice__ = make_wrapper('__getslice__') del method, make_wrapper def _escape_argspec(obj, iterable, escape): """Helper for various string-wrapped functions.""" for key, value in iterable: if hasattr(value, '__html__') or isinstance(value, string_types): obj[key] = escape(value) return obj class _MarkupEscapeHelper(object): """Helper for Markup.__mod__""" def __init__(self, obj, escape): self.obj = obj self.escape = escape __getitem__ = lambda s, x: _MarkupEscapeHelper(s.obj[x], s.escape) __unicode__ = __str__ = lambda s: text_type(s.escape(s.obj)) __repr__ = lambda s: str(s.escape(repr(s.obj))) __int__ = lambda s: int(s.obj) __float__ = lambda s: float(s.obj) # we have to import it down here as the speedups and native # modules imports the markup type which is define above. try: from markupsafe._speedups import escape, escape_silent, soft_unicode except ImportError: from markupsafe._native import escape, escape_silent, soft_unicode if not PY2: soft_str = soft_unicode __all__.append('soft_str')
#!/usr/bin/env python # -*- coding: utf-8 -*- import clr SWF = clr.AddReference("System.Windows.Forms") print (SWF.Location) import System.Windows.Forms as WinForms from System.Drawing import Size, Point class HelloApp(WinForms.Form): """A simple hello world app that demonstrates the essentials of winforms programming and event-based programming in Python.""" def __init__(self): self.Text = "Hello World From Python" self.AutoScaleBaseSize = Size(5, 13) self.ClientSize = Size(392, 117) h = WinForms.SystemInformation.CaptionHeight self.MinimumSize = Size(392, (117 + h)) # Create the button self.button = WinForms.Button() self.button.Location = Point(160, 64) self.button.Size = Size(820, 20) self.button.TabIndex = 2 self.button.Text = "Click Me!" # Register the event handler self.button.Click += self.button_Click # Create the text box self.textbox = WinForms.TextBox() self.textbox.Text = "Hello World" self.textbox.TabIndex = 1 self.textbox.Size = Size(1260, 40) self.textbox.Location = Point(160, 24) # Add the controls to the form self.AcceptButton = self.button self.Controls.Add(self.button) self.Controls.Add(self.textbox) def button_Click(self, sender, args): """Button click event handler""" print ("Click") WinForms.MessageBox.Show("Please do not press this button again.") def run(self): WinForms.Application.Run(self) def main(): form = HelloApp() print ("form created") app = WinForms.Application print ("app referenced") app.Run(form) if __name__ == '__main__': main()
#!/usr/bin/env python # -*- coding: latin-1 -*- """ Copyright  2003 Bogdan Sumanariu <zarrok@yahoo.com> This file is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA. script name : evolutionvcard2claws.py script purpose : convert an evolution addressbook VCARD file into a Claws Mail addressbook tested with evolution 1.2.x, and 1.4.x """ import string import sys import time import os import StringIO keywds = ('x-evolution-file-as','fn', 'n','email;internet','nickname', 'url', 'org') def normalizeLongLines(file): """ Skip line breaks after 72 chars """ buf = '' line = file.readline() while line: if line[0] == ' ': buf = buf.rstrip('\n') line = line.lstrip(); buf += line else: buf += line line = file.readline() return buf def getEmailAddress(vcard): """ Get email address. Supported formats: - email;something - email;type=something something := (internet,work,home, other) """ for key in vcard: items = key.split(';') if len(items) == 2: if items[0].lower() == 'email': list = vcard[key] return list[0] else: if key.lower() == 'email': list = vcard[key] return list[0] return "" def findName(vcard): """ Find a version 3.0 name """ for key in vcard: items = key.split(';') if len(items) == 2: if items[0].lower() == 'n': return vcard[key] else: if key.lower() == 'n': return vcard[key] return None ################################################################################ ## reads a vcard and stores as hash pairs key/value where value is a list ## ################################################################################ def readVCARD (buffer) : """ skips fom <file> until a 'begin' tag from VCARD is encountered. from this point starts constructing a map (key, [values] ) VCARD entry format -> tag:value key <- tag [values] <- list with the values of <tag> if there are more tags with the same name """ r=' ' bgn,end = -1, -1; d = dict() while r and bgn < 0 : r = buffer.readline() if len (r) == 0 : return dict() if string.find('begin',string.lower(string.strip(r))) : bgn = 1 while r and end < 0 : r = buffer.readline() s = string.split(string.lower(string.strip(r)),':') if s[0] <> '' : if d.has_key(s[0]) : d[s[0]].append(s[1]) elif len(s) > 1: d[s[0]] = [s[1]] else : d[s[0]] = [''] if s[0] == 'end' : end = 1 return d ################################################################################## ############################################################################################### ## writes on a given file an xml representation for claws-mail addressbook received as a hash ## ############################################################################################### def writeXMLREPR (vcard,file,uid) : """ based on <vcard> and <uid> writes only recognized tags (the ones defined in <keywds> list) NOTE: <url> and <org> tag will be written as attributes (there are such tags in claws-mail's XML schema) """ if len (vcard.keys()) == 0 : return item = vcard.get(keywds[2]); if item: name = string.split(item[0],';') else: """ version 3.0 n ?""" name = findName(vcard) if not name: return fn, ln, nick, cn, a = '', '', '', '', '' if len(name) >= 2 : fn = name[0] ln = name[1] elif len(name) ==1 : fn = name[0] if vcard.has_key(keywds[4]) : nick = vcard.get(keywds[4])[0] if len(vcard.get(keywds[1])[0]) : cn = vcard.get(keywds[1])[0] else : cn = vcard.get(keywds[0])[0]; a += str('\n<person uid=\"' + str(uid[0]) + '\" first-name=\"' + fn + '\" last-name=\"' + ln + '\" nick-name=\"' + nick + '\" cn=\"' + cn + '\" >\n') a += '\t<address-list>\n' if vcard.get(keywds[3]) : for c in vcard.get(keywds[3]) : uid[0] = uid[0] + 1 a += '\t\t<address uid=\"' + str(uid[0]) + '\" alias=\"' + nick + '\" email=\"' + c + '\" remarks=\"\" />\n' else : email = getEmailAddress(vcard) uid[0] = uid[0]+1 a += '\t\t<address uid=\"' + str(uid[0]) + '\" alias=\"' + nick + '\" email=\"' + email + '\" remarks=\"\" />\n' a += '\t</address-list>\n' a += '\t<attribute-list>\n' for key in keywds[5:] : if vcard.get(key) : for c in vcard.get(key) : uid[0] = uid[0] + 1 a += '\t\t<attribute uid=\"' + str(uid[0]) + '\" name=\"' + key +'\">'+c+'</attribute>\n' a += '\t</attribute-list>\n' a += '</person>\n' file.write(a) file.flush() ################################################################################################### def convert (in_f, o_f, name='INBOX') : d = {'d':1} uid = [int(time.time())] try : print 'proccessing...\n' o_f.write('<?xml version="1.0" encoding="ISO-8859-1" ?>\n<address-book name="'+name+'" >\n'); buf = normalizeLongLines(in_f) buffer = StringIO.StringIO(buf) while len(d.keys()) > 0 : d = readVCARD(buffer) writeXMLREPR (d, o_f, uid) uid[0] = uid [0]+1 o_f.write('\n</address-book>') print 'finished processing...\n' except IOError, err : print 'Caught an IOError : ',err,'\t ABORTING!!!' raise err ################################################################################################# def execute () : if len(sys.argv) <> 3 and len(sys.argv) <> 2 : print str("\nUsage: vcard2xml.py source_file [destination_file]\n\n" + '\tWhen only <source_file> is specified will overwrite the existing addressbook.\n'+ '\tWhen both arguments are suplied will create a new additional addressbook named \n\tas the destination file.'+'\n\tNOTE: in both cases the Claws Mail must be closed and ran at least once.\n\n') sys.exit(1) in_file = None out_file = None path_to_out = os.environ['HOME']+'/.claws-mail/' adr_idx = 'addrbook--index.xml' adr_idx_file = None tmp_adr_idx_file= None got_ex = 0 try : in_file = open(sys.argv[1]) except IOError, e: print 'Could not open input file <',sys.argv[1],'> ABORTING' sys.exit(1) if len(sys.argv) == 2 : try : dlist = os.listdir(path_to_out); flist=[] for l in dlist : if l.find('addrbook') == 0 and l.find("addrbook--index.xml") < 0 and l.find('bak') < 0 : flist.append(l) flist.sort() out_file = flist.pop() os.rename(path_to_out+out_file, path_to_out+out_file+'.tmp') out_file = open(path_to_out+out_file,'w') convert(in_file, out_file) except Exception, e: got_ex = 1 print 'got exception: ', e else : try : os.rename(path_to_out+adr_idx, path_to_out+adr_idx+'.tmp') tmp_adr_idx_file = open(path_to_out+adr_idx+'.tmp') adr_idx_file = open(path_to_out+adr_idx,'w') except Exception, e : print 'Could not open <', path_to_out+adr_idx,'> file. Make sure you started Claws Mail at least once.' sys.exit(1) try : out_file = open(path_to_out+sys.argv[2],'w') convert(in_file, out_file, sys.argv[2].split('.xml')[0]) l = tmp_adr_idx_file.readline() while l : if l.strip() == '</book_list>' : adr_idx_file.write('\t<book name="'+sys.argv[2].split('.xml')[0] +'" file="'+sys.argv[2]+'" />\n') adr_idx_file.write(l) else : adr_idx_file.write(l) l = tmp_adr_idx_file.readline() except Exception, e: got_ex = 1 print 'got exception: ', e if got_ex : #clean up the mess print 'got exception, cleaning up the mess... changed files will be restored...\n' if adr_idx_file : adr_idx_file.close() if out_file : out_file.close() if len(sys.argv) == 2 : os.rename(out_file.name+'.tmp', out_file.name) else : os.remove(out_file.name) os.rename(path_to_out+adr_idx+'.tmp', path_to_out+adr_idx) if tmp_adr_idx_file : tmp_adr_idx_file.close() else : #closing all and moving temporary data into place print 'closing open files...\n' in_file.close() out_file.close() if len(sys.argv) == 3 : os.rename(path_to_out+adr_idx+'.tmp',path_to_out+adr_idx+'.bak' ) if len(sys.argv) == 2 : os.rename(out_file.name+'.tmp', out_file.name+'.bak') if adr_idx_file : adr_idx_file.close() if tmp_adr_idx_file : tmp_adr_idx_file.close() print 'done!' if __name__ == '__main__': execute ()
""" South Africa-specific Form helpers """ from django.core.validators import EMPTY_VALUES from django.forms import ValidationError from django.forms.fields import CharField, RegexField from django.utils.checksums import luhn from django.utils.translation import gettext as _ import re from datetime import date id_re = re.compile(r'^(?P<yy>\d\d)(?P<mm>\d\d)(?P<dd>\d\d)(?P<mid>\d{4})(?P<end>\d{3})') class ZAIDField(CharField): """A form field for South African ID numbers -- the checksum is validated using the Luhn checksum, and uses a simlistic (read: not entirely accurate) check for the birthdate """ default_error_messages = { 'invalid': _(u'Enter a valid South African ID number'), } def clean(self, value): super(ZAIDField, self).clean(value) if value in EMPTY_VALUES: return u'' # strip spaces and dashes value = value.strip().replace(' ', '').replace('-', '') match = re.match(id_re, value) if not match: raise ValidationError(self.error_messages['invalid']) g = match.groupdict() try: # The year 2000 is conveniently a leapyear. # This algorithm will break in xx00 years which aren't leap years # There is no way to guess the century of a ZA ID number d = date(int(g['yy']) + 2000, int(g['mm']), int(g['dd'])) except ValueError: raise ValidationError(self.error_messages['invalid']) if not luhn(value): raise ValidationError(self.error_messages['invalid']) return value class ZAPostCodeField(RegexField): default_error_messages = { 'invalid': _(u'Enter a valid South African postal code'), } def __init__(self, max_length=None, min_length=None, *args, **kwargs): super(ZAPostCodeField, self).__init__(r'^\d{4}$', max_length, min_length, *args, **kwargs)
# This Source Code Form is subject to the terms of the Mozilla Public # License, v. 2.0. If a copy of the MPL was not distributed with this # file, You can obtain one at http://mozilla.org/MPL/2.0/. """this is the basis for any app that follows the fetch/transform/save model * the configman versions of the crash mover and the processor apps will derive from this class The form of fetch/transform/save, of course, in three parts 1) fetch - some iterating or streaming function or object fetches packets of from data a source 2) transform - some function transforms each packet of data into a new form 3) save - some function or class saves or streams the packet to some data sink. For the crash mover, the fetch phase is reading new crashes from the collector's file system datastore. The transform phase is the degenerate case of identity: no transformation. The save phase is just sending the crashes to HBase. For the processor, the fetch phase is reading from the new crash queue. In, 2012, that's the union of reading a postgres jobs/crash_id table and fetching the crash from HBase. The transform phase is the running of minidump stackwalk and production of the processed crash data. The save phase is the union of sending new crash records to Postgres; sending the processed crash to HBase; the the submission of the crash_id to Elastic Search.""" import signal from functools import partial from configman import Namespace from configman.converters import class_converter from socorro.lib.task_manager import respond_to_SIGTERM from socorro.app.generic_app import App, main # main not used here, but # is imported from generic_app # into this scope to offer to # apps that derive from the # class defined here. #============================================================================== class FetchTransformSaveApp(App): """base class for apps that follow the fetch/transform/save model""" app_name = 'generic_fetch_transform_save_app' app_version = '0.1' app_description = __doc__ required_config = Namespace() # the required config is broken into two parts: the source and the # destination. Each of those gets assigned a crasnstorage class. required_config.source = Namespace() # For source, the storage class should be one that defines a method # of fetching new crashes through the three storage api methods: the # iterator 'new_crashes' and the accessors 'get_raw_crash' and # 'get_raw_dumps' required_config.source.add_option( 'crashstorage_class', doc='the source storage class', default=None, from_string_converter=class_converter ) required_config.destination = Namespace() # For destination, the storage class should implement the 'save_raw_crash' # method. Of course, a subclass may redefine either the source_iterator # or transform methods and therefore completely redefine what api calls # are relevant. required_config.destination.add_option( 'crashstorage_class', doc='the destination storage class', default=None, from_string_converter=class_converter ) required_config.producer_consumer = Namespace() required_config.producer_consumer.add_option( 'producer_consumer_class', doc='the class implements a threaded producer consumer queue', default='socorro.lib.threaded_task_manager.ThreadedTaskManager', from_string_converter=class_converter ) ########################################################################### ### TODO: add a feature where clients of this class may register a waiting ### function. The MainThread will run all the registered waiting ### functions at their configured interval. A first application of this ### feature will be to allow periodic reloading of config data from a ### database. Specifically, the skip list rules could be reloaded without ### having to restart the processor. ########################################################################### #-------------------------------------------------------------------------- @staticmethod def get_application_defaults(): """this method allows an app to inject defaults into the configuration that can override defaults not under the direct control of the app. For example, if an app were to use a class that had a config default of X and that was not appropriate as a default for this app, then this method could be used to override that default. This is a technique of getting defaults into an app that replaces an older method of going to the configman option and using the 'set_default' method with 'force=True'""" return { 'source.crashstorage_class': 'socorro.external.fs.crashstorage.FSPermanentStorage', 'destination.crashstorage_class': 'socorro.external.fs.crashstorage.FSPermanentStorage', } #-------------------------------------------------------------------------- def __init__(self, config): super(FetchTransformSaveApp, self).__init__(config) self.waiting_func = None #-------------------------------------------------------------------------- def source_iterator(self): """this iterator yields individual crash_ids from the source crashstorage class's 'new_crashes' method.""" while(True): # loop forever and never raise StopIteration for x in self.source.new_crashes(): if x is None: yield None elif isinstance(x, tuple): yield x # already in (args, kwargs) form else: yield ((x,), {}) # (args, kwargs) else: yield None # if the inner iterator yielded nothing at all, # yield None to give the caller the chance to sleep #-------------------------------------------------------------------------- def transform( self, crash_id, finished_func=(lambda: None), ): try: self._transform(crash_id) finally: # no matter what causes this method to end, we need to make sure # that the finished_func gets called. If the new crash source is # RabbitMQ, this is what removes the job from the queue. try: finished_func() except Exception, x: # when run in a thread, a failure here is not a problem, but if # we're running all in the same thread, a failure here could # derail the the whole processor. Best just log the problem # so that we can continue. self.config.logger.error( 'Error completing job %s: %s', crash_id, x, exc_info=True ) #-------------------------------------------------------------------------- def _transform(self, crash_id): """this default transform function only transfers raw data from the source to the destination without changing the data. While this may be good enough for the raw crashmover, the processor would override this method to create and save processed crashes""" try: raw_crash = self.source.get_raw_crash(crash_id) except Exception as x: self.config.logger.error( "reading raw_crash: %s", str(x), exc_info=True ) raw_crash = {} try: dumps = self.source.get_raw_dumps(crash_id) except Exception as x: self.config.logger.error( "reading dump: %s", str(x), exc_info=True ) dumps = {} try: self.destination.save_raw_crash(raw_crash, dumps, crash_id) self.config.logger.info('saved - %s', crash_id) except Exception as x: self.config.logger.error( "writing raw: %s", str(x), exc_info=True ) else: try: self.source.remove(crash_id) except Exception as x: self.config.logger.error( "removing raw: %s", str(x), exc_info=True ) #-------------------------------------------------------------------------- def quit_check(self): self.task_manager.quit_check() #-------------------------------------------------------------------------- def signal_quit(self): self.task_manager.stop() #-------------------------------------------------------------------------- def _setup_source_and_destination(self): """instantiate the classes that implement the source and destination crash storage systems.""" try: self.source = self.config.source.crashstorage_class( self.config.source, quit_check_callback=self.quit_check ) except Exception: self.config.logger.critical( 'Error in creating crash source', exc_info=True ) raise try: self.destination = self.config.destination.crashstorage_class( self.config.destination, quit_check_callback=self.quit_check ) except Exception: self.config.logger.critical( 'Error in creating crash destination', exc_info=True ) raise #-------------------------------------------------------------------------- def _setup_task_manager(self): """instantiate the threaded task manager to run the producer/consumer queue that is the heart of the processor.""" self.config.logger.info('installing signal handers') respond_to_SIGTERM_with_logging = partial( respond_to_SIGTERM, logger=self.config.logger ) signal.signal(signal.SIGTERM, respond_to_SIGTERM_with_logging) self.task_manager = \ self.config.producer_consumer.producer_consumer_class( self.config.producer_consumer, job_source_iterator=self.source_iterator, task_func=self.transform ) self.config.executor_identity = self.task_manager.executor_identity #-------------------------------------------------------------------------- def _cleanup(self): pass #-------------------------------------------------------------------------- def main(self): """this main routine sets up the signal handlers, the source and destination crashstorage systems at the theaded task manager. That starts a flock of threads that are ready to shepherd crashes from the source to the destination.""" self._setup_task_manager() self._setup_source_and_destination() self.task_manager.blocking_start(waiting_func=self.waiting_func) self._cleanup()
#!/usr/bin/env python """Fortran to Python Interface Generator. """ from __future__ import division, absolute_import, print_function __all__ = ['run_main', 'compile', 'f2py_testing'] import sys from . import f2py2e from . import f2py_testing from . import diagnose run_main = f2py2e.run_main main = f2py2e.main def compile(source, modulename='untitled', extra_args='', verbose=True, source_fn=None, extension='.f' ): """ Build extension module from processing source with f2py. Parameters ---------- source : str Fortran source of module / subroutine to compile modulename : str, optional The name of the compiled python module extra_args : str, optional Additional parameters passed to f2py verbose : bool, optional Print f2py output to screen source_fn : str, optional Name of the file where the fortran source is written. The default is to use a temporary file with the extension provided by the `extension` parameter extension : {'.f', '.f90'}, optional Filename extension if `source_fn` is not provided. The extension tells which fortran standard is used. The default is `.f`, which implies F77 standard. .. versionadded:: 1.11.0 """ from numpy.distutils.exec_command import exec_command import tempfile if source_fn is None: f = tempfile.NamedTemporaryFile(suffix=extension) else: f = open(source_fn, 'w') try: f.write(source) f.flush() args = ' -c -m {} {} {}'.format(modulename, f.name, extra_args) c = '{} -c "import numpy.f2py as f2py2e;f2py2e.main()" {}' c = c.format(sys.executable, args) status, output = exec_command(c) if verbose: print(output) finally: f.close() return status from numpy.testing.nosetester import _numpy_tester test = _numpy_tester().test bench = _numpy_tester().bench
# Copyright (c) Microsoft Corporation. # # This source code is subject to terms and conditions of the Apache License, Version 2.0. A # copy of the license can be found in the LICENSE.txt file at the root of this distribution. If # you cannot locate the Apache License, Version 2.0, please send an email to # vspython@microsoft.com. By using this source code in any fashion, you are agreeing to be bound # by the terms of the Apache License, Version 2.0. # # You must not remove this notice, or any other, from this software. """Pyvot - Pythonic interface for data exploration in Excel The user-level API for the `xl` package follows. For interactive use, consider running the :ref:`interactive shell <interactive>`:: python -m xl.shell **Managing Excel workbooks**: - :class:`xl.Workbook() <xl.sheet.Workbook>` opens a new workbook - xl.Workbook("filename") attaches to an existing workbook, or opens it - :func:`xl.workbooks() <xl.tools.workbooks>` returns a Workbook for each that is currently open **Excel Ranges**: - :class:`xl.Range <xl.range.Range>` is the base type for a contiguous range of Excel cells. - :func:`xl.get() <xl.tools.get>` / :meth:`Workbook.get <xl.sheet.Workbook.get>` / etc. return Ranges; namely, subclasses such as :class:`xl.RowVector <xl.range.RowVector>`, :class:`xl.ColumnVector <xl.range.ColumnVector>`, :class:`xl.Matrix <xl.range.Matrix>`, or :class:`xl.Scalar <xl.range.Scalar>` - :meth:`xl.Range.get` / :meth:`xl.Range.set` allow reading from / writing to Excel **Tools**: - :func:`xl.map <xl.tools.map>` / :func:`xl.apply <xl.tools.apply>` / :func:`xl.filter <xl.tools.filter>` operate like their Python counterparts, but read and write from an Excel workbook ``from xl import *`` imports :func:`xlmap`, etc. instead, to avoid overriding builtins. - :func:`xl.join() <xl.tools.join>` allows joining two Excel tables by a pair of key columns - :func:`xl.get() <xl.tools.get>` fetches a Range for a table column (by column name), named Excel range, or for an Excel address (ex. A1:B1). It attempts to guess the active Workbook, and begins looking in the active sheet. See also :meth:`Workbook.get <xl.sheet.Workbook.get>` - :func:`xl.view() <xl.tools.view>` splats a list of Python values to an empty column in Excel - :func:`xl.selected_range() <xl.tools.selected_range>` / :func:`xl.selected_value() <xl.tools.selected_value>` provide the active sheet's selection""" try: __import__('win32com') except ImportError as e: import ctypes import sys is_64bit = ctypes.sizeof(ctypes.c_voidp) > 4 arch_str = "64-bit" if is_64bit else "32-bit" ver = "%d.%d" % (sys.version_info.major, sys.version_info.minor) raise Exception("pywin32 does not appear to be installed. Visit http://sourceforge.net/projects/pywin32/ and download " "build 216 or above for Python %s (%s)" % (ver, arch_str), e) from .version import __version__ # Conventions: # - prefix excel COM objectss with "xl". Apply to field and method names. # Design conventions: # - Very low activation energy for users. # Layer between "precise (dumb)" operations (which are often not useful) and "guess user intent (smart)" operations # (which can be much more useful). # Users start with "smart" general operations and work towards the precise ones. # - Global functions user "current" workbook, which iterates all sheets. from .range import Range, Vector, Scalar, RowVector, ColumnVector, Matrix, ExcelRangeError from .cache import CacheManager, enable_caching, cache_result from .tools import get, view, join, map, apply, filter, selected_range, selected_value, workbooks from .sheet import Workbook # We want to allow 'from xl import *' without clobbering builtin map / apply / filter. # We define these aliases, and exclude map / apply / filter from __all__. # This way xl.map works, but 'from xl import *' imports xlmap instead xlmap, xlapply, xlfilter = map, apply, filter __all__ = ['Range', 'Vector', 'Scalar', 'RowVector', 'ColumnVector', 'Matrix', 'ExcelRangeError', 'CacheManager', 'enable_caching', 'cache_result', 'get', 'view', 'join', 'selected_range', 'selected_value', 'workbooks', 'xlmap', 'xlapply', 'xlfilter', # We omit map / apply / filter from __all__ but include these. See above 'Workbook']
# -*- coding: utf-8 -*- ############################################################################## # # OpenERP, Open Source Management Solution # Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>). # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## from openerp.osv import fields, osv class account_tax_chart(osv.osv_memory): """ For Chart of taxes """ _name = "account.tax.chart" _description = "Account tax chart" _columns = { 'period_id': fields.many2one('account.period', \ 'Period', \ ), 'target_move': fields.selection([('posted', 'All Posted Entries'), ('all', 'All Entries'), ], 'Target Moves', required=True), } def _get_period(self, cr, uid, context=None): """Return default period value""" period_ids = self.pool.get('account.period').find(cr, uid, context=context) return period_ids and period_ids[0] or False def account_tax_chart_open_window(self, cr, uid, ids, context=None): """ Opens chart of Accounts @param cr: the current row, from the database cursor, @param uid: the current users ID for security checks, @param ids: List of account charts IDs @return: dictionary of Open account chart window on given fiscalyear and all Entries or posted entries """ mod_obj = self.pool.get('ir.model.data') act_obj = self.pool.get('ir.actions.act_window') if context is None: context = {} data = self.browse(cr, uid, ids, context=context)[0] result = mod_obj.get_object_reference(cr, uid, 'account', 'action_tax_code_tree') id = result and result[1] or False result = act_obj.read(cr, uid, [id], context=context)[0] if data.period_id: result['context'] = str({'period_id': data.period_id.id, \ 'fiscalyear_id': data.period_id.fiscalyear_id.id, \ 'state': data.target_move}) period_code = data.period_id.code result['name'] += period_code and (':' + period_code) or '' else: result['context'] = str({'state': data.target_move}) return result _defaults = { 'period_id': _get_period, 'target_move': 'posted' } # vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
# =============================================================================== # Copyright (C) 2010 Diego Duclos # # This file is part of eos. # # eos is free software: you can redistribute it and/or modify # it under the terms of the GNU Lesser General Public License as published by # the Free Software Foundation, either version 2 of the License, or # (at your option) any later version. # # eos is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Lesser General Public License for more details. # # You should have received a copy of the GNU Lesser General Public License # along with eos. If not, see <http://www.gnu.org/licenses/>. # =============================================================================== from logbook import Logger from sqlalchemy.orm import reconstructor from eos.utils.stats import DmgTypes pyfalog = Logger(__name__) class FighterAbility(object): # We aren't able to get data on the charges that can be stored with fighters. So we hardcode that data here, keyed # with the fighter squadron role NUM_SHOTS_MAPPING = { 1: 0, # Superiority fighter / Attack 2: 12, # Light fighter / Attack 4: 6, # Heavy fighter / Heavy attack 5: 3, # Heavy fighter / Long range attack } # Same as above REARM_TIME_MAPPING = { 1: 0, # Superiority fighter / Attack 2: 4000, # Light fighter / Attack 4: 6000, # Heavy fighter / Heavy attack 5: 20000, # Heavy fighter / Long range attack } def __init__(self, effect): """Initialize from the program""" self.__effect = effect self.effectID = effect.ID if effect is not None else None self.active = False self.build() @reconstructor def init(self): """Initialize from the database""" self.__effect = None if self.effectID: self.__effect = next((x for x in self.fighter.item.effects.values() if x.ID == self.effectID), None) if self.__effect is None: pyfalog.error("Effect (id: {0}) does not exist", self.effectID) return self.build() def build(self): pass @property def effect(self): return self.__effect @property def name(self): return self.__effect.getattr('displayName') or self.__effect.name @property def attrPrefix(self): return self.__effect.getattr('prefix') @property def dealsDamage(self): attr = "{}DamageMultiplier".format(self.attrPrefix) return attr in self.fighter.itemModifiedAttributes or self.fighter.charge is not None @property def grouped(self): # is the ability applied per fighter (webs, returns False), or as a group (MWD, returned True) return self.__effect.getattr('grouped') @property def hasCharges(self): return self.__effect.getattr('hasCharges') @property def reloadTime(self): rearm_time = (self.REARM_TIME_MAPPING[self.fighter.getModifiedItemAttr("fighterSquadronRole")] or 0 if self.hasCharges else 0) return self.fighter.getModifiedItemAttr("fighterRefuelingTime") + rearm_time * self.numShots @property def numShots(self): return self.NUM_SHOTS_MAPPING[self.fighter.getModifiedItemAttr("fighterSquadronRole")] or 0 if self.hasCharges else 0 @property def cycleTime(self): speed = self.fighter.getModifiedItemAttr("{}Duration".format(self.attrPrefix)) # Factor in reload ''' reload = self.reloadTime if self.fighter.owner.factorReload: numShots = self.numShots # Speed here already takes into consideration reactivation time speed = (speed * numShots + reload) / numShots if numShots > 0 else speed ''' return speed def getVolley(self, targetResists=None): if not self.dealsDamage or not self.active: return DmgTypes(0, 0, 0, 0) if self.attrPrefix == "fighterAbilityLaunchBomb": em = self.fighter.getModifiedChargeAttr("emDamage", 0) therm = self.fighter.getModifiedChargeAttr("thermalDamage", 0) kin = self.fighter.getModifiedChargeAttr("kineticDamage", 0) exp = self.fighter.getModifiedChargeAttr("explosiveDamage", 0) else: em = self.fighter.getModifiedItemAttr("{}DamageEM".format(self.attrPrefix), 0) therm = self.fighter.getModifiedItemAttr("{}DamageTherm".format(self.attrPrefix), 0) kin = self.fighter.getModifiedItemAttr("{}DamageKin".format(self.attrPrefix), 0) exp = self.fighter.getModifiedItemAttr("{}DamageExp".format(self.attrPrefix), 0) dmgMult = self.fighter.amountActive * self.fighter.getModifiedItemAttr("{}DamageMultiplier".format(self.attrPrefix), 1) volley = DmgTypes( em=em * dmgMult * (1 - getattr(targetResists, "emAmount", 0)), thermal=therm * dmgMult * (1 - getattr(targetResists, "thermalAmount", 0)), kinetic=kin * dmgMult * (1 - getattr(targetResists, "kineticAmount", 0)), explosive=exp * dmgMult * (1 - getattr(targetResists, "explosiveAmount", 0))) return volley def getDps(self, targetResists=None): volley = self.getVolley(targetResists=targetResists) if not volley: return DmgTypes(0, 0, 0, 0) dpsFactor = 1 / (self.cycleTime / 1000) dps = DmgTypes( em=volley.em * dpsFactor, thermal=volley.thermal * dpsFactor, kinetic=volley.kinetic * dpsFactor, explosive=volley.explosive * dpsFactor) return dps def clear(self): self.__dps = None self.__volley = None
# coding: utf-8 # Copyright (c) Pymatgen Development Team. # Distributed under the terms of the MIT License. import os import tempfile import unittest import numpy as np from pymatgen.core.structure import Structure from pymatgen.io.abinit.inputs import ( BasicAbinitInput, BasicMultiDataset, ShiftMode, calc_shiftk, ebands_input, gs_input, ion_ioncell_relax_input, num_valence_electrons, ) from pymatgen.util.testing import PymatgenTest _test_dir = os.path.join(os.path.dirname(__file__), "..", "..", "..", "..", "test_files", "abinit") def abiref_file(filename): """Return absolute path to filename in ~pymatgen/test_files/abinit""" return os.path.join(_test_dir, filename) def abiref_files(*filenames): """Return list of absolute paths to filenames in ~pymatgen/test_files/abinit""" return [os.path.join(_test_dir, f) for f in filenames] class AbinitInputTestCase(PymatgenTest): """Unit tests for BasicAbinitInput.""" def test_api(self): """Testing BasicAbinitInput API.""" # Build simple input with structure and pseudos unit_cell = { "acell": 3 * [10.217], "rprim": [[0.0, 0.5, 0.5], [0.5, 0.0, 0.5], [0.5, 0.5, 0.0]], "ntypat": 1, "znucl": [14], "natom": 2, "typat": [1, 1], "xred": [[0.0, 0.0, 0.0], [0.25, 0.25, 0.25]], } inp = BasicAbinitInput(structure=unit_cell, pseudos=abiref_file("14si.pspnc")) shiftk = [[0.5, 0.5, 0.5], [0.5, 0.0, 0.0], [0.0, 0.5, 0.0], [0.0, 0.0, 0.5]] self.assertArrayEqual(calc_shiftk(inp.structure), shiftk) assert num_valence_electrons(inp.structure, inp.pseudos) == 8 repr(inp), str(inp) assert len(inp) == 0 and not inp assert inp.get("foo", "bar") == "bar" and inp.pop("foo", "bar") == "bar" assert inp.comment is None inp.set_comment("This is a comment") assert inp.comment == "This is a comment" assert inp.isnc and not inp.ispaw inp["ecut"] = 1 assert inp.get("ecut") == 1 and len(inp) == 1 and "ecut" in inp.keys() and "foo" not in inp # Test to_string assert inp.to_string(with_structure=True, with_pseudos=True) assert inp.to_string(with_structure=False, with_pseudos=False) inp.set_vars(ecut=5, toldfe=1e-6) assert inp["ecut"] == 5 inp.set_vars_ifnotin(ecut=-10) assert inp["ecut"] == 5 _, tmpname = tempfile.mkstemp(text=True) inp.write(filepath=tmpname) # Cannot change structure variables directly. with self.assertRaises(inp.Error): inp.set_vars(unit_cell) with self.assertRaises(TypeError): inp.add_abiobjects({}) with self.assertRaises(KeyError): inp.remove_vars("foo", strict=True) assert not inp.remove_vars("foo", strict=False) # Test deepcopy and remove_vars. inp["bdgw"] = [1, 2] inp_copy = inp.deepcopy() inp_copy["bdgw"][1] = 3 assert inp["bdgw"] == [1, 2] assert inp.remove_vars("bdgw") and "bdgw" not in inp removed = inp.pop_tolerances() assert len(removed) == 1 and removed["toldfe"] == 1e-6 # Test set_spin_mode old_vars = inp.set_spin_mode("polarized") assert "nsppol" in inp and inp["nspden"] == 2 and inp["nspinor"] == 1 inp.set_vars(old_vars) # Test set_structure new_structure = inp.structure.copy() new_structure.perturb(distance=0.1) inp.set_structure(new_structure) assert inp.structure == new_structure # Compatible with Pickle and MSONable? self.serialize_with_pickle(inp, test_eq=False) def test_input_errors(self): """Testing typical BasicAbinitInput Error""" si_structure = Structure.from_file(abiref_file("si.cif")) # Ambiguous list of pseudos. with self.assertRaises(BasicAbinitInput.Error): BasicAbinitInput(si_structure, pseudos=abiref_files("14si.pspnc", "14si.4.hgh")) # Pseudos do not match structure. with self.assertRaises(BasicAbinitInput.Error): BasicAbinitInput(si_structure, pseudos=abiref_file("H-wdr.oncvpsp")) si1_negative_volume = dict( ntypat=1, natom=1, typat=[1], znucl=14, acell=3 * [7.60], rprim=[[0.0, 0.5, 0.5], [-0.5, -0.0, -0.5], [0.5, 0.5, 0.0]], xred=[[0.0, 0.0, 0.0]], ) # Negative triple product. with self.assertRaises(BasicAbinitInput.Error): BasicAbinitInput(si1_negative_volume, pseudos=abiref_files("14si.pspnc")) def test_helper_functions(self): """Testing BasicAbinitInput helper functions.""" inp = BasicAbinitInput(structure=abiref_file("si.cif"), pseudos="14si.pspnc", pseudo_dir=_test_dir) inp.set_kmesh(ngkpt=(1, 2, 3), shiftk=(1, 2, 3, 4, 5, 6)) assert inp["kptopt"] == 1 and inp["nshiftk"] == 2 inp.set_gamma_sampling() assert inp["kptopt"] == 1 and inp["nshiftk"] == 1 assert np.all(inp["shiftk"] == 0) inp.set_kpath(ndivsm=3, kptbounds=None) assert inp["ndivsm"] == 3 and inp["iscf"] == -2 and len(inp["kptbounds"]) == 12 class TestMultiDataset(PymatgenTest): """Unit tests for BasicMultiDataset.""" def test_api(self): """Testing BasicMultiDataset API.""" structure = Structure.from_file(abiref_file("si.cif")) pseudo = abiref_file("14si.pspnc") pseudo_dir = os.path.dirname(pseudo) multi = BasicMultiDataset(structure=structure, pseudos=pseudo) with self.assertRaises(ValueError): BasicMultiDataset(structure=structure, pseudos=pseudo, ndtset=-1) multi = BasicMultiDataset(structure=structure, pseudos=pseudo, pseudo_dir=pseudo_dir) assert len(multi) == 1 and multi.ndtset == 1 assert multi.isnc for i, inp in enumerate(multi): assert list(inp.keys()) == list(multi[i].keys()) multi.addnew_from(0) assert multi.ndtset == 2 and multi[0] is not multi[1] assert multi[0].structure == multi[1].structure assert multi[0].structure is not multi[1].structure multi.set_vars(ecut=2) assert all(inp["ecut"] == 2 for inp in multi) self.assertEqual(multi.get("ecut"), [2, 2]) multi[1].set_vars(ecut=1) assert multi[0]["ecut"] == 2 and multi[1]["ecut"] == 1 self.assertEqual(multi.get("ecut"), [2, 1]) self.assertEqual(multi.get("foo", "default"), ["default", "default"]) multi[1].set_vars(paral_kgb=1) assert "paral_kgb" not in multi[0] self.assertEqual(multi.get("paral_kgb"), [None, 1]) pert_structure = structure.copy() pert_structure.perturb(distance=0.1) assert structure != pert_structure assert multi.set_structure(structure) == multi.ndtset * [structure] assert all(s == structure for s in multi.structure) assert multi.has_same_structures multi[1].set_structure(pert_structure) assert multi[0].structure != multi[1].structure and multi[1].structure == pert_structure assert not multi.has_same_structures split = multi.split_datasets() assert len(split) == 2 and all(split[i] == multi[i] for i in range(multi.ndtset)) repr(multi) str(multi) assert multi.to_string(with_pseudos=False) tmpdir = tempfile.mkdtemp() filepath = os.path.join(tmpdir, "run.abi") inp.write(filepath=filepath) multi.write(filepath=filepath) new_multi = BasicMultiDataset.from_inputs([inp for inp in multi]) assert new_multi.ndtset == multi.ndtset assert new_multi.structure == multi.structure for old_inp, new_inp in zip(multi, new_multi): assert old_inp is not new_inp self.assertDictEqual(old_inp.as_dict(), new_inp.as_dict()) ref_input = multi[0] new_multi = BasicMultiDataset.replicate_input(input=ref_input, ndtset=4) assert new_multi.ndtset == 4 for inp in new_multi: assert ref_input is not inp self.assertDictEqual(ref_input.as_dict(), inp.as_dict()) # Compatible with Pickle and MSONable? self.serialize_with_pickle(multi, test_eq=False) class ShiftModeTest(PymatgenTest): def test_shiftmode(self): """Testing shiftmode""" gamma = ShiftMode.GammaCentered assert ShiftMode.from_object("G") == gamma assert ShiftMode.from_object(gamma) == gamma with self.assertRaises(TypeError): ShiftMode.from_object({}) class FactoryTest(PymatgenTest): def setUp(self): # Si ebands self.si_structure = Structure.from_file(abiref_file("si.cif")) self.si_pseudo = abiref_file("14si.pspnc") def test_gs_input(self): """Testing gs_input factory.""" inp = gs_input(self.si_structure, self.si_pseudo, kppa=10, ecut=10, spin_mode="polarized") str(inp) assert inp["nsppol"] == 2 assert inp["nband"] == 14 self.assertArrayEqual(inp["ngkpt"], [2, 2, 2]) def test_ebands_input(self): """Testing ebands_input factory.""" multi = ebands_input(self.si_structure, self.si_pseudo, kppa=10, ecut=2) str(multi) scf_inp, nscf_inp = multi.split_datasets() # Test dos_kppa and other options. multi_dos = ebands_input( self.si_structure, self.si_pseudo, nscf_nband=10, kppa=10, ecut=2, spin_mode="unpolarized", smearing=None, charge=2.0, dos_kppa=50, ) assert len(multi_dos) == 3 assert all(i["charge"] == 2 for i in multi_dos) self.assertEqual(multi_dos.get("nsppol"), [1, 1, 1]) self.assertEqual(multi_dos.get("iscf"), [None, -2, -2]) multi_dos = ebands_input( self.si_structure, self.si_pseudo, nscf_nband=10, kppa=10, ecut=2, spin_mode="unpolarized", smearing=None, charge=2.0, dos_kppa=[50, 100], ) assert len(multi_dos) == 4 self.assertEqual(multi_dos.get("iscf"), [None, -2, -2, -2]) str(multi_dos) def test_ion_ioncell_relax_input(self): """Testing ion_ioncell_relax_input factory.""" multi = ion_ioncell_relax_input(self.si_structure, self.si_pseudo, kppa=10, ecut=2) str(multi) ion_inp, ioncell_inp = multi.split_datasets() assert ion_inp["chksymbreak"] == 0 assert ion_inp["ionmov"] == 3 and ion_inp["optcell"] == 0 assert ioncell_inp["ionmov"] == 3 and ioncell_inp["optcell"] == 2
# coding: utf-8 from __future__ import unicode_literals from .common import InfoExtractor class GlideIE(InfoExtractor): IE_DESC = 'Glide mobile video messages (glide.me)' _VALID_URL = r'https?://share\.glide\.me/(?P<id>[A-Za-z0-9\-=_+]+)' _TEST = { 'url': 'http://share.glide.me/UZF8zlmuQbe4mr+7dCiQ0w==', 'md5': '4466372687352851af2d131cfaa8a4c7', 'info_dict': { 'id': 'UZF8zlmuQbe4mr+7dCiQ0w==', 'ext': 'mp4', 'title': "Damon's Glide message", 'thumbnail': r're:^https?://.*?\.cloudfront\.net/.*\.jpg$', } } def _real_extract(self, url): video_id = self._match_id(url) webpage = self._download_webpage(url, video_id) title = self._html_search_regex( r'<title>(.+?)</title>', webpage, 'title', default=None) or self._og_search_title(webpage) video_url = self._proto_relative_url(self._search_regex( r'<source[^>]+src=(["\'])(?P<url>.+?)\1', webpage, 'video URL', default=None, group='url')) or self._og_search_video_url(webpage) thumbnail = self._proto_relative_url(self._search_regex( r'<img[^>]+id=["\']video-thumbnail["\'][^>]+src=(["\'])(?P<url>.+?)\1', webpage, 'thumbnail url', default=None, group='url')) or self._og_search_thumbnail(webpage) return { 'id': video_id, 'title': title, 'url': video_url, 'thumbnail': thumbnail, }
# -*- coding: utf-8 -*- """Contains helper functions for generating correctly formatted hgrid list/folders. """ import logging import hurry.filesize from django.utils import timezone from framework import sentry from framework.auth.decorators import Auth from django.apps import apps from website import settings from website.util import paths from website.util import sanitize from website.settings import DISK_SAVING_MODE logger = logging.getLogger(__name__) FOLDER = 'folder' FILE = 'file' KIND = 'kind' # TODO: Validate the JSON schema, esp. for addons DEFAULT_PERMISSIONS = { 'view': True, 'edit': False, } def format_filesize(size): return hurry.filesize.size(size, system=hurry.filesize.alternative) def default_urls(node_api, short_name): return { 'fetch': u'{node_api}{addonshort}/hgrid/'.format(node_api=node_api, addonshort=short_name), 'upload': u'{node_api}{addonshort}/'.format(node_api=node_api, addonshort=short_name), } def to_hgrid(node, auth, **data): """Converts a node into a rubeus grid format :param Node node: the node to be parsed :param Auth auth: the user authorization object :returns: rubeus-formatted dict """ return NodeFileCollector(node, auth, **data).to_hgrid() def build_addon_root(node_settings, name, permissions=None, urls=None, extra=None, buttons=None, user=None, private_key=None, **kwargs): """Builds the root or "dummy" folder for an addon. :param addonNodeSettingsBase node_settings: Addon settings :param String name: Additional information for the folder title eg. Repo name for Github or bucket name for S3 :param dict or Auth permissions: Dictionary of permissions for the addon's content or Auth for use in node.can_X methods :param dict urls: Hgrid related urls :param String extra: Html to be appened to the addon folder name eg. Branch switcher for github :param list of dicts buttons: List of buttons to appear in HGrid row. Each dict must have 'text', a string that will appear on the button, and 'action', the name of a function in :param bool private_key: Used to check if information should be stripped from anonymous links :param dict kwargs: Any additional information to add to the root folder :return dict: Hgrid formatted dictionary for the addon root folder """ from website.util import check_private_key_for_anonymized_link permissions = permissions or DEFAULT_PERMISSIONS if name and not check_private_key_for_anonymized_link(private_key): name = u'{0}: {1}'.format(node_settings.config.full_name, name) else: name = node_settings.config.full_name if hasattr(node_settings.config, 'urls') and node_settings.config.urls: urls = node_settings.config.urls if urls is None: urls = default_urls(node_settings.owner.api_url, node_settings.config.short_name) forbid_edit = DISK_SAVING_MODE if node_settings.config.short_name == 'osfstorage' else False if isinstance(permissions, Auth): auth = permissions permissions = { 'view': node_settings.owner.can_view(auth), 'edit': (node_settings.owner.can_edit(auth) and not node_settings.owner.is_registration and not forbid_edit), } max_size = node_settings.config.max_file_size if user and 'high_upload_limit' in user.system_tags: max_size = node_settings.config.high_max_file_size ret = { 'provider': node_settings.config.short_name, 'addonFullname': node_settings.config.full_name, 'name': name, 'iconUrl': node_settings.config.icon_url, KIND: FOLDER, 'extra': extra, 'buttons': buttons, 'isAddonRoot': True, 'permissions': permissions, 'accept': { 'maxSize': max_size, 'acceptedFiles': node_settings.config.accept_extensions, }, 'urls': urls, 'isPointer': False, 'nodeId': node_settings.owner._id, 'nodeUrl': node_settings.owner.url, 'nodeApiUrl': node_settings.owner.api_url, } ret.update(kwargs) return ret def build_addon_button(text, action, title=''): """Builds am action button to be rendered in HGrid :param str text: A string or html to appear on the button itself :param str action: The name of the HGrid action for the button to call. The callback for the HGrid action must be defined as a member of HGrid.Actions :return dict: Hgrid formatted dictionary for custom buttons """ button = { 'text': text, 'action': action, } if title: button['attributes'] = 'title="{title}" data-toggle="tooltip" data-placement="right" '.format(title=title) return button def sort_by_name(hgrid_data): return_value = hgrid_data if hgrid_data is not None: return_value = sorted(hgrid_data, key=lambda item: item['name'].lower()) return return_value class NodeFileCollector(object): """A utility class for creating rubeus formatted node data""" def __init__(self, node, auth, **kwargs): NodeRelation = apps.get_model('osf.NodeRelation') self.node = node.child if isinstance(node, NodeRelation) else node self.auth = auth self.extra = kwargs self.can_view = self.node.can_view(auth) self.can_edit = self.node.can_edit(auth) and not self.node.is_registration def to_hgrid(self): """Return the Rubeus.JS representation of the node's file data, including addons and components """ root = self._serialize_node(self.node) return [root] def _collect_components(self, node, visited): rv = [] if not node.can_view(self.auth): return rv for child in node.get_nodes(is_deleted=False): if not child.can_view(self.auth): if child.primary: for desc in child.find_readable_descendants(self.auth): visited.append(desc.resolve()._id) rv.append(self._serialize_node(desc, visited=visited, parent=node)) elif child.resolve()._id not in visited: visited.append(child.resolve()._id) rv.append(self._serialize_node(child, visited=visited, parent=node)) return rv def _get_node_name(self, node): """Input node object, return the project name to be display. """ NodeRelation = apps.get_model('osf.NodeRelation') is_node_relation = isinstance(node, NodeRelation) node = node.child if is_node_relation else node can_view = node.can_view(auth=self.auth) if can_view: node_name = sanitize.unescape_entities(node.title) elif node.is_registration: node_name = u'Private Registration' elif node.is_fork: node_name = u'Private Fork' elif is_node_relation: node_name = u'Private Link' else: node_name = u'Private Component' return node_name def _serialize_node(self, node, visited=None, parent=None): """Returns the rubeus representation of a node folder. """ visited = visited or [] visited.append(node.resolve()._id) can_view = node.can_view(auth=self.auth) if can_view: children = self._collect_addons(node) + self._collect_components(node, visited) else: children = [] is_pointer = parent and parent.has_node_link_to(node) return { # TODO: Remove safe_unescape_html when mako html safe comes in 'name': self._get_node_name(node), 'category': node.category, 'kind': FOLDER, 'permissions': { 'edit': node.can_edit(self.auth) and not node.is_registration, 'view': can_view, }, 'urls': { 'upload': None, 'fetch': None, }, 'children': children, 'isPointer': is_pointer, 'isSmartFolder': False, 'nodeType': node.project_or_component, 'nodeID': node.resolve()._id, } def _collect_addons(self, node): rv = [] for addon in node.get_addons(): if addon.config.has_hgrid_files: # WARNING: get_hgrid_data can return None if the addon is added but has no credentials. try: temp = addon.config.get_hgrid_data(addon, self.auth, **self.extra) except Exception as e: logger.warn( getattr( e, 'data', 'Unexpected error when fetching file contents for {0}.'.format(addon.config.full_name) ) ) sentry.log_exception() rv.append({ KIND: FOLDER, 'unavailable': True, 'iconUrl': addon.config.icon_url, 'provider': addon.config.short_name, 'addonFullname': addon.config.full_name, 'permissions': {'view': False, 'edit': False}, 'name': '{} is currently unavailable'.format(addon.config.full_name), }) continue rv.extend(sort_by_name(temp) or []) return rv # TODO: these might belong in addons module def collect_addon_assets(node): """Return a dictionary containing lists of JS and CSS assets for a node's addons. :rtype: {'tree_js': <list of JS scripts>, 'tree_css': <list of CSS files>} """ return { 'tree_js': list(collect_addon_js(node)), 'tree_css': list(collect_addon_css(node)), } # TODO: Abstract static collectors def collect_addon_js(node, visited=None, filename='files.js', config_entry='files'): """Collect JavaScript includes for all add-ons implementing HGrid views. :return list: List of JavaScript include paths """ js = [] for addon_config in settings.ADDONS_AVAILABLE_DICT.values(): # JS modules configured in each addon's __init__ file js.extend(addon_config.include_js.get(config_entry, [])) # Webpack bundle js_path = paths.resolve_addon_path(addon_config, filename) if js_path: js.append(js_path) return js def collect_addon_css(node, visited=None): """Collect CSS includes for all addons-ons implementing Hgrid views. :return: List of CSS include paths :rtype: list """ css = [] for addon_config in settings.ADDONS_AVAILABLE_DICT.values(): # CSS modules configured in each addon's __init__ file css.extend(addon_config.include_css.get('files', [])) return css def delta_date(d): diff = d - timezone.now() s = diff.total_seconds() return s
import bencode import hashlib import json from datetime import datetime from itertools import chain from django.core.cache import cache from django.conf import settings from django.core.paginator import Paginator from django.db.models import Q from util import convert_to_utc, convert_to_dict from ..models import Statement, Agent from ..objects.AgentManager import AgentManager from ..exceptions import NotFound, IDNotFoundError MORE_ENDPOINT = '/xapi/statements/more/' def complex_get(param_dict, limit, language, format, attachments): # Tests if value is True or "true" voidQ = Q(voided=False) # keep track if a filter other than time or sequence is used reffilter = False sinceQ = Q() if 'since' in param_dict: sinceQ = Q(stored__gt=convert_to_utc(param_dict['since'])) untilQ = Q() if 'until' in param_dict: untilQ = Q(stored__lte=convert_to_utc(param_dict['until'])) # For statements/read/mine oauth scope authQ = Q() if 'auth' in param_dict and (param_dict['auth'] and 'statements_mine_only' in param_dict['auth']): q_auth = param_dict['auth']['authority'] # If oauth - set authority to look for as the user if q_auth.oauth_identifier: authQ = Q(authority=q_auth) | Q(authority=q_auth.get_user_from_oauth_group()) # Chain all of user's oauth clients as well else: oauth_clients = Agent.objects.filter(member__in=[q_auth]) authQ = Q(authority=q_auth) for client in oauth_clients: authQ = authQ | Q(authority=client.get_user_from_oauth_group()) agentQ = Q() if 'agent' in param_dict: reffilter = True agent = None data = param_dict['agent'] related = 'related_agents' in param_dict and param_dict['related_agents'] if not type(data) is dict: data = convert_to_dict(data) try: agent = AgentManager(data).Agent if agent.objectType == "Group": groups = [] else: groups = agent.member.all() agentQ = Q(actor=agent) for g in groups: agentQ = agentQ | Q(actor=g) if related: me = chain([agent], groups) for a in me: agentQ = agentQ | Q(object_agent=a) | Q(authority=a) \ | Q(context_instructor=a) | Q(context_team=a) \ | Q(object_substatement__actor=a) \ | Q(object_substatement__object_agent=a) \ | Q(object_substatement__context_instructor=a) \ | Q(object_substatement__context_team=a) except IDNotFoundError: return[] verbQ = Q() if 'verb' in param_dict: reffilter = True verbQ = Q(verb__verb_id=param_dict['verb']) # activity activityQ = Q() if 'activity' in param_dict: reffilter = True activityQ = Q(object_activity__activity_id=param_dict['activity']) if 'related_activities' in param_dict and param_dict['related_activities']: activityQ = activityQ | Q(statementcontextactivity__context_activity__activity_id=param_dict['activity']) \ | Q(object_substatement__object_activity__activity_id=param_dict['activity']) \ | Q(object_substatement__substatementcontextactivity__context_activity__activity_id=param_dict['activity']) registrationQ = Q() if 'registration' in param_dict: reffilter = True registrationQ = Q(context_registration=param_dict['registration']) # If want ordered by ascending stored_param = '-stored' if 'ascending' in param_dict and param_dict['ascending']: stored_param = 'stored' stmtset = Statement.objects.filter(voidQ & untilQ & sinceQ & authQ & agentQ & verbQ & activityQ & registrationQ) # only find references when a filter other than # since, until, or limit was used if reffilter: stmtset = findstmtrefs(stmtset.distinct(), sinceQ, untilQ) # Calculate limit of stmts to return return_limit = set_limit(limit) # If there are more stmts than the limit, need to break it up and return more id if stmtset.count() > return_limit: return initial_cache_return(stmtset, stored_param, return_limit, language, format, attachments) else: return create_stmt_result(stmtset, stored_param, language, format) def create_stmt_result(stmt_set, stored, language, format): stmt_result = {} # blows up if the idlist is empty... so i gotta check for that idlist = stmt_set.values_list('id', flat=True) if idlist > 0: if format == 'exact': stmt_result = '{"statements": [%s], "more": ""}' % ",".join([json.dumps(stmt.full_statement) for stmt in \ Statement.objects.filter(id__in=idlist).order_by(stored)]) else: stmt_result['statements'] = [stmt.to_dict(language, format) for stmt in \ Statement.objects.filter(id__in=idlist).order_by(stored)] stmt_result['more'] = "" else: stmt_result['statements'] = [] stmt_result['more'] = "" return stmt_result def findstmtrefs(stmtset, sinceQ, untilQ): if stmtset.count() == 0: return stmtset q = Q() for s in stmtset: q = q | Q(object_statementref__ref_id=s.statement_id) if sinceQ and untilQ: q = q & Q(sinceQ, untilQ) elif sinceQ: q = q & sinceQ elif untilQ: q = q & untilQ # finally weed out voided statements in this lookup q = q & Q(voided=False) return findstmtrefs(Statement.objects.filter(q).distinct(), sinceQ, untilQ) | stmtset def create_cache_key(stmt_list): # Create unique hash data to use for the cache key hash_data = [] hash_data.append(str(datetime.now())) hash_data.append(str(stmt_list)) # Create cache key from hashed data (always 32 digits) key = hashlib.md5(bencode.bencode(hash_data)).hexdigest() return key def initial_cache_return(stmt_list, stored, limit, language, format, attachments): # First time someone queries POST/GET result = {} cache_list = [] cache_list.append([s for s in stmt_list.order_by(stored).values_list('id', flat=True)]) stmt_pager = Paginator(cache_list[0], limit) # Always start on first page current_page = 1 total_pages = stmt_pager.num_pages # Create cache key from hashed data (always 32 digits) cache_key = create_cache_key(cache_list[0]) # Add data to cache cache_list.append(current_page) cache_list.append(total_pages) cache_list.append(limit) cache_list.append(attachments) cache_list.append(language) cache_list.append(format) cache_list.append(stored) # Encode data encoded_info = json.dumps(cache_list) # Save encoded_dict in cache cache.set(cache_key,encoded_info) # Return first page of results if format == 'exact': result = '{"statements": [%s], "more": "%s"}' % (",".join([json.dumps(stmt.full_statement) for stmt in \ Statement.objects.filter(id__in=stmt_pager.page(1).object_list).order_by(stored)]), MORE_ENDPOINT + cache_key) else: result['statements'] = [stmt.to_dict(language, format) for stmt in \ Statement.objects.filter(id__in=stmt_pager.page(1).object_list).order_by(stored)] result['more'] = MORE_ENDPOINT + cache_key return result def set_limit(req_limit): if not req_limit or req_limit > settings.SERVER_STMT_LIMIT: req_limit = settings.SERVER_STMT_LIMIT return req_limit def get_more_statement_request(req_id): # Retrieve encoded info for statements encoded_info = cache.get(req_id) # Could have expired or never existed if not encoded_info: raise NotFound("List does not exist - may have expired after 24 hours") # Decode info decoded_info = json.loads(encoded_info) # Info is always cached as [stmt_list, start_page, total_pages, limit, attachments, language, format] stmt_list = decoded_info[0] start_page = decoded_info[1] total_pages = decoded_info[2] limit = decoded_info[3] attachments = decoded_info[4] language = decoded_info[5] format = decoded_info[6] stored = decoded_info[7] # Build statementResult stmt_result = build_statement_result(stmt_list, start_page, total_pages, limit, attachments, language, format, stored, req_id) return stmt_result, attachments # Gets called from req_process after complex_get with list of django objects and also gets called from get_more_statement_request when # more_id is used so list will be serialized def build_statement_result(stmt_list, start_page, total_pages, limit, attachments, language, format, stored, more_id): result = {} current_page = start_page + 1 # If that was the last page to display then just return the remaining stmts if current_page == total_pages: stmt_pager = Paginator(stmt_list, limit) # Return first page of results if format == 'exact': result = '{"statements": [%s], "more": ""}' % ",".join([json.dumps(stmt.to_dict(language, format)) for stmt in \ Statement.objects.filter(id__in=stmt_pager.page(current_page).object_list).order_by(stored)]) else: result['statements'] = [stmt.to_dict(language, format) for stmt in \ Statement.objects.filter(id__in=stmt_pager.page(current_page).object_list).order_by(stored)] result['more'] = "" # Set current page back for when someone hits the URL again current_page -= 1 # Retrieve list stored in cache encoded_list = cache.get(more_id) # Decode info to set the current page back then encode again decoded_list = json.loads(encoded_list) decoded_list[1] = current_page encoded_list = json.dumps(decoded_list) cache.set(more_id, encoded_list) # There are more pages to display else: stmt_pager = Paginator(stmt_list, limit) # Create cache key from hashed data (always 32 digits) cache_key = create_cache_key(stmt_list) # Return first page of results if format == 'exact': result = '{"statements": [%s], "more": "%s"}' % (",".join([json.dumps(stmt.to_dict(language, format)) for stmt in \ Statement.objects.filter(id__in=stmt_pager.page(current_page).object_list).order_by(stored)]), MORE_ENDPOINT + cache_key) else: # Set result to have selected page of stmts and more endpoint result['statements'] = [stmt.to_dict(language, format) for stmt in \ Statement.objects.filter(id__in=stmt_pager.page(current_page).object_list).order_by(stored)] result['more'] = MORE_ENDPOINT + cache_key more_cache_list = [] # Increment next page start_page = current_page more_cache_list.append(stmt_list) more_cache_list.append(start_page) more_cache_list.append(total_pages) more_cache_list.append(limit) more_cache_list.append(attachments) more_cache_list.append(language) more_cache_list.append(format) more_cache_list.append(stored) # Encode info encoded_list = json.dumps(more_cache_list) cache.set(cache_key, encoded_list) return result
# -*- coding: utf-8 -*- # Copyright 2012 Nicolas Wack <wackou@gmail.com> # # This file is part of subliminal. # # subliminal is free software; you can redistribute it and/or modify it under # the terms of the GNU Lesser General Public License as published by # the Free Software Foundation; either version 3 of the License, or # (at your option) any later version. # # subliminal is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Lesser General Public License for more details. # # You should have received a copy of the GNU Lesser General Public License # along with subliminal. If not, see <http://www.gnu.org/licenses/>. from . import ServiceBase from ..cache import cachedmethod from ..language import language_set, Language from ..subtitles import get_subtitle_path, ResultSubtitle from ..utils import get_keywords from ..videos import Episode from bs4 import BeautifulSoup import logging import re logger = logging.getLogger(__name__) def match(pattern, string): try: return re.search(pattern, string).group(1) except AttributeError: logger.debug(u'Could not match %r on %r' % (pattern, string)) return None class TvSubtitles(ServiceBase): server_url = 'http://www.tvsubtitles.net' api_based = False languages = language_set(['ar', 'bg', 'cs', 'da', 'de', 'el', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja', 'ko', 'nl', 'pl', 'pt', 'ro', 'ru', 'sv', 'tr', 'uk', 'zh', 'pt-br']) #TODO: Find more exceptions language_map = {'gr': Language('gre'), 'cz': Language('cze'), 'ua': Language('ukr'), 'cn': Language('chi')} videos = [Episode] require_video = False required_features = ['permissive'] @cachedmethod def get_likely_series_id(self, name): r = self.session.post('%s/search.php' % self.server_url, data={'q': name}) soup = BeautifulSoup(r.content, self.required_features) maindiv = soup.find('div', 'left') results = [] for elem in maindiv.find_all('li'): sid = int(match('tvshow-([0-9]+)\.html', elem.a['href'])) show_name = match('(.*) \(', elem.a.text) results.append((show_name, sid)) #TODO: pick up the best one in a smart way result = results[0] return result[1] @cachedmethod def get_episode_id(self, series_id, season, number): """Get the TvSubtitles id for the given episode. Raises KeyError if none could be found.""" # download the page of the season, contains ids for all episodes episode_id = None r = self.session.get('%s/tvshow-%d-%d.html' % (self.server_url, series_id, season)) soup = BeautifulSoup(r.content, self.required_features) table = soup.find('table', id='table5') for row in table.find_all('tr'): cells = row.find_all('td') if not cells: continue episode_number = match('x([0-9]+)', cells[0].text) if not episode_number: continue episode_number = int(episode_number) episode_id = int(match('episode-([0-9]+)', cells[1].a['href'])) # we could just return the id of the queried episode, but as we # already downloaded the whole page we might as well fill in the # information for all the episodes of the season self.cache_for(self.get_episode_id, args=(series_id, season, episode_number), result=episode_id) # raises KeyError if not found return self.cached_value(self.get_episode_id, args=(series_id, season, number)) # Do not cache this method in order to always check for the most recent # subtitles def get_sub_ids(self, episode_id): subids = [] r = self.session.get('%s/episode-%d.html' % (self.server_url, episode_id)) epsoup = BeautifulSoup(r.content, self.required_features) for subdiv in epsoup.find_all('a'): if 'href' not in subdiv.attrs or not subdiv['href'].startswith('/subtitle'): continue subid = int(match('([0-9]+)', subdiv['href'])) lang = self.get_language(match('flags/(.*).gif', subdiv.img['src'])) result = {'subid': subid, 'language': lang} for p in subdiv.find_all('p'): if 'alt' in p.attrs and p['alt'] == 'rip': result['rip'] = p.text.strip() if 'alt' in p.attrs and p['alt'] == 'release': result['release'] = p.text.strip() subids.append(result) return subids def list_checked(self, video, languages): return self.query(video.path or video.release, languages, get_keywords(video.guess), video.series, video.season, video.episode) def query(self, filepath, languages, keywords, series, season, episode): logger.debug(u'Getting subtitles for %s season %d episode %d with languages %r' % (series, season, episode, languages)) self.init_cache() sid = self.get_likely_series_id(series.lower()) try: ep_id = self.get_episode_id(sid, season, episode) except KeyError: logger.debug(u'Could not find episode id for %s season %d episode %d' % (series, season, episode)) return [] subids = self.get_sub_ids(ep_id) # filter the subtitles with our queried languages subtitles = [] for subid in subids: language = subid['language'] if language not in languages: continue path = get_subtitle_path(filepath, language, self.config.multi) subtitle = ResultSubtitle(path, language, self.__class__.__name__.lower(), '%s/download-%d.html' % (self.server_url, subid['subid']), keywords=[subid['rip'], subid['release']]) subtitles.append(subtitle) return subtitles def download(self, subtitle): self.download_zip_file(subtitle.link, subtitle.path) return subtitle Service = TvSubtitles
#!/usr/bin/env python # Copyright (c) 2012 The Chromium Authors. All rights reserved. # Use of this source code is governed by a BSD-style license that can be # found in the LICENSE file. """Generates .msi from a .zip archive or an unpacked directory. The structure of the input archive or directory should look like this: +- archive.zip +- archive +- parameters.json The name of the archive and the top level directory in the archive must match. When an unpacked directory is used as the input "archive.zip/archive" should be passed via the command line. 'parameters.json' specifies the parameters to be passed to candle/light and must have the following structure: { "defines": { "name": "value" }, "extensions": [ "WixFirewallExtension.dll" ], "switches": [ '-nologo' ], "source": "chromoting.wxs", "bind_path": "files", "sign": [ ... ], "candle": { ... }, "light": { ... } } "source" specifies the name of the input .wxs relative to "archive.zip/archive". "bind_path" specifies the path where to look for binary files referenced by .wxs relative to "archive.zip/archive". This script is used for both building Chromoting Host installation during Chromuim build and for signing Chromoting Host installation later. There are two copies of this script because of that: - one in Chromium tree at src/remoting/tools/zip2msi.py. - another one next to the signing scripts. The copies of the script can be out of sync so make sure that a newer version is compatible with the older ones when updating the script. """ import copy import json from optparse import OptionParser import os import re import subprocess import sys import zipfile def UnpackZip(target, source): """Unpacks |source| archive to |target| directory.""" target = os.path.normpath(target) archive = zipfile.ZipFile(source, 'r') for f in archive.namelist(): target_file = os.path.normpath(os.path.join(target, f)) # Sanity check to make sure .zip uses relative paths. if os.path.commonprefix([target_file, target]) != target: print "Failed to unpack '%s': '%s' is not under '%s'" % ( source, target_file, target) return 1 # Create intermediate directories. target_dir = os.path.dirname(target_file) if not os.path.exists(target_dir): os.makedirs(target_dir) archive.extract(f, target) return 0 def Merge(left, right): """Merges two values. Raises: TypeError: |left| and |right| cannot be merged. Returns: - if both |left| and |right| are dictionaries, they are merged recursively. - if both |left| and |right| are lists, the result is a list containing elements from both lists. - if both |left| and |right| are simple value, |right| is returned. - |TypeError| exception is raised if a dictionary or a list are merged with a non-dictionary or non-list correspondingly. """ if isinstance(left, dict): if isinstance(right, dict): retval = copy.copy(left) for key, value in right.iteritems(): if key in retval: retval[key] = Merge(retval[key], value) else: retval[key] = value return retval else: raise TypeError('Error: merging a dictionary and non-dictionary value') elif isinstance(left, list): if isinstance(right, list): return left + right else: raise TypeError('Error: merging a list and non-list value') else: if isinstance(right, dict): raise TypeError('Error: merging a dictionary and non-dictionary value') elif isinstance(right, list): raise TypeError('Error: merging a dictionary and non-dictionary value') else: return right quote_matcher_regex = re.compile(r'\s|"') quote_replacer_regex = re.compile(r'(\\*)"') def QuoteArgument(arg): """Escapes a Windows command-line argument. So that the Win32 CommandLineToArgv function will turn the escaped result back into the original string. See http://msdn.microsoft.com/en-us/library/17w5ykft.aspx ("Parsing C++ Command-Line Arguments") to understand why we have to do this. Args: arg: the string to be escaped. Returns: the escaped string. """ def _Replace(match): # For a literal quote, CommandLineToArgv requires an odd number of # backslashes preceding it, and it produces half as many literal backslashes # (rounded down). So we need to produce 2n+1 backslashes. return 2 * match.group(1) + '\\"' if re.search(quote_matcher_regex, arg): # Escape all quotes so that they are interpreted literally. arg = quote_replacer_regex.sub(_Replace, arg) # Now add unescaped quotes so that any whitespace is interpreted literally. return '"' + arg + '"' else: return arg def GenerateCommandLine(tool, source, dest, parameters): """Generates the command line for |tool|.""" # Merge/apply tool-specific parameters params = copy.copy(parameters) if tool in parameters: params = Merge(params, params[tool]) wix_path = os.path.normpath(params.get('wix_path', '')) switches = [os.path.join(wix_path, tool), '-nologo'] # Append the list of defines and extensions to the command line switches. for name, value in params.get('defines', {}).iteritems(): switches.append('-d%s=%s' % (name, value)) for ext in params.get('extensions', []): switches += ('-ext', os.path.join(wix_path, ext)) # Append raw switches switches += params.get('switches', []) # Append the input and output files switches += ('-out', dest, source) # Generate the actual command line #return ' '.join(map(QuoteArgument, switches)) return switches def Run(args): """Runs a command interpreting the passed |args| as a command line.""" command = ' '.join(map(QuoteArgument, args)) popen = subprocess.Popen( command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) out, _ = popen.communicate() if popen.returncode: print command for line in out.splitlines(): print line print '%s returned %d' % (args[0], popen.returncode) return popen.returncode def GenerateMsi(target, source, parameters): """Generates .msi from the installation files prepared by Chromium build.""" parameters['basename'] = os.path.splitext(os.path.basename(source))[0] # The script can handle both forms of input a directory with unpacked files or # a ZIP archive with the same files. In the latter case the archive should be # unpacked to the intermediate directory. source_dir = None if os.path.isdir(source): # Just use unpacked files from the supplied directory. source_dir = source else: # Unpack .zip rc = UnpackZip(parameters['intermediate_dir'], source) if rc != 0: return rc source_dir = '%(intermediate_dir)s\\%(basename)s' % parameters # Read parameters from 'parameters.json'. f = open(os.path.join(source_dir, 'parameters.json')) parameters = Merge(json.load(f), parameters) f.close() if 'source' not in parameters: print 'The source .wxs is not specified' return 1 if 'bind_path' not in parameters: print 'The binding path is not specified' return 1 wxs = os.path.join(source_dir, parameters['source']) # Add the binding path to the light-specific parameters. bind_path = os.path.join(source_dir, parameters['bind_path']) parameters = Merge(parameters, {'light': {'switches': ['-b', bind_path]}}) target_arch = parameters['target_arch'] if target_arch == 'ia32': arch_param = 'x86' elif target_arch == 'x64': arch_param = 'x64' else: print 'Invalid target_arch parameter value' return 1 # Add the architecture to candle-specific parameters. parameters = Merge( parameters, {'candle': {'switches': ['-arch', arch_param]}}) # Run candle and light to generate the installation. wixobj = '%(intermediate_dir)s\\%(basename)s.wixobj' % parameters args = GenerateCommandLine('candle', wxs, wixobj, parameters) rc = Run(args) if rc: return rc args = GenerateCommandLine('light', wixobj, target, parameters) rc = Run(args) if rc: return rc return 0 def main(): usage = 'Usage: zip2msi [options] <input.zip> <output.msi>' parser = OptionParser(usage=usage) parser.add_option('--intermediate_dir', dest='intermediate_dir', default='.') parser.add_option('--wix_path', dest='wix_path', default='.') parser.add_option('--target_arch', dest='target_arch', default='x86') options, args = parser.parse_args() if len(args) != 2: parser.error('two positional arguments expected') return GenerateMsi(args[1], args[0], dict(options.__dict__)) if __name__ == '__main__': sys.exit(main())
""" This module is used to build queries for Postgresql. You shouldn't really need to import anything from this file because they can all be built using dictorm.Table. Sqlite queries are slightly different, but use these methods as their base. """ from copy import copy from typing import Union global sort_keys sort_keys = False __all__ = [ 'And', 'Column', 'Comparison', 'Delete', 'Insert', 'Null', 'Operator', 'Or', 'Select', 'set_sort_keys', 'Update', ] def set_sort_keys(val): """Used only for testing""" global sort_keys sort_keys = val class Select(object): query = 'SELECT * FROM "{table}"' def __init__(self, table, operators_or_comp=None, returning=None): self.table = table self.operators_or_comp = operators_or_comp or [] self.returning = returning self._order_by = None self._limit = None self._offset = None def __repr__(self): # pragma: no cover return 'Select({0}, {1}, ret:{2}, order:{3}, limit:{4}, offset:{5}'.format( self.table, repr(self.operators_or_comp), self.returning, self._order_by, self._limit, self._offset) def _copy(self): try: ooc = self.operators_or_comp[:] except TypeError: ooc = self.operators_or_comp._copy() new = type(self)(self.table, ooc, copy(self.returning)) new._order_by = copy(self._order_by) new._limit = copy(self._limit) new._offset = copy(self._offset) return new def __str__(self): parts = [] formats = {'table': self.table, } ooc = self.operators_or_comp if (isinstance(ooc, Operator) and ooc.operators_or_comp) or ( isinstance(ooc, Comparison) ): parts.append(' WHERE {comp}') formats['comp'] = str(ooc) if self._order_by: parts.append(' ORDER BY {0}'.format(str(self._order_by))) if self.returning == '*': parts.append(' RETURNING *') elif self.returning: parts.append(' RETURNING "{0}"'.format(str(self.returning))) if self._limit: parts.append(' LIMIT {0}'.format(str(self._limit))) if self._offset: parts.append(' OFFSET {0}'.format(str(self._offset))) sql = self.query + ''.join(parts) return sql.format(**formats) def values(self): return list(self.operators_or_comp or []) def build(self): return (str(self), self.values()) def order_by(self, order_by): self._order_by = order_by return self def limit(self, limit): self._limit = limit return self def offset(self, offset): self._offset = offset return self def __add__(self, item): self.operators_or_comp += item return self class Insert(object): query = 'INSERT INTO "{table}" {cvp}' cvp = '({0}) VALUES ({1})' interpolation_str = '%s' append_returning = None last_row = 'SELECT {0} FROM "{1}" WHERE "rowid" = last_insert_rowid()' def __init__(self, table, **values): self.table = table self._values = values self._returning = None self._ordered_keys = values.keys() if sort_keys: self._ordered_keys = sorted(self._ordered_keys) def _build_cvp(self): return (', '.join(['"{}"'.format(i) for i in self._ordered_keys]), ', '.join([self.interpolation_str, ] * len(self._values))) def __str__(self): sql = self.query if self._returning == '*': sql += ' RETURNING *' elif self._returning: sql += ' RETURNING "{0}"'.format(self._returning) if not self._values: return sql.format(table=self.table, cvp='DEFAULT VALUES') return sql.format(table=self.table, cvp=self.cvp.format(*self._build_cvp())) def values(self): return [self._values[k] for k in self._ordered_keys] def build(self): sql, values = str(self), self.values() if self.append_returning: ret = [(sql, values), ] ret.append((self.last_row.format( self.append_returning, self.table), [])) return ret return (sql, values) def returning(self, returning): self._returning = returning return self class Update(Insert): query = 'UPDATE "{table}" SET {cvp}' interpolation_str = '%s' def __init__(self, table, **values): self.operators_or_comp = None super(Update, self).__init__(table, **values) def _build_cvp(self): return ', '.join(('"{0}"={1}'.format(k, self.interpolation_str) \ for k in self._ordered_keys)) def __str__(self): parts = [] formats = {'table': self.table, 'cvp': self._build_cvp()} if self.operators_or_comp: parts.append(' WHERE {comps}') formats['comps'] = str(self.operators_or_comp) if self._returning == '*': parts.append(' RETURNING *') elif self._returning: parts.append(' RETURNING "{0}"'.format(self._returning)) sql = self.query + ''.join(parts) return sql.format(**formats) def values(self): values = super(Update, self).values() if self.operators_or_comp: values.extend(list(self.operators_or_comp)) return values def where(self, operators_or_comp): self.operators_or_comp = operators_or_comp return self class Delete(Update): query = 'DELETE FROM "{table}"' QueryHint = Union[Select, Insert, Update, Delete] class Comparison(object): interpolation_str = '%s' many = False def __init__(self, column1, column2, kind): self.column1 = column1 self.column2 = column2 self.kind = kind self._substratum = None self._aggregate = False self._array_exp = False def __repr__(self): # pragma: no cover if isinstance(self.column2, Null): ret = 'Comparison({0}{1})'.format(self.column1, self.kind) ret = 'Comparison{0}({1}{2}{3})'.format('Many' if self.many else '', self.column1, str(self.kind), self.column2) if self._substratum: ret += '.substratum({0})'.format(self._substratum) return ret def __str__(self): c1 = self.column1.column if self._null_kind(): return '"{0}"{1}'.format(c1, self.kind) # Surround the expression with parentheses if self._array_exp: return '"{0}"{1}({2})'.format(c1, self.kind, self.interpolation_str) return '"{0}"{1}{2}'.format(c1, self.kind, self.interpolation_str) def _copy(self): new = type(self)(self.column1, self.column2, self.kind) new._substratum = self._substratum new._aggregate = self._aggregate return new def value(self): return self.column2 def __iter__(self): if self._null_kind(): return iter([]) return iter([self.column2, ]) def substratum(self, column): comp = Comparison(self.column1, self.column2, self.kind) comp._substratum = column comp.many = self.many return comp def aggregate(self, column): comp = self.substratum(column) comp._aggregate = True return comp def _null_kind(self): return isinstance(self.column2, Null) def Or(self, comp2): return Or(self, comp2) def And(self, comp2): return And(self, comp2) class Null(): pass class Column(object): comparison = Comparison def __init__(self, table, column): self.table = table self.column = column def __repr__(self): # pragma: no cover return '{0}.{1}'.format(self.table, self.column) def many(self, column): c = self.comparison(self, column, '=') c.many = True return c def __eq__(self, column): return self.comparison(self, column, '=') def __gt__(self, column): return self.comparison(self, column, '>') def __ge__(self, column): return self.comparison(self, column, '>=') def __lt__(self, column): return self.comparison(self, column, '<') def __le__(self, column): return self.comparison(self, column, '<=') def __ne__(self, column): return self.comparison(self, column, '!=') def Is(self, column): return self.comparison(self, column, ' IS ') def IsNot(self, column): return self.comparison(self, column, ' IS NOT ') def IsDistinct(self, column): return self.comparison(self, column, ' IS DISTINCT FROM ') def IsNotDistinct(self, column): return self.comparison(self, column, ' IS NOT DISTINCT FROM ') def IsNull(self): return self.comparison(self, Null(), ' IS NULL') def IsNotNull(self): return self.comparison(self, Null(), ' IS NOT NULL') def In(self, tup): if isinstance(tup, list): tup = tuple(tup) return self.comparison(self, tup, ' IN ') def Like(self, column): return self.comparison(self, column, ' LIKE ') def Ilike(self, column): return self.comparison(self, column, ' ILIKE ') def Any(self, column): comp = self.comparison(self, column, ' = ANY ') comp._array_exp = True return comp def wrap_ooc(ooc): if isinstance(ooc, Comparison): return '%s' % str(ooc) return '(%s)' % str(ooc) class Operator(object): def __init__(self, kind, operators_or_comp): self.kind = kind self.operators_or_comp = operators_or_comp def __repr__(self): # pragma: no cover return '{0}{1}'.format(self.kind, repr(self.operators_or_comp)) def __str__(self): kind = ' {0} '.format(self.kind) return kind.join(map(wrap_ooc, self.operators_or_comp)) def __iter__(self): i = [] for comp in self.operators_or_comp: if isinstance(comp, Operator): i.extend(comp) elif isinstance(comp, Comparison) and not comp._null_kind(): i.append(comp.value()) return iter(i) def __add__(self, i): if isinstance(i, tuple): self.operators_or_comp += i else: self.operators_or_comp += (i,) return self def _copy(self): new = type(self)() new.operators_or_comp = tuple(i._copy() for i in self.operators_or_comp) return new class Or(Operator): def __init__(self, *operators_or_comp): super(Or, self).__init__('OR', operators_or_comp) class And(Operator): def __init__(self, *operators_or_comp): super(And, self).__init__('AND', operators_or_comp)
#!/usr/bin/python -u # # this test exercise the XPath basic engine, parser, etc, and # allows to detect memory leaks # import sys import libxml2 # Memory debug specific libxml2.debugMemory(1) doc = libxml2.parseFile("tst.xml") if doc.name != "tst.xml": print "doc.name error" sys.exit(1); ctxt = doc.xpathNewContext() res = ctxt.xpathEval("//*") if len(res) != 2: print "xpath query: wrong node set size" sys.exit(1) if res[0].name != "doc" or res[1].name != "foo": print "xpath query: wrong node set value" sys.exit(1) ctxt.setContextNode(res[0]) res = ctxt.xpathEval("foo") if len(res) != 1: print "xpath query: wrong node set size" sys.exit(1) if res[0].name != "foo": print "xpath query: wrong node set value" sys.exit(1) doc.freeDoc() ctxt.xpathFreeContext() i = 1000 while i > 0: doc = libxml2.parseFile("tst.xml") ctxt = doc.xpathNewContext() res = ctxt.xpathEval("//*") doc.freeDoc() ctxt.xpathFreeContext() i = i -1 del ctxt # Memory debug specific libxml2.cleanupParser() if libxml2.debugMemory(1) == 0: print "OK" else: print "Memory leak %d bytes" % (libxml2.debugMemory(1)) libxml2.dumpMemory()
# Copyright 2014 Google Inc. # # Use of this source code is governed by a BSD-style license that can be # found in the LICENSE file. """Module to host the VerboseSubprocess, ChangeDir, and ReSearch classes. """ import os import re import subprocess def print_subprocess_args(prefix, *args, **kwargs): """Print out args in a human-readable manner.""" def quote_and_escape(string): """Quote and escape a string if necessary.""" if ' ' in string or '\n' in string: string = '"%s"' % string.replace('"', '\\"') return string if 'cwd' in kwargs: print '%scd %s' % (prefix, kwargs['cwd']) print prefix + ' '.join(quote_and_escape(arg) for arg in args[0]) if 'cwd' in kwargs: print '%scd -' % prefix class VerboseSubprocess(object): """Call subprocess methods, but print out command before executing. Attributes: verbose: (boolean) should we print out the command or not. If not, this is the same as calling the subprocess method quiet: (boolean) suppress stdout on check_call and call. prefix: (string) When verbose, what to print before each command. """ def __init__(self, verbose): self.verbose = verbose self.quiet = not verbose self.prefix = '~~$ ' def check_call(self, *args, **kwargs): """Wrapper for subprocess.check_call(). Args: *args: to be passed to subprocess.check_call() **kwargs: to be passed to subprocess.check_call() Returns: Whatever subprocess.check_call() returns. Raises: OSError or subprocess.CalledProcessError: raised by check_call. """ if self.verbose: print_subprocess_args(self.prefix, *args, **kwargs) if self.quiet: with open(os.devnull, 'w') as devnull: return subprocess.check_call(*args, stdout=devnull, **kwargs) else: return subprocess.check_call(*args, **kwargs) def call(self, *args, **kwargs): """Wrapper for subprocess.check(). Args: *args: to be passed to subprocess.check_call() **kwargs: to be passed to subprocess.check_call() Returns: Whatever subprocess.call() returns. Raises: OSError or subprocess.CalledProcessError: raised by call. """ if self.verbose: print_subprocess_args(self.prefix, *args, **kwargs) if self.quiet: with open(os.devnull, 'w') as devnull: return subprocess.call(*args, stdout=devnull, **kwargs) else: return subprocess.call(*args, **kwargs) def check_output(self, *args, **kwargs): """Wrapper for subprocess.check_output(). Args: *args: to be passed to subprocess.check_output() **kwargs: to be passed to subprocess.check_output() Returns: Whatever subprocess.check_output() returns. Raises: OSError or subprocess.CalledProcessError: raised by check_output. """ if self.verbose: print_subprocess_args(self.prefix, *args, **kwargs) return subprocess.check_output(*args, **kwargs) def strip_output(self, *args, **kwargs): """Wrap subprocess.check_output and str.strip(). Pass the given arguments into subprocess.check_output() and return the results, after stripping any excess whitespace. Args: *args: to be passed to subprocess.check_output() **kwargs: to be passed to subprocess.check_output() Returns: The output of the process as a string without leading or trailing whitespace. Raises: OSError or subprocess.CalledProcessError: raised by check_output. """ if self.verbose: print_subprocess_args(self.prefix, *args, **kwargs) return str(subprocess.check_output(*args, **kwargs)).strip() def popen(self, *args, **kwargs): """Wrapper for subprocess.Popen(). Args: *args: to be passed to subprocess.Popen() **kwargs: to be passed to subprocess.Popen() Returns: The output of subprocess.Popen() Raises: OSError or subprocess.CalledProcessError: raised by Popen. """ if self.verbose: print_subprocess_args(self.prefix, *args, **kwargs) return subprocess.Popen(*args, **kwargs) class ChangeDir(object): """Use with a with-statement to temporarily change directories.""" # pylint: disable=I0011,R0903 def __init__(self, directory, verbose=False): self._directory = directory self._verbose = verbose def __enter__(self): if self._directory != os.curdir: if self._verbose: print '~~$ cd %s' % self._directory cwd = os.getcwd() os.chdir(self._directory) self._directory = cwd def __exit__(self, etype, value, traceback): if self._directory != os.curdir: if self._verbose: print '~~$ cd %s' % self._directory os.chdir(self._directory) class ReSearch(object): """A collection of static methods for regexing things.""" @staticmethod def search_within_stream(input_stream, pattern, default=None): """Search for regular expression in a file-like object. Opens a file for reading and searches line by line for a match to the regex and returns the parenthesized group named return for the first match. Does not search across newlines. For example: pattern = '^root(:[^:]*){4}:(?P<return>[^:]*)' with open('/etc/passwd', 'r') as stream: return search_within_file(stream, pattern) should return root's home directory (/root on my system). Args: input_stream: file-like object to be read pattern: (string) to be passed to re.compile default: what to return if no match Returns: A string or whatever default is """ pattern_object = re.compile(pattern) for line in input_stream: match = pattern_object.search(line) if match: return match.group('return') return default @staticmethod def search_within_string(input_string, pattern, default=None): """Search for regular expression in a string. Args: input_string: (string) to be searched pattern: (string) to be passed to re.compile default: what to return if no match Returns: A string or whatever default is """ match = re.search(pattern, input_string) return match.group('return') if match else default @staticmethod def search_within_output(verbose, pattern, default, *args, **kwargs): """Search for regular expression in a process output. Does not search across newlines. Args: verbose: (boolean) shoule we call print_subprocess_args? pattern: (string) to be passed to re.compile default: what to return if no match *args: to be passed to subprocess.Popen() **kwargs: to be passed to subprocess.Popen() Returns: A string or whatever default is """ if verbose: print_subprocess_args('~~$ ', *args, **kwargs) proc = subprocess.Popen(*args, stdout=subprocess.PIPE, **kwargs) return ReSearch.search_within_stream(proc.stdout, pattern, default)
from __future__ import unicode_literals import frappe from frappe.utils import cint from frappe.model import default_fields def execute(): for table in frappe.db.get_tables(): doctype = table[3:] if frappe.db.exists("DocType", doctype): fieldnames = [df["fieldname"] for df in frappe.get_all("DocField", fields=["fieldname"], filters={"parent": doctype})] custom_fieldnames = [df["fieldname"] for df in frappe.get_all("Custom Field", fields=["fieldname"], filters={"dt": doctype})] else: fieldnames = custom_fieldnames = [] for column in frappe.db.sql("""desc `{0}`""".format(table), as_dict=True): if column["Type"]=="int(1)": fieldname = column["Field"] # only change for defined fields, ignore old fields that don't exist in meta if not (fieldname in default_fields or fieldname in fieldnames or fieldname in custom_fieldnames): continue # set 0 frappe.db.sql("""update `{table}` set `{column}`=0 where `{column}` is null"""\ .format(table=table, column=fieldname)) frappe.db.commit() # change definition frappe.db.sql_ddl("""alter table `{table}` modify `{column}` int(1) not null default {default}"""\ .format(table=table, column=fieldname, default=cint(column["Default"])))
#!/usr/bin/env python # Copyright 2013 The Swarming Authors. All rights reserved. # Use of this source code is governed under the Apache License, Version 2.0 that # can be found in the LICENSE file. import json import logging import os import sys import tempfile import unittest ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) sys.path.insert(0, ROOT_DIR) from utils import lru class LRUDictTest(unittest.TestCase): @staticmethod def prepare_lru_dict(keys): """Returns new LRUDict with given |keys| added one by one.""" lru_dict = lru.LRUDict() for key in keys: lru_dict.add(key, None) return lru_dict def assert_order(self, lru_dict, expected_keys): """Asserts order of keys in |lru_dict| is |expected_keys|. expected_keys[0] is supposedly oldest key, expected_keys[-1] is newest. Destroys |lru_dict| state in the process. """ # Check keys iteration works. self.assertEqual(lru_dict.keys_set(), set(expected_keys)) # Check pop_oldest returns keys in expected order. actual_keys = [] while lru_dict: oldest_key, _ = lru_dict.pop_oldest() actual_keys.append(oldest_key) self.assertEqual(actual_keys, expected_keys) def assert_same_data(self, lru_dict, regular_dict): """Asserts that given |lru_dict| contains same data as |regular_dict|.""" self.assertEqual(lru_dict.keys_set(), set(regular_dict.keys())) self.assertEqual(set(lru_dict.itervalues()), set(regular_dict.values())) for k, v in regular_dict.items(): self.assertEqual(lru_dict.get(k), v) def test_basic_dict_funcs(self): lru_dict = lru.LRUDict() # Add a bunch. data = {1: 'one', 2: 'two', 3: 'three'} for k, v in data.items(): lru_dict.add(k, v) # Check its there. self.assert_same_data(lru_dict, data) # Replace value. lru_dict.add(1, 'one!!!') data[1] = 'one!!!' self.assert_same_data(lru_dict, data) # Check pop works. self.assertEqual(lru_dict.pop(2), 'two') data.pop(2) self.assert_same_data(lru_dict, data) # Pop missing key. with self.assertRaises(KeyError): lru_dict.pop(2) # Touch has no effect on set of keys and values. lru_dict.touch(1) self.assert_same_data(lru_dict, data) # Touch fails on missing key. with self.assertRaises(KeyError): lru_dict.touch(22) def test_magic_methods(self): # Check __nonzero__, __len__ and __contains__ for empty dict. lru_dict = lru.LRUDict() self.assertFalse(lru_dict) self.assertEqual(len(lru_dict), 0) self.assertFalse(1 in lru_dict) # Dict with one item. lru_dict.add(1, 'one') self.assertTrue(lru_dict) self.assertEqual(len(lru_dict), 1) self.assertTrue(1 in lru_dict) self.assertFalse(2 in lru_dict) def test_order(self): data = [1, 2, 3] # Edge cases. self.assert_order(self.prepare_lru_dict([]), []) self.assert_order(self.prepare_lru_dict([1]), [1]) # No touches. self.assert_order(self.prepare_lru_dict(data), data) # Touching newest item is noop. lru_dict = self.prepare_lru_dict(data) lru_dict.touch(3) self.assert_order(lru_dict, data) # Touch to move to newest. lru_dict = self.prepare_lru_dict(data) lru_dict.touch(2) self.assert_order(lru_dict, [1, 3, 2]) # Pop newest. lru_dict = self.prepare_lru_dict(data) lru_dict.pop(1) self.assert_order(lru_dict, [2, 3]) # Pop in the middle. lru_dict = self.prepare_lru_dict(data) lru_dict.pop(2) self.assert_order(lru_dict, [1, 3]) # Pop oldest. lru_dict = self.prepare_lru_dict(data) lru_dict.pop(3) self.assert_order(lru_dict, [1, 2]) # Add oldest. lru_dict = self.prepare_lru_dict(data) lru_dict.batch_insert_oldest([(4, 4), (5, 5)]) self.assert_order(lru_dict, [4, 5] + data) # Add newest. lru_dict = self.prepare_lru_dict(data) lru_dict.add(4, 4) self.assert_order(lru_dict, data + [4]) def test_load_save(self): def save_and_load(lru_dict): handle, tmp_name = tempfile.mkstemp(prefix=u'lru_test') os.close(handle) try: lru_dict.save(tmp_name) return lru.LRUDict.load(tmp_name) finally: try: os.unlink(tmp_name) except OSError: pass data = [1, 2, 3] # Edge case. empty = save_and_load(lru.LRUDict()) self.assertFalse(empty) # Normal flow. lru_dict = save_and_load(self.prepare_lru_dict(data)) self.assert_order(lru_dict, data) # After touches. lru_dict = self.prepare_lru_dict(data) lru_dict.touch(2) lru_dict = save_and_load(lru_dict) self.assert_order(lru_dict, [1, 3, 2]) # After pop. lru_dict = self.prepare_lru_dict(data) lru_dict.pop(2) lru_dict = save_and_load(lru_dict) self.assert_order(lru_dict, [1, 3]) # After add. lru_dict = self.prepare_lru_dict(data) lru_dict.add(4, 4) lru_dict.batch_insert_oldest([(5, 5), (6, 6)]) lru_dict = save_and_load(lru_dict) self.assert_order(lru_dict, [5, 6] + data + [4]) def test_corrupted_state_file(self): def load_from_state(state_text): handle, tmp_name = tempfile.mkstemp(prefix=u'lru_test') os.close(handle) try: with open(tmp_name, 'w') as f: f.write(state_text) return lru.LRUDict.load(tmp_name) finally: os.unlink(tmp_name) # Loads correct state just fine. self.assertIsNotNone(load_from_state(json.dumps([ ['key1', 'value1'], ['key2', 'value2'], ]))) # Not a json. with self.assertRaises(ValueError): load_from_state('garbage, not a state') # Not a list. with self.assertRaises(ValueError): load_from_state('{}') # Not a list of pairs. with self.assertRaises(ValueError): load_from_state(json.dumps([ ['key', 'value', 'and whats this?'], ])) # Duplicate keys. with self.assertRaises(ValueError): load_from_state(json.dumps([ ['key', 'value'], ['key', 'another_value'], ])) if __name__ == '__main__': VERBOSE = '-v' in sys.argv logging.basicConfig(level=logging.DEBUG if VERBOSE else logging.ERROR) unittest.main()
# # This file is part of Plinth. # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # """ Views for the Matrix Synapse module. """ from django.shortcuts import redirect from django.urls import reverse_lazy from django.views.generic import FormView from plinth import actions from plinth import views from plinth.modules import matrixsynapse from plinth.forms import DomainSelectionForm from plinth.utils import get_domain_names class SetupView(FormView): """Show matrix-synapse setup page.""" template_name = 'matrix-synapse-pre-setup.html' form_class = DomainSelectionForm success_url = reverse_lazy('matrixsynapse:index') def form_valid(self, form): """Handle valid form submission.""" domain_name = form.cleaned_data['domain_name'] actions.superuser_run('matrixsynapse', ['setup', '--domain-name', domain_name]) return super().form_valid(form) def get_context_data(self, *args, **kwargs): """Provide context data to the template.""" context = super().get_context_data(**kwargs) context['title'] = matrixsynapse.name context['description'] = matrixsynapse.description context['domain_names'] = get_domain_names() return context class ServiceView(views.ServiceView): """Show matrix-synapse service page.""" service_id = matrixsynapse.managed_services[0] template_name = 'matrix-synapse.html' description = matrixsynapse.description diagnostics_module_name = 'matrixsynapse' def dispatch(self, request, *args, **kwargs): """Redirect to setup page if setup is not done yet.""" if not matrixsynapse.is_setup(): return redirect('matrixsynapse:setup') return super().dispatch(request, *args, **kwargs) def get_context_data(self, *args, **kwargs): """Add additional context data for template.""" context = super().get_context_data(**kwargs) context['domain_name'] = matrixsynapse.get_configured_domain_name() return context
# Licensed to the Apache Software Foundation (ASF) under one # or more contributor license agreements. See the NOTICE file # distributed with this work for additional information # regarding copyright ownership. The ASF licenses this file # to you under the Apache License, Version 2.0 (the # "License"); you may not use this file except in compliance # with the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, # software distributed under the License is distributed on an # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY # KIND, either express or implied. See the License for the # specific language governing permissions and limitations # under the License. # coding: utf-8 # pylint: disable=fixme, invalid-name, too-many-arguments, too-many-locals, too-many-lines # pylint: disable=too-many-branches, too-many-statements """Profiler setting methods.""" from __future__ import absolute_import import ctypes from .base import _LIB, check_call, c_str def profiler_set_config(mode='symbolic', filename='profile.json'): """Set up the configure of profiler. Parameters ---------- mode : string, optional Indicates whether to enable the profiler, can be 'symbolic', or 'all'. Defaults to `symbolic`. filename : string, optional The name of output trace file. Defaults to 'profile.json'. """ mode2int = {'symbolic': 0, 'all': 1} check_call(_LIB.MXSetProfilerConfig( ctypes.c_int(mode2int[mode]), c_str(filename))) def profiler_set_state(state='stop'): """Set up the profiler state to record operator. Parameters ---------- state : string, optional Indicates whether to run the profiler, can be 'stop' or 'run'. Default is `stop`. """ state2int = {'stop': 0, 'run': 1} check_call(_LIB.MXSetProfilerState(ctypes.c_int(state2int[state]))) def dump_profile(): """Dump profile and stop profiler. Use this to save profile in advance in case your program cannot exit normally.""" check_call(_LIB.MXDumpProfile())
# Copyright 2015 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== """Gradients for operators defined in linalg_ops.py. Useful reference for derivative formulas is An extended collection of matrix derivative results for forward and reverse mode algorithmic differentiation by Mike Giles: http://eprints.maths.ox.ac.uk/1079/1/NA-08-01.pdf A detailed derivation of formulas for backpropagating through spectral layers (SVD and Eig) by Ionescu, Vantzos & Sminchisescu: https://arxiv.org/pdf/1509.07838v4.pdf """ from __future__ import absolute_import from __future__ import division from __future__ import print_function from tensorflow.python.framework import ops from tensorflow.python.ops import array_ops from tensorflow.python.ops import control_flow_ops from tensorflow.python.ops import linalg_ops from tensorflow.python.ops import math_ops @ops.RegisterGradient("MatrixInverse") def _MatrixInverseGrad(op, grad): """Gradient for MatrixInverse.""" ainv = op.outputs[0] return -math_ops.matmul( ainv, math_ops.matmul( grad, ainv, adjoint_b=True), adjoint_a=True) @ops.RegisterGradient("MatrixDeterminant") def _MatrixDeterminantGrad(op, grad): """Gradient for MatrixDeterminant.""" a = op.inputs[0] c = op.outputs[0] a_adj_inv = linalg_ops.matrix_inverse(a, adjoint=True) multipliers = array_ops.reshape( grad * c, array_ops.concat([array_ops.shape(c), [1, 1]], 0)) return multipliers * a_adj_inv @ops.RegisterGradient("Cholesky") def _CholeskyGrad(op, grad): """Gradient for Cholesky.""" # Gradient is l^{-H} @ ((l^{H} @ grad) * (tril(ones)-1/2*eye)) @ l^{-1} l = op.outputs[0] num_rows = array_ops.shape(l)[-1] batch_shape = array_ops.shape(l)[:-2] l_inverse = linalg_ops.matrix_triangular_solve( l, linalg_ops.eye(num_rows, batch_shape=batch_shape, dtype=l.dtype)) middle = math_ops.matmul(l, grad, adjoint_a=True) middle = array_ops.matrix_set_diag(middle, 0.5 * array_ops.matrix_diag_part(middle)) middle = array_ops.matrix_band_part(middle, -1, 0) grad_a = math_ops.matmul( math_ops.matmul(l_inverse, middle, adjoint_a=True), l_inverse) grad_a += math_ops.conj(array_ops.matrix_transpose(grad_a)) return grad_a * 0.5 @ops.RegisterGradient("MatrixSolve") def _MatrixSolveGrad(op, grad): """Gradient for MatrixSolve.""" a = op.inputs[0] adjoint_a = op.get_attr("adjoint") c = op.outputs[0] grad_b = linalg_ops.matrix_solve(a, grad, adjoint=not adjoint_a) if adjoint_a: grad_a = -math_ops.matmul(c, grad_b, adjoint_b=True) else: grad_a = -math_ops.matmul(grad_b, c, adjoint_b=True) return (grad_a, grad_b) @ops.RegisterGradient("MatrixSolveLs") def _MatrixSolveLsGrad(op, grad): """Gradients for MatrixSolveLs.""" # TODO(rmlarsen): The implementation could be more efficient: # a) Output the Cholesky factorization from forward op instead of # recomputing it here. # b) Implement a symmetric rank-k update op instead of computing # x*z + transpose(x*z). This pattern occurs other places in TensorFlow. def _overdetermined(op, grad): """Gradients for the overdetermined case of MatrixSolveLs. This is the backprop for the solution to the normal equations of the first kind: X = F(A, B) = (A^T * A + lambda * I)^{-1} * A^T * B which solve the least squares problem min ||A * X - B||_F^2 + lambda ||X||_F^2. """ a = op.inputs[0] b = op.inputs[1] l2_regularizer = math_ops.cast(op.inputs[2], a.dtype.base_dtype) x = op.outputs[0] a_shape = array_ops.shape(a) batch_shape = a_shape[:-2] n = a_shape[-1] identity = linalg_ops.eye(n, batch_shape=batch_shape, dtype=a.dtype) gramian = math_ops.matmul(a, a, adjoint_a=True) + l2_regularizer * identity chol = linalg_ops.cholesky(gramian) # Temporary z = (A^T * A + lambda * I)^{-1} * grad. z = linalg_ops.cholesky_solve(chol, grad) xzt = math_ops.matmul(x, z, adjoint_b=True) zx_sym = xzt + array_ops.matrix_transpose(xzt) grad_a = -math_ops.matmul(a, zx_sym) + math_ops.matmul(b, z, adjoint_b=True) grad_b = math_ops.matmul(a, z) return (grad_a, grad_b, None) def _underdetermined(op, grad): """Gradients for the underdetermined case of MatrixSolveLs. This is the backprop for the solution to the normal equations of the second kind: X = F(A, B) = A * (A*A^T + lambda*I)^{-1} * B that (for lambda=0) solve the least squares problem min ||X||_F subject to A*X = B. """ a = op.inputs[0] b = op.inputs[1] l2_regularizer = math_ops.cast(op.inputs[2], a.dtype.base_dtype) a_shape = array_ops.shape(a) batch_shape = a_shape[:-2] m = a_shape[-2] identity = linalg_ops.eye(m, batch_shape=batch_shape, dtype=a.dtype) gramian = math_ops.matmul(a, a, adjoint_b=True) + l2_regularizer * identity chol = linalg_ops.cholesky(gramian) grad_b = linalg_ops.cholesky_solve(chol, math_ops.matmul(a, grad)) # Temporary tmp = (A * A^T + lambda * I)^{-1} * B. tmp = linalg_ops.cholesky_solve(chol, b) a1 = math_ops.matmul(tmp, a, adjoint_a=True) a1 = -math_ops.matmul(grad_b, a1) a2 = grad - math_ops.matmul(a, grad_b, adjoint_a=True) a2 = math_ops.matmul(tmp, a2, adjoint_b=True) grad_a = a1 + a2 return (grad_a, grad_b, None) fast = op.get_attr("fast") if fast is False: raise ValueError("Gradient not defined for fast=False") matrix_shape = op.inputs[0].get_shape()[-2:] if matrix_shape.is_fully_defined(): if matrix_shape[-2] >= matrix_shape[-1]: return _overdetermined(op, grad) else: return _underdetermined(op, grad) else: # We have to defer determining the shape to runtime and use # conditional execution of the appropriate graph. matrix_shape = array_ops.shape(op.inputs[0])[-2:] return control_flow_ops.cond(matrix_shape[-2] >= matrix_shape[-1], lambda: _overdetermined(op, grad), lambda: _underdetermined(op, grad)) @ops.RegisterGradient("MatrixTriangularSolve") def _MatrixTriangularSolveGrad(op, grad): """Gradient for MatrixTriangularSolve.""" a = op.inputs[0] adjoint_a = op.get_attr("adjoint") lower_a = op.get_attr("lower") c = op.outputs[0] grad_b = linalg_ops.matrix_triangular_solve( a, grad, lower=lower_a, adjoint=not adjoint_a) if adjoint_a: grad_a = -math_ops.matmul(c, grad_b, adjoint_b=True) else: grad_a = -math_ops.matmul(grad_b, c, adjoint_b=True) if lower_a: grad_a = array_ops.matrix_band_part(grad_a, -1, 0) else: grad_a = array_ops.matrix_band_part(grad_a, 0, -1) return (grad_a, grad_b) @ops.RegisterGradient("SelfAdjointEigV2") def _SelfAdjointEigV2Grad(op, grad_e, grad_v): """Gradient for SelfAdjointEigV2.""" e = op.outputs[0] v = op.outputs[1] # a = op.inputs[0], which satisfies # a[...,:,:] * v[...,:,i] = e[...,i] * v[...,i] with ops.control_dependencies([grad_e.op, grad_v.op]): if grad_v is not None: # Construct the matrix f(i,j) = (i != j ? 1 / (e_i - e_j) : 0). # Notice that because of the term involving f, the gradient becomes # infinite (or NaN in practice) when eigenvalues are not unique. # Mathematically this should not be surprising, since for (k-fold) # degenerate eigenvalues, the corresponding eigenvectors are only defined # up to arbitrary rotation in a (k-dimensional) subspace. f = array_ops.matrix_set_diag( math_ops.reciprocal( array_ops.expand_dims(e, -2) - array_ops.expand_dims(e, -1)), array_ops.zeros_like(e)) grad_a = math_ops.matmul( v, math_ops.matmul( array_ops.matrix_diag(grad_e) + f * math_ops.matmul( v, grad_v, adjoint_a=True), v, adjoint_b=True)) else: grad_a = math_ops.matmul( v, math_ops.matmul( array_ops.matrix_diag(grad_e), v, adjoint_b=True)) # The forward op only depends on the lower triangular part of a, so here we # symmetrize and take the lower triangle grad_a = array_ops.matrix_band_part( grad_a + math_ops.conj(array_ops.matrix_transpose(grad_a)), -1, 0) grad_a = array_ops.matrix_set_diag(grad_a, 0.5 * array_ops.matrix_diag_part(grad_a)) return grad_a
from moves import find_possible_moves, find_impossible_moves, resolve_moves def test_find_possible_moves_1(): # the first item, with value 0, is 'missing'. find_possible_moves will tell us it should be a 9. state = [((i,i), i) for i in range(9)] moves = find_possible_moves(state) assert len(moves) == 1 assert moves[0][0] == (0,0) assert moves[0][1] == 9 def test_find_possible_moves_2(): # the first and last items have value 0 state = [((i,i), i if i < 8 else 0) for i in range(9)] moves = find_possible_moves(state) moves = sorted(moves) assert len(moves) == 4 assert moves[0] == ((0,0), 8) assert moves[1] == ((0,0), 9) assert moves[2] == ((8,8), 8) assert moves[3] == ((8,8), 9) def test_resolve_moves_0(): possible_moves = [[((0,0), 1)], [((0,0), 1)]] moves = resolve_moves(possible_moves) assert len(moves) == 1 assert moves[0] == ((0,0), 1) def test_resolve_moves_1(): possible_moves = [[((0,0), 1), ((0,0), 2)], [((0,0), 1), ((0,0), 3)]] moves = resolve_moves(possible_moves) assert len(moves) == 1 assert moves[0] == ((0,0), 1) def test_resolve_moves_2(): possible_moves = [[((0,0), 1), ((0,0), 2)], [((0,0), 1), ((0,0), 2), ((0,0), 3)]] moves = resolve_moves(possible_moves) assert len(moves) == 0 def test_resolve_moves_confounding_0(): possible_moves = [[((0,0), 1), ((0,0), 2), ((1,1), 1), ((1,1), 2)], [((0,0), 1), ((0,0), 2), ((0,0), 3), ((1,1), 1), ((1,1), 2)]] moves = resolve_moves(possible_moves) assert len(moves) == 0 def test_find_impossible_moves_1(): # the first item, with value 0, is 'missing'. find_impossible_moves will tell us it cannot be [1,2,3,4,5,6,7,8]. state = [((i,i), i) for i in range(9)] moves = find_impossible_moves(state) assert len(moves) == 1 assert moves[0][0] == (0,0) assert moves[0][1] == [1,2,3,4,5,6,7,8] def test_find_impossible_moves_2(): state = [((i,i), 0 if i%2==0 else i) for i in range(9)] moves = find_impossible_moves(state) assert len(moves) == 5 for i, m in enumerate(moves): assert m[0] == (2*i,2*i) assert m[1] == [1,3, 5, 7] def test_find_impossible_moves_3(): state = [((i,i), 0) for i in range(9)] moves = find_impossible_moves(state) assert len(moves) == 9 for i, m in enumerate(moves): assert m[0] == (i, i) # there are no impossible moves in this case assert m[1] == [] def test_resolve_impossible_moves_1(): possible_moves = [[((0,0), 1), ((0,0), 2)], [((0,0), 1), ((0,0), 2), ((0,0), 3)]] impossible_moves = [((0,0), [2, 3])] moves = resolve_moves(possible_moves, impossible_moves) assert len(moves) == 1 def test_resolve_impossible_moves_2(): possible_moves = [[((0,0), 1), ((0,0), 2), ((1,1), 1), ((1,1), 2)], [((0,0), 1), ((0,0), 2), ((0,0), 3), ((1,1), 1), ((1,1), 2)]] impossible_moves = [((0,0), [1, 5, 6, 7, 8])] moves = resolve_moves(possible_moves, impossible_moves) assert len(moves) == 1 def test_resolve_impossible_moves_3(): possible_moves = [[((0,0), 1), ((0,0), 2), ((1,1), 1), ((1,1), 2)], [((0,0), 1), ((0,0), 2), ((0,0), 3), ((1,1), 1), ((1,1), 2)]] impossible_moves = [((0,0), [1, 5, 6, 7, 8]), ((1,1), [1, 3, 5, 6, 7, 8, 9])] moves = resolve_moves(possible_moves, impossible_moves) assert len(moves) == 2
#!/usr/bin/python # Copyright 2015 The Chromium Authors. All rights reserved. # Use of this source code is governed by a BSD-style license that can be # found in the LICENSE file. """Symbolizes a log file produced by cyprofile instrumentation. Given a log file and the binary being profiled, creates an orderfile. """ import logging import multiprocessing import optparse import os import tempfile import string import sys import cygprofile_utils import symbol_extractor def _ParseLogLines(log_file_lines): """Parses a merged cyglog produced by mergetraces.py. Args: log_file_lines: array of lines in log file produced by profiled run lib_name: library or executable containing symbols Below is an example of a small log file: 5086e000-52e92000 r-xp 00000000 b3:02 51276 libchromeview.so secs usecs pid:threadid func START 1314897086 795828 3587:1074648168 0x509e105c 1314897086 795874 3587:1074648168 0x509e0eb4 1314897086 796326 3587:1074648168 0x509e0e3c 1314897086 796552 3587:1074648168 0x509e07bc END Returns: An ordered list of callee offsets. """ call_lines = [] vm_start = 0 line = log_file_lines[0] assert 'r-xp' in line end_index = line.find('-') vm_start = int(line[:end_index], 16) for line in log_file_lines[3:]: fields = line.split() if len(fields) == 4: call_lines.append(fields) else: assert fields[0] == 'END' # Convert strings to int in fields. call_info = [] for call_line in call_lines: addr = int(call_line[3], 16) if vm_start < addr: addr -= vm_start call_info.append(addr) return call_info def _GroupLibrarySymbolInfosByOffset(lib_filename): """Returns a dict {offset: [SymbolInfo]} from a library.""" symbol_infos = symbol_extractor.SymbolInfosFromBinary(lib_filename) return symbol_extractor.GroupSymbolInfosByOffset(symbol_infos) class SymbolNotFoundException(Exception): def __init__(self, value): super(SymbolNotFoundException, self).__init__(value) self.value = value def __str__(self): return repr(self.value) def _FindSymbolInfosAtOffset(offset_to_symbol_infos, offset): """Finds all SymbolInfo at a given offset. Args: offset_to_symbol_infos: {offset: [SymbolInfo]} offset: offset to look the symbols at Returns: The list of SymbolInfo at the given offset Raises: SymbolNotFoundException if the offset doesn't match any symbol. """ if offset in offset_to_symbol_infos: return offset_to_symbol_infos[offset] elif offset % 2 and (offset - 1) in offset_to_symbol_infos: # On ARM, odd addresses are used to signal thumb instruction. They are # generated by setting the LSB to 1 (see # http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0471e/Babfjhia.html). # TODO(lizeb): Make sure this hack doesn't propagate to other archs. return offset_to_symbol_infos[offset - 1] else: raise SymbolNotFoundException(offset) def _GetObjectFileNames(obj_dir): """Returns the list of object files in a directory.""" obj_files = [] for (dirpath, _, filenames) in os.walk(obj_dir): for file_name in filenames: if file_name.endswith('.o'): obj_files.append(os.path.join(dirpath, file_name)) return obj_files def _AllSymbolInfos(object_filenames): """Returns a list of SymbolInfo from an iterable of filenames.""" pool = multiprocessing.Pool() # Hopefully the object files are in the page cache at this step, so IO should # not be a problem (hence no concurrency limit on the pool). symbol_infos_nested = pool.map( symbol_extractor.SymbolInfosFromBinary, object_filenames) result = [] for symbol_infos in symbol_infos_nested: result += symbol_infos return result def _GetSymbolToSectionMapFromObjectFiles(obj_dir): """ Creates a mapping from symbol to linker section name by scanning all the object files. """ object_files = _GetObjectFileNames(obj_dir) symbol_to_section_map = {} symbol_warnings = cygprofile_utils.WarningCollector(300) symbol_infos = _AllSymbolInfos(object_files) for symbol_info in symbol_infos: symbol = symbol_info.name if symbol.startswith('.LTHUNK'): continue section = symbol_info.section if ((symbol in symbol_to_section_map) and (symbol_to_section_map[symbol] != symbol_info.section)): symbol_warnings.Write('Symbol ' + symbol + ' in conflicting sections ' + section + ' and ' + symbol_to_section_map[symbol]) elif not section.startswith('.text'): symbol_warnings.Write('Symbol ' + symbol + ' in incorrect section ' + section) else: symbol_to_section_map[symbol] = section symbol_warnings.WriteEnd('bad sections') return symbol_to_section_map def _WarnAboutDuplicates(offsets): """Warns about duplicate offsets. Args: offsets: list of offsets to check for duplicates Returns: True if there are no duplicates, False otherwise. """ seen_offsets = set() ok = True for offset in offsets: if offset not in seen_offsets: seen_offsets.add(offset) else: ok = False logging.warning('Duplicate offset: ' + hex(offset)) return ok def _OutputOrderfile(offsets, offset_to_symbol_infos, symbol_to_section_map, output_file): """Outputs the orderfile to output_file. Args: offsets: Iterable of offsets to match to section names offset_to_symbol_infos: {offset: [SymbolInfo]} symbol_to_section_map: {name: section} output_file: file-like object to write the results to """ success = True unknown_symbol_warnings = cygprofile_utils.WarningCollector(300) symbol_not_found_warnings = cygprofile_utils.WarningCollector(300) output_sections = set() for offset in offsets: try: symbol_infos = _FindSymbolInfosAtOffset(offset_to_symbol_infos, offset) for symbol_info in symbol_infos: if symbol_info.name in symbol_to_section_map: section = symbol_to_section_map[symbol_info.name] if not section in output_sections: output_file.write(section + '\n') output_sections.add(section) else: unknown_symbol_warnings.Write( 'No known section for symbol ' + symbol_info.name) except SymbolNotFoundException: symbol_not_found_warnings.Write( 'Did not find function in binary. offset: ' + hex(offset)) success = False unknown_symbol_warnings.WriteEnd('no known section for symbol.') symbol_not_found_warnings.WriteEnd('symbol not found in the binary.') return success def main(): parser = optparse.OptionParser(usage= 'usage: %prog [options] <merged_cyglog> <library> <output_filename>') parser.add_option('--target-arch', action='store', dest='arch', choices=['arm', 'arm64', 'x86', 'x86_64', 'x64', 'mips'], help='The target architecture for libchrome.so') options, argv = parser.parse_args(sys.argv) if not options.arch: options.arch = cygprofile_utils.DetectArchitecture() if len(argv) != 4: parser.print_help() return 1 (log_filename, lib_filename, output_filename) = argv[1:] symbol_extractor.SetArchitecture(options.arch) obj_dir = os.path.abspath(os.path.join( os.path.dirname(lib_filename), '../obj')) log_file_lines = map(string.rstrip, open(log_filename).readlines()) offsets = _ParseLogLines(log_file_lines) _WarnAboutDuplicates(offsets) offset_to_symbol_infos = _GroupLibrarySymbolInfosByOffset(lib_filename) symbol_to_section_map = _GetSymbolToSectionMapFromObjectFiles(obj_dir) success = False temp_filename = None output_file = None try: (fd, temp_filename) = tempfile.mkstemp(dir=os.path.dirname(output_filename)) output_file = os.fdopen(fd, 'w') ok = _OutputOrderfile( offsets, offset_to_symbol_infos, symbol_to_section_map, output_file) output_file.close() os.rename(temp_filename, output_filename) temp_filename = None success = ok finally: if output_file: output_file.close() if temp_filename: os.remove(temp_filename) return 0 if success else 1 if __name__ == '__main__': logging.basicConfig(level=logging.INFO) sys.exit(main())
from __future__ import absolute_import import logging import os import tempfile # TODO: Get this into six.moves.urllib.parse try: from urllib import parse as urllib_parse except ImportError: import urlparse as urllib_parse from pip.utils import rmtree, display_path from pip.vcs import vcs, VersionControl from pip.download import path_to_url logger = logging.getLogger(__name__) class Bazaar(VersionControl): name = 'bzr' dirname = '.bzr' repo_name = 'branch' schemes = ( 'bzr', 'bzr+http', 'bzr+https', 'bzr+ssh', 'bzr+sftp', 'bzr+ftp', 'bzr+lp', ) def __init__(self, url=None, *args, **kwargs): super(Bazaar, self).__init__(url, *args, **kwargs) # Python >= 2.7.4, 3.3 doesn't have uses_fragment or non_hierarchical # Register lp but do not expose as a scheme to support bzr+lp. if getattr(urllib_parse, 'uses_fragment', None): urllib_parse.uses_fragment.extend(['lp']) urllib_parse.non_hierarchical.extend(['lp']) def export(self, location): """ Export the Bazaar repository at the url to the destination location """ temp_dir = tempfile.mkdtemp('-export', 'pip-') self.unpack(temp_dir) if os.path.exists(location): # Remove the location to make sure Bazaar can export it correctly rmtree(location) try: self.run_command(['export', location], cwd=temp_dir, show_stdout=False) finally: rmtree(temp_dir) def switch(self, dest, url, rev_options): self.run_command(['switch', url], cwd=dest) def update(self, dest, rev_options): self.run_command(['pull', '-q'] + rev_options, cwd=dest) def obtain(self, dest): url, rev = self.get_url_rev() if rev: rev_options = ['-r', rev] rev_display = ' (to revision %s)' % rev else: rev_options = [] rev_display = '' if self.check_destination(dest, url, rev_options, rev_display): logger.info( 'Checking out %s%s to %s', url, rev_display, display_path(dest), ) self.run_command(['branch', '-q'] + rev_options + [url, dest]) def get_url_rev(self): # hotfix the URL scheme after removing bzr+ from bzr+ssh:// readd it url, rev = super(Bazaar, self).get_url_rev() if url.startswith('ssh://'): url = 'bzr+' + url return url, rev def get_url(self, location): urls = self.run_command(['info'], show_stdout=False, cwd=location) for line in urls.splitlines(): line = line.strip() for x in ('checkout of branch: ', 'parent branch: '): if line.startswith(x): repo = line.split(x)[1] if self._is_local_repository(repo): return path_to_url(repo) return repo return None def get_revision(self, location): revision = self.run_command( ['revno'], show_stdout=False, cwd=location) return revision.splitlines()[-1] def get_src_requirement(self, dist, location): repo = self.get_url(location) if not repo: return None if not repo.lower().startswith('bzr:'): repo = 'bzr+' + repo egg_project_name = dist.egg_name().split('-', 1)[0] current_rev = self.get_revision(location) return '%s@%s#egg=%s' % (repo, current_rev, egg_project_name) def check_version(self, dest, rev_options): """Always assume the versions don't match""" return False vcs.register(Bazaar)
from django.test import TestCase from viewflow import flow from viewflow.base import Flow, this from .. import integration_test def create_test_flow(activation): activation.prepare() activation.done() return activation @flow.flow_func(task_loader=lambda flow_task, process: process.get_task(FunctionFlow.func_task)) def function_task(activation, process): activation.prepare() activation.done() def handler(activation): pass class FunctionFlow(Flow): start = flow.StartFunction(create_test_flow).Next(this.func_task) default_start = flow.StartFunction().Next(this.func_task) inline_start = flow.StartFunction().Next(this.func_task) func_task = flow.Function(function_task).Next(this.handler_task) handler_task = flow.Handler(handler).Next(this.end) end = flow.End() def inline_start_func(self, activation): activation.prepare() activation.done() self.inline_start_func_called = True return activation @integration_test class TestFunctionFlow(TestCase): def test_function_flow(self): act = FunctionFlow.start.run() FunctionFlow.func_task.run(act.process) tasks = act.process.task_set.all() self.assertEqual(4, tasks.count()) self.assertTrue(all(task.finished is not None for task in tasks)) def test_function_default_start(self): act = FunctionFlow.default_start.run() FunctionFlow.func_task.run(act.process) tasks = act.process.task_set.all() self.assertEqual(4, tasks.count()) self.assertTrue(all(task.finished is not None for task in tasks)) def test_function_inline_start(self): act = FunctionFlow.inline_start.run() self.assertTrue(getattr(FunctionFlow.instance, 'inline_start_func_called', False)) FunctionFlow.func_task.run(act.process) tasks = act.process.task_set.all() self.assertEqual(4, tasks.count()) self.assertTrue(all(task.finished is not None for task in tasks))
# # This file is part of Ansible # # Ansible is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Ansible is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Ansible. If not, see <http://www.gnu.org/licenses/>. # Make coding more python3-ish from __future__ import (absolute_import, division, print_function) __metaclass__ = type from os.path import basename from ansible.errors import AnsibleParserError from ansible.playbook.attribute import FieldAttribute from ansible.playbook.task_include import TaskInclude from ansible.playbook.role import Role from ansible.playbook.role.include import RoleInclude try: from __main__ import display except ImportError: from ansible.utils.display import Display display = Display() __all__ = ['IncludeRole'] class IncludeRole(TaskInclude): """ A Role include is derived from a regular role to handle the special circumstances related to the `- include_role: ...` """ BASE = ('name', 'role') # directly assigned FROM_ARGS = ('tasks_from', 'vars_from', 'defaults_from') # used to populate from dict in role OTHER_ARGS = ('private', 'allow_duplicates') # assigned to matching property VALID_ARGS = tuple(frozenset(BASE + FROM_ARGS + OTHER_ARGS)) # all valid args _inheritable = False # ================================================================================= # ATTRIBUTES # private as this is a 'module options' vs a task property _allow_duplicates = FieldAttribute(isa='bool', default=True, private=True) _private = FieldAttribute(isa='bool', default=None, private=True) def __init__(self, block=None, role=None, task_include=None): super(IncludeRole, self).__init__(block=block, role=role, task_include=task_include) self._from_files = {} self._parent_role = role self._role_name = None self._role_path = None def get_block_list(self, play=None, variable_manager=None, loader=None): # only need play passed in when dynamic if play is None: myplay = self._parent._play else: myplay = play ri = RoleInclude.load(self._role_name, play=myplay, variable_manager=variable_manager, loader=loader) ri.vars.update(self.vars) # build role actual_role = Role.load(ri, myplay, parent_role=self._parent_role, from_files=self._from_files) actual_role._metadata.allow_duplicates = self.allow_duplicates # save this for later use self._role_path = actual_role._role_path # compile role with parent roles as dependencies to ensure they inherit # variables if not self._parent_role: dep_chain = [] else: dep_chain = list(self._parent_role._parents) dep_chain.append(self._parent_role) blocks = actual_role.compile(play=myplay, dep_chain=dep_chain) for b in blocks: b._parent = self # updated available handlers in play handlers = actual_role.get_handler_blocks(play=myplay) myplay.handlers = myplay.handlers + handlers return blocks, handlers @staticmethod def load(data, block=None, role=None, task_include=None, variable_manager=None, loader=None): ir = IncludeRole(block, role, task_include=task_include).load_data(data, variable_manager=variable_manager, loader=loader) # Validate options my_arg_names = frozenset(ir.args.keys()) # name is needed, or use role as alias ir._role_name = ir.args.get('name', ir.args.get('role')) if ir._role_name is None: raise AnsibleParserError("'name' is a required field for include_role.") # validate bad args, otherwise we silently ignore bad_opts = my_arg_names.difference(IncludeRole.VALID_ARGS) if bad_opts: raise AnsibleParserError('Invalid options for include_role: %s' % ','.join(list(bad_opts))) # build options for role includes for key in my_arg_names.intersection(IncludeRole.FROM_ARGS): from_key = key.replace('_from', '') ir._from_files[from_key] = basename(ir.args.get(key)) # manual list as otherwise the options would set other task parameters we don't want. for option in my_arg_names.intersection(IncludeRole.OTHER_ARGS): setattr(ir, option, ir.args.get(option)) return ir def copy(self, exclude_parent=False, exclude_tasks=False): new_me = super(IncludeRole, self).copy(exclude_parent=exclude_parent, exclude_tasks=exclude_tasks) new_me.statically_loaded = self.statically_loaded new_me._from_files = self._from_files.copy() new_me._parent_role = self._parent_role new_me._role_name = self._role_name new_me._role_path = self._role_path return new_me def get_include_params(self): v = super(IncludeRole, self).get_include_params() if self._parent_role: v.update(self._parent_role.get_role_params()) return v
# Copied from https://github.com/ohmu/ohmu_common_py ohmu_common_py/pgutil.py version 0.0.1-0-unknown-fa54b44 """ pghoard - postgresql utility functions Copyright (c) 2015 Ohmu Ltd See LICENSE for details """ try: from urllib.parse import urlparse, parse_qs # pylint: disable=no-name-in-module, import-error except ImportError: from urlparse import urlparse, parse_qs # pylint: disable=no-name-in-module, import-error def create_connection_string(connection_info): return " ".join("{}='{}'".format(k, str(v).replace("'", "\\'")) for k, v in sorted(connection_info.items())) def mask_connection_info(info): masked_info = get_connection_info(info) password = masked_info.pop("password", None) return "{0}; {1} password".format( create_connection_string(masked_info), "no" if password is None else "hidden") def get_connection_info_from_config_line(line): _, value = line.split("=", 1) value = value.strip()[1:-1].replace("''", "'") return get_connection_info(value) def get_connection_info(info): """turn a connection info object into a dict or return it if it was a dict already. supports both the traditional libpq format and the new url format""" if isinstance(info, dict): return info.copy() elif info.startswith("postgres://") or info.startswith("postgresql://"): return parse_connection_string_url(info) else: return parse_connection_string_libpq(info) def parse_connection_string_url(url): # drop scheme from the url as some versions of urlparse don't handle # query and path properly for urls with a non-http scheme schemeless_url = url.split(":", 1)[1] p = urlparse(schemeless_url) fields = {} if p.hostname: fields["host"] = p.hostname if p.port: fields["port"] = str(p.port) if p.username: fields["user"] = p.username if p.password is not None: fields["password"] = p.password if p.path and p.path != "/": fields["dbname"] = p.path[1:] for k, v in parse_qs(p.query).items(): fields[k] = v[-1] return fields def parse_connection_string_libpq(connection_string): """parse a postgresql connection string as defined in http://www.postgresql.org/docs/current/static/libpq-connect.html#LIBPQ-CONNSTRING""" fields = {} while True: connection_string = connection_string.strip() if not connection_string: break if "=" not in connection_string: raise ValueError("expecting key=value format in connection_string fragment {!r}".format(connection_string)) key, rem = connection_string.split("=", 1) if rem.startswith("'"): asis, value = False, "" for i in range(1, len(rem)): if asis: value += rem[i] asis = False elif rem[i] == "'": break # end of entry elif rem[i] == "\\": asis = True else: value += rem[i] else: raise ValueError("invalid connection_string fragment {!r}".format(rem)) connection_string = rem[i + 1:] # pylint: disable=undefined-loop-variable else: res = rem.split(None, 1) if len(res) > 1: value, connection_string = res else: value, connection_string = rem, "" fields[key] = value return fields
#!/usr/bin/python2.7 import argparse import json import sys from collections import defaultdict class Node(object): def __init__(self, node, datacenter): self.node = node self.datacenter = datacenter self.partitions = set() @property def hostname(self): return self.node["hostname"] @property def port(self): return self.node["port"] @property def rack_id(self): if "rackId" in self.node: return self.node["rackId"] return -1 @property def datacenter_name(self): return self.datacenter["name"] def add_partition(self, partition): self.partitions.add(partition) def __repr__(self): return "[hostname: {}, port: {}, dc: {}]".format( self.hostname, self.port, self.datacenter_name) __str__ = __repr__ class Partition(object): def __init__(self, partition): self.partition = partition self.nodes_by_datacenter = defaultdict(set) @property def id(self): return self.partition["id"] def add_node(self, node): self.nodes_by_datacenter[node.datacenter_name].add(node) node.add_partition(self) def racks_used(self, datacenter_name): return {node.rack_id for node in self.nodes_by_datacenter[datacenter_name]} def __repr__(self): return "[id: {}]".format(self.id) __str__ = __repr__ class Layout(object): BALANCE_THRESHOLD = 4.0 def __init__(self, hardware_layout_filename, partition_layout_filename): with open(hardware_layout_filename) as f: self.hardware_layout = json.load(f) with open(partition_layout_filename) as f: self.partition_layout = json.load(f) self.setup() def setup(self): self.node_map = {} self.partition_map = {} self.dc_node_combo_map = defaultdict(lambda: defaultdict(set)) for datacenter_struct in self.hardware_layout["datacenters"]: for node_struct in datacenter_struct["dataNodes"]: k = (node_struct["hostname"], node_struct["port"]) self.node_map[k] = Node(node_struct, datacenter_struct) for partition_struct in self.partition_layout["partitions"]: partition = Partition(partition_struct) if len(partition_struct["replicas"]) == 0: raise Exception("No replicas assigned to partition {}".format(partition.id)) for replica_struct in partition_struct["replicas"]: k = (replica_struct["hostname"], replica_struct["port"]) node = self.node_map[k] partition.add_node(node) for dc, nodes in partition.nodes_by_datacenter.items(): self.dc_node_combo_map[dc][frozenset(nodes)].add(partition) self.partition_map[partition_struct["id"]] = partition def rack_id(self, node_host, node_port): k = (node_host, node_port) if k in self.node_map: return self.node_map[k].rack_id raise Exception("Node {}:{} not found".format(node_host, node_port)) def racks_used(self, partition_id, datacenter_name): return self.partition_map[partition_id].racks_used(datacenter_name) def shared_partitions(self, *nodes): return set.intersection( *(self.node_map[node].partitions for node in nodes) ) def print_report(self): for dc, node_combo_map in self.dc_node_combo_map.items(): print("In datacenter: {}".format(dc)) max_combo = max(node_combo_map, key=lambda k: len(node_combo_map[k])) avg_per_combo = sum(len(partitions) for partitions in node_combo_map.values()) / float(len(node_combo_map)) max_per_combo = len(node_combo_map[max_combo]) print("Num node combos used: {}".format(len(node_combo_map))) print("Average partitions sharing a node combo: {}".format(avg_per_combo)) print("Max partitions sharing a node combo: {} on the following nodes:".format(max_per_combo)) for node in max_combo: print(node) if (float(max_per_combo) / avg_per_combo) > self.BALANCE_THRESHOLD: print("The ratio of max to average number of partitions sharing a node combo " + "exceeds the threshold: {} on this datacenter".format(self.BALANCE_THRESHOLD)) sum_racks, n_partitions, min_racks = 0, 0, sys.maxsize for partition in self.partition_map.values(): num_racks = len(partition.racks_used(dc)) n_partitions += 1 sum_racks += num_racks if num_racks < min_racks: min_racks = num_racks print("Min racks used: {}".format(min_racks)) print("Average racks used: {}".format( float(sum_racks) / n_partitions)) partitions_per_node = [len(node.partitions) for node in self.node_map.values() if node.datacenter_name == dc] print("") def interactive(self): while True: cmd = raw_input(">> ").split() try: if len(cmd) == 0: continue elif cmd[0] == "report": self.print_report() elif cmd[0] == "rack_id": print("Node {}:{} is on rack {}".format( cmd[1], cmd[2], self.rack_id(cmd[1], int(cmd[2])))) elif cmd[0] == "racks_used": print("Partition {} in datacenter {} uses the following racks: {}".format( cmd[1], cmd[2], self.racks_used(int(cmd[1]), cmd[2]))) elif cmd[0] == "shared_partitions": args = [(cmd[i + 1], int(cmd[i + 2])) for i in range(0, len(cmd) - 1, 2)] print("The following nodes:") for hostname, port in args: print(" {}:{}".format(hostname, port)) print("share the following partitions:") print(self.shared_partitions(*args)) else: print("Command not recognized") except Exception: print("Invalid input") print("") def main(): parser = argparse.ArgumentParser( description='Analyze node distribution in a partition layout') parser.add_argument("--interactive", "-i", action="store_true") parser.add_argument('hardware_layout', help='the path to the hardware layout file') parser.add_argument('partition_layout', help='the path to the partition layout file') args = parser.parse_args() layout = Layout(args.hardware_layout, args.partition_layout) if args.interactive: layout.interactive() else: layout.print_report() if __name__ == "__main__": main()
# Licensed to the Apache Software Foundation (ASF) under one # or more contributor license agreements. See the NOTICE file # distributed with this work for additional information # regarding copyright ownership. The ASF licenses this file # to you under the Apache License, Version 2.0 (the # "License"); you may not use this file except in compliance # with the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, # software distributed under the License is distributed on an # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY # KIND, either express or implied. See the License for the # specific language governing permissions and limitations # under the License. # pylint: disable=invalid-name, too-many-nested-blocks "Roi align in python" import math import numpy as np def roi_align_nchw_python(a_np, rois_np, pooled_size, spatial_scale, sample_ratio): """Roi align in python""" _, channel, height, width = a_np.shape num_roi = rois_np.shape[0] b_np = np.zeros((num_roi, channel, pooled_size, pooled_size), dtype=a_np.dtype) if isinstance(pooled_size, int): pooled_size_h = pooled_size_w = pooled_size else: pooled_size_h, pooled_size_w = pooled_size def _bilinear(b, c, y, x): if y < -1 or y > height or x < -1 or x > width: return 0 y = max(y, 0.0) x = max(x, 0.0) y_low = int(y) x_low = int(x) y_high = min(y_low + 1, height - 1) x_high = min(x_low + 1, width - 1) ly = y - y_low lx = x - x_low return (1 - ly) * (1 - lx) * a_np[b, c, y_low, x_low] + \ (1 - ly) * lx * a_np[b, c, y_low, x_high] + \ ly * (1 - lx) * a_np[b, c, y_high, x_low] + \ ly * lx * a_np[b, c, y_high, x_high] for i in range(num_roi): roi = rois_np[i] batch_index = int(roi[0]) roi_start_w, roi_start_h, roi_end_w, roi_end_h = roi[1:] * spatial_scale roi_h = max(roi_end_h - roi_start_h, 1.0) roi_w = max(roi_end_w - roi_start_w, 1.0) bin_h = roi_h / pooled_size_h bin_w = roi_w / pooled_size_w if sample_ratio > 0: roi_bin_grid_h = roi_bin_grid_w = int(sample_ratio) else: roi_bin_grid_h = int(math.ceil(roi_h / pooled_size)) roi_bin_grid_w = int(math.ceil(roi_w / pooled_size)) count = roi_bin_grid_h * roi_bin_grid_w for c in range(channel): for ph in range(pooled_size_h): for pw in range(pooled_size_w): total = 0. for iy in range(roi_bin_grid_h): for ix in range(roi_bin_grid_w): y = roi_start_h + ph * bin_h + (iy + 0.5) * bin_h / roi_bin_grid_h x = roi_start_w + pw * bin_w + (ix + 0.5) * bin_w / roi_bin_grid_w total += _bilinear(batch_index, c, y, x) b_np[i, c, ph, pw] = total / count return b_np
# Copyright (c) 2016 PaddlePaddle Authors. All Rights Reserved # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. from paddle.trainer_config_helpers import * # 1. read data. Suppose you saved above python code as dataprovider.py data_file = 'empty.list' with open(data_file, 'w') as f: f.writelines(' ') define_py_data_sources2( train_list=data_file, test_list=None, module='dataprovider', obj='process', args={}) # 2. learning algorithm settings(batch_size=12, learning_rate=1e-3, learning_method=MomentumOptimizer()) # 3. Network configuration x = data_layer(name='x', size=1) y = data_layer(name='y', size=1) y_predict = fc_layer( input=x, param_attr=ParamAttr(name='w'), size=1, act=LinearActivation(), bias_attr=ParamAttr(name='b')) cost = regression_cost(input=y_predict, label=y) outputs(cost)
from abc import ABCMeta, abstractmethod, abstractproperty import importlib import os import shlex import subprocess import socket import time import rpyc from dpa.app.entity import EntityRegistry from dpa.env.vars import DpaVars from dpa.ptask.area import PTaskArea from dpa.ptask import PTaskError, PTask from dpa.singleton import Singleton # ----------------------------------------------------------------------------- class SessionRegistry(Singleton): # ------------------------------------------------------------------------- def init(self): self._registry = {} # ------------------------------------------------------------------------- def current(self): for registered_cls in self._registry.values(): if registered_cls.current(): return registered_cls() return None # ------------------------------------------------------------------------- def register(self, cls): self._registry[cls.app_name] = cls # ----------------------------------------------------------------------------- class Session(object): __metaclass__ = ABCMeta app_name = None # ------------------------------------------------------------------------- @classmethod def current(cls): return None # ------------------------------------------------------------------------- def __init__(self): pass # ------------------------------------------------------------------------- @abstractmethod def close(self): """Close the current file.""" # ------------------------------------------------------------------------- def list_entities(self, categories=None): """List entities in the session.""" entities = [] entity_classes = EntityRegistry().get_entity_classes( self.__class__.app_name) for entity_class in entity_classes: entities.extend(entity_class.list(self)) if categories: filtered = [e for e in entities if e.category in categories] else: filtered = entities return filtered # ------------------------------------------------------------------------- @classmethod def open_file(self, filepath): """Open a new session with the supplied file.""" # ------------------------------------------------------------------------- @abstractmethod def save(self, filepath=None): """Save the current session. Save to the file path if provided.""" # ------------------------------------------------------------------------- @abstractproperty def in_session(self): """Returns True if inside a current app session.""" # ------------------------------------------------------------------------- def init_module(self, module_path): _module = None if self.in_session: try: _module = importlib.import_module(module_path) except ImportError: pass # will raise below if not _module: raise SessionError( "Failed to initialize session. " + \ "'{mod}' module could not be imported.".format(mod=module_path) ) return _module # ------------------------------------------------------------------------- def require_executable(self, executable): """Returns the full path for the supplied executable name.""" (path, file_name) = os.path.split(executable) # path already included if path: if not os.path.isfile(executable): raise SessionError("Unable to locate executable: " + executable) elif not os.access(executable, os.X_OK): raise SessionError("File is not executable: " + executable) else: return executable else: bin_paths = DpaVars.path() bin_paths.get() for path in bin_paths.list: executable_path = os.path.join(path, executable) if (os.path.isfile(executable_path) and os.access(executable_path, os.X_OK)): return executable_path raise SessionError("Unable to locate executable: " + executable) # ------------------------------------------------------------------------- @property def app_name(self): return self.__class__.app_name # ------------------------------------------------------------------------- @property def ptask_area(self): """Return the current ptask area for this session.""" if not hasattr(self, '_ptask_area'): self._ptask_area = PTaskArea.current() return self._ptask_area # ------------------------------------------------------------------------- @property def ptask(self): if not hasattr(self, '_ptask'): ptask_area = self.ptask_area if not ptask_area.spec: self._ptask = None else: try: self._ptask = PTask.get(ptask_area.spec) except PTaskError as e: raise SessionError("Unable to determine ptask.") return self._ptask # ------------------------------------------------------------------------- @property def ptask_version(self): """Return the current ptask version for this session.""" if not hasattr(self, '_ptask_version'): ptask = self.ptask if not ptask: self._ptask_version = None else: self._ptask_version = ptask.latest_version return self._ptask_version # ----------------------------------------------------------------------------- class RemoteMixin(object): __metaclass__ = ABCMeta # ------------------------------------------------------------------------- def __init__(self, remote=False): self._remote = remote # ------------------------------------------------------------------------- def __del__(self): self.shutdown() # ------------------------------------------------------------------------- def __enter__(self): return self # ------------------------------------------------------------------------- def __exit__(self, exc_type, exc_value, traceback): self.shutdown() # ------------------------------------------------------------------------- @staticmethod def _get_port(): sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) sock.bind(("",0)) port = sock.getsockname()[1] sock.close() return port # ------------------------------------------------------------------------- @property def remote(self): """Returns True if in a session, False otherwise.""" return self._remote # ------------------------------------------------------------------------- @property def remote_connection(self): if not hasattr(self, '_remote_connection'): self._remote_connection = self._connect_remote() return self._remote_connection # ------------------------------------------------------------------------- @abstractproperty def server_executable(self): """The executable for starting the remote app server.""" # ------------------------------------------------------------------------- def shutdown(self): if hasattr(self, '_remote_connection'): try: self._remote_connection.root.shutdown() except EOFError: # this is the expected error on shutdown pass else: self._remote_connection = None # ------------------------------------------------------------------------- def init_module(self, module_path): _module = None if self.remote: # need to give time for standalone app to import properly tries = 0 while not _module or tries < 30: try: self.remote_connection.execute("import " + module_path) _module = getattr( self.remote_connection.modules, module_path) break except ImportError: tries += 1 time.sleep(1) if not _module: self.shutdown() elif self.in_session: try: _module = importlib.import_module(module_path) except ImportError: pass # will raise below if not _module: raise SessionError( "Failed to initialize session. " + \ "'{mod}' module could not be imported.".format(mod=module_path) ) return _module # ------------------------------------------------------------------------- def _connect_remote(self): port = self._get_port() cmd = "{cmd} {port}".format(cmd=self.server_executable, port=port) args = shlex.split(cmd) subprocess.Popen(args) connection = None tries = 0 while not connection or tries < 30: try: connection = rpyc.classic.connect("localhost", port) break except socket.error: tries += 1 time.sleep(1) if not connection: raise SessionError("Unable connect to remote session.") return connection # ----------------------------------------------------------------------------- class SessionError(Exception): pass
# -*- coding: utf-8 -*- import asyncio from .assertions import isiter from .concurrent import ConcurrentExecutor @asyncio.coroutine def wait(*coros_or_futures, limit=0, timeout=None, loop=None, return_exceptions=False, return_when='ALL_COMPLETED'): """ Wait for the Futures and coroutine objects given by the sequence futures to complete, with optional concurrency limit. Coroutines will be wrapped in Tasks. ``timeout`` can be used to control the maximum number of seconds to wait before returning. timeout can be an int or float. If timeout is not specified or None, there is no limit to the wait time. If ``return_exceptions`` is True, exceptions in the tasks are treated the same as successful results, and gathered in the result list; otherwise, the first raised exception will be immediately propagated to the returned future. ``return_when`` indicates when this function should return. It must be one of the following constants of the concurrent.futures module. All futures must share the same event loop. This functions is mostly compatible with Python standard ``asyncio.wait()``. Arguments: *coros_or_futures (iter|list): an iterable collection yielding coroutines functions. limit (int): optional concurrency execution limit. Use ``0`` for no limit. timeout (int/float): maximum number of seconds to wait before returning. return_exceptions (bool): exceptions in the tasks are treated the same as successful results, instead of raising them. return_when (str): indicates when this function should return. loop (asyncio.BaseEventLoop): optional event loop to use. *args (mixed): optional variadic argument to pass to the coroutines function. Returns: tuple: Returns two sets of Future: (done, pending). Raises: TypeError: in case of invalid coroutine object. ValueError: in case of empty set of coroutines or futures. TimeoutError: if execution takes more than expected. Usage:: async def sum(x, y): return x + y done, pending = await paco.wait( sum(1, 2), sum(3, 4)) [task.result() for task in done] # => [3, 7] """ # Support iterable as first argument for better interoperability if len(coros_or_futures) == 1 and isiter(coros_or_futures[0]): coros_or_futures = coros_or_futures[0] # If no coroutines to schedule, return empty list # Mimics asyncio behaviour. if len(coros_or_futures) == 0: raise ValueError('paco: set of coroutines/futures is empty') # Create concurrent executor pool = ConcurrentExecutor(limit=limit, loop=loop, coros=coros_or_futures) # Wait until all the tasks finishes return (yield from pool.run(timeout=timeout, return_when=return_when, return_exceptions=return_exceptions))
""" Grades Application Configuration Signal handlers are connected here. """ from django.apps import AppConfig from django.conf import settings from edx_proctoring.runtime import set_runtime_service from openedx.core.djangoapps.plugins.constants import ProjectType, SettingsType, PluginURLs, PluginSettings class GradesConfig(AppConfig): """ Application Configuration for Grades. """ name = u'lms.djangoapps.grades' plugin_app = { PluginURLs.CONFIG: { ProjectType.LMS: { PluginURLs.NAMESPACE: u'grades_api', PluginURLs.REGEX: u'api/grades/', PluginURLs.RELATIVE_PATH: u'api.urls', } }, PluginSettings.CONFIG: { ProjectType.LMS: { SettingsType.AWS: {PluginSettings.RELATIVE_PATH: u'settings.aws'}, SettingsType.COMMON: {PluginSettings.RELATIVE_PATH: u'settings.common'}, SettingsType.TEST: {PluginSettings.RELATIVE_PATH: u'settings.test'}, } } } def ready(self): """ Connect handlers to recalculate grades. """ # Can't import models at module level in AppConfigs, and models get # included from the signal handlers from .signals import handlers # pylint: disable=unused-variable if settings.FEATURES.get('ENABLE_SPECIAL_EXAMS'): from .services import GradesService set_runtime_service('grades', GradesService())
# # Licensed to the Apache Software Foundation (ASF) under one or more # contributor license agreements. See the NOTICE file distributed with # this work for additional information regarding copyright ownership. # The ASF licenses this file to You under the Apache License, Version 2.0 # (the "License"); you may not use this file except in compliance with # the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # import atexit import os import sys import select import signal import shlex import socket import platform from subprocess import Popen, PIPE if sys.version >= '3': xrange = range from py4j.java_gateway import java_import, JavaGateway, GatewayClient from pyspark.find_spark_home import _find_spark_home from pyspark.serializers import read_int def launch_gateway(conf=None): """ launch jvm gateway :param conf: spark configuration passed to spark-submit :return: """ if "PYSPARK_GATEWAY_PORT" in os.environ: gateway_port = int(os.environ["PYSPARK_GATEWAY_PORT"]) else: SPARK_HOME = _find_spark_home() # Launch the Py4j gateway using Spark's run command so that we pick up the # proper classpath and settings from spark-env.sh on_windows = platform.system() == "Windows" script = "./bin/spark-submit.cmd" if on_windows else "./bin/spark-submit" command = [os.path.join(SPARK_HOME, script)] if conf: for k, v in conf.getAll(): command += ['--conf', '%s=%s' % (k, v)] submit_args = os.environ.get("PYSPARK_SUBMIT_ARGS", "pyspark-shell") if os.environ.get("SPARK_TESTING"): submit_args = ' '.join([ "--conf spark.ui.enabled=false", submit_args ]) command = command + shlex.split(submit_args) # Start a socket that will be used by PythonGatewayServer to communicate its port to us callback_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) callback_socket.bind(('127.0.0.1', 0)) callback_socket.listen(1) callback_host, callback_port = callback_socket.getsockname() env = dict(os.environ) env['_PYSPARK_DRIVER_CALLBACK_HOST'] = callback_host env['_PYSPARK_DRIVER_CALLBACK_PORT'] = str(callback_port) # Launch the Java gateway. # We open a pipe to stdin so that the Java gateway can die when the pipe is broken if not on_windows: # Don't send ctrl-c / SIGINT to the Java gateway: def preexec_func(): signal.signal(signal.SIGINT, signal.SIG_IGN) proc = Popen(command, stdin=PIPE, preexec_fn=preexec_func, env=env) else: # preexec_fn not supported on Windows proc = Popen(command, stdin=PIPE, env=env) gateway_port = None # We use select() here in order to avoid blocking indefinitely if the subprocess dies # before connecting while gateway_port is None and proc.poll() is None: timeout = 1 # (seconds) readable, _, _ = select.select([callback_socket], [], [], timeout) if callback_socket in readable: gateway_connection = callback_socket.accept()[0] # Determine which ephemeral port the server started on: gateway_port = read_int(gateway_connection.makefile(mode="rb")) gateway_connection.close() callback_socket.close() if gateway_port is None: raise Exception("Java gateway process exited before sending the driver its port number") # In Windows, ensure the Java child processes do not linger after Python has exited. # In UNIX-based systems, the child process can kill itself on broken pipe (i.e. when # the parent process' stdin sends an EOF). In Windows, however, this is not possible # because java.lang.Process reads directly from the parent process' stdin, contending # with any opportunity to read an EOF from the parent. Note that this is only best # effort and will not take effect if the python process is violently terminated. if on_windows: # In Windows, the child process here is "spark-submit.cmd", not the JVM itself # (because the UNIX "exec" command is not available). This means we cannot simply # call proc.kill(), which kills only the "spark-submit.cmd" process but not the # JVMs. Instead, we use "taskkill" with the tree-kill option "/t" to terminate all # child processes in the tree (http://technet.microsoft.com/en-us/library/bb491009.aspx) def killChild(): Popen(["cmd", "/c", "taskkill", "/f", "/t", "/pid", str(proc.pid)]) atexit.register(killChild) # Connect to the gateway gateway = JavaGateway(GatewayClient(port=gateway_port), auto_convert=True) # Import the classes used by PySpark java_import(gateway.jvm, "org.apache.spark.SparkConf") java_import(gateway.jvm, "org.apache.spark.api.java.*") java_import(gateway.jvm, "org.apache.spark.api.python.*") java_import(gateway.jvm, "org.apache.spark.ml.python.*") java_import(gateway.jvm, "org.apache.spark.mllib.api.python.*") # TODO(davies): move into sql java_import(gateway.jvm, "org.apache.spark.sql.*") java_import(gateway.jvm, "org.apache.spark.sql.hive.*") java_import(gateway.jvm, "scala.Tuple2") return gateway
from django.test import TestCase from oscar.apps.offer import models from oscar.test.factories import ( create_order, OrderDiscountFactory, UserFactory) class TestAPerUserConditionalOffer(TestCase): def setUp(self): self.offer = models.ConditionalOffer(max_user_applications=1) self.user = UserFactory() def test_is_available_with_no_applications(self): self.assertTrue(self.offer.is_available()) def test_max_applications_is_correct_when_no_applications(self): self.assertEqual(1, self.offer.get_max_applications(self.user)) def test_max_applications_is_correct_when_equal_applications(self): order = create_order(user=self.user) OrderDiscountFactory( order=order, offer_id=self.offer.id, frequency=1) self.assertEqual(0, self.offer.get_max_applications(self.user)) def test_max_applications_is_correct_when_more_applications(self): order = create_order(user=self.user) OrderDiscountFactory( order=order, offer_id=self.offer.id, frequency=5) self.assertEqual(0, self.offer.get_max_applications(self.user))
# coding: utf-8 from __future__ import unicode_literals from .common import InfoExtractor from ..utils import ( int_or_none, mimetype2ext, ) class AparatIE(InfoExtractor): _VALID_URL = r'https?://(?:www\.)?aparat\.com/(?:v/|video/video/embed/videohash/)(?P<id>[a-zA-Z0-9]+)' _TEST = { 'url': 'http://www.aparat.com/v/wP8On', 'md5': '131aca2e14fe7c4dcb3c4877ba300c89', 'info_dict': { 'id': 'wP8On', 'ext': 'mp4', 'title': '  11 - ', 'age_limit': 0, }, # 'skip': 'Extremely unreliable', } def _real_extract(self, url): video_id = self._match_id(url) # Note: There is an easier-to-parse configuration at # http://www.aparat.com/video/video/config/videohash/%video_id # but the URL in there does not work webpage = self._download_webpage( 'http://www.aparat.com/video/video/embed/vt/frame/showvideo/yes/videohash/' + video_id, video_id) title = self._search_regex(r'\s+title:\s*"([^"]+)"', webpage, 'title') file_list = self._parse_json( self._search_regex( r'fileList\s*=\s*JSON\.parse\(\'([^\']+)\'\)', webpage, 'file list'), video_id) formats = [] for item in file_list[0]: file_url = item.get('file') if not file_url: continue ext = mimetype2ext(item.get('type')) label = item.get('label') formats.append({ 'url': file_url, 'ext': ext, 'format_id': label or ext, 'height': int_or_none(self._search_regex( r'(\d+)[pP]', label or '', 'height', default=None)), }) self._sort_formats(formats) thumbnail = self._search_regex( r'image:\s*"([^"]+)"', webpage, 'thumbnail', fatal=False) return { 'id': video_id, 'title': title, 'thumbnail': thumbnail, 'age_limit': self._family_friendly_search(webpage), 'formats': formats, }
#!/usr/bin/env python # pylint: disable=W0212 import leather from agate import utils def bar_chart(self, label=0, value=1, path=None, width=None, height=None): """ Render a bar chart using :class:`leather.Chart`. :param label: The name or index of a column to plot as the labels of the chart. Defaults to the first column in the table. :param value: The name or index of a column to plot as the values of the chart. Defaults to the second column in the table. :param path: If specified, the resulting SVG will be saved to this location. If :code:`None` and running in IPython, then the SVG will be rendered inline. Otherwise, the SVG data will be returned as a string. :param width: The width of the output SVG. :param height: The height of the output SVG. """ if type(label) is int: label_name = self.column_names[label] else: label_name = label if type(value) is int: value_name = self.column_names[value] else: value_name = value chart = leather.Chart() chart.add_x_axis(name=value_name) chart.add_y_axis(name=label_name) chart.add_bars(self, x=value, y=label) return chart.to_svg(path=path, width=width, height=height)
#!/usr/bin/python # Software License Agreement (BSD License) # # Copyright (c) 2012, Willow Garage, Inc. # All rights reserved. # # Redistribution and use in source and binary forms, with or without # modification, are permitted provided that the following conditions # are met: # # * Redistributions of source code must retain the above copyright # notice, this list of conditions and the following disclaimer. # * Redistributions in binary form must reproduce the above # copyright notice, this list of conditions and the following # disclaimer in the documentation and/or other materials provided # with the distribution. # * Neither the name of Willow Garage, Inc. nor the names of its # contributors may be used to endorse or promote products derived # from this software without specific prior written permission. # # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS # "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT # LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS # FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE # COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, # INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, # BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; # LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER # CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT # LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN # ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE # POSSIBILITY OF SUCH DAMAGE. '''This file generates shell code for the setup.SHELL scripts to set environment variables''' from __future__ import print_function import argparse import copy import errno import os import platform import sys CATKIN_MARKER_FILE = '.catkin' system = platform.system() IS_DARWIN = (system == 'Darwin') IS_WINDOWS = (system == 'Windows') # subfolder of workspace prepended to CMAKE_PREFIX_PATH ENV_VAR_SUBFOLDERS = { 'CMAKE_PREFIX_PATH': '', 'CPATH': 'include', 'LD_LIBRARY_PATH' if not IS_DARWIN else 'DYLD_LIBRARY_PATH': ['lib', os.path.join('lib', 'x86_64-linux-gnu')], 'PATH': 'bin', 'PKG_CONFIG_PATH': [os.path.join('lib', 'pkgconfig'), os.path.join('lib', 'x86_64-linux-gnu', 'pkgconfig')], 'PYTHONPATH': 'lib/python2.7/dist-packages', } def rollback_env_variables(environ, env_var_subfolders): ''' Generate shell code to reset environment variables by unrolling modifications based on all workspaces in CMAKE_PREFIX_PATH. This does not cover modifications performed by environment hooks. ''' lines = [] unmodified_environ = copy.copy(environ) for key in sorted(env_var_subfolders.keys()): subfolders = env_var_subfolders[key] if not isinstance(subfolders, list): subfolders = [subfolders] for subfolder in subfolders: value = _rollback_env_variable(unmodified_environ, key, subfolder) if value is not None: environ[key] = value lines.append(assignment(key, value)) if lines: lines.insert(0, comment('reset environment variables by unrolling modifications based on all workspaces in CMAKE_PREFIX_PATH')) return lines def _rollback_env_variable(environ, name, subfolder): ''' For each catkin workspace in CMAKE_PREFIX_PATH remove the first entry from env[NAME] matching workspace + subfolder. :param subfolder: str '' or subfoldername that may start with '/' :returns: the updated value of the environment variable. ''' value = environ[name] if name in environ else '' env_paths = [path for path in value.split(os.pathsep) if path] value_modified = False if subfolder: if subfolder.startswith(os.path.sep) or (os.path.altsep and subfolder.startswith(os.path.altsep)): subfolder = subfolder[1:] if subfolder.endswith(os.path.sep) or (os.path.altsep and subfolder.endswith(os.path.altsep)): subfolder = subfolder[:-1] for ws_path in _get_workspaces(environ, include_fuerte=True, include_non_existing=True): path_to_find = os.path.join(ws_path, subfolder) if subfolder else ws_path path_to_remove = None for env_path in env_paths: env_path_clean = env_path[:-1] if env_path and env_path[-1] in [os.path.sep, os.path.altsep] else env_path if env_path_clean == path_to_find: path_to_remove = env_path break if path_to_remove: env_paths.remove(path_to_remove) value_modified = True new_value = os.pathsep.join(env_paths) return new_value if value_modified else None def _get_workspaces(environ, include_fuerte=False, include_non_existing=False): ''' Based on CMAKE_PREFIX_PATH return all catkin workspaces. :param include_fuerte: The flag if paths starting with '/opt/ros/fuerte' should be considered workspaces, ``bool`` ''' # get all cmake prefix paths env_name = 'CMAKE_PREFIX_PATH' value = environ[env_name] if env_name in environ else '' paths = [path for path in value.split(os.pathsep) if path] # remove non-workspace paths workspaces = [path for path in paths if os.path.isfile(os.path.join(path, CATKIN_MARKER_FILE)) or (include_fuerte and path.startswith('/opt/ros/fuerte')) or (include_non_existing and not os.path.exists(path))] return workspaces def prepend_env_variables(environ, env_var_subfolders, workspaces): ''' Generate shell code to prepend environment variables for the all workspaces. ''' lines = [] lines.append(comment('prepend folders of workspaces to environment variables')) paths = [path for path in workspaces.split(os.pathsep) if path] prefix = _prefix_env_variable(environ, 'CMAKE_PREFIX_PATH', paths, '') lines.append(prepend(environ, 'CMAKE_PREFIX_PATH', prefix)) for key in sorted([key for key in env_var_subfolders.keys() if key != 'CMAKE_PREFIX_PATH']): subfolder = env_var_subfolders[key] prefix = _prefix_env_variable(environ, key, paths, subfolder) lines.append(prepend(environ, key, prefix)) return lines def _prefix_env_variable(environ, name, paths, subfolders): ''' Return the prefix to prepend to the environment variable NAME, adding any path in NEW_PATHS_STR without creating duplicate or empty items. ''' value = environ[name] if name in environ else '' environ_paths = [path for path in value.split(os.pathsep) if path] checked_paths = [] for path in paths: if not isinstance(subfolders, list): subfolders = [subfolders] for subfolder in subfolders: path_tmp = path if subfolder: path_tmp = os.path.join(path_tmp, subfolder) # exclude any path already in env and any path we already added if path_tmp not in environ_paths and path_tmp not in checked_paths: checked_paths.append(path_tmp) prefix_str = os.pathsep.join(checked_paths) if prefix_str != '' and environ_paths: prefix_str += os.pathsep return prefix_str def assignment(key, value): if not IS_WINDOWS: return 'export %s="%s"' % (key, value) else: return 'set %s=%s' % (key, value) def comment(msg): if not IS_WINDOWS: return '# %s' % msg else: return 'REM %s' % msg def prepend(environ, key, prefix): if key not in environ or not environ[key]: return assignment(key, prefix) if not IS_WINDOWS: return 'export %s="%s$%s"' % (key, prefix, key) else: return 'set %s=%s%%%s%%' % (key, prefix, key) def find_env_hooks(environ, cmake_prefix_path): ''' Generate shell code with found environment hooks for the all workspaces. ''' lines = [] lines.append(comment('found environment hooks in workspaces')) generic_env_hooks = [] generic_env_hooks_workspace = [] specific_env_hooks = [] specific_env_hooks_workspace = [] generic_env_hooks_by_filename = {} specific_env_hooks_by_filename = {} generic_env_hook_ext = 'bat' if IS_WINDOWS else 'sh' specific_env_hook_ext = environ['CATKIN_SHELL'] if not IS_WINDOWS and 'CATKIN_SHELL' in environ and environ['CATKIN_SHELL'] else None # remove non-workspace paths workspaces = [path for path in cmake_prefix_path.split(os.pathsep) if path and os.path.isfile(os.path.join(path, CATKIN_MARKER_FILE))] for workspace in reversed(workspaces): env_hook_dir = os.path.join(workspace, 'etc', 'catkin', 'profile.d') if os.path.isdir(env_hook_dir): for filename in sorted(os.listdir(env_hook_dir)): if filename.endswith('.%s' % generic_env_hook_ext): # remove previous env hook with same name if present if filename in generic_env_hooks_by_filename: i = generic_env_hooks.index(generic_env_hooks_by_filename[filename]) generic_env_hooks.pop(i) generic_env_hooks_workspace.pop(i) # append env hook generic_env_hooks.append(os.path.join(env_hook_dir, filename)) generic_env_hooks_workspace.append(workspace) generic_env_hooks_by_filename[filename] = generic_env_hooks[-1] elif specific_env_hook_ext is not None and filename.endswith('.%s' % specific_env_hook_ext): # remove previous env hook with same name if present if filename in specific_env_hooks_by_filename: i = specific_env_hooks.index(specific_env_hooks_by_filename[filename]) specific_env_hooks.pop(i) specific_env_hooks_workspace.pop(i) # append env hook specific_env_hooks.append(os.path.join(env_hook_dir, filename)) specific_env_hooks_workspace.append(workspace) specific_env_hooks_by_filename[filename] = specific_env_hooks[-1] env_hooks = generic_env_hooks + specific_env_hooks env_hooks_workspace = generic_env_hooks_workspace + specific_env_hooks_workspace count = len(env_hooks) lines.append(assignment('_CATKIN_ENVIRONMENT_HOOKS_COUNT', count)) for i in range(count): lines.append(assignment('_CATKIN_ENVIRONMENT_HOOKS_%d' % i, env_hooks[i])) lines.append(assignment('_CATKIN_ENVIRONMENT_HOOKS_%d_WORKSPACE' % i, env_hooks_workspace[i])) return lines def _parse_arguments(args=None): parser = argparse.ArgumentParser(description='Generates code blocks for the setup.SHELL script.') parser.add_argument('--extend', action='store_true', help='Skip unsetting previous environment variables to extend context') return parser.parse_known_args(args=args)[0] if __name__ == '__main__': try: try: args = _parse_arguments() except Exception as e: print(e, file=sys.stderr) sys.exit(1) # environment at generation time CMAKE_PREFIX_PATH = '/home/yuhan/catkin_ws/devel;/opt/ros/indigo'.split(';') # prepend current workspace if not already part of CPP base_path = os.path.dirname(__file__) if base_path not in CMAKE_PREFIX_PATH: CMAKE_PREFIX_PATH.insert(0, base_path) CMAKE_PREFIX_PATH = os.pathsep.join(CMAKE_PREFIX_PATH) environ = dict(os.environ) lines = [] if not args.extend: lines += rollback_env_variables(environ, ENV_VAR_SUBFOLDERS) lines += prepend_env_variables(environ, ENV_VAR_SUBFOLDERS, CMAKE_PREFIX_PATH) lines += find_env_hooks(environ, CMAKE_PREFIX_PATH) print('\n'.join(lines)) # need to explicitly flush the output sys.stdout.flush() except IOError as e: # and catch potantial "broken pipe" if stdout is not writable # which can happen when piping the output to a file but the disk is full if e.errno == errno.EPIPE: print(e, file=sys.stderr) sys.exit(2) raise sys.exit(0)
# pylint: disable=W0622 # Copyright (c) 2004-2013 LOGILAB S.A. (Paris, FRANCE). # http://www.logilab.fr/ -- mailto:contact@logilab.fr # # This program is free software; you can redistribute it and/or modify it under # the terms of the GNU General Public License as published by the Free Software # Foundation; either version 2 of the License, or (at your option) any later # version. # # This program is distributed in the hope that it will be useful, but WITHOUT # ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS # FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details # # You should have received a copy of the GNU General Public License along with # this program; if not, write to the Free Software Foundation, Inc., # 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA. """a similarities / code duplication command line tool and pylint checker """ from __future__ import print_function import sys from collections import defaultdict from logilab.common.ureports import Table from pylint.interfaces import IRawChecker from pylint.checkers import BaseChecker, table_lines_from_stats import six from six.moves import zip class Similar(object): """finds copy-pasted lines of code in a project""" def __init__(self, min_lines=4, ignore_comments=False, ignore_docstrings=False, ignore_imports=False): self.min_lines = min_lines self.ignore_comments = ignore_comments self.ignore_docstrings = ignore_docstrings self.ignore_imports = ignore_imports self.linesets = [] def append_stream(self, streamid, stream, encoding=None): """append a file to search for similarities""" if encoding is None: readlines = stream.readlines else: readlines = lambda: [line.decode(encoding) for line in stream] try: self.linesets.append(LineSet(streamid, readlines(), self.ignore_comments, self.ignore_docstrings, self.ignore_imports)) except UnicodeDecodeError: pass def run(self): """start looking for similarities and display results on stdout""" self._display_sims(self._compute_sims()) def _compute_sims(self): """compute similarities in appended files""" no_duplicates = defaultdict(list) for num, lineset1, idx1, lineset2, idx2 in self._iter_sims(): duplicate = no_duplicates[num] for couples in duplicate: if (lineset1, idx1) in couples or (lineset2, idx2) in couples: couples.add((lineset1, idx1)) couples.add((lineset2, idx2)) break else: duplicate.append(set([(lineset1, idx1), (lineset2, idx2)])) sims = [] for num, ensembles in six.iteritems(no_duplicates): for couples in ensembles: sims.append((num, couples)) sims.sort() sims.reverse() return sims def _display_sims(self, sims): """display computed similarities on stdout""" nb_lignes_dupliquees = 0 for num, couples in sims: print() print(num, "similar lines in", len(couples), "files") couples = sorted(couples) for lineset, idx in couples: print("==%s:%s" % (lineset.name, idx)) # pylint: disable=W0631 for line in lineset._real_lines[idx:idx+num]: print(" ", line.rstrip()) nb_lignes_dupliquees += num * (len(couples)-1) nb_total_lignes = sum([len(lineset) for lineset in self.linesets]) print("TOTAL lines=%s duplicates=%s percent=%.2f" \ % (nb_total_lignes, nb_lignes_dupliquees, nb_lignes_dupliquees*100. / nb_total_lignes)) def _find_common(self, lineset1, lineset2): """find similarities in the two given linesets""" lines1 = lineset1.enumerate_stripped lines2 = lineset2.enumerate_stripped find = lineset2.find index1 = 0 min_lines = self.min_lines while index1 < len(lineset1): skip = 1 num = 0 for index2 in find(lineset1[index1]): non_blank = 0 for num, ((_, line1), (_, line2)) in enumerate( zip(lines1(index1), lines2(index2))): if line1 != line2: if non_blank > min_lines: yield num, lineset1, index1, lineset2, index2 skip = max(skip, num) break if line1: non_blank += 1 else: # we may have reach the end num += 1 if non_blank > min_lines: yield num, lineset1, index1, lineset2, index2 skip = max(skip, num) index1 += skip def _iter_sims(self): """iterate on similarities among all files, by making a cartesian product """ for idx, lineset in enumerate(self.linesets[:-1]): for lineset2 in self.linesets[idx+1:]: for sim in self._find_common(lineset, lineset2): yield sim def stripped_lines(lines, ignore_comments, ignore_docstrings, ignore_imports): """return lines with leading/trailing whitespace and any ignored code features removed """ strippedlines = [] docstring = None for line in lines: line = line.strip() if ignore_docstrings: if not docstring and \ (line.startswith('"""') or line.startswith("'''")): docstring = line[:3] line = line[3:] if docstring: if line.endswith(docstring): docstring = None line = '' if ignore_imports: if line.startswith("import ") or line.startswith("from "): line = '' if ignore_comments: # XXX should use regex in checkers/format to avoid cutting # at a "#" in a string line = line.split('#', 1)[0].strip() strippedlines.append(line) return strippedlines class LineSet(object): """Holds and indexes all the lines of a single source file""" def __init__(self, name, lines, ignore_comments=False, ignore_docstrings=False, ignore_imports=False): self.name = name self._real_lines = lines self._stripped_lines = stripped_lines(lines, ignore_comments, ignore_docstrings, ignore_imports) self._index = self._mk_index() def __str__(self): return '<Lineset for %s>' % self.name def __len__(self): return len(self._real_lines) def __getitem__(self, index): return self._stripped_lines[index] def __lt__(self, other): return self.name < other.name def __hash__(self): return id(self) def enumerate_stripped(self, start_at=0): """return an iterator on stripped lines, starting from a given index if specified, else 0 """ idx = start_at if start_at: lines = self._stripped_lines[start_at:] else: lines = self._stripped_lines for line in lines: #if line: yield idx, line idx += 1 def find(self, stripped_line): """return positions of the given stripped line in this set""" return self._index.get(stripped_line, ()) def _mk_index(self): """create the index for this set""" index = defaultdict(list) for line_no, line in enumerate(self._stripped_lines): if line: index[line].append(line_no) return index MSGS = {'R0801': ('Similar lines in %s files\n%s', 'duplicate-code', 'Indicates that a set of similar lines has been detected \ among multiple file. This usually means that the code should \ be refactored to avoid this duplication.')} def report_similarities(sect, stats, old_stats): """make a layout with some stats about duplication""" lines = ['', 'now', 'previous', 'difference'] lines += table_lines_from_stats(stats, old_stats, ('nb_duplicated_lines', 'percent_duplicated_lines')) sect.append(Table(children=lines, cols=4, rheaders=1, cheaders=1)) # wrapper to get a pylint checker from the similar class class SimilarChecker(BaseChecker, Similar): """checks for similarities and duplicated code. This computation may be memory / CPU intensive, so you should disable it if you experiment some problems. """ __implements__ = (IRawChecker,) # configuration section name name = 'similarities' # messages msgs = MSGS # configuration options # for available dict keys/values see the optik parser 'add_option' method options = (('min-similarity-lines', {'default' : 4, 'type' : "int", 'metavar' : '<int>', 'help' : 'Minimum lines number of a similarity.'}), ('ignore-comments', {'default' : True, 'type' : 'yn', 'metavar' : '<y or n>', 'help': 'Ignore comments when computing similarities.'} ), ('ignore-docstrings', {'default' : True, 'type' : 'yn', 'metavar' : '<y or n>', 'help': 'Ignore docstrings when computing similarities.'} ), ('ignore-imports', {'default' : False, 'type' : 'yn', 'metavar' : '<y or n>', 'help': 'Ignore imports when computing similarities.'} ), ) # reports reports = (('RP0801', 'Duplication', report_similarities),) def __init__(self, linter=None): BaseChecker.__init__(self, linter) Similar.__init__(self, min_lines=4, ignore_comments=True, ignore_docstrings=True) self.stats = None def set_option(self, optname, value, action=None, optdict=None): """method called to set an option (registered in the options list) overridden to report options setting to Similar """ BaseChecker.set_option(self, optname, value, action, optdict) if optname == 'min-similarity-lines': self.min_lines = self.config.min_similarity_lines elif optname == 'ignore-comments': self.ignore_comments = self.config.ignore_comments elif optname == 'ignore-docstrings': self.ignore_docstrings = self.config.ignore_docstrings elif optname == 'ignore-imports': self.ignore_imports = self.config.ignore_imports def open(self): """init the checkers: reset linesets and statistics information""" self.linesets = [] self.stats = self.linter.add_stats(nb_duplicated_lines=0, percent_duplicated_lines=0) def process_module(self, node): """process a module the module's content is accessible via the stream object stream must implement the readlines method """ with node.stream() as stream: self.append_stream(self.linter.current_name, stream, node.file_encoding) def close(self): """compute and display similarities on closing (i.e. end of parsing)""" total = sum([len(lineset) for lineset in self.linesets]) duplicated = 0 stats = self.stats for num, couples in self._compute_sims(): msg = [] for lineset, idx in couples: msg.append("==%s:%s" % (lineset.name, idx)) msg.sort() # pylint: disable=W0631 for line in lineset._real_lines[idx:idx+num]: msg.append(line.rstrip()) self.add_message('R0801', args=(len(couples), '\n'.join(msg))) duplicated += num * (len(couples) - 1) stats['nb_duplicated_lines'] = duplicated stats['percent_duplicated_lines'] = total and duplicated * 100. / total def register(linter): """required method to auto register this checker """ linter.register_checker(SimilarChecker(linter)) def usage(status=0): """display command line usage information""" print("finds copy pasted blocks in a set of files") print() print('Usage: symilar [-d|--duplicates min_duplicated_lines] \ [-i|--ignore-comments] [--ignore-docstrings] [--ignore-imports] file1...') sys.exit(status) def Run(argv=None): """standalone command line access point""" if argv is None: argv = sys.argv[1:] from getopt import getopt s_opts = 'hdi' l_opts = ('help', 'duplicates=', 'ignore-comments', 'ignore-imports', 'ignore-docstrings') min_lines = 4 ignore_comments = False ignore_docstrings = False ignore_imports = False opts, args = getopt(argv, s_opts, l_opts) for opt, val in opts: if opt in ('-d', '--duplicates'): min_lines = int(val) elif opt in ('-h', '--help'): usage() elif opt in ('-i', '--ignore-comments'): ignore_comments = True elif opt in ('--ignore-docstrings',): ignore_docstrings = True elif opt in ('--ignore-imports',): ignore_imports = True if not args: usage(1) sim = Similar(min_lines, ignore_comments, ignore_docstrings, ignore_imports) for filename in args: with open(filename) as stream: sim.append_stream(filename, stream) sim.run() sys.exit(0) if __name__ == '__main__': Run()
#!/usr/bin/env python from gnuradio import gr, gr_unittest class test_hier_block2(gr_unittest.TestCase): def setUp(self): pass def tearDown(self): pass def test_001_make(self): hblock = gr.hier_block2("test_block", gr.io_signature(1,1,gr.sizeof_int), gr.io_signature(1,1,gr.sizeof_int)) self.assertEqual("test_block", hblock.name()) self.assertEqual(1, hblock.input_signature().max_streams()) self.assertEqual(1, hblock.output_signature().min_streams()) self.assertEqual(1, hblock.output_signature().max_streams()) self.assertEqual(gr.sizeof_int, hblock.output_signature().sizeof_stream_item(0)) def test_002_connect_input(self): hblock = gr.hier_block2("test_block", gr.io_signature(1,1,gr.sizeof_int), gr.io_signature(1,1,gr.sizeof_int)) nop1 = gr.nop(gr.sizeof_int) hblock.connect(hblock, nop1) def test_004_connect_output(self): hblock = gr.hier_block2("test_block", gr.io_signature(1,1,gr.sizeof_int), gr.io_signature(1,1,gr.sizeof_int)) nop1 = gr.nop(gr.sizeof_int) hblock.connect(nop1, hblock) def test_005_connect_output_in_use(self): hblock = gr.hier_block2("test_block", gr.io_signature(1,1,gr.sizeof_int), gr.io_signature(1,1,gr.sizeof_int)) nop1 = gr.nop(gr.sizeof_int) nop2 = gr.nop(gr.sizeof_int) hblock.connect(nop1, hblock) self.assertRaises(ValueError, lambda: hblock.connect(nop2, hblock)) def test_006_connect_invalid_src_port_neg(self): hblock = gr.hier_block2("test_block", gr.io_signature(1,1,gr.sizeof_int), gr.io_signature(1,1,gr.sizeof_int)) nop1 = gr.nop(gr.sizeof_int) self.assertRaises(ValueError, lambda: hblock.connect((hblock, -1), nop1)) def test_005_connect_invalid_src_port_exceeds(self): hblock = gr.hier_block2("test_block", gr.io_signature(1,1,gr.sizeof_int), gr.io_signature(1,1,gr.sizeof_int)) nop1 = gr.nop(gr.sizeof_int) self.assertRaises(ValueError, lambda: hblock.connect((hblock, 1), nop1)) def test_007_connect_invalid_dst_port_neg(self): hblock = gr.hier_block2("test_block", gr.io_signature(1,1,gr.sizeof_int), gr.io_signature(1,1,gr.sizeof_int)) nop1 = gr.nop(gr.sizeof_int) nop2 = gr.nop(gr.sizeof_int) self.assertRaises(ValueError, lambda: hblock.connect(nop1, (nop2, -1))) def test_008_connect_invalid_dst_port_exceeds(self): hblock = gr.hier_block2("test_block", gr.io_signature(1,1,gr.sizeof_int), gr.io_signature(1,1,gr.sizeof_int)) nop1 = gr.null_sink(gr.sizeof_int) nop2 = gr.null_sink(gr.sizeof_int) self.assertRaises(ValueError, lambda: hblock.connect(nop1, (nop2, 1))) def test_009_check_topology(self): hblock = gr.top_block("test_block") hblock.check_topology(0, 0) def test_010_run(self): expected = (1.0, 2.0, 3.0, 4.0) hblock = gr.top_block("test_block") src = gr.vector_source_f(expected, False) sink1 = gr.vector_sink_f() sink2 = gr.vector_sink_f() hblock.connect(src, sink1) hblock.connect(src, sink2) hblock.run() actual1 = sink1.data() actual2 = sink2.data() self.assertEquals(expected, actual1) self.assertEquals(expected, actual2) def test_012_disconnect_input(self): hblock = gr.hier_block2("test_block", gr.io_signature(1,1,gr.sizeof_int), gr.io_signature(1,1,gr.sizeof_int)) nop1 = gr.nop(gr.sizeof_int) hblock.connect(hblock, nop1) hblock.disconnect(hblock, nop1) def test_013_disconnect_input_not_connected(self): hblock = gr.hier_block2("test_block", gr.io_signature(1,1,gr.sizeof_int), gr.io_signature(1,1,gr.sizeof_int)) nop1 = gr.nop(gr.sizeof_int) nop2 = gr.nop(gr.sizeof_int) hblock.connect(hblock, nop1) self.assertRaises(ValueError, lambda: hblock.disconnect(hblock, nop2)) def test_014_disconnect_input_neg(self): hblock = gr.hier_block2("test_block", gr.io_signature(1,1,gr.sizeof_int), gr.io_signature(1,1,gr.sizeof_int)) nop1 = gr.nop(gr.sizeof_int) hblock.connect(hblock, nop1) self.assertRaises(ValueError, lambda: hblock.disconnect((hblock, -1), nop1)) def test_015_disconnect_input_exceeds(self): hblock = gr.hier_block2("test_block", gr.io_signature(1,1,gr.sizeof_int), gr.io_signature(1,1,gr.sizeof_int)) nop1 = gr.nop(gr.sizeof_int) hblock.connect(hblock, nop1) self.assertRaises(ValueError, lambda: hblock.disconnect((hblock, 1), nop1)) def test_016_disconnect_output(self): hblock = gr.hier_block2("test_block", gr.io_signature(1,1,gr.sizeof_int), gr.io_signature(1,1,gr.sizeof_int)) nop1 = gr.nop(gr.sizeof_int) hblock.connect(nop1, hblock) hblock.disconnect(nop1, hblock) def test_017_disconnect_output_not_connected(self): hblock = gr.hier_block2("test_block", gr.io_signature(1,1,gr.sizeof_int), gr.io_signature(1,1,gr.sizeof_int)) nop1 = gr.nop(gr.sizeof_int) nop2 = gr.nop(gr.sizeof_int) hblock.connect(nop1, hblock) self.assertRaises(ValueError, lambda: hblock.disconnect(nop2, hblock)) def test_018_disconnect_output_neg(self): hblock = gr.hier_block2("test_block", gr.io_signature(1,1,gr.sizeof_int), gr.io_signature(1,1,gr.sizeof_int)) nop1 = gr.nop(gr.sizeof_int) hblock.connect(hblock, nop1) self.assertRaises(ValueError, lambda: hblock.disconnect(nop1, (hblock, -1))) def test_019_disconnect_output_exceeds(self): hblock = gr.hier_block2("test_block", gr.io_signature(1,1,gr.sizeof_int), gr.io_signature(1,1,gr.sizeof_int)) nop1 = gr.nop(gr.sizeof_int) hblock.connect(nop1, hblock) self.assertRaises(ValueError, lambda: hblock.disconnect(nop1, (hblock, 1))) def test_020_run(self): hblock = gr.top_block("test_block") data = (1.0, 2.0, 3.0, 4.0) src = gr.vector_source_f(data, False) dst = gr.vector_sink_f() hblock.connect(src, dst) hblock.run() self.assertEquals(data, dst.data()) def test_021_connect_single(self): hblock = gr.top_block("test_block") blk = gr.hier_block2("block", gr.io_signature(0, 0, 0), gr.io_signature(0, 0, 0)) hblock.connect(blk) def test_022_connect_single_with_ports(self): hblock = gr.top_block("test_block") blk = gr.hier_block2("block", gr.io_signature(1, 1, 1), gr.io_signature(1, 1, 1)) self.assertRaises(ValueError, lambda: hblock.connect(blk)) def test_023_connect_single_twice(self): hblock = gr.top_block("test_block") blk = gr.hier_block2("block", gr.io_signature(0, 0, 0), gr.io_signature(0, 0, 0)) hblock.connect(blk) self.assertRaises(ValueError, lambda: hblock.connect(blk)) def test_024_disconnect_single(self): hblock = gr.top_block("test_block") blk = gr.hier_block2("block", gr.io_signature(0, 0, 0), gr.io_signature(0, 0, 0)) hblock.connect(blk) hblock.disconnect(blk) def test_025_disconnect_single_not_connected(self): hblock = gr.top_block("test_block") blk = gr.hier_block2("block", gr.io_signature(0, 0, 0), gr.io_signature(0, 0, 0)) self.assertRaises(ValueError, lambda: hblock.disconnect(blk)) def test_026_run_single(self): expected_data = (1.0,) tb = gr.top_block("top_block") hb = gr.hier_block2("block", gr.io_signature(0, 0, 0), gr.io_signature(0, 0, 0)) src = gr.vector_source_f(expected_data) dst = gr.vector_sink_f() hb.connect(src, dst) tb.connect(hb) tb.run() self.assertEquals(expected_data, dst.data()) def test_027a_internally_unconnected_input(self): tb = gr.top_block() hb = gr.hier_block2("block", gr.io_signature(1, 1, 1), gr.io_signature(1, 1, 1)) hsrc = gr.vector_source_b([1,]) hb.connect(hsrc, hb) # wire output internally src = gr.vector_source_b([1, ]) dst = gr.vector_sink_b() tb.connect(src, hb, dst) # hb's input is not connected internally self.assertRaises(RuntimeError, lambda: tb.run()) def test_027b_internally_unconnected_output(self): tb = gr.top_block() hb = gr.hier_block2("block", gr.io_signature(1, 1, 1), gr.io_signature(1, 1, 1)) hdst = gr.vector_sink_b() hb.connect(hb, hdst) # wire input internally src = gr.vector_source_b([1, ]) dst = gr.vector_sink_b() tb.connect(src, hb, dst) # hb's output is not connected internally self.assertRaises(RuntimeError, lambda: tb.run()) def test_027c_fully_unconnected_output(self): tb = gr.top_block() hb = gr.hier_block2("block", gr.io_signature(1, 1, 1), gr.io_signature(1, 1, 1)) hsrc = gr.vector_sink_b() hb.connect(hb, hsrc) # wire input internally src = gr.vector_source_b([1, ]) dst = gr.vector_sink_b() tb.connect(src, hb) # hb's output is not connected internally or externally self.assertRaises(RuntimeError, lambda: tb.run()) def test_027d_fully_unconnected_input(self): tb = gr.top_block() hb = gr.hier_block2("block", gr.io_signature(1, 1, 1), gr.io_signature(1, 1, 1)) hdst = gr.vector_source_b([1,]) hb.connect(hdst, hb) # wire output internally dst = gr.vector_sink_b() tb.connect(hb, dst) # hb's input is not connected internally or externally self.assertRaises(RuntimeError, lambda: tb.run()) def test_028_singleton_reconfigure(self): tb = gr.top_block() hb = gr.hier_block2("block", gr.io_signature(0, 0, 0), gr.io_signature(0, 0, 0)) src = gr.vector_source_b([1, ]) dst = gr.vector_sink_b() hb.connect(src, dst) tb.connect(hb) # Singleton connect tb.lock() tb.disconnect_all() tb.connect(src, dst) tb.unlock() def test_029_singleton_disconnect(self): tb = gr.top_block() src = gr.vector_source_b([1, ]) dst = gr.vector_sink_b() tb.connect(src, dst) tb.disconnect(src) # Singleton disconnect tb.connect(src, dst) tb.run() self.assertEquals(dst.data(), (1,)) def test_030_nested_input(self): tb = gr.top_block() src = gr.vector_source_b([1,]) hb1 = gr.hier_block2("hb1", gr.io_signature(1, 1, gr.sizeof_char), gr.io_signature(0, 0, 0)) hb2 = gr.hier_block2("hb2", gr.io_signature(1, 1, gr.sizeof_char), gr.io_signature(0, 0, 0)) dst = gr.vector_sink_b() tb.connect(src, hb1) hb1.connect(hb1, hb2) hb2.connect(hb2, gr.kludge_copy(gr.sizeof_char), dst) tb.run() self.assertEquals(dst.data(), (1,)) def test_031_multiple_internal_inputs(self): tb = gr.top_block() src = gr.vector_source_f([1.0,]) hb = gr.hier_block2("hb", gr.io_signature(1, 1, gr.sizeof_float), gr.io_signature(1, 1, gr.sizeof_float)) m1 = gr.multiply_const_ff(1.0) m2 = gr.multiply_const_ff(2.0) add = gr.add_ff() hb.connect(hb, m1) # m1 is connected to hb external input #0 hb.connect(hb, m2) # m2 is also connected to hb external input #0 hb.connect(m1, (add, 0)) hb.connect(m2, (add, 1)) hb.connect(add, hb) # add is connected to hb external output #0 dst = gr.vector_sink_f() tb.connect(src, hb, dst) tb.run() self.assertEquals(dst.data(), (3.0,)) def test_032_nested_multiple_internal_inputs(self): tb = gr.top_block() src = gr.vector_source_f([1.0,]) hb = gr.hier_block2("hb", gr.io_signature(1, 1, gr.sizeof_float), gr.io_signature(1, 1, gr.sizeof_float)) hb2 = gr.hier_block2("hb", gr.io_signature(1, 1, gr.sizeof_float), gr.io_signature(1, 1, gr.sizeof_float)) m1 = gr.multiply_const_ff(1.0) m2 = gr.multiply_const_ff(2.0) add = gr.add_ff() hb2.connect(hb2, m1) # m1 is connected to hb2 external input #0 hb2.connect(hb2, m2) # m2 is also connected to hb2 external input #0 hb2.connect(m1, (add, 0)) hb2.connect(m2, (add, 1)) hb2.connect(add, hb2) # add is connected to hb2 external output #0 hb.connect(hb, hb2, hb) # hb as hb2 as nested internal block dst = gr.vector_sink_f() tb.connect(src, hb, dst) tb.run() self.assertEquals(dst.data(), (3.0,)) if __name__ == "__main__": gr_unittest.run(test_hier_block2, "test_hier_block2.xml")
from ..backend_object import BackendObject class BoolResult(BackendObject): def __init__(self, op=None, args=None): self._op = op self._args = args def value(self): raise NotImplementedError() def __len__(self): return BackendError() def __eq__(self, other): raise NotImplementedError() def __and__(self, other): raise NotImplementedError() def __invert__(self): raise NotImplementedError() def __or__(self, other): raise NotImplementedError() def identical(self, other): if self.value != other.value: return False if self._op != other._op: return False if self._args != other._args: return False return True def union(self, other): raise NotImplementedError() def size(self): #pylint:disable=no-self-use return None @staticmethod def is_maybe(o): if isinstance(o, Base): raise ClaripyValueError("BoolResult can't handle AST objects directly") return isinstance(o, MaybeResult) @staticmethod def has_true(o): if isinstance(o, Base): raise ClaripyValueError("BoolResult can't handle AST objects directly") return o is True or (isinstance(o, BoolResult) and True in o.value) @staticmethod def has_false(o): if isinstance(o, Base): raise ClaripyValueError("BoolResult can't handle AST objects directly") return o is False or (isinstance(o, BoolResult) and False in o.value) @staticmethod def is_true(o): if isinstance(o, Base): raise ClaripyValueError("BoolResult can't handle AST objects directly") return o is True or (isinstance(o, TrueResult)) @staticmethod def is_false(o): if isinstance(o, Base): raise ClaripyValueError("BoolResult can't handle AST objects directly") return o is False or (isinstance(o, FalseResult)) class TrueResult(BoolResult): cardinality = 1 @property def value(self): return (True, ) def identical(self, other): return isinstance(other, TrueResult) def __eq__(self, other): if isinstance(other, FalseResult): return FalseResult() elif isinstance(other, TrueResult): return TrueResult() else: return MaybeResult() def __invert__(self): return FalseResult() def __or__(self, other): return TrueResult() def __and__(self, other): if BoolResult.is_maybe(other): return MaybeResult() elif BoolResult.is_false(other): return FalseResult() else: return TrueResult() def union(self, other): if other is True or type(other) is TrueResult: return TrueResult() elif other is False or type(other) is FalseResult: return MaybeResult() elif type(other) is MaybeResult: return MaybeResult() else: return NotImplemented def __repr__(self): return '<True>' class FalseResult(BoolResult): cardinality = 1 @property def value(self): return (False, ) def identical(self, other): return isinstance(other, FalseResult) def __eq__(self, other): if isinstance(other, FalseResult): return TrueResult() elif isinstance(other, TrueResult): return FalseResult() else: return MaybeResult() def __invert__(self): return TrueResult() def __and__(self, other): return FalseResult() def __or__(self, other): return other def __repr__(self): return '<False>' def union(self, other): if other is True or type(other) is TrueResult: return MaybeResult() elif other is False or type(other) is FalseResult: return FalseResult() elif type(other) is MaybeResult: return MaybeResult() else: return NotImplemented class MaybeResult(BoolResult): cardinality = 2 @property def value(self): return (True, False) def identical(self, other): return isinstance(other, MaybeResult) def __eq__(self, other): return MaybeResult() def __invert__(self): return MaybeResult() def __and__(self, other): if BoolResult.is_false(other): return FalseResult() else: return MaybeResult() def union(self, other): return MaybeResult() def __or__(self, other): if BoolResult.is_true(other): return TrueResult() else: return self def __repr__(self): if self._op is None: return '<Maybe>' else: return '<Maybe(%s, %s)>' % (self._op, self._args) from ..errors import BackendError, ClaripyValueError from ..ast.base import Base
# -*- coding: utf-8 -*- """ Created on 2018/3/14 @author: will4906  """ from bs4 import BeautifulSoup from controller.url_config import url_search, url_detail, url_related_info, url_full_text from crawler.items import DataItem from entity.crawler_item import BaseItem, ResultItem class PatentId(BaseItem): is_required = True crawler_id = url_search.get('crawler_id') english = 'patent_id' chinese = ['', 'id', 'ID', 'Id'] @classmethod def parse(cls, raw, item, process=None): if process is not None: patent_id = process.find(attrs={'name': 'idHidden'}).get('value') item.patent_id = ResultItem(title=cls.title, value=str(patent_id)) return item class PatentName(BaseItem): is_required = True crawler_id = url_detail.get('crawler_id') english = ['patent_name', 'invention_name'] chinese = '' @classmethod def parse(cls, raw, item, process=None): if process is not None: patent_name = process.get('abstractInfoDTO').get('tioIndex').get('value') item.patent_name = ResultItem(title=cls.title, value=str(patent_name)) return item class Abstract(BaseItem): crawler_id = url_detail.get('crawler_id') english = 'abstract' chinese = '' @classmethod def parse(cls, raw, item, process=None): if process is not None: abstract = BeautifulSoup(process.get('abstractInfoDTO').get('abIndexList')[0].get('value'), 'lxml').text.replace('\n', '').strip() item.abstract = ResultItem(title=cls.title, value=abstract) return item def push_item(json_list, item: DataItem, title, name): """ detail :param json_list: :param item: :param title: :param name: :return: """ if json_list is not None: aitem_list = json_list.get('abstractInfoDTO').get('abstractItemList') for a_item in aitem_list: if a_item.get('indexCnName').find(name) != -1: item.__setattr__(title, ResultItem(title=name, value=a_item.get('value'))) break if not hasattr(item, title): item.__setattr__(title, ResultItem(title=name, value="")) return item class RequestNumber(BaseItem): crawler_id = url_detail.get('crawler_id') english = ['request_number', 'application_number'] chinese = '' @classmethod def parse(cls, raw, item, process=None): return push_item(process, item, 'request_number', '') class RequestDate(BaseItem): crawler_id = url_detail.get('crawler_id') english = 'request_date' chinese = '' @classmethod def parse(cls, raw, item, process=None): return push_item(process, item, 'request_date', '') class PublishNumber(BaseItem): crawler_id = url_detail.get('crawler_id') english = 'publish_number' chinese = ['', '', ''] @classmethod def parse(cls, raw, item, process=None): return push_item(process, item, 'publish_number', '') class PublishDate(BaseItem): crawler_id = url_detail.get('crawler_id') english = 'publish_date' chinese = ['', '', ''] @classmethod def parse(cls, raw, item, process=None): return push_item(process, item, 'publish_date', '') class IpcClassificationNumber(BaseItem): crawler_id = url_detail.get('crawler_id') english = ['ipc_class_number', 'IPC', 'ipc', 'Ipc'] chinese = 'IPC' @classmethod def parse(cls, raw, item, process=None): return push_item(process, item, 'ipc_class_number', 'IPC') class Applicant(BaseItem): crawler_id = url_detail.get('crawler_id') english = ['Applicant', 'applicant', 'assignee', 'Assignee', 'proposer'] chinese = ['', '', '', ''] @classmethod def parse(cls, raw, item, process=None): return push_item(process, item, 'applicant', '') class Inventor(BaseItem): crawler_id = url_detail.get('crawler_id') english = ['Inventor', 'inventor'] chinese = '' @classmethod def parse(cls, raw, item, process=None): return push_item(process, item, 'inventor', '') class PriorityNumber(BaseItem): crawler_id = url_detail.get('crawler_id') english = 'priority_number' chinese = '' @classmethod def parse(cls, raw, item, process=None): return push_item(process, item, 'priority_number', '') class PriorityDate(BaseItem): crawler_id = url_detail.get('crawler_id') english = 'priority_date' chinese = '' @classmethod def parse(cls, raw, item, process=None): return push_item(process, item, 'priority_date', '') class AddressOfApplicant(BaseItem): crawler_id = url_detail.get('crawler_id') english = ['proposer_address', 'address_of_the_Applicant', 'applicant_address'] chinese = '' @classmethod def parse(cls, raw, item, process=None): if process is not None: item = push_item(process, item, 'proposer_address', '') return item class ZipCodeOfTheApplicant(BaseItem): crawler_id = url_detail.get('crawler_id') english = ['proposer_post_code', 'zip_code_of_the_applicant', 'proposer_zip_code'] chinese = '' @classmethod def parse(cls, raw, item, process=None): return push_item(process, item, 'proposer_zip_code', '') class CountryOfTheApplicant(BaseItem): crawler_id = url_detail.get('crawler_id') english = ['proposer_location', 'country_of_the_applicant', 'country_of_the_assignee'] chinese = ['', ''] @classmethod def parse(cls, raw, item, process=None): return push_item(process, item, 'proposer_location', '') class CpcClassificationNumber(BaseItem): crawler_id = url_detail.get('crawler_id') english = ['cpc_class_number', 'cpc', 'CPC', 'Cpc'] chinese = 'CPC' @classmethod def parse(cls, raw, item, process=None): return push_item(process, item, 'cpc_class_number', 'CPC') class Cognation(BaseItem): crawler_id = url_related_info.get('crawler_id') table_name = 'cognation' english = 'cognation_list' chinese = '' title = '' @classmethod def parse(cls, raw, item, process=None): if process is not None: cognation_list = process.get('cognationList') # print('cognation', cognation_list) if cognation_list is not None: pn_list = [] for cog in cognation_list: pn_list.append(cog.get('pn')) item.cognation_list = ResultItem(table=cls.table_name, title=cls.title, value=pn_list) return item class LawStateList(BaseItem): crawler_id = url_related_info.get('crawler_id') table_name = 'law_state' english = 'law_state_list' chinese = '' title = ['', ''] @classmethod def set_title(cls, title): if title == cls.english: cls.title = ['law_status', 'law_status_date'] elif title == cls.chinese: cls.title = ['', ''] @classmethod def parse(cls, raw, item, process=None): if process is not None: law_state_list = process.get('lawStateList') if law_state_list is not None: tmp_list = [] for law in law_state_list: mean = law.get('lawStateCNMeaning') law_date = law.get('prsDate') part = (ResultItem(table=cls.table_name, title=cls.title[0], value=mean), ResultItem(table=cls.table_name, title=cls.title[1], value=law_date)) tmp_list.append(part) item.law_state_list = tmp_list return item class FullText(BaseItem): crawler_id = url_full_text.get('crawler_id') english = ['full_text', 'whole_text'] chinese = ['', ''] @classmethod def parse(cls, raw, item, process=None): if process is not None: item.full_text = ResultItem(table=cls.table_name, title=cls.title, value=BeautifulSoup(str(process.get('fullTextDTO').get('literaInfohtml')), 'lxml') .get_text().replace("'", '"').replace(';', ',')) return item
"""Minio Test event.""" TEST_EVENT = { "Records": [ { "eventVersion": "2.0", "eventSource": "minio:s3", "awsRegion": "", "eventTime": "2019-05-02T11:05:07Z", "eventName": "s3:ObjectCreated:Put", "userIdentity": {"principalId": "SO9KNO6YT9OGE39PQCZW"}, "requestParameters": { "accessKey": "SO9KNO6YT9OGE39PQCZW", "region": "", "sourceIPAddress": "172.27.0.1", }, "responseElements": { "x-amz-request-id": "159AD8E6F6805783", "x-minio-deployment-id": "90b265b8-bac5-413a-b12a-8915469fd769", "x-minio-origin-endpoint": "http://172.27.0.2:9000", }, "s3": { "s3SchemaVersion": "1.0", "configurationId": "Config", "bucket": { "name": "test", "ownerIdentity": {"principalId": "SO9KNO6YT9OGE39PQCZW"}, "arn": "arn:aws:s3:::test", }, "object": { "key": "5jJkTAo.jpg", "size": 108368, "eTag": "1af324731637228cbbb0b2e8c07d4e50", "contentType": "image/jpeg", "userMetadata": {"content-type": "image/jpeg"}, "versionId": "1", "sequencer": "159AD8E6F76DD9C4", }, }, "source": { "host": "", "port": "", "userAgent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) " "AppleWebKit/605.1.15 (KHTML, like Gecko) " "Version/12.0.3 Safari/605.1.15", }, } ] }
"""Tests for distutils.pypirc.pypirc.""" import sys import os import unittest import tempfile from distutils.core import PyPIRCCommand from distutils.core import Distribution from distutils.log import set_threshold from distutils.log import WARN from distutils.tests import support from test.support import run_unittest PYPIRC = """\ [distutils] index-servers = server1 server2 [server1] username:me password:secret [server2] username:meagain password: secret realm:acme repository:http://another.pypi/ """ PYPIRC_OLD = """\ [server-login] username:tarek password:secret """ WANTED = """\ [distutils] index-servers = pypi [pypi] username:tarek password:xxx """ class PyPIRCCommandTestCase(support.TempdirManager, support.LoggingSilencer, support.EnvironGuard, unittest.TestCase): def setUp(self): """Patches the environment.""" super(PyPIRCCommandTestCase, self).setUp() self.tmp_dir = self.mkdtemp() os.environ['HOME'] = self.tmp_dir self.rc = os.path.join(self.tmp_dir, '.pypirc') self.dist = Distribution() class command(PyPIRCCommand): def __init__(self, dist): PyPIRCCommand.__init__(self, dist) def initialize_options(self): pass finalize_options = initialize_options self._cmd = command self.old_threshold = set_threshold(WARN) def tearDown(self): """Removes the patch.""" set_threshold(self.old_threshold) super(PyPIRCCommandTestCase, self).tearDown() def test_server_registration(self): # This test makes sure PyPIRCCommand knows how to: # 1. handle several sections in .pypirc # 2. handle the old format # new format self.write_file(self.rc, PYPIRC) cmd = self._cmd(self.dist) config = cmd._read_pypirc() config = list(sorted(config.items())) waited = [('password', 'secret'), ('realm', 'pypi'), ('repository', 'https://pypi.python.org/pypi'), ('server', 'server1'), ('username', 'me')] self.assertEqual(config, waited) # old format self.write_file(self.rc, PYPIRC_OLD) config = cmd._read_pypirc() config = list(sorted(config.items())) waited = [('password', 'secret'), ('realm', 'pypi'), ('repository', 'https://pypi.python.org/pypi'), ('server', 'server-login'), ('username', 'tarek')] self.assertEqual(config, waited) def test_server_empty_registration(self): cmd = self._cmd(self.dist) rc = cmd._get_rc_file() self.assertFalse(os.path.exists(rc)) cmd._store_pypirc('tarek', 'xxx') self.assertTrue(os.path.exists(rc)) f = open(rc) try: content = f.read() self.assertEqual(content, WANTED) finally: f.close() def test_suite(): return unittest.makeSuite(PyPIRCCommandTestCase) if __name__ == "__main__": run_unittest(test_suite())
from django.conf.urls import include, url from .utils import URLObject from .views import empty_view, view_class_instance testobj3 = URLObject('testapp', 'test-ns3') testobj4 = URLObject('testapp', 'test-ns4') app_name = 'included_namespace_urls' urlpatterns = [ url(r'^normal/$', empty_view, name='inc-normal-view'), url(r'^normal/(?P<arg1>[0-9]+)/(?P<arg2>[0-9]+)/$', empty_view, name='inc-normal-view'), url(r'^\+\\\$\*/$', empty_view, name='inc-special-view'), url(r'^mixed_args/([0-9]+)/(?P<arg2>[0-9]+)/$', empty_view, name='inc-mixed-args'), url(r'^no_kwargs/([0-9]+)/([0-9]+)/$', empty_view, name='inc-no-kwargs'), url(r'^view_class/(?P<arg1>[0-9]+)/(?P<arg2>[0-9]+)/$', view_class_instance, name='inc-view-class'), url(r'^test3/', include(*testobj3.urls)), url(r'^test4/', include(*testobj4.urls)), url(r'^ns-included3/', include(('urlpatterns_reverse.included_urls', 'included_urls'), namespace='inc-ns3')), url(r'^ns-included4/', include('urlpatterns_reverse.namespace_urls', namespace='inc-ns4')), ]
# ex:ts=4:sw=4:sts=4:et # -*- tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- from __future__ import absolute_import import subprocess import shlex from svtplay_dl.log import log from svtplay_dl.utils import is_py2 from svtplay_dl.fetcher import VideoRetriever from svtplay_dl.output import output class RTMP(VideoRetriever): def name(self): return "rtmp" def download(self): """ Get the stream from RTMP """ args = [] if self.options.live: args.append("-v") if self.options.resume: args.append("-e") file_d = output(self.options, "flv", False) if file_d is None: return args += ["-o", self.options.output] if self.options.silent or self.options.output == "-": args.append("-q") if self.options.other: if is_py2: args += shlex.split(self.options.other.encode("utf-8")) else: args += shlex.split(self.options.other) if self.options.verbose: args.append("-V") command = ["rtmpdump", "-r", self.url] + args log.debug("Running: %s", " ".join(command)) try: subprocess.call(command) except OSError as e: log.error("Could not execute rtmpdump: " + e.strerror) return self.finished = True
# -*- coding: utf-8 -*- ############################################################################## # # OpenERP, Open Source Management Solution # Copyright (C) 2004-2009 P. Christeas, Tiny SPRL (<http://tiny.be>). # Copyright (C) 2010-2013 OpenERP SA. (http://www.openerp.com) # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## import glob import logging import os import platform from reportlab import rl_config from openerp.tools import config #.apidoc title: TTF Font Table """This module allows the mapping of some system-available TTF fonts to the reportlab engine. This file could be customized per distro (although most Linux/Unix ones) should have the same filenames, only need the code below). Due to an awful configuration that ships with reportlab at many Linux and Ubuntu distros, we have to override the search path, too. """ _logger = logging.getLogger(__name__) CustomTTFonts = [ ('Helvetica',"DejaVu Sans", "DejaVuSans.ttf", 'normal'), ('Helvetica',"DejaVu Sans Bold", "DejaVuSans-Bold.ttf", 'bold'), ('Helvetica',"DejaVu Sans Oblique", "DejaVuSans-Oblique.ttf", 'italic'), ('Helvetica',"DejaVu Sans BoldOblique", "DejaVuSans-BoldOblique.ttf", 'bolditalic'), ('Times',"Liberation Serif", "LiberationSerif-Regular.ttf", 'normal'), ('Times',"Liberation Serif Bold", "LiberationSerif-Bold.ttf", 'bold'), ('Times',"Liberation Serif Italic", "LiberationSerif-Italic.ttf", 'italic'), ('Times',"Liberation Serif BoldItalic", "LiberationSerif-BoldItalic.ttf", 'bolditalic'), ('Times-Roman',"Liberation Serif", "LiberationSerif-Regular.ttf", 'normal'), ('Times-Roman',"Liberation Serif Bold", "LiberationSerif-Bold.ttf", 'bold'), ('Times-Roman',"Liberation Serif Italic", "LiberationSerif-Italic.ttf", 'italic'), ('Times-Roman',"Liberation Serif BoldItalic", "LiberationSerif-BoldItalic.ttf", 'bolditalic'), ('Courier',"FreeMono", "FreeMono.ttf", 'normal'), ('Courier',"FreeMono Bold", "FreeMonoBold.ttf", 'bold'), ('Courier',"FreeMono Oblique", "FreeMonoOblique.ttf", 'italic'), ('Courier',"FreeMono BoldOblique", "FreeMonoBoldOblique.ttf", 'bolditalic'), # Sun-ExtA can be downloaded from http://okuc.net/SunWb/ ('Sun-ExtA',"Sun-ExtA", "Sun-ExtA.ttf", 'normal'), ] TTFSearchPath_Linux = [ '/usr/share/fonts/truetype', # SuSE '/usr/share/fonts/dejavu', '/usr/share/fonts/liberation', # Fedora, RHEL '/usr/share/fonts/truetype/*', # Ubuntu, '/usr/share/fonts/TTF/*', # at Mandriva/Mageia '/usr/share/fonts/TTF', # Arch Linux ] TTFSearchPath_Windows = [ 'c:/winnt/fonts', 'c:/windows/fonts' ] TTFSearchPath_Darwin = [ #mac os X - from #http://developer.apple.com/technotes/tn/tn2024.html '~/Library/Fonts', '/Library/Fonts', '/Network/Library/Fonts', '/System/Library/Fonts', ] TTFSearchPathMap = { 'Darwin': TTFSearchPath_Darwin, 'Windows': TTFSearchPath_Windows, 'Linux': TTFSearchPath_Linux, } # ----- The code below is less distro-specific, please avoid editing! ------- __foundFonts = None def FindCustomFonts(): """Fill the __foundFonts list with those filenames, whose fonts can be found in the reportlab ttf font path. This process needs only be done once per loading of this module, it is cached. But, if the system admin adds some font in the meanwhile, the server must be restarted eventually. """ dirpath = [] global __foundFonts __foundFonts = {} searchpath = [] if config.get('fonts_search_path'): searchpath += map(str.strip, config.get('fonts_search_path').split(',')) local_platform = platform.system() if local_platform in TTFSearchPathMap: searchpath += TTFSearchPathMap[local_platform] # Append the original search path of reportlab (at the end) searchpath += rl_config.TTFSearchPath # Perform the search for font files ourselves, as reportlab's # TTFOpenFile is not very good at it. for dirglob in searchpath: dirglob = os.path.expanduser(dirglob) for dirname in glob.iglob(dirglob): abp = os.path.abspath(dirname) if os.path.isdir(abp): dirpath.append(abp) for k, (name, font, filename, mode) in enumerate(CustomTTFonts): if filename in __foundFonts: continue for d in dirpath: abs_filename = os.path.join(d, filename) if os.path.exists(abs_filename): _logger.debug("Found font %s at %s", filename, abs_filename) __foundFonts[filename] = abs_filename break def SetCustomFonts(rmldoc): """ Map some font names to the corresponding TTF fonts The ttf font may not even have the same name, as in Times -> Liberation Serif. This function is called once per report, so it should avoid system-wide processing (cache it, instead). """ global __foundFonts if __foundFonts is None: FindCustomFonts() for name, font, filename, mode in CustomTTFonts: if os.path.isabs(filename) and os.path.exists(filename): rmldoc.setTTFontMapping(name, font, filename, mode) elif filename in __foundFonts: rmldoc.setTTFontMapping(name, font, __foundFonts[filename], mode) return True #eof # vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
#!/usr/bin/env python # (c) 2014, Timothy Vandenbrande <timothy.vandenbrande@gmail.com> # # This file is part of Ansible # # Ansible is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Ansible is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Ansible. If not, see <http://www.gnu.org/licenses/>. ANSIBLE_METADATA = {'metadata_version': '1.0', 'status': ['preview'], 'supported_by': 'community'} DOCUMENTATION = r''' --- module: win_firewall_rule version_added: "2.0" author: Timothy Vandenbrande short_description: Windows firewall automation description: - allows you to create/remove/update firewall rules options: enable: description: - is this firewall rule enabled or disabled default: true required: false state: description: - should this rule be added or removed default: "present" required: true choices: ['present', 'absent'] name: description: - the rules name default: null required: true direction: description: - is this rule for inbound or outbound traffic default: null required: true choices: ['in', 'out'] action: description: - what to do with the items this rule is for default: null required: true choices: ['allow', 'block', 'bypass'] description: description: - description for the firewall rule default: null required: false localip: description: - the local ip address this rule applies to default: 'any' required: false remoteip: description: - the remote ip address/range this rule applies to default: 'any' required: false localport: description: - the local port this rule applies to default: 'any' required: false remoteport: description: - the remote port this rule applies to default: 'any' required: false program: description: - the program this rule applies to default: null required: false service: description: - the service this rule applies to default: 'any' required: false protocol: description: - the protocol this rule applies to default: 'any' required: false profile: description: - the profile this rule applies to, e.g. Domain,Private,Public default: 'any' required: false force: description: - Enforces the change if a rule with different values exists default: false required: false ''' EXAMPLES = r''' - name: Firewall rule to allow smtp on TCP port 25 action: win_firewall_rule args: name: smtp enable: yes state: present localport: 25 action: allow direction: In protocol: TCP '''
# Copyright David Abrahams 2004. Distributed under the Boost # Software License, Version 1.0. (See accompanying # file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt) ''' >>> from shared_ptr_ext import * Test that shared_ptr<Derived> can be converted to shared_ptr<Base> >>> Y.store(YYY(42)) >>> x = X(17) >>> null_x = null(x) >>> null_x # should be None >>> identity(null_x) # should also be None >>> a = New(1) >>> A.call_f(a) 1 >>> New(0) >>> type(factory(3)) <class 'shared_ptr_ext.Y'> >>> type(factory(42)) <class 'shared_ptr_ext.YY'> >>> class P(Z): ... def v(self): ... return -Z.v(self); ... def __del__(self): ... print 'bye' ... >>> p = P(12) >>> p.value() 12 >>> p.v() -12 >>> look(p) 12 >>> try: modify(p) ... except TypeError: pass ... else: 'print expected a TypeError' >>> look(None) -1 >>> store(p) >>> del p >>> Z.get().v() -12 >>> Z.count() 1 >>> Z.look_store() 12 >>> Z.release() bye >>> Z.count() 0 >>> z = Z(13) >>> z.value() 13 >>> z.v() 13 >>> try: modify(z) ... except TypeError: pass ... else: 'print expected a TypeError' >>> Z.get() # should be None >>> store(z) >>> assert Z.get() is z # show that deleter introspection works >>> del z >>> Z.get().value() 13 >>> Z.count() 1 >>> Z.look_store() 13 >>> Z.release() >>> Z.count() 0 >>> x = X(17) >>> x.value() 17 >>> look(x) 17 >>> try: modify(x) ... except TypeError: pass ... else: 'print expected a TypeError' >>> look(None) -1 >>> store(x) >>> del x >>> X.count() 1 >>> X.look_store() 17 >>> X.release() >>> X.count() 0 >>> y = Y(19) >>> y.value() 19 >>> modify(y) >>> look(y) -1 >>> store(Y(23)) >>> Y.count() 1 >>> Y.look_store() 23 >>> Y.release() >>> Y.count() 0 ''' def run(args = None): import sys import doctest if args is not None: sys.argv = args return doctest.testmod(sys.modules.get(__name__)) if __name__ == '__main__': print "running..." import sys status = run()[0] if (status == 0): print "Done." sys.exit(status)
# -*- coding: utf-8 -*- from __future__ import unicode_literals from pelican.tests.support import unittest from pelican.urlwrappers import Author, Category, Tag, URLWrapper class TestURLWrapper(unittest.TestCase): def test_ordering(self): # URLWrappers are sorted by name wrapper_a = URLWrapper(name='first', settings={}) wrapper_b = URLWrapper(name='last', settings={}) self.assertFalse(wrapper_a > wrapper_b) self.assertFalse(wrapper_a >= wrapper_b) self.assertFalse(wrapper_a == wrapper_b) self.assertTrue(wrapper_a != wrapper_b) self.assertTrue(wrapper_a <= wrapper_b) self.assertTrue(wrapper_a < wrapper_b) wrapper_b.name = 'first' self.assertFalse(wrapper_a > wrapper_b) self.assertTrue(wrapper_a >= wrapper_b) self.assertTrue(wrapper_a == wrapper_b) self.assertFalse(wrapper_a != wrapper_b) self.assertTrue(wrapper_a <= wrapper_b) self.assertFalse(wrapper_a < wrapper_b) wrapper_a.name = 'last' self.assertTrue(wrapper_a > wrapper_b) self.assertTrue(wrapper_a >= wrapper_b) self.assertFalse(wrapper_a == wrapper_b) self.assertTrue(wrapper_a != wrapper_b) self.assertFalse(wrapper_a <= wrapper_b) self.assertFalse(wrapper_a < wrapper_b) def test_equality(self): tag = Tag('test', settings={}) cat = Category('test', settings={}) author = Author('test', settings={}) # same name, but different class self.assertNotEqual(tag, cat) self.assertNotEqual(tag, author) # should be equal vs text representing the same name self.assertEqual(tag, u'test') # should not be equal vs binary self.assertNotEqual(tag, b'test') # Tags describing the same should be equal tag_equal = Tag('Test', settings={}) self.assertEqual(tag, tag_equal) # Author describing the same should be equal author_equal = Author('Test', settings={}) self.assertEqual(author, author_equal) cat_ascii = Category('', settings={}) self.assertEqual(cat_ascii, u'zhi-dao-shu') def test_slugify_with_substitutions_and_dots(self): tag = Tag('Tag Dot', settings={ 'TAG_SUBSTITUTIONS': [('Tag Dot', 'tag.dot', True)] }) cat = Category('Category Dot', settings={ 'CATEGORY_SUBSTITUTIONS': (('Category Dot', 'cat.dot', True),) }) self.assertEqual(tag.slug, 'tag.dot') self.assertEqual(cat.slug, 'cat.dot') def test_author_slug_substitutions(self): settings = { 'AUTHOR_SUBSTITUTIONS': [ ('Alexander Todorov', 'atodorov', False), ('Krasimir Tsonev', 'krasimir', False), ] } author1 = Author('Mr. Senko', settings=settings) author2 = Author('Alexander Todorov', settings=settings) author3 = Author('Krasimir Tsonev', settings=settings) self.assertEqual(author1.slug, 'mr-senko') self.assertEqual(author2.slug, 'atodorov') self.assertEqual(author3.slug, 'krasimir')
""" This module defines a class used to represent a choice for an enumerated-value field. """ from __future__ import unicode_literals from __future__ import absolute_import, division, print_function __author__ = "Graham Klyne (GK@ACM.ORG)" __copyright__ = "Copyright 2015, G. Klyne" __license__ = "MIT (http://opensource.org/licenses/MIT)" import re import logging log = logging.getLogger(__name__) from collections import namedtuple from utils.py3porting import to_unicode from django.utils.html import format_html, mark_safe, escape _FieldChoice_tuple = namedtuple("FieldChoice", ("id", "value", "label", "link", "choice_value")) class FieldChoice(_FieldChoice_tuple): """ Class representing a choice for an enumerated field. >>> c1 = FieldChoice('id1', 'value1', 'label1', 'link1', choice_value=True) >>> c1 == FieldChoice(id='id1', value='value1', label='label1', link='link1', choice_value=True) True >>> c1.id == 'id1' True >>> c1.value == 'value1' True >>> c1.label == 'label1' True >>> c1.link == 'link1' True >>> c1.choice_html() == u'label1&nbsp;&nbsp;&nbsp;(value1)' True >>> c2 = FieldChoice('id2', 'value2', 'label2', 'link2', choice_value=False) >>> c2 == FieldChoice(id='id2', value='value2', label='label2', link='link2', choice_value=False) True >>> c2.id == 'id2' True >>> c2.value == 'value2' True >>> c2.label == 'label2' True >>> c2.link == 'link2' True >>> c2.choice() == u'label2' True >>> c3 = FieldChoice(id='id3', value='value3', link='link3') >>> c3 == FieldChoice(id='id3', value='value3', label='value3', link='link3', choice_value=False) True >>> c3.id == 'id3' True >>> c3.value == 'value3' True >>> c3.label == 'value3' True >>> c3.link == 'link3' True >>> c4 = FieldChoice('id4', link='link4') >>> c4 == FieldChoice(id='id4', value='id4', label='id4', link='link4', choice_value=False) True >>> c4.id == 'id4' True >>> c4.value == 'id4' True >>> c4.label == 'id4' True >>> c4.link == 'link4' True >>> c5 = FieldChoice('') >>> c5 == FieldChoice(id='', value='', label='', link=None, choice_value=False) True """ def __new__(_cls, id=None, value=None, label=None, link=None, choice_value=False): if value is None: value = id if label is None: label = value result = super(FieldChoice, _cls).__new__(_cls, id, value, label, link, choice_value) return result def __eq__(self, other): """ Returns True if self == other for sorting and equivalence purposes """ return self.id.__eq__(other.id) def __ne__(self, other): """ Returns True if self != other for sorting and equivalence purposes Note: required for Python2. """ return self.id.__ne__(other.id) def __lt__(self, other): """ Returns True if self < other for sorting purposes """ return self.id.__lt__(other.id) def __hash__(self): """ pylint says this should be defined if __eq__ is defined. Something to do with sets? """ return hash(self.id) def choice(self, sep=u"\xa0\xa0\xa0"): """ Return choice string """ if self.choice_value: choice_text = self.option_label(sep=sep) else: choice_text = to_unicode(self.label) return choice_text def choice_html(self, sep=u"&nbsp;&nbsp;&nbsp;"): """ Return choice string HTML for option in drop-down list. """ return self.choice(sep=sep) def add_link(self, link=None): return FieldChoice(self.id, self.value, self.label, link) def option_label(self, sep=u"\xa0\xa0\xa0"): """ Returns string used for displayed option label. This function is used mainly for testing, to isolate details of option presentation from the majority of test cases. """ if self.label: return format_html(u"{}{}({})", self.label, mark_safe(sep), self.value) else: return escape(self.value) def option_label_html(self, sep=u"&nbsp;&nbsp;&nbsp;"): """ Variation of option_label returns HTML-encoded form of label text """ return self.option_label(sep=sep) def update_choice_labels(fieldchoices): """ Update choice labels in supplied list of FieldChoice values so that duplicate labels can be distinguished. Returns an updated list of options. """ # Detect non-unique labels labels = {} for o in fieldchoices: l = o.label labels[l] = labels.get(l, 0) + 1 # Generate updated choice values new_choices = [] for o in fieldchoices: if labels[o.label] > 1: new_choices.append( FieldChoice(id=o.id, value=o.value, label=o.label, link=o.link, choice_value=True) ) else: new_choices.append(o) return new_choices def get_choice_labels(fieldchoices): """ Return a list of choice labels based on the supplied list of FieldChoice values >>> c1 = FieldChoice('id1', 'value1', 'label1', 'link1') >>> c2a = FieldChoice('id2a', 'value2a', 'label2', 'link2') >>> c2b = FieldChoice('id2b', 'value2b', 'label2', 'link2') >>> labels = get_choice_labels([c1,c2a,c2b]) >>> labels == ['label1', u'label2\\xa0\\xa0\\xa0(value2a)', u'label2\\xa0\\xa0\\xa0(value2b)'] True """ return [ fc.choice() for fc in update_choice_labels(fieldchoices) ] if __name__ == "__main__": import doctest doctest.testmod() # End.
#!/usr/bin/python2 # -*- coding: utf-8 -*- import sys import matplotlib.pyplot as plt import numpy as np from sklearn import datasets from sklearn.cross_validation import cross_val_predict from sklearn import linear_model from sklearn import datasets X = [] Y = [] for line in sys.stdin: line = line.rstrip() X.append([len(line.split())]) Y.append(line.count(",")) lr = linear_model.LinearRegression() predicted = cross_val_predict(lr, X, Y) FILE = open(sys.argv[1], "r") X_TEST = [] Y_TEST = [] for line in FILE: line = line.rstrip() Y_TEST.append(line.count(",")) line = line.replace(",", "") X_TEST.append([len(line.split())]) regr = linear_model.LinearRegression() regr.fit(X, Y) print "Coefficients: ", regr.coef_ print "Residual sum of squares: %.2f" % np.mean((regr.predict(X_TEST) - Y_TEST) ** 2) print "Variance score: %.2f" % regr.score(X_TEST, Y_TEST) plt.scatter(X_TEST, Y_TEST, color='black') plt.plot(X_TEST, regr.predict(X_TEST), color='green', linewidth=2) plt.xticks(()) plt.yticks(()) plt.show()
# vim: tabstop=4 shiftwidth=4 softtabstop=4 # Copyright 2010 OpenStack LLC. # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from xml.dom import minidom import webob import webob.dec import webob.exc from nova.api.openstack import common from nova.api.openstack import wsgi from nova.openstack.common import jsonutils from nova import test class TestFaults(test.TestCase): """Tests covering `nova.api.openstack.faults:Fault` class.""" def _prepare_xml(self, xml_string): """Remove characters from string which hinder XML equality testing.""" xml_string = xml_string.replace(" ", "") xml_string = xml_string.replace("\n", "") xml_string = xml_string.replace("\t", "") return xml_string def test_400_fault_json(self): # Test fault serialized to JSON via file-extension and/or header. requests = [ webob.Request.blank('/.json'), webob.Request.blank('/', headers={"Accept": "application/json"}), ] for request in requests: fault = wsgi.Fault(webob.exc.HTTPBadRequest(explanation='scram')) response = request.get_response(fault) expected = { "badRequest": { "message": "scram", "code": 400, }, } actual = jsonutils.loads(response.body) self.assertEqual(response.content_type, "application/json") self.assertEqual(expected, actual) def test_413_fault_json(self): # Test fault serialized to JSON via file-extension and/or header. requests = [ webob.Request.blank('/.json'), webob.Request.blank('/', headers={"Accept": "application/json"}), ] for request in requests: exc = webob.exc.HTTPRequestEntityTooLarge fault = wsgi.Fault(exc(explanation='sorry', headers={'Retry-After': 4})) response = request.get_response(fault) expected = { "overLimit": { "message": "sorry", "code": 413, "retryAfter": 4, }, } actual = jsonutils.loads(response.body) self.assertEqual(response.content_type, "application/json") self.assertEqual(expected, actual) def test_raise(self): # Ensure the ability to raise :class:`Fault` in WSGI-ified methods. @webob.dec.wsgify def raiser(req): raise wsgi.Fault(webob.exc.HTTPNotFound(explanation='whut?')) req = webob.Request.blank('/.xml') resp = req.get_response(raiser) self.assertEqual(resp.content_type, "application/xml") self.assertEqual(resp.status_int, 404) self.assertTrue('whut?' in resp.body) def test_raise_403(self): # Ensure the ability to raise :class:`Fault` in WSGI-ified methods. @webob.dec.wsgify def raiser(req): raise wsgi.Fault(webob.exc.HTTPForbidden(explanation='whut?')) req = webob.Request.blank('/.xml') resp = req.get_response(raiser) self.assertEqual(resp.content_type, "application/xml") self.assertEqual(resp.status_int, 403) self.assertTrue('resizeNotAllowed' not in resp.body) self.assertTrue('forbidden' in resp.body) def test_fault_has_status_int(self): # Ensure the status_int is set correctly on faults. fault = wsgi.Fault(webob.exc.HTTPBadRequest(explanation='what?')) self.assertEqual(fault.status_int, 400) def test_xml_serializer(self): # Ensure that a v1.1 request responds with a v1.1 xmlns. request = webob.Request.blank('/v1.1', headers={"Accept": "application/xml"}) fault = wsgi.Fault(webob.exc.HTTPBadRequest(explanation='scram')) response = request.get_response(fault) self.assertTrue(common.XML_NS_V11 in response.body) self.assertEqual(response.content_type, "application/xml") self.assertEqual(response.status_int, 400) class FaultsXMLSerializationTestV11(test.TestCase): """Tests covering `nova.api.openstack.faults:Fault` class.""" def _prepare_xml(self, xml_string): xml_string = xml_string.replace(" ", "") xml_string = xml_string.replace("\n", "") xml_string = xml_string.replace("\t", "") return xml_string def test_400_fault(self): metadata = {'attributes': {"badRequest": 'code'}} serializer = wsgi.XMLDictSerializer(metadata=metadata, xmlns=common.XML_NS_V11) fixture = { "badRequest": { "message": "scram", "code": 400, }, } output = serializer.serialize(fixture) actual = minidom.parseString(self._prepare_xml(output)) expected = minidom.parseString(self._prepare_xml(""" <badRequest code="400" xmlns="%s"> <message>scram</message> </badRequest> """) % common.XML_NS_V11) self.assertEqual(expected.toxml(), actual.toxml()) def test_413_fault(self): metadata = {'attributes': {"overLimit": 'code'}} serializer = wsgi.XMLDictSerializer(metadata=metadata, xmlns=common.XML_NS_V11) fixture = { "overLimit": { "message": "sorry", "code": 413, "retryAfter": 4, }, } output = serializer.serialize(fixture) actual = minidom.parseString(self._prepare_xml(output)) expected = minidom.parseString(self._prepare_xml(""" <overLimit code="413" xmlns="%s"> <message>sorry</message> <retryAfter>4</retryAfter> </overLimit> """) % common.XML_NS_V11) self.assertEqual(expected.toxml(), actual.toxml()) def test_404_fault(self): metadata = {'attributes': {"itemNotFound": 'code'}} serializer = wsgi.XMLDictSerializer(metadata=metadata, xmlns=common.XML_NS_V11) fixture = { "itemNotFound": { "message": "sorry", "code": 404, }, } output = serializer.serialize(fixture) actual = minidom.parseString(self._prepare_xml(output)) expected = minidom.parseString(self._prepare_xml(""" <itemNotFound code="404" xmlns="%s"> <message>sorry</message> </itemNotFound> """) % common.XML_NS_V11) self.assertEqual(expected.toxml(), actual.toxml())
# Copyright 2016 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== """Basic loop for training.""" from __future__ import absolute_import from __future__ import division from __future__ import print_function from tensorflow.python.framework import errors from tensorflow.python.util.tf_export import tf_export @tf_export(v1=["train.basic_train_loop"]) def basic_train_loop(supervisor, train_step_fn, args=None, kwargs=None, master=""): """Basic loop to train a model. Calls `train_step_fn` in a loop to train a model. The function is called as: ```python train_step_fn(session, *args, **kwargs) ``` It is passed a `tf.Session` in addition to `args` and `kwargs`. The function typically runs one training step in the session. Args: supervisor: `tf.train.Supervisor` to run the training services. train_step_fn: Callable to execute one training step. Called repeatedly as `train_step_fn(session, *args **kwargs)`. args: Optional positional arguments passed to `train_step_fn`. kwargs: Optional keyword arguments passed to `train_step_fn`. master: Master to use to create the training session. Defaults to `""` which causes the session to be created in the local process. """ if args is None: args = [] if kwargs is None: kwargs = {} should_retry = True while should_retry: try: should_retry = False with supervisor.managed_session(master) as sess: while not supervisor.should_stop(): train_step_fn(sess, *args, **kwargs) except errors.AbortedError: # Always re-run on AbortedError as it indicates a restart of one of the # distributed tensorflow servers. should_retry = True
from __future__ import unicode_literals import re from .common import InfoExtractor from ..utils import ( compat_urllib_request, unified_strdate, str_to_int, parse_duration, clean_html, ) class FourTubeIE(InfoExtractor): IE_NAME = '4tube' _VALID_URL = r'https?://(?:www\.)?4tube\.com/videos/(?P<id>\d+)' _TEST = { 'url': 'http://www.4tube.com/videos/209733/hot-babe-holly-michaels-gets-her-ass-stuffed-by-black', 'md5': '6516c8ac63b03de06bc8eac14362db4f', 'info_dict': { 'id': '209733', 'ext': 'mp4', 'title': 'Hot Babe Holly Michaels gets her ass stuffed by black', 'uploader': 'WCP Club', 'uploader_id': 'wcp-club', 'upload_date': '20131031', 'duration': 583, } } def _real_extract(self, url): mobj = re.match(self._VALID_URL, url) video_id = mobj.group('id') webpage_url = 'http://www.4tube.com/videos/' + video_id webpage = self._download_webpage(webpage_url, video_id) self.report_extraction(video_id) playlist_json = self._html_search_regex(r'var playerConfigPlaylist\s+=\s+([^;]+)', webpage, 'Playlist') media_id = self._search_regex(r'idMedia:\s*(\d+)', playlist_json, 'Media Id') sources = self._search_regex(r'sources:\s*\[([^\]]*)\]', playlist_json, 'Sources').split(',') title = self._search_regex(r'title:\s*"([^"]*)', playlist_json, 'Title') thumbnail_url = self._search_regex(r'image:\s*"([^"]*)', playlist_json, 'Thumbnail', fatal=False) uploader_str = self._search_regex(r'<span>Uploaded by</span>(.*?)<span>', webpage, 'uploader', fatal=False) mobj = re.search(r'<a href="/sites/(?P<id>[^"]+)"><strong>(?P<name>[^<]+)</strong></a>', uploader_str) (uploader, uploader_id) = (mobj.group('name'), mobj.group('id')) if mobj else (clean_html(uploader_str), None) upload_date = None view_count = None duration = None description = self._html_search_meta('description', webpage, 'description') if description: upload_date = self._search_regex(r'Published Date: (\d{2} [a-zA-Z]{3} \d{4})', description, 'upload date', fatal=False) if upload_date: upload_date = unified_strdate(upload_date) view_count = self._search_regex(r'Views: ([\d,\.]+)', description, 'view count', fatal=False) if view_count: view_count = str_to_int(view_count) duration = parse_duration(self._search_regex(r'Length: (\d+m\d+s)', description, 'duration', fatal=False)) token_url = "http://tkn.4tube.com/{0}/desktop/{1}".format(media_id, "+".join(sources)) headers = { b'Content-Type': b'application/x-www-form-urlencoded', b'Origin': b'http://www.4tube.com', } token_req = compat_urllib_request.Request(token_url, b'{}', headers) tokens = self._download_json(token_req, video_id) formats = [{ 'url': tokens[format]['token'], 'format_id': format + 'p', 'resolution': format + 'p', 'quality': int(format), } for format in sources] self._sort_formats(formats) return { 'id': video_id, 'title': title, 'formats': formats, 'thumbnail': thumbnail_url, 'uploader': uploader, 'uploader_id': uploader_id, 'upload_date': upload_date, 'view_count': view_count, 'duration': duration, 'age_limit': 18, 'webpage_url': webpage_url, }
#!/usr/bin/env python2 # vim:fileencoding=UTF-8:ts=4:sw=4:sta:et:sts=4:ai __license__ = 'GPL v3' __copyright__ = '2010, Kovid Goyal <kovid@kovidgoyal.net>' __docformat__ = 'restructuredtext en' import re, os, traceback, shutil from threading import Thread from operator import itemgetter from calibre.ptempfile import TemporaryDirectory from calibre.ebooks.metadata.opf2 import OPF from calibre.library.database2 import LibraryDatabase2 from calibre.library.prefs import DBPrefs from calibre.constants import filesystem_encoding from calibre.utils.date import utcfromtimestamp from calibre import isbytestring NON_EBOOK_EXTENSIONS = frozenset([ 'jpg', 'jpeg', 'gif', 'png', 'bmp', 'opf', 'swp', 'swo' ]) class RestoreDatabase(LibraryDatabase2): PATH_LIMIT = 10 WINDOWS_LIBRARY_PATH_LIMIT = 180 def set_path(self, *args, **kwargs): pass def dirtied(self, *args, **kwargs): pass class Restore(Thread): def __init__(self, library_path, progress_callback=None): super(Restore, self).__init__() if isbytestring(library_path): library_path = library_path.decode(filesystem_encoding) self.src_library_path = os.path.abspath(library_path) self.progress_callback = progress_callback self.db_id_regexp = re.compile(r'^.* \((\d+)\)$') self.bad_ext_pat = re.compile(r'[^a-z0-9_]+') if not callable(self.progress_callback): self.progress_callback = lambda x, y: x self.dirs = [] self.ignored_dirs = [] self.failed_dirs = [] self.books = [] self.conflicting_custom_cols = {} self.failed_restores = [] self.mismatched_dirs = [] self.successes = 0 self.tb = None self.authors_links = {} @property def errors_occurred(self): return self.failed_dirs or self.mismatched_dirs or \ self.conflicting_custom_cols or self.failed_restores @property def report(self): ans = '' failures = list(self.failed_dirs) + [(x['dirpath'], tb) for x, tb in self.failed_restores] if failures: ans += 'Failed to restore the books in the following folders:\n' for dirpath, tb in failures: ans += '\t' + dirpath + ' with error:\n' ans += '\n'.join('\t\t'+x for x in tb.splitlines()) ans += '\n\n' if self.conflicting_custom_cols: ans += '\n\n' ans += 'The following custom columns have conflicting definitions ' \ 'and were not fully restored:\n' for x in self.conflicting_custom_cols: ans += '\t#'+x+'\n' ans += '\tused:\t%s, %s, %s, %s\n'%(self.custom_columns[x][1], self.custom_columns[x][2], self.custom_columns[x][3], self.custom_columns[x][5]) for coldef in self.conflicting_custom_cols[x]: ans += '\tother:\t%s, %s, %s, %s\n'%(coldef[1], coldef[2], coldef[3], coldef[5]) if self.mismatched_dirs: ans += '\n\n' ans += 'The following folders were ignored:\n' for x in self.mismatched_dirs: ans += '\t'+x+'\n' return ans def run(self): try: with TemporaryDirectory('_library_restore') as tdir: self.library_path = tdir self.scan_library() if not self.load_preferences(): # Something went wrong with preferences restore. Start over # with a new database and attempt to rebuild the structure # from the metadata in the opf dbpath = os.path.join(self.library_path, 'metadata.db') if os.path.exists(dbpath): os.remove(dbpath) self.create_cc_metadata() self.restore_books() if self.successes == 0 and len(self.dirs) > 0: raise Exception(('Something bad happened')) self.replace_db() except: self.tb = traceback.format_exc() def load_preferences(self): self.progress_callback(None, 1) self.progress_callback(_('Starting restoring preferences and column metadata'), 0) prefs_path = os.path.join(self.src_library_path, 'metadata_db_prefs_backup.json') if not os.path.exists(prefs_path): self.progress_callback(_('Cannot restore preferences. Backup file not found.'), 1) return False try: prefs = DBPrefs.read_serialized(self.src_library_path, recreate_prefs=False) db = RestoreDatabase(self.library_path, default_prefs=prefs, restore_all_prefs=True, progress_callback=self.progress_callback) db.commit() db.close() self.progress_callback(None, 1) if 'field_metadata' in prefs: self.progress_callback(_('Finished restoring preferences and column metadata'), 1) return True self.progress_callback(_('Finished restoring preferences'), 1) return False except: traceback.print_exc() self.progress_callback(None, 1) self.progress_callback(_('Restoring preferences and column metadata failed'), 0) return False def scan_library(self): for dirpath, dirnames, filenames in os.walk(self.src_library_path): leaf = os.path.basename(dirpath) m = self.db_id_regexp.search(leaf) if m is None or 'metadata.opf' not in filenames: self.ignored_dirs.append(dirpath) continue self.dirs.append((dirpath, filenames, m.group(1))) self.progress_callback(None, len(self.dirs)) for i, x in enumerate(self.dirs): dirpath, filenames, book_id = x try: self.process_dir(dirpath, filenames, book_id) except: self.failed_dirs.append((dirpath, traceback.format_exc())) self.progress_callback(_('Processed') + ' ' + dirpath, i+1) def is_ebook_file(self, filename): ext = os.path.splitext(filename)[1] if not ext: return False ext = ext[1:].lower() if ext in NON_EBOOK_EXTENSIONS or \ self.bad_ext_pat.search(ext) is not None: return False return True def process_dir(self, dirpath, filenames, book_id): book_id = int(book_id) formats = filter(self.is_ebook_file, filenames) fmts = [os.path.splitext(x)[1][1:].upper() for x in formats] sizes = [os.path.getsize(os.path.join(dirpath, x)) for x in formats] names = [os.path.splitext(x)[0] for x in formats] opf = os.path.join(dirpath, 'metadata.opf') mi = OPF(opf, basedir=dirpath).to_book_metadata() timestamp = os.path.getmtime(opf) path = os.path.relpath(dirpath, self.src_library_path).replace(os.sep, '/') if int(mi.application_id) == book_id: self.books.append({ 'mi': mi, 'timestamp': timestamp, 'formats': list(zip(fmts, sizes, names)), 'id': book_id, 'dirpath': dirpath, 'path': path, }) else: self.mismatched_dirs.append(dirpath) alm = mi.get('author_link_map', {}) for author, link in alm.iteritems(): existing_link, timestamp = self.authors_links.get(author, (None, None)) if existing_link is None or existing_link != link and timestamp < mi.timestamp: self.authors_links[author] = (link, mi.timestamp) def create_cc_metadata(self): self.books.sort(key=itemgetter('timestamp')) self.custom_columns = {} fields = ('label', 'name', 'datatype', 'is_multiple', 'is_editable', 'display') for b in self.books: for key in b['mi'].custom_field_keys(): cfm = b['mi'].metadata_for_field(key) args = [] for x in fields: if x in cfm: if x == 'is_multiple': args.append(bool(cfm[x])) else: args.append(cfm[x]) if len(args) == len(fields): # TODO: Do series type columns need special handling? label = cfm['label'] if label in self.custom_columns and args != self.custom_columns[label]: if label not in self.conflicting_custom_cols: self.conflicting_custom_cols[label] = [] if self.custom_columns[label] not in self.conflicting_custom_cols[label]: self.conflicting_custom_cols[label].append(self.custom_columns[label]) self.custom_columns[label] = args db = RestoreDatabase(self.library_path) self.progress_callback(None, len(self.custom_columns)) if len(self.custom_columns): for i,args in enumerate(self.custom_columns.values()): db.create_custom_column(*args) self.progress_callback(_('creating custom column ')+args[0], i+1) db.close() def restore_books(self): self.progress_callback(None, len(self.books)) self.books.sort(key=itemgetter('id')) db = RestoreDatabase(self.library_path) for i, book in enumerate(self.books): try: self.restore_book(book, db) except: self.failed_restores.append((book, traceback.format_exc())) self.progress_callback(book['mi'].title, i+1) for author in self.authors_links.iterkeys(): link, ign = self.authors_links[author] db.conn.execute('UPDATE authors SET link=? WHERE name=?', (link, author.replace(',', '|'))) db.conn.commit() db.close() def restore_book(self, book, db): db.create_book_entry(book['mi'], add_duplicates=True, force_id=book['id']) if book['mi'].uuid: db.set_uuid(book['id'], book['mi'].uuid, commit=False, notify=False) db.conn.execute('UPDATE books SET path=?,last_modified=? WHERE id=?', (book['path'], utcfromtimestamp(book['timestamp']), book['id'])) for fmt, size, name in book['formats']: db.conn.execute(''' INSERT INTO data (book,format,uncompressed_size,name) VALUES (?,?,?,?)''', (book['id'], fmt, size, name)) db.conn.commit() self.successes += 1 def replace_db(self): dbpath = os.path.join(self.src_library_path, 'metadata.db') ndbpath = os.path.join(self.library_path, 'metadata.db') save_path = self.olddb = os.path.splitext(dbpath)[0]+'_pre_restore.db' if os.path.exists(save_path): os.remove(save_path) os.rename(dbpath, save_path) shutil.copyfile(ndbpath, dbpath)
# Copyright 2017 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== """Tests for third_party.tensorflow.contrib.kernel_methods.python.losses.""" from __future__ import absolute_import from __future__ import division from __future__ import print_function import numpy as np from tensorflow.contrib.kernel_methods.python import losses from tensorflow.python.framework import constant_op from tensorflow.python.framework import dtypes from tensorflow.python.framework import errors from tensorflow.python.ops import array_ops from tensorflow.python.platform import test class SparseMulticlassHingeLossTest(test.TestCase): def testInvalidLogitsShape(self): """An error is raised when logits have invalid shape.""" with self.test_session(): logits = constant_op.constant([-1.0, 2.1], shape=(2,)) labels = constant_op.constant([0, 1]) with self.assertRaises(ValueError): _ = losses.sparse_multiclass_hinge_loss(labels, logits) def testInvalidLabelsShape(self): """An error is raised when labels have invalid shape.""" with self.test_session(): logits = constant_op.constant([-1.0, 2.1], shape=(2, 1)) labels = constant_op.constant([1, 0], shape=(1, 1, 2)) with self.assertRaises(ValueError): _ = losses.sparse_multiclass_hinge_loss(labels, logits) def testInvalidWeightsShape(self): """An error is raised when weights have invalid shape.""" with self.test_session(): logits = constant_op.constant([-1.0, 2.1], shape=(2, 1)) labels = constant_op.constant([1, 0], shape=(2,)) weights = constant_op.constant([1.5, 0.2], shape=(2, 1, 1)) with self.assertRaises(ValueError): _ = losses.sparse_multiclass_hinge_loss(labels, logits, weights) def testInvalidLabelsDtype(self): """An error is raised when labels have invalid shape.""" with self.test_session(): logits = constant_op.constant([-1.0, 2.1], shape=(2, 1)) labels = constant_op.constant([1, 0], dtype=dtypes.float32) with self.assertRaises(ValueError): _ = losses.sparse_multiclass_hinge_loss(labels, logits) def testNoneWeightRaisesValueError(self): """An error is raised when weights are None.""" with self.test_session(): logits = constant_op.constant([-1.0, 2.1], shape=(2, 1)) labels = constant_op.constant([1, 0]) with self.assertRaises(ValueError): _ = losses.sparse_multiclass_hinge_loss(labels, logits, weights=None) def testInconsistentLabelsAndWeightsShapesSameRank(self): """Error raised when weights and labels have same ranks, different sizes.""" with self.test_session(): logits = constant_op.constant([-1.0, 2.1, 4.1], shape=(3, 1)) labels = constant_op.constant([1, 0, 2], shape=(3, 1)) weights = constant_op.constant([1.1, 2.0], shape=(2, 1)) with self.assertRaises(ValueError): _ = losses.sparse_multiclass_hinge_loss(labels, logits, weights) def testInconsistentLabelsAndWeightsShapesDifferentRank(self): """Error raised when weights and labels have different ranks and sizes.""" with self.test_session(): logits = constant_op.constant([-1.0, 2.1], shape=(2, 1)) labels = constant_op.constant([1, 0], shape=(2, 1)) weights = constant_op.constant([1.1, 2.0, 2.8], shape=(3,)) with self.assertRaises(ValueError): _ = losses.sparse_multiclass_hinge_loss(labels, logits, weights) def testOutOfRangeLabels(self): """An error is raised when labels are not in [0, num_classes).""" with self.test_session(): logits = constant_op.constant([[1.2, -1.4, -1.0], [1.4, 1.8, 4.0], [0.5, 1.8, -1.0]]) labels = constant_op.constant([1, 0, 4]) loss = losses.sparse_multiclass_hinge_loss(labels, logits) with self.assertRaises(errors.InvalidArgumentError): loss.eval() def testZeroLossInt32Labels(self): """Loss is 0 if true class logits sufficiently higher than other classes.""" with self.test_session(): logits = constant_op.constant([[1.2, -1.4, -1.0], [1.4, 1.8, 4.0], [0.5, 1.8, -1.0]]) labels = constant_op.constant([0, 2, 1], dtype=dtypes.int32) loss = losses.sparse_multiclass_hinge_loss(labels, logits) self.assertAlmostEqual(loss.eval(), 0.0, 3) def testZeroLossInt64Labels(self): """Loss is 0 if true class logits sufficiently higher than other classes.""" with self.test_session(): logits = constant_op.constant([[2.1, -0.4, -1.0], [1.4, 2.8, 4.0], [-0.5, 0.8, -1.0]]) labels = constant_op.constant([0, 2, 1], dtype=dtypes.int64) loss = losses.sparse_multiclass_hinge_loss(labels, logits) self.assertAlmostEqual(loss.eval(), 0.0, 3) def testUnknownShape(self): """Result keeps same with `testZeroLossInt32Labels`""" logits_np = np.array([[1.2, -1.4, -1.0], [1.4, 1.8, 4.0], [0.5, 1.8, -1.0]]) labels_np = np.array([0, 2, 1], dtype=np.int32) logits_shapes = [ [3, 3], # batch_size, num_classes [None, 3], [3, None], [None, None] ] for batch_size, num_classes in logits_shapes: with self.test_session(): logits = array_ops.placeholder( dtypes.float32, shape=(batch_size, num_classes)) labels = array_ops.placeholder(dtypes.int32, shape=(batch_size,)) loss = losses.sparse_multiclass_hinge_loss(labels, logits) result = loss.eval(feed_dict={logits: logits_np, labels: labels_np}) self.assertAlmostEqual(result, 0.0, 3) def testCorrectPredictionsSomeClassesInsideMargin(self): """Loss is > 0 even if true class logits are higher than other classes.""" with self.test_session(): logits = constant_op.constant([[1.2, -1.4, 0.8], [1.4, 1.8, 4.0], [1.5, 1.8, -1.0]]) labels = constant_op.constant([0, 2, 1]) loss = losses.sparse_multiclass_hinge_loss(labels, logits) # The first and third samples incur some loss (0.6 and 0.7 respectively). self.assertAlmostEqual(loss.eval(), 0.4333, 3) def testIncorrectPredictions(self): """Loss is >0 when an incorrect class has higher logits than true class.""" with self.test_session(): logits = constant_op.constant([[2.6, 0.4, 0.8], [1.4, 0.8, -1.0], [0.5, -1.8, 2.0]]) labels = constant_op.constant([1, 0, 2]) loss = losses.sparse_multiclass_hinge_loss(labels, logits) # The first examples incurs a high loss (3.2) since the logits of an # incorrect class (0) are higher than the logits of the ground truth. The # second example also incures a (smaller) loss (0.4). self.assertAlmostEqual(loss.eval(), 1.2, 3) def testIncorrectPredictionsColumnLabels(self): """Same as above but labels is a rank-2 tensor.""" with self.test_session(): logits = constant_op.constant([[1.6, -0.4, 0.8], [1.5, 0.8, -1.0], [0.2, -1.8, 4.0]]) labels = constant_op.constant([1, 0, 2], shape=(3, 1)) loss = losses.sparse_multiclass_hinge_loss(labels, logits) # The first examples incurs a high loss (3.0) since the logits of an # incorrect class (0) are higher than the logits of the ground truth. The # second example also incures a (smaller) loss (0.3). self.assertAlmostEqual(loss.eval(), 1.1, 3) def testIncorrectPredictionsZeroWeights(self): """Loss is 0 when all weights are missing even if predictions are wrong.""" with self.test_session(): logits = constant_op.constant([[1.6, -0.4, 0.8], [1.5, 0.8, -1.0], [0.2, -1.8, 4.0]]) labels = constant_op.constant([1, 0, 2], shape=(3, 1)) weights = constant_op.constant([0.0, 0.0, 0.0], shape=(3, 1)) loss = losses.sparse_multiclass_hinge_loss(labels, logits, weights) # No overall loss since all weights are 0. self.assertAlmostEqual(loss.eval(), 0.0, 3) def testNonZeroLossWithPythonScalarWeights(self): """Weighted loss is correctly computed when weights is a python scalar.""" with self.test_session(): logits = constant_op.constant([[1.6, -0.4, 0.8], [1.5, 0.8, -1.0], [0.2, -1.8, 4.0]]) labels = constant_op.constant([1, 0, 2], shape=(3, 1)) weights = 10.0 loss = losses.sparse_multiclass_hinge_loss(labels, logits, weights) self.assertAlmostEqual(loss.eval(), 11.0, 3) def testNonZeroLossWithScalarTensorWeights(self): """Weighted loss is correctly computed when weights is a rank-0 tensor.""" with self.test_session(): logits = constant_op.constant([[1.6, -0.4, 0.8], [1.5, 0.8, -1.0], [0.2, -1.8, 4.0]]) labels = constant_op.constant([1, 0, 2], shape=(3, 1)) weights = constant_op.constant(5.0) loss = losses.sparse_multiclass_hinge_loss(labels, logits, weights) self.assertAlmostEqual(loss.eval(), 5.5, 3) def testNonZeroLossWith1DTensorWeightsColumnLabels(self): """Weighted loss is correctly computed when weights is a rank-0 tensor.""" with self.test_session(): logits = constant_op.constant([[1.6, -0.4, 0.8], [1.5, 0.8, -1.0], [0.2, -1.8, 4.0]]) labels = constant_op.constant([1, 0, 2], shape=(3, 1)) weights = constant_op.constant([1.0, 0.5, 2.0], shape=(3,)) loss = losses.sparse_multiclass_hinge_loss(labels, logits, weights) # The overall loss is 1/3 *(3.0*1.0 + 0.5*0.3+ 2.0*0.0) = 1.05 self.assertAlmostEqual(loss.eval(), 1.05, 3) def testNonZeroLossWith2DTensorWeights1DLabelsSomeWeightsMissing(self): """Weighted loss is correctly computed when weights is a rank-0 tensor.""" with self.test_session(): logits = constant_op.constant([[1.6, -0.4, 0.8], [1.5, 0.8, -1.0], [0.2, -1.8, 4.0], [1.6, 1.8, -4.0]]) labels = constant_op.constant([1, 0, 2, 1]) weights = constant_op.constant([[1.0], [0.0], [2.0], [4.0]]) loss = losses.sparse_multiclass_hinge_loss(labels, logits, weights) # The overall loss is 1/3 *(3.0*1.0 + 0.0*0.3+ 2.0*0.0 + 4.0*0.8) = 6.2/3. self.assertAlmostEqual(loss.eval(), 2.06666, 3) if __name__ == '__main__': test.main()
# Copyright (c) 2001-2007 Twisted Matrix Laboratories. # See LICENSE for details. from twisted.trial import unittest # system imports import os, time, stat # twisted imports from twisted.python import logfile, runtime class LogFileTestCase(unittest.TestCase): """ Test the rotating log file. """ def setUp(self): self.dir = self.mktemp() os.makedirs(self.dir) self.name = "test.log" self.path = os.path.join(self.dir, self.name) def tearDown(self): """ Restore back write rights on created paths: if tests modified the rights, that will allow the paths to be removed easily afterwards. """ os.chmod(self.dir, 0777) if os.path.exists(self.path): os.chmod(self.path, 0777) def testWriting(self): log = logfile.LogFile(self.name, self.dir) log.write("123") log.write("456") log.flush() log.write("7890") log.close() f = open(self.path, "r") self.assertEquals(f.read(), "1234567890") f.close() def testRotation(self): # this logfile should rotate every 10 bytes log = logfile.LogFile(self.name, self.dir, rotateLength=10) # test automatic rotation log.write("123") log.write("4567890") log.write("1" * 11) self.assert_(os.path.exists("%s.1" % self.path)) self.assert_(not os.path.exists("%s.2" % self.path)) log.write('') self.assert_(os.path.exists("%s.1" % self.path)) self.assert_(os.path.exists("%s.2" % self.path)) self.assert_(not os.path.exists("%s.3" % self.path)) log.write("3") self.assert_(not os.path.exists("%s.3" % self.path)) # test manual rotation log.rotate() self.assert_(os.path.exists("%s.3" % self.path)) self.assert_(not os.path.exists("%s.4" % self.path)) log.close() self.assertEquals(log.listLogs(), [1, 2, 3]) def testAppend(self): log = logfile.LogFile(self.name, self.dir) log.write("0123456789") log.close() log = logfile.LogFile(self.name, self.dir) self.assertEquals(log.size, 10) self.assertEquals(log._file.tell(), log.size) log.write("abc") self.assertEquals(log.size, 13) self.assertEquals(log._file.tell(), log.size) f = log._file f.seek(0, 0) self.assertEquals(f.read(), "0123456789abc") log.close() def testLogReader(self): log = logfile.LogFile(self.name, self.dir) log.write("abc\n") log.write("def\n") log.rotate() log.write("ghi\n") log.flush() # check reading logs self.assertEquals(log.listLogs(), [1]) reader = log.getCurrentLog() reader._file.seek(0) self.assertEquals(reader.readLines(), ["ghi\n"]) self.assertEquals(reader.readLines(), []) reader.close() reader = log.getLog(1) self.assertEquals(reader.readLines(), ["abc\n", "def\n"]) self.assertEquals(reader.readLines(), []) reader.close() # check getting illegal log readers self.assertRaises(ValueError, log.getLog, 2) self.assertRaises(TypeError, log.getLog, "1") # check that log numbers are higher for older logs log.rotate() self.assertEquals(log.listLogs(), [1, 2]) reader = log.getLog(1) reader._file.seek(0) self.assertEquals(reader.readLines(), ["ghi\n"]) self.assertEquals(reader.readLines(), []) reader.close() reader = log.getLog(2) self.assertEquals(reader.readLines(), ["abc\n", "def\n"]) self.assertEquals(reader.readLines(), []) reader.close() def testModePreservation(self): """ Check rotated files have same permissions as original. """ f = open(self.path, "w").close() os.chmod(self.path, 0707) mode = os.stat(self.path)[stat.ST_MODE] log = logfile.LogFile(self.name, self.dir) log.write("abc") log.rotate() self.assertEquals(mode, os.stat(self.path)[stat.ST_MODE]) def test_noPermission(self): """ Check it keeps working when permission on dir changes. """ log = logfile.LogFile(self.name, self.dir) log.write("abc") # change permissions so rotation would fail os.chmod(self.dir, 0555) # if this succeeds, chmod doesn't restrict us, so we can't # do the test try: f = open(os.path.join(self.dir,"xxx"), "w") except (OSError, IOError): pass else: f.close() return log.rotate() # this should not fail log.write("def") log.flush() f = log._file self.assertEquals(f.tell(), 6) f.seek(0, 0) self.assertEquals(f.read(), "abcdef") log.close() def test_maxNumberOfLog(self): """ Test it respect the limit on the number of files when maxRotatedFiles is not None. """ log = logfile.LogFile(self.name, self.dir, rotateLength=10, maxRotatedFiles=3) log.write("1" * 11) log.write("2" * 11) self.failUnless(os.path.exists("%s.1" % self.path)) log.write("3" * 11) self.failUnless(os.path.exists("%s.2" % self.path)) log.write("4" * 11) self.failUnless(os.path.exists("%s.3" % self.path)) self.assertEquals(file("%s.3" % self.path).read(), "1" * 11) log.write("5" * 11) self.assertEquals(file("%s.3" % self.path).read(), "2" * 11) self.failUnless(not os.path.exists("%s.4" % self.path)) def test_fromFullPath(self): """ Test the fromFullPath method. """ log1 = logfile.LogFile(self.name, self.dir, 10, defaultMode=0777) log2 = logfile.LogFile.fromFullPath(self.path, 10, defaultMode=0777) self.assertEquals(log1.name, log2.name) self.assertEquals(os.path.abspath(log1.path), log2.path) self.assertEquals(log1.rotateLength, log2.rotateLength) self.assertEquals(log1.defaultMode, log2.defaultMode) def test_defaultPermissions(self): """ Test the default permission of the log file: if the file exist, it should keep the permission. """ f = file(self.path, "w") os.chmod(self.path, 0707) currentMode = stat.S_IMODE(os.stat(self.path)[stat.ST_MODE]) f.close() log1 = logfile.LogFile(self.name, self.dir) self.assertEquals(stat.S_IMODE(os.stat(self.path)[stat.ST_MODE]), currentMode) def test_specifiedPermissions(self): """ Test specifying the permissions used on the log file. """ log1 = logfile.LogFile(self.name, self.dir, defaultMode=0066) mode = stat.S_IMODE(os.stat(self.path)[stat.ST_MODE]) if runtime.platform.isWindows(): # The only thing we can get here is global read-only self.assertEquals(mode, 0444) else: self.assertEquals(mode, 0066) def test_reopen(self): """ L{logfile.LogFile.reopen} allows to rename the currently used file and make L{logfile.LogFile} create a new file. """ log1 = logfile.LogFile(self.name, self.dir) log1.write("hello1") savePath = os.path.join(self.dir, "save.log") os.rename(self.path, savePath) log1.reopen() log1.write("hello2") log1.close() f = open(self.path, "r") self.assertEquals(f.read(), "hello2") f.close() f = open(savePath, "r") self.assertEquals(f.read(), "hello1") f.close() if runtime.platform.isWindows(): test_reopen.skip = "Can't test reopen on Windows" class RiggedDailyLogFile(logfile.DailyLogFile): _clock = 0.0 def _openFile(self): logfile.DailyLogFile._openFile(self) # rig the date to match _clock, not mtime self.lastDate = self.toDate() def toDate(self, *args): if args: return time.gmtime(*args)[:3] return time.gmtime(self._clock)[:3] class DailyLogFileTestCase(unittest.TestCase): """ Test rotating log file. """ def setUp(self): self.dir = self.mktemp() os.makedirs(self.dir) self.name = "testdaily.log" self.path = os.path.join(self.dir, self.name) def testWriting(self): log = RiggedDailyLogFile(self.name, self.dir) log.write("123") log.write("456") log.flush() log.write("7890") log.close() f = open(self.path, "r") self.assertEquals(f.read(), "1234567890") f.close() def testRotation(self): # this logfile should rotate every 10 bytes log = RiggedDailyLogFile(self.name, self.dir) days = [(self.path + '.' + log.suffix(day * 86400)) for day in range(3)] # test automatic rotation log._clock = 0.0 # 1970/01/01 00:00.00 log.write("123") log._clock = 43200 # 1970/01/01 12:00.00 log.write("4567890") log._clock = 86400 # 1970/01/02 00:00.00 log.write("1" * 11) self.assert_(os.path.exists(days[0])) self.assert_(not os.path.exists(days[1])) log._clock = 172800 # 1970/01/03 00:00.00 log.write('') self.assert_(os.path.exists(days[0])) self.assert_(os.path.exists(days[1])) self.assert_(not os.path.exists(days[2])) log._clock = 259199 # 1970/01/03 23:59.59 log.write("3") self.assert_(not os.path.exists(days[2]))
import sys import os from optparse import make_option, OptionParser from django.conf import settings from django.core.management.base import BaseCommand from django.test.utils import get_runner class Command(BaseCommand): option_list = BaseCommand.option_list + ( make_option('--noinput', action='store_false', dest='interactive', default=True, help='Tells Django to NOT prompt the user for input of any kind.'), make_option('--failfast', action='store_true', dest='failfast', default=False, help='Tells Django to stop running the test suite after first ' 'failed test.'), make_option('--testrunner', action='store', dest='testrunner', help='Tells Django to use specified test runner class instead of ' 'the one specified by the TEST_RUNNER setting.'), make_option('--liveserver', action='store', dest='liveserver', default=None, help='Overrides the default address where the live server (used ' 'with LiveServerTestCase) is expected to run from. The ' 'default value is localhost:8081.'), ) help = ('Runs the test suite for the specified applications, or the ' 'entire site if no apps are specified.') args = '[appname ...]' requires_model_validation = False def __init__(self): self.test_runner = None super(Command, self).__init__() def run_from_argv(self, argv): """ Pre-parse the command line to extract the value of the --testrunner option. This allows a test runner to define additional command line arguments. """ option = '--testrunner=' for arg in argv[2:]: if arg.startswith(option): self.test_runner = arg[len(option):] break super(Command, self).run_from_argv(argv) def create_parser(self, prog_name, subcommand): test_runner_class = get_runner(settings, self.test_runner) options = self.option_list + getattr( test_runner_class, 'option_list', ()) return OptionParser(prog=prog_name, usage=self.usage(subcommand), version=self.get_version(), option_list=options) def handle(self, *test_labels, **options): from django.conf import settings from django.test.utils import get_runner TestRunner = get_runner(settings, options.get('testrunner')) options['verbosity'] = int(options.get('verbosity')) if options.get('liveserver') is not None: os.environ['DJANGO_LIVE_TEST_SERVER_ADDRESS'] = options['liveserver'] del options['liveserver'] test_runner = TestRunner(**options) failures = test_runner.run_tests(test_labels) if failures: sys.exit(bool(failures))
# Generated by h2py from /usr/include/dlfcn.h _DLFCN_H = 1 # Included from features.h _FEATURES_H = 1 __USE_ANSI = 1 __FAVOR_BSD = 1 _ISOC99_SOURCE = 1 _POSIX_SOURCE = 1 _POSIX_C_SOURCE = 199506L _XOPEN_SOURCE = 600 _XOPEN_SOURCE_EXTENDED = 1 _LARGEFILE64_SOURCE = 1 _BSD_SOURCE = 1 _SVID_SOURCE = 1 _BSD_SOURCE = 1 _SVID_SOURCE = 1 __USE_ISOC99 = 1 _POSIX_SOURCE = 1 _POSIX_C_SOURCE = 2 _POSIX_C_SOURCE = 199506L __USE_POSIX = 1 __USE_POSIX2 = 1 __USE_POSIX199309 = 1 __USE_POSIX199506 = 1 __USE_XOPEN = 1 __USE_XOPEN_EXTENDED = 1 __USE_UNIX98 = 1 _LARGEFILE_SOURCE = 1 __USE_XOPEN2K = 1 __USE_ISOC99 = 1 __USE_XOPEN_EXTENDED = 1 __USE_LARGEFILE = 1 __USE_LARGEFILE64 = 1 __USE_FILE_OFFSET64 = 1 __USE_MISC = 1 __USE_BSD = 1 __USE_SVID = 1 __USE_GNU = 1 __USE_REENTRANT = 1 __STDC_IEC_559__ = 1 __STDC_IEC_559_COMPLEX__ = 1 __STDC_ISO_10646__ = 200009L __GNU_LIBRARY__ = 6 __GLIBC__ = 2 __GLIBC_MINOR__ = 2 # Included from sys/cdefs.h _SYS_CDEFS_H = 1 def __PMT(args): return args def __P(args): return args def __PMT(args): return args def __STRING(x): return #x __flexarr = [] __flexarr = [0] __flexarr = [] __flexarr = [1] def __ASMNAME(cname): return __ASMNAME2 (__USER_LABEL_PREFIX__, cname) def __attribute__(xyz): return def __attribute_format_arg__(x): return __attribute__ ((__format_arg__ (x))) def __attribute_format_arg__(x): return __USE_LARGEFILE = 1 __USE_LARGEFILE64 = 1 __USE_EXTERN_INLINES = 1 # Included from gnu/stubs.h # Included from bits/dlfcn.h RTLD_LAZY = 0x00001 RTLD_NOW = 0x00002 RTLD_BINDING_MASK = 0x3 RTLD_NOLOAD = 0x00004 RTLD_GLOBAL = 0x00100 RTLD_LOCAL = 0 RTLD_NODELETE = 0x01000
# # Copyright (c) 2012 Tom Keffer <tkeffer@gmail.com> # # See the file LICENSE.txt for your full rights. # # $Revision: 829 $ # $Author: tkeffer $ # $Date: 2013-01-19 08:05:49 -0800 (Sat, 19 Jan 2013) $ # """Driver for sqlite""" import os.path # Import sqlite3. If it does not support the 'with' statement, then # import pysqlite2, which might... import sqlite3 if not hasattr(sqlite3.Connection, "__exit__"): del sqlite3 from pysqlite2 import dbapi2 as sqlite3 #@Reimport @UnresolvedImport import weedb def connect(database='', root='', driver='', **argv): """Factory function, to keep things compatible with DBAPI. """ return Connection(database=database, root=root, **argv) def create(database='', root='', driver='', **argv): """Create the database specified by the db_dict. If it already exists, an exception of type DatabaseExists will be thrown.""" file_path = os.path.join(root, database) # Check whether the database file exists: if os.path.exists(file_path): raise weedb.DatabaseExists("Database %s already exists" % (file_path,)) else: # If it doesn't exist, create the parent directories fileDirectory = os.path.dirname(file_path) if not os.path.exists(fileDirectory): os.makedirs(fileDirectory) connection = sqlite3.connect(file_path, **argv) connection.close() def drop(database='', root='', driver='', **argv): file_path = os.path.join(root, database) try: os.remove(file_path) except OSError: raise weedb.NoDatabase("""Attempt to drop non-existent database %s""" % (file_path,)) class Connection(weedb.Connection): """A wrapper around a sqlite3 connection object.""" def __init__(self, database='', root='', **argv): """Initialize an instance of Connection. Parameters: file: Path to the sqlite file (required) fileroot: An optional path to be prefixed to parameter 'file'. If not given, nothing will be prefixed. If the operation fails, an exception of type weedb.OperationalError will be raised. """ self.file_path = os.path.join(root, database) if not os.path.exists(self.file_path): raise weedb.OperationalError("Attempt to open a non-existent database %s" % database) try: connection = sqlite3.connect(self.file_path, **argv) except sqlite3.OperationalError: # The Pysqlite driver does not include the database file path. # Include it in case it might be useful. raise weedb.OperationalError("Unable to open database '%s'" % (self.file_path,)) weedb.Connection.__init__(self, connection, database, 'sqlite') def cursor(self): """Return a cursor object.""" return Cursor(self.connection) def tables(self): """Returns a list of tables in the database.""" table_list = list() for row in self.connection.execute("""SELECT tbl_name FROM sqlite_master WHERE type='table';"""): # Extract the table name. Sqlite returns unicode, so always # convert to a regular string: table_list.append(str(row[0])) return table_list def columnsOf(self, table): """Return a list of columns in the specified table. If the table does not exist, None is returned.""" column_list = list() for row in self.connection.execute("""PRAGMA table_info(%s);""" % table): # Append this column to the list of columns. column_list.append(str(row[1])) # If there are no columns (which means the table did not exist) raise an exceptional if not column_list: raise weedb.OperationalError("No such table %s" % table) return column_list def begin(self): self.connection.execute("BEGIN TRANSACTION") class Cursor(sqlite3.Cursor): """A wrapper around the sqlite cursor object""" # The sqlite3 cursor object is very full featured. We need only turn # the sqlite exceptions into weedb exceptions. def __init__(self, *args, **kwargs): sqlite3.Cursor.__init__(self, *args, **kwargs) def execute(self, *args, **kwargs): try: return sqlite3.Cursor.execute(self, *args, **kwargs) except sqlite3.OperationalError, e: # Convert to a weedb exception raise weedb.OperationalError(e)
from __future__ import absolute_import from __future__ import with_statement import sys import socket from nose import SkipTest from celery.exceptions import ImproperlyConfigured from celery import states from celery.utils import uuid from celery.backends import redis from celery.backends.redis import RedisBackend from celery.tests.utils import Case, mask_modules _no_redis_msg = "* Redis %s. Will not execute related tests." _no_redis_msg_emitted = False try: from redis.exceptions import ConnectionError except ImportError: class ConnectionError(socket.error): # noqa pass class SomeClass(object): def __init__(self, data): self.data = data def get_redis_or_SkipTest(): def emit_no_redis_msg(reason): global _no_redis_msg_emitted if not _no_redis_msg_emitted: sys.stderr.write("\n" + _no_redis_msg % reason + "\n") _no_redis_msg_emitted = True if redis.redis is None: emit_no_redis_msg("not installed") raise SkipTest("redis library not installed") try: tb = RedisBackend(redis_db="celery_unittest") try: # Evaluate lazy connection tb.client.info() except ConnectionError, exc: emit_no_redis_msg("not running") raise SkipTest("can't connect to redis: %s" % (exc, )) return tb except ImproperlyConfigured, exc: if "need to install" in str(exc): return emit_no_redis_msg("not installed") return emit_no_redis_msg("not configured") class TestRedisBackend(Case): def test_mark_as_done(self): tb = get_redis_or_SkipTest() tid = uuid() self.assertEqual(tb.get_status(tid), states.PENDING) self.assertIsNone(tb.get_result(tid)) tb.mark_as_done(tid, 42) self.assertEqual(tb.get_status(tid), states.SUCCESS) self.assertEqual(tb.get_result(tid), 42) def test_is_pickled(self): tb = get_redis_or_SkipTest() tid2 = uuid() result = {"foo": "baz", "bar": SomeClass(12345)} tb.mark_as_done(tid2, result) # is serialized properly. rindb = tb.get_result(tid2) self.assertEqual(rindb.get("foo"), "baz") self.assertEqual(rindb.get("bar").data, 12345) def test_mark_as_failure(self): tb = get_redis_or_SkipTest() tid3 = uuid() try: raise KeyError("foo") except KeyError, exception: pass tb.mark_as_failure(tid3, exception) self.assertEqual(tb.get_status(tid3), states.FAILURE) self.assertIsInstance(tb.get_result(tid3), KeyError) class TestRedisBackendNoRedis(Case): def test_redis_None_if_redis_not_installed(self): prev = sys.modules.pop("celery.backends.redis") try: with mask_modules("redis"): from celery.backends.redis import redis self.assertIsNone(redis) finally: sys.modules["celery.backends.redis"] = prev def test_constructor_raises_if_redis_not_installed(self): from celery.backends import redis prev = redis.RedisBackend.redis redis.RedisBackend.redis = None try: with self.assertRaises(ImproperlyConfigured): redis.RedisBackend() finally: redis.RedisBackend.redis = prev
#!/usr/bin/python # -*- coding: utf-8 -*- # Copyright: (c) 2014, Trond Hindenes <trond@hindenes.com>, and others # GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt) # this is a windows documentation stub. actual code lives in the .ps1 # file of the same name ANSIBLE_METADATA = {'metadata_version': '1.1', 'status': ['preview'], 'supported_by': 'core'} DOCUMENTATION = r''' --- module: win_package version_added: "1.7" short_description: Installs/uninstalls an installable package description: - Installs or uninstalls a package in either an MSI or EXE format. - These packages can be sources from the local file system, network file share or a url. - Please read the notes section around some caveats with this module. options: arguments: description: - Any arguments the installer needs to either install or uninstall the package. - If the package is an MSI do not supply the C(/qn), C(/log) or C(/norestart) arguments. - As of Ansible 2.5, this parameter can be a list of arguments and the module will escape the arguments as necessary, it is recommended to use a string when dealing with MSI packages due to the unique escaping issues with msiexec. creates_path: description: - Will check the existance of the path specified and use the result to determine whether the package is already installed. - You can use this in conjunction with C(product_id) and other C(creates_*). type: path version_added: '2.4' creates_service: description: - Will check the existing of the service specified and use the result to determine whether the package is already installed. - You can use this in conjunction with C(product_id) and other C(creates_*). version_added: '2.4' creates_version: description: - Will check the file version property of the file at C(creates_path) and use the result to determine whether the package is already installed. - C(creates_path) MUST be set and is a file. - You can use this in conjunction with C(product_id) and other C(creates_*). version_added: '2.4' expected_return_code: description: - One or more return codes from the package installation that indicates success. - Before Ansible 2.4 this was just 0 but since 2.4 this is both C(0) and C(3010). - A return code of C(3010) usually means that a reboot is required, the C(reboot_required) return value is set if the return code is C(3010). type: list default: [0, 3010] password: description: - The password for C(user_name), must be set when C(user_name) is. aliases: [ user_password ] path: description: - Location of the package to be installed or uninstalled. - This package can either be on the local file system, network share or a url. - If the path is on a network share and the current WinRM transport doesn't support credential delegation, then C(user_name) and C(user_password) must be set to access the file. - There are cases where this file will be copied locally to the server so it can access it, see the notes for more info. - If C(state=present) then this value MUST be set. - If C(state=absent) then this value does not need to be set if C(product_id) is. product_id: description: - The product id of the installed packaged. - This is used for checking whether the product is already installed and getting the uninstall information if C(state=absent). - You can find product ids for installed programs in the Windows registry editor either at C(HKLM:Software\Microsoft\Windows\CurrentVersion\Uninstall) or for 32 bit programs at C(HKLM:Software\Wow6432Node\Microsoft\Windows\CurrentVersion\Uninstall). - This SHOULD be set when the package is not an MSI, or the path is a url or a network share and credential delegation is not being used. The C(creates_*) options can be used instead but is not recommended. aliases: [ productid ] state: description: - Whether to install or uninstall the package. - The module uses C(product_id) and whether it exists at the registry path to see whether it needs to install or uninstall the package. default: present aliases: [ ensure ] username: description: - Username of an account with access to the package if it is located on a file share. - This is only needed if the WinRM transport is over an auth method that does not support credential delegation like Basic or NTLM. aliases: [ user_name ] validate_certs: description: - If C(no), SSL certificates will not be validated. This should only be used on personally controlled sites using self-signed certificates. - Before Ansible 2.4 this defaulted to C(no). type: bool default: 'yes' version_added: '2.4' notes: - For non Windows targets, use the M(package) module instead. - When C(state=absent) and the product is an exe, the path may be different from what was used to install the package originally. If path is not set then the path used will be what is set under C(UninstallString) in the registry for that product_id. - Not all product ids are in a GUID form, some programs incorrectly use a different structure but this module should support any format. - By default all msi installs and uninstalls will be run with the options C(/log, /qn, /norestart). - It is recommended you download the package first from the URL using the M(win_get_url) module as it opens up more flexibility with what must be set when calling C(win_package). - Packages will be temporarily downloaded or copied locally when path is a network location and credential delegation is not set, or path is a URL and the file is not an MSI. - All the installation checks under C(product_id) and C(creates_*) add together, if one fails then the program is considered to be absent. author: - Trond Hindenes (@trondhindenes) - Jordan Borean (@jborean93) ''' EXAMPLES = r''' - name: Install the Visual C thingy win_package: path: http://download.microsoft.com/download/1/6/B/16B06F60-3B20-4FF2-B699-5E9B7962F9AE/VSU_4/vcredist_x64.exe product_id: '{CF2BEA3C-26EA-32F8-AA9B-331F7E34BA97}' arguments: /install /passive /norestart - name: Install Visual C thingy with list of arguments instead of a string win_package: path: http://download.microsoft.com/download/1/6/B/16B06F60-3B20-4FF2-B699-5E9B7962F9AE/VSU_4/vcredist_x64.exe product_id: '{CF2BEA3C-26EA-32F8-AA9B-331F7E34BA97}' arguments: - /install - /passive - /norestart - name: Install Remote Desktop Connection Manager from msi win_package: path: https://download.microsoft.com/download/A/F/0/AF0071F3-B198-4A35-AA90-C68D103BDCCF/rdcman.msi product_id: '{0240359E-6A4C-4884-9E94-B397A02D893C}' state: present - name: Uninstall Remote Desktop Connection Manager win_package: product_id: '{0240359E-6A4C-4884-9E94-B397A02D893C}' state: absent - name: Install Remote Desktop Connection Manager locally omitting the product_id win_package: path: C:\temp\rdcman.msi state: present - name: Uninstall Remote Desktop Connection Manager from local MSI omitting the product_id win_package: path: C:\temp\rdcman.msi state: absent # 7-Zip exe doesn't use a guid for the Product ID - name: Install 7zip from a network share specifying the credentials win_package: path: \\domain\programs\7z.exe product_id: 7-Zip arguments: /S state: present user_name: DOMAIN\User user_password: Password - name: Install 7zip and use a file version for the installation check win_package: path: C:\temp\7z.exe creates_path: C:\Program Files\7-Zip\7z.exe creates_version: 16.04 state: present - name: Uninstall 7zip from the exe win_package: path: C:\Program Files\7-Zip\Uninstall.exe product_id: 7-Zip arguments: /S state: absent - name: Uninstall 7zip without specifying the path win_package: product_id: 7-Zip arguments: /S state: absent - name: Install application and override expected return codes win_package: path: https://download.microsoft.com/download/1/6/7/167F0D79-9317-48AE-AEDB-17120579F8E2/NDP451-KB2858728-x86-x64-AllOS-ENU.exe product_id: '{7DEBE4EB-6B40-3766-BB35-5CBBC385DA37}' arguments: '/q /norestart' state: present expected_return_code: [0, 666, 3010] ''' RETURN = r''' exit_code: description: See rc, this will be removed in favour of rc in Ansible 2.6. returned: change occured type: int sample: 0 log: description: The contents of the MSI log. returned: change occured and package is an MSI type: str sample: Installation completed successfully rc: description: The return code of the package process. returned: change occured type: int sample: 0 reboot_required: description: Whether a reboot is required to finalise package. This is set to true if the executable return code is 3010. returned: always type: bool sample: True restart_required: description: See reboot_required, this will be removed in favour of reboot_required in Ansible 2.6 returned: always type: bool sample: True stdout: description: The stdout stream of the package process. returned: failure during install or uninstall type: str sample: Installing program stderr: description: The stderr stream of the package process. returned: failure during install or uninstall type: str sample: Failed to install program '''
#!/usr/bin/env python # -*- coding: utf-8 -*- ################################################################################ # # RMG - Reaction Mechanism Generator # # Copyright (c) 2002-2010 Prof. William H. Green (whgreen@mit.edu) and the # RMG Team (rmg_dev@mit.edu) # # Permission is hereby granted, free of charge, to any person obtaining a # copy of this software and associated documentation files (the 'Software'), # to deal in the Software without restriction, including without limitation # the rights to use, copy, modify, merge, publish, distribute, sublicense, # and/or sell copies of the Software, and to permit persons to whom the # Software is furnished to do so, subject to the following conditions: # # The above copyright notice and this permission notice shall be included in # all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING # FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER # DEALINGS IN THE SOFTWARE. # ################################################################################ from .base import ReactionSystem, TerminationTime, TerminationConversion from .simple import SimpleReactor
# Fluorescence Analysis import os import cv2 import numpy as np import pandas as pd from plotnine import ggplot, geom_label, aes, geom_line from plantcv.plantcv import print_image from plantcv.plantcv import plot_image from plantcv.plantcv import fatal_error from plantcv.plantcv import params from plantcv.plantcv import outputs def analyze_fvfm(fdark, fmin, fmax, mask, bins=256, label="default"): """Analyze PSII camera images. Inputs: fdark = grayscale fdark image fmin = grayscale fmin image fmax = grayscale fmax image mask = mask of plant (binary, single channel) bins = number of bins (1 to 256 for 8-bit; 1 to 65,536 for 16-bit; default is 256) label = optional label parameter, modifies the variable name of observations recorded Returns: analysis_images = list of images (fv image and fvfm histogram image) :param fdark: numpy.ndarray :param fmin: numpy.ndarray :param fmax: numpy.ndarray :param mask: numpy.ndarray :param bins: int :param label: str :return analysis_images: numpy.ndarray """ # Auto-increment the device counter params.device += 1 # Check that fdark, fmin, and fmax are grayscale (single channel) if not all(len(np.shape(i)) == 2 for i in [fdark, fmin, fmax]): fatal_error("The fdark, fmin, and fmax images must be grayscale images.") # QC Fdark Image fdark_mask = cv2.bitwise_and(fdark, fdark, mask=mask) if np.amax(fdark_mask) > 2000: qc_fdark = False else: qc_fdark = True # Mask Fmin and Fmax Image fmin_mask = cv2.bitwise_and(fmin, fmin, mask=mask) fmax_mask = cv2.bitwise_and(fmax, fmax, mask=mask) # Calculate Fvariable, where Fv = Fmax - Fmin (masked) fv = np.subtract(fmax_mask, fmin_mask) # When Fmin is greater than Fmax, a negative value is returned. # Because the data type is unsigned integers, negative values roll over, resulting in nonsensical values # Wherever Fmin is greater than Fmax, set Fv to zero fv[np.where(fmax_mask < fmin_mask)] = 0 analysis_images = [] # Calculate Fv/Fm (Fvariable / Fmax) where Fmax is greater than zero # By definition above, wherever Fmax is zero, Fvariable will also be zero # To calculate the divisions properly we need to change from unit16 to float64 data types fvfm = fv.astype(np.float64) analysis_images.append(fvfm) fmax_flt = fmax_mask.astype(np.float64) fvfm[np.where(fmax_mask > 0)] /= fmax_flt[np.where(fmax_mask > 0)] # Calculate the median Fv/Fm value for non-zero pixels fvfm_median = np.median(fvfm[np.where(fvfm > 0)]) # Calculate the histogram of Fv/Fm non-zero values fvfm_hist, fvfm_bins = np.histogram(fvfm[np.where(fvfm > 0)], bins, range=(0, 1)) # fvfm_bins is a bins + 1 length list of bin endpoints, so we need to calculate bin midpoints so that # the we have a one-to-one list of x (FvFm) and y (frequency) values. # To do this we add half the bin width to each lower bin edge x-value midpoints = fvfm_bins[:-1] + 0.5 * np.diff(fvfm_bins) # Calculate which non-zero bin has the maximum Fv/Fm value max_bin = midpoints[np.argmax(fvfm_hist)] # Create a dataframe dataset = pd.DataFrame({'Plant Pixels': fvfm_hist, 'Fv/Fm': midpoints}) # Make the histogram figure using plotnine fvfm_hist_fig = (ggplot(data=dataset, mapping=aes(x='Fv/Fm', y='Plant Pixels')) + geom_line(color='green', show_legend=True) + geom_label(label='Peak Bin Value: ' + str(max_bin), x=.15, y=205, size=8, color='green')) analysis_images.append(fvfm_hist_fig) if params.debug == 'print': print_image(fmin_mask, os.path.join(params.debug_outdir, str(params.device) + '_fmin_mask.png')) print_image(fmax_mask, os.path.join(params.debug_outdir, str(params.device) + '_fmax_mask.png')) print_image(fv, os.path.join(params.debug_outdir, str(params.device) + '_fv_convert.png')) fvfm_hist_fig.save(os.path.join(params.debug_outdir, str(params.device) + '_fv_hist.png'), verbose=False) elif params.debug == 'plot': plot_image(fmin_mask, cmap='gray') plot_image(fmax_mask, cmap='gray') plot_image(fv, cmap='gray') print(fvfm_hist_fig) outputs.add_observation(sample=label, variable='fvfm_hist', trait='Fv/Fm frequencies', method='plantcv.plantcv.fluor_fvfm', scale='none', datatype=list, value=fvfm_hist.tolist(), label=np.around(midpoints, decimals=len(str(bins))).tolist()) outputs.add_observation(sample=label, variable='fvfm_hist_peak', trait='peak Fv/Fm value', method='plantcv.plantcv.fluor_fvfm', scale='none', datatype=float, value=float(max_bin), label='none') outputs.add_observation(sample=label, variable='fvfm_median', trait='Fv/Fm median', method='plantcv.plantcv.fluor_fvfm', scale='none', datatype=float, value=float(np.around(fvfm_median, decimals=4)), label='none') outputs.add_observation(sample=label, variable='fdark_passed_qc', trait='Fdark passed QC', method='plantcv.plantcv.fluor_fvfm', scale='none', datatype=bool, value=qc_fdark, label='none') # Store images outputs.images.append(analysis_images) return analysis_images
from __future__ import absolute_import from .packages.six.moves.http_client import ( IncompleteRead as httplib_IncompleteRead ) # Base Exceptions class HTTPError(Exception): "Base exception used by this module." pass class HTTPWarning(Warning): "Base warning used by this module." pass class PoolError(HTTPError): "Base exception for errors caused within a pool." def __init__(self, pool, message): self.pool = pool HTTPError.__init__(self, "%s: %s" % (pool, message)) def __reduce__(self): # For pickling purposes. return self.__class__, (None, None) class RequestError(PoolError): "Base exception for PoolErrors that have associated URLs." def __init__(self, pool, url, message): self.url = url PoolError.__init__(self, pool, message) def __reduce__(self): # For pickling purposes. return self.__class__, (None, self.url, None) class SSLError(HTTPError): "Raised when SSL certificate fails in an HTTPS connection." pass class ProxyError(HTTPError): "Raised when the connection to a proxy fails." pass class DecodeError(HTTPError): "Raised when automatic decoding based on Content-Type fails." pass class ProtocolError(HTTPError): "Raised when something unexpected happens mid-request/response." pass #: Renamed to ProtocolError but aliased for backwards compatibility. ConnectionError = ProtocolError # Leaf Exceptions class MaxRetryError(RequestError): """Raised when the maximum number of retries is exceeded. :param pool: The connection pool :type pool: :class:`~urllib3.connectionpool.HTTPConnectionPool` :param string url: The requested Url :param exceptions.Exception reason: The underlying error """ def __init__(self, pool, url, reason=None): self.reason = reason message = "Max retries exceeded with url: %s (Caused by %r)" % ( url, reason) RequestError.__init__(self, pool, url, message) class HostChangedError(RequestError): "Raised when an existing pool gets a request for a foreign host." def __init__(self, pool, url, retries=3): message = "Tried to open a foreign host with url: %s" % url RequestError.__init__(self, pool, url, message) self.retries = retries class TimeoutStateError(HTTPError): """ Raised when passing an invalid state to a timeout """ pass class TimeoutError(HTTPError): """ Raised when a socket timeout error occurs. Catching this error will catch both :exc:`ReadTimeoutErrors <ReadTimeoutError>` and :exc:`ConnectTimeoutErrors <ConnectTimeoutError>`. """ pass class ReadTimeoutError(TimeoutError, RequestError): "Raised when a socket timeout occurs while receiving data from a server" pass # This timeout error does not have a URL attached and needs to inherit from the # base HTTPError class ConnectTimeoutError(TimeoutError): "Raised when a socket timeout occurs while connecting to a server" pass class NewConnectionError(ConnectTimeoutError, PoolError): "Raised when we fail to establish a new connection. Usually ECONNREFUSED." pass class EmptyPoolError(PoolError): "Raised when a pool runs out of connections and no more are allowed." pass class ClosedPoolError(PoolError): "Raised when a request enters a pool after the pool has been closed." pass class LocationValueError(ValueError, HTTPError): "Raised when there is something wrong with a given URL input." pass class LocationParseError(LocationValueError): "Raised when get_host or similar fails to parse the URL input." def __init__(self, location): message = "Failed to parse: %s" % location HTTPError.__init__(self, message) self.location = location class ResponseError(HTTPError): "Used as a container for an error reason supplied in a MaxRetryError." GENERIC_ERROR = 'too many error responses' SPECIFIC_ERROR = 'too many {status_code} error responses' class SecurityWarning(HTTPWarning): "Warned when perfoming security reducing actions" pass class SubjectAltNameWarning(SecurityWarning): "Warned when connecting to a host with a certificate missing a SAN." pass class InsecureRequestWarning(SecurityWarning): "Warned when making an unverified HTTPS request." pass class SystemTimeWarning(SecurityWarning): "Warned when system time is suspected to be wrong" pass class InsecurePlatformWarning(SecurityWarning): "Warned when certain SSL configuration is not available on a platform." pass class SNIMissingWarning(HTTPWarning): "Warned when making a HTTPS request without SNI available." pass class DependencyWarning(HTTPWarning): """ Warned when an attempt is made to import a module with missing optional dependencies. """ pass class ResponseNotChunked(ProtocolError, ValueError): "Response needs to be chunked in order to read it as chunks." pass class BodyNotHttplibCompatible(HTTPError): """ Body should be httplib.HTTPResponse like (have an fp attribute which returns raw chunks) for read_chunked(). """ pass class IncompleteRead(HTTPError, httplib_IncompleteRead): """ Response length doesn't match expected Content-Length Subclass of http_client.IncompleteRead to allow int value for `partial` to avoid creating large objects on streamed reads. """ def __init__(self, partial, expected): super(IncompleteRead, self).__init__(partial, expected) def __repr__(self): return ('IncompleteRead(%i bytes read, ' '%i more expected)' % (self.partial, self.expected)) class InvalidHeader(HTTPError): "The header provided was somehow invalid." pass class ProxySchemeUnknown(AssertionError, ValueError): "ProxyManager does not support the supplied scheme" # TODO(t-8ch): Stop inheriting from AssertionError in v2.0. def __init__(self, scheme): message = "Not supported proxy scheme %s" % scheme super(ProxySchemeUnknown, self).__init__(message) class HeaderParsingError(HTTPError): "Raised by assert_header_parsing, but we convert it to a log.warning statement." def __init__(self, defects, unparsed_data): message = '%s, unparsed data: %r' % (defects or 'Unknown', unparsed_data) super(HeaderParsingError, self).__init__(message) class UnrewindableBodyError(HTTPError): "urllib3 encountered an error when trying to rewind a body" pass
# -*- coding: utf-8 -*- # Open Source Initiative OSI - The MIT License (MIT):Licensing # # The MIT License (MIT) # Copyright (c) 2012 DotCloud Inc (opensource@dotcloud.com) # # Permission is hereby granted, free of charge, to any person obtaining a copy of # this software and associated documentation files (the "Software"), to deal in # the Software without restriction, including without limitation the rights to # use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies # of the Software, and to permit persons to whom the Software is furnished to do # so, subject to the following conditions: # # The above copyright notice and this permission notice shall be included in all # copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE # SOFTWARE. # # Based on https://github.com/traviscline/gevent-zeromq/blob/master/gevent_zeromq/core.py import zmq import gevent.event import gevent.core STOP_EVERYTHING = False class ZMQSocket(zmq.Socket): def __init__(self, context, socket_type): super(ZMQSocket, self).__init__(context, socket_type) on_state_changed_fd = self.getsockopt(zmq.FD) self._readable = gevent.event.Event() self._writable = gevent.event.Event() try: # gevent>=1.0 self._state_event = gevent.hub.get_hub().loop.io( on_state_changed_fd, gevent.core.READ) self._state_event.start(self._on_state_changed) except AttributeError: # gevent<1.0 self._state_event = gevent.core.read_event(on_state_changed_fd, self._on_state_changed, persist=True) def _on_state_changed(self, event=None, _evtype=None): if self.closed: self._writable.set() self._readable.set() return events = self.getsockopt(zmq.EVENTS) if events & zmq.POLLOUT: self._writable.set() if events & zmq.POLLIN: self._readable.set() def close(self): if not self.closed and getattr(self, '_state_event', None): try: # gevent>=1.0 self._state_event.stop() except AttributeError: # gevent<1.0 self._state_event.cancel() super(ZMQSocket, self).close() def send(self, data, flags=0, copy=True, track=False): if flags & zmq.NOBLOCK: return super(ZMQSocket, self).send(data, flags, copy, track) flags |= zmq.NOBLOCK while True: try: return super(ZMQSocket, self).send(data, flags, copy, track) except zmq.ZMQError, e: if e.errno != zmq.EAGAIN: raise self._writable.clear() self._writable.wait() def recv(self, flags=0, copy=True, track=False): if flags & zmq.NOBLOCK: return super(ZMQSocket, self).recv(flags, copy, track) flags |= zmq.NOBLOCK while True: try: return super(ZMQSocket, self).recv(flags, copy, track) except zmq.ZMQError, e: if e.errno != zmq.EAGAIN: raise self._readable.clear() while not self._readable.wait(timeout=10): events = self.getsockopt(zmq.EVENTS) if bool(events & zmq.POLLIN): print "here we go, nobody told me about new messages!" global STOP_EVERYTHING STOP_EVERYTHING = True raise gevent.GreenletExit() zmq_context = zmq.Context() def server(): socket = ZMQSocket(zmq_context, zmq.REP) socket.bind('ipc://zmqbug') class Cnt: responded = 0 cnt = Cnt() def responder(): while not STOP_EVERYTHING: msg = socket.recv() socket.send(msg) cnt.responded += 1 gevent.spawn(responder) while not STOP_EVERYTHING: print "cnt.responded=", cnt.responded gevent.sleep(0.5) def client(): socket = ZMQSocket(zmq_context, zmq.DEALER) socket.connect('ipc://zmqbug') class Cnt: recv = 0 send = 0 cnt = Cnt() def recvmsg(): while not STOP_EVERYTHING: socket.recv() socket.recv() cnt.recv += 1 def sendmsg(): while not STOP_EVERYTHING: socket.send('', flags=zmq.SNDMORE) socket.send('hello') cnt.send += 1 gevent.sleep(0) gevent.spawn(recvmsg) gevent.spawn(sendmsg) while not STOP_EVERYTHING: print "cnt.recv=", cnt.recv, "cnt.send=", cnt.send gevent.sleep(0.5) gevent.spawn(server) client()
# -*- coding: utf-8 -*- from __future__ import unicode_literals from django.db import models, migrations import django.utils.timezone class Migration(migrations.Migration): dependencies = [ ] operations = [ migrations.CreateModel( name='Notification', fields=[ ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)), ('added', models.DateTimeField(default=django.utils.timezone.now, verbose_name='created on')), ('updated', models.DateTimeField(default=django.utils.timezone.now, verbose_name='updated on')), ('type', models.CharField(max_length=64, verbose_name='type', choices=[(b'node_created', 'node_created'), (b'node_status_changed', 'node_status_changed'), (b'node_own_status_changed', 'node_own_status_changed'), (b'node_deleted', 'node_deleted'), (b'custom', 'custom')])), ('object_id', models.PositiveIntegerField(null=True, blank=True)), ('text', models.CharField(max_length=120, verbose_name='text', blank=True)), ('is_read', models.BooleanField(default=False, verbose_name='read?')), ], options={ 'ordering': ('-id',), }, ), migrations.CreateModel( name='UserEmailNotificationSettings', fields=[ ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)), ('node_created', models.IntegerField(default=30, help_text='-1 (less than 0): disabled; 0: enabled for all; 1 (less than 0): enabled for those in the specified distance range (km)', verbose_name='node_created')), ('node_status_changed', models.IntegerField(default=30, help_text='-1 (less than 0): disabled; 0: enabled for all; 1 (less than 0): enabled for those in the specified distance range (km)', verbose_name='node_status_changed')), ('node_own_status_changed', models.BooleanField(default=True, verbose_name='node_own_status_changed')), ('node_deleted', models.IntegerField(default=30, help_text='-1 (less than 0): disabled; 0: enabled for all; 1 (less than 0): enabled for those in the specified distance range (km)', verbose_name='node_deleted')), ], options={ 'db_table': 'notifications_user_email_settings', 'verbose_name': 'user email notification settings', 'verbose_name_plural': 'user email notification settings', }, ), migrations.CreateModel( name='UserWebNotificationSettings', fields=[ ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)), ('node_created', models.IntegerField(default=30, help_text='-1 (less than 0): disabled; 0: enabled for all; 1 (less than 0): enabled for those in the specified distance range (km)', verbose_name='node_created')), ('node_status_changed', models.IntegerField(default=30, help_text='-1 (less than 0): disabled; 0: enabled for all; 1 (less than 0): enabled for those in the specified distance range (km)', verbose_name='node_status_changed')), ('node_own_status_changed', models.BooleanField(default=True, verbose_name='node_own_status_changed')), ('node_deleted', models.IntegerField(default=30, help_text='-1 (less than 0): disabled; 0: enabled for all; 1 (less than 0): enabled for those in the specified distance range (km)', verbose_name='node_deleted')), ], options={ 'db_table': 'notifications_user_web_settings', 'verbose_name': 'user web notification settings', 'verbose_name_plural': 'user web notification settings', }, ), ]
# vim: set et sw=4 sts=4 fileencoding=utf-8: # # Copyright (c) 2013-2017 Dave Jones <dave@waveform.org.uk> # Copyright (c) 2013 Mime Consulting Ltd. <info@mimeconsulting.co.uk> # All rights reserved. # # Permission is hereby granted, free of charge, to any person obtaining a copy # of this software and associated documentation files (the "Software"), to deal # in the Software without restriction, including without limitation the rights # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell # copies of the Software, and to permit persons to whom the Software is # furnished to do so, subject to the following conditions: # # The above copyright notice and this permission notice shall be included in # all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE # SOFTWARE. """ Defines the URL parsing specific parts of :mod:`lars.datatypes`. """ from __future__ import ( unicode_literals, absolute_import, print_function, division, ) from collections import namedtuple try: from urllib import parse except ImportError: import urlparse as parse from .ipaddress import hostname str = type('') # pylint: disable=redefined-builtin,invalid-name def path(s): """ Returns a :class:`Path` object for the given string. :param str s: The string containing the path to parse :returns: A :class:`Path` object representing the path """ i = s.rfind('/') + 1 dirname, basename = s[:i], s[i:] if dirname and dirname != '/' * len(dirname): dirname = dirname.rstrip('/') i = basename.rfind('.') if i > 0: ext = basename[i:] else: ext = '' return Path(dirname, basename, ext) def url(s): """ Returns a :class:`Url` object for the given string. :param str s: The string containing the URL to parse :returns: A :class:`Url` tuple representing the URL """ return Url(*parse.urlparse(s)) def request(s): """ Returns a :class:`Request` object for the given string. :param str s: The string containing the request line to parse :returns: A :class:`Request` tuple representing the request line """ try: method, s = s.split(' ', 1) except ValueError: raise ValueError('Request line is missing a space separated method') try: s, protocol = s.rsplit(' ', 1) except ValueError: raise ValueError('Request line is missing a space separated protocol') s = s.strip() if not s: raise ValueError('Request line URL cannot be blank') return Request(method, url(s) if s != '*' else None, protocol) class Path(namedtuple('Path', 'dirname basename ext')): """ Represents a path. This type is returned by the :func:`path` function and represents a path in POSIX format (forward slash separators and no drive portion). It is used to represent the path portion of URLs and provides attributes for extracting parts of the path there-in. The original path can be obtained as a string by asking for the string conversion of this class, like so:: p = datatypes.path('/foo/bar/baz.ext') assert p.dirname == '/foo/bar' assert p.basename == 'baz.ext' assert str(p) == '/foo/bar/baz.ext' .. attribute:: dirname A string containing all of the path except the basename at the end .. attribute:: basename A string containing the basename (filename and extension) at the end of the path .. attribute:: ext A string containing the filename's extension (including the leading dot) """ __slots__ = () @property def dirs(self): """ Returns a sequence of the directories making up :attr:`dirname` """ return [d for d in self.dirname.split('/') if d] @property def basename_no_ext(self): """ Returns a string containing basename with the extension removed (including the final dot separator). """ if self.ext: return self.basename[:-len(self.ext)] else: return self.basename @property def isabs(self): """ Returns True if the path is absolute (dirname begins with one or more forward slashes). """ return self.dirname.startswith('/') def join(self, *paths): """ Joins this path with the specified parts, returning a new :class:`Path` object. :param \\*paths: The parts to append to this path :returns: A new :class:`Path` object representing the extended path """ # pylint: disable=invalid-name result = str(self) for p in paths: if not isinstance(p, str): p = str(p) # Strip doubled slashes? Or leave this to normpath? if p.startswith('/'): result = p elif not result or result.endswith('/'): result += p else: result += '/' + p return path(result) def __str__(self): result = self.dirname if not result or result.endswith('/'): return result + self.basename else: return result + '/' + self.basename # This is rather hackish; in Python 2.x, urlparse.ResultMixin provides # functionality for extracting username, password, hostname and port from a # parsed URL. In Python 3 this changed to ResultBase, then to a whole bunch of # undocumented classes (split between strings and bytes) with ResultBase as an # alias try: _ResultMixin = parse.ResultBase # pylint: disable=invalid-name except AttributeError: _ResultMixin = parse.ResultMixin # pylint: disable=invalid-name class Url(namedtuple('Url', ('scheme', 'netloc', 'path_str', 'params', 'query_str', 'fragment')), _ResultMixin): """ Represents a URL. This type is returned by the :func:`url` function and represents the parts of the URL. You can obtain the original URL as a string by requesting the string conversion of this class, for example:: >>> u = datatypes.url('http://foo/bar/baz') >>> print u.scheme http >>> print u.hostname foo >>> print str(u) http://foo/bar/baz .. attribute:: scheme The scheme of the URL, before the first ``:`` .. attribute:: netloc The "network location" of the URL, comprising the hostname and port (separated by a colon), and historically the username and password (prefixed to the hostname and separated with an ampersand) .. attribute:: path_str The path of the URL from the first slash after the network location .. attribute:: path The path of the URL, parsed into a tuple which splits out the directory, filename, and extension:: >>> u = datatypes.url('foo/bar/baz.html') >>> u.path Path(dirname='foo/bar', basename='baz.html', ext='.html') >>> u.path.isabs False .. attribute:: params The parameters of the URL .. attribute:: query_str The query string of the URL from the first question-mark in the path .. attribute:: query The query string, parsed into a mapping of keys to lists of values. For example:: >>> u = datatypes.url('foo/bar?a=1&a=2&b=3&c=') >>> print u.query {'a': ['1', '2'], 'c': [''], 'b': ['3']} >>> print 'a' in u.query True .. attribute:: fragment The fragment of the URL from the last hash-mark to the end of the URL Additionally, the following attributes can be used to separate out the various parts of the :attr:`netloc` attribute: .. attribute:: username The username (historical, rare to see this used on the modern web) .. attribute:: password The password (historical, almost unheard of on the modern web as it's extremely insecure to include credentials in the URL) .. attribute:: hostname The hostname from the network location. This attribute returns a :class:`Hostname` object which can be used to resolve the hostname into an IP address if required. .. attribute:: port The optional network port """ __slots__ = () def geturl(self): """ Return the URL as a string string. """ return parse.urlunparse(self) def __str__(self): return self.geturl() @property def hostname(self): return hostname(super(Url, self).hostname) @property def query(self): # pylint: disable=missing-docstring return parse.parse_qs(self.query_str, keep_blank_values=True) @property def path(self): # pylint: disable=missing-docstring return path(self.path_str) class Request(namedtuple('Request', 'method url protocol')): """ Represents an HTTP request line. This type is returned by the :func:`request` function and represents the three parts of an HTTP request line: the method, the URL (optional, can be None in the case of methods like OPTIONS), and the protocol. The following attributes exist: .. attribute:: method The method of the request (typically GET, POST, or PUT but can technically be any valid HTTP token) .. attribute:: url The requested URL. May be an absolute URL, an absolute path, an authority token, or None in the case that the request line contained "*" for the URL. .. attribute:: protocol The HTTP protocol version requested. A string of the format 'HTTP/x.y' where x.y is the version number. At the time of writing only HTTP/1.0 and HTTP/1.1 are defined. """ def __str__(self): return '%s %s %s' % (self.method, self.url, self.protocol)
#!/usr/bin/env python2 # -*- coding: utf-8 -*- # Copyright 2016 Mirantis, Inc. # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from timmy.analyze_health import GREEN, UNKNOWN, YELLOW, RED from timmy.env import project_name import logging import re import yaml logger = logging.getLogger(project_name) def register(function_mapping): function_mapping['rabbitmqctl-list-queues'] = parse_list_queues function_mapping['rabbitmqctl-status'] = parse_status def parse_list_queues(stdout, script, node, stderr=None, exitcode=None): warning = 100 error = 1000 health = GREEN details = [] if exitcode: health = UNKNOWN return health, details data = [l.rstrip() for l in stdout.splitlines()] for line in data[1:]: elements = line.rstrip().split() if len(elements) < 2: logger.warning('no value in list_queues: "%s"' % line.rstrip()) else: count = int(elements[1]) if count < error and count >= warning: health = max(health, YELLOW) details.append(line) return health, details def prepare_status(stdout): bad_yaml = ''.join(stdout.splitlines()[1:]) # quoting string elements bad_yaml = re.sub(r'([,{])([a-z_A-Z]+)([,}])', r'\1"\2"\3', bad_yaml) # changing first element int a key - replacing , with : bad_yaml = re.sub(r'({[^,]+),', r'\1:', bad_yaml) bad_yaml_list = list(bad_yaml) good_yaml, _ = fix_dicts(bad_yaml_list, 0) status_list = yaml.load(''.join(good_yaml)) status_dict = squash_dicts(status_list) return status_dict def fix_dicts(json_str_list, pos): '''recursively puts all comma-separted values into square brackets to make data look like normal 'key: value' dicts ''' quoted_string = False value = True value_pos = 0 commas = False is_list = False in_list = 0 while pos < len(json_str_list): if not quoted_string: if json_str_list[pos] == '{': json_str_list, pos = fix_dicts(json_str_list, pos+1) elif json_str_list[pos] == '"': quoted_string = True elif json_str_list[pos] == ':': value = True value_pos = pos + 1 elif json_str_list[pos] == '[': if value and not commas: is_list = True in_list += 1 elif json_str_list[pos] == ']': in_list -= 1 elif json_str_list[pos] == ',': commas = True if not in_list: is_list = False elif json_str_list[pos] == '}': if not is_list and commas: json_str_list = (json_str_list[:value_pos] + ['['] + json_str_list[value_pos:pos] + [']'] + json_str_list[pos:]) pos += 2 return json_str_list, pos elif json_str_list[pos] == '"': quoted_string = False pos += 1 return json_str_list, pos def squash_dicts(input_data): # recursively converts [{a:1},{b:2},{c:3}...] into {a:1, b:2, c:3}''' if type(input_data) is list: for i in range(len(input_data)): input_data[i] = squash_dicts(input_data[i]) if all([type(i) is dict for i in input_data]): kv_list = [(k, v) for i in input_data for k, v in i.items()] input_data = dict(kv_list) elif type(input_data) is dict: for k, v in input_data.items(): input_data[k] = squash_dicts(v) return input_data def parse_status(stdout, script, node, stderr=None, exitcode=None): health = GREEN details = [] status = prepare_status(stdout) if not status: health = UNKNOWN details = ['Status unavailable'] if exitcode: if exitcode == 69: health = RED details = ['RabbitMQ is not running'] return health, details # disk free check try: dfree = int(status['disk_free']) dlimit = int(status['disk_free_limit']) dfree_ok = 10**9 # 1GB if dfree > dlimit and dfree < dfree_ok: health = max(health, YELLOW) details.append('disk_free: %s, disk_free_limit: %s' % (dfree, dlimit)) elif dfree <= dlimit: health = max(health, RED) details.append('disk_free: %s, disk_free_limit: %s' % (dfree, dlimit)) except ValueError: details.append('cannot convert disk_free* to int') health = max(health, UNKNOWN) except KeyError: details.append('disk_free* not present') health = max(health, UNKNOWN) # process limit check try: pused = float(status['processes']['used']) plimit = float(status['processes']['limit']) ok_ratio = 0.9 if pused < plimit and pused/plimit > ok_ratio: health = max(health, YELLOW) details.append('processes used: %s, processes limit: %s' % (pused, plimit)) elif pused >= plimit: health = max(health, RED) details.append('processes used: %s, processes limit: %s' % (pused, plimit)) except ValueError: details.append('cannot convert processes* to numbers') health = max(health, UNKNOWN) except KeyError: details.append('processes* not present') health = max(health, UNKNOWN) # fd check try: sused = float(status['file_descriptors']['sockets_used']) slimit = float(status['file_descriptors']['sockets_limit']) ok_ratio = 0.9 if sused < slimit and sused/slimit > ok_ratio: health = max(health, YELLOW) details.append('sockets used: %s, sockets limit: %s' % (sused, slimit)) elif sused >= slimit: health = max(health, RED) details.append('sockets used: %s, sockets limit: %s' % (sused, slimit)) fdused = float(status['file_descriptors']['total_used']) fdlimit = float(status['file_descriptors']['total_limit']) ok_ratio = 0.9 if fdused < fdlimit and fdused/fdlimit > ok_ratio: health = max(health, YELLOW) details.append('fd total used: %s, fd total limit: %s' % (fdused, fdlimit)) elif fdused >= fdlimit: health = max(health, RED) details.append('fd total used: %s, fd total limit: %s' % (fdused, fdlimit)) except ValueError: details.append('cannot convert file_descriptors* to numbers') health = max(health, UNKNOWN) except KeyError: details.append('file_descriptors* not present') health = max(health, UNKNOWN) return health, details
from pele.amber import amberSystem as amb # create a new amber system and load database to be pruned sys = amb.AMBERSystem('coords.prmtop', 'coords.inpcrd') dbcurr = sys.create_database(db="aladipep.db") print 'Collecting minima to delete .. ' listTODel = [] for minimum in dbcurr.minima(): testOutCome1 = sys.check_cistrans(minimum.coords) testOutCome2 = sys.check_CAchirality(minimum.coords) if testOutCome1 and testOutCome2: print 'PASS', minimum._id, minimum.energy else: listTODel.append(minimum) print 'FAIL', minimum._id, minimum.energy print '------------' print 'Number of minima to be deleted = ', len(listTODel) # now delete for minn in listTODel: dbcurr.removeMinimum(minn) #print 'Checking transition states .. ' #ct = 0 #print len(dbcurr.transition_states()) #for ts in dbcurr.transition_states() : # if sys.check_cistrans(ts.coords ): # print 'PASS', ts._id, ts.energy # ct = ct + 1 # # dbcurr.removeTS(ts) # not implemented yet # else: # print 'FAIL', ts._id, ts.energy # # print '------------' # #print 'Number of TS deleted = ', ct
# Copyright (c) 2017-18, Ansible Project # Copyright (c) 2017-18, Abhijeet Kasurde (akasurde@redhat.com) # GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt) class ModuleDocFragment(object): # Parameters for FreeIPA/IPA modules DOCUMENTATION = ''' options: ipa_port: description: - Port of FreeIPA / IPA server. - If the value is not specified in the task, the value of environment variable C(IPA_PORT) will be used instead. - If both the environment variable C(IPA_PORT) and the value are not specified in the task, then default value is set. - 'Environment variable fallback mechanism is added in version 2.5.' default: 443 ipa_host: description: - IP or hostname of IPA server. - If the value is not specified in the task, the value of environment variable C(IPA_HOST) will be used instead. - If both the environment variable C(IPA_HOST) and the value are not specified in the task, then default value is set. - 'Environment variable fallback mechanism is added in version 2.5.' default: ipa.example.com ipa_user: description: - Administrative account used on IPA server. - If the value is not specified in the task, the value of environment variable C(IPA_USER) will be used instead. - If both the environment variable C(IPA_USER) and the value are not specified in the task, then default value is set. - 'Environment variable fallback mechanism is added in version 2.5.' default: admin ipa_pass: description: - Password of administrative user. - If the value is not specified in the task, the value of environment variable C(IPA_PASS) will be used instead. - If both the environment variable C(IPA_PASS) and the value are not specified in the task, then default value is set. - 'Environment variable fallback mechanism is added in version 2.5.' required: true ipa_prot: description: - Protocol used by IPA server. - If the value is not specified in the task, the value of environment variable C(IPA_PROT) will be used instead. - If both the environment variable C(IPA_PROT) and the value are not specified in the task, then default value is set. - 'Environment variable fallback mechanism is added in version 2.5.' default: https choices: ["http", "https"] validate_certs: description: - This only applies if C(ipa_prot) is I(https). - If set to C(no), the SSL certificates will not be validated. - This should only set to C(no) used on personally controlled sites using self-signed certificates. default: true '''
"""Word completion for GNU readline. The completer completes keywords, built-ins and globals in a selectable namespace (which defaults to __main__); when completing NAME.NAME..., it evaluates (!) the expression up to the last dot and completes its attributes. It's very cool to do "import sys" type "sys.", hit the completion key (twice), and see the list of names defined by the sys module! Tip: to use the tab key as the completion key, call readline.parse_and_bind("tab: complete") Notes: - Exceptions raised by the completer function are *ignored* (and generally cause the completion to fail). This is a feature -- since readline sets the tty device in raw (or cbreak) mode, printing a traceback wouldn't work well without some complicated hoopla to save, reset and restore the tty state. - The evaluation of the NAME.NAME... form may cause arbitrary application defined code to be executed if an object with a __getattr__ hook is found. Since it is the responsibility of the application (or the user) to enable this feature, I consider this an acceptable risk. More complicated expressions (e.g. function calls or indexing operations) are *not* evaluated. - When the original stdin is not a tty device, GNU readline is never used, and this module (and the readline module) are silently inactive. """ import atexit import builtins import __main__ __all__ = ["Completer"] class Completer: def __init__(self, namespace = None): """Create a new completer for the command line. Completer([namespace]) -> completer instance. If unspecified, the default namespace where completions are performed is __main__ (technically, __main__.__dict__). Namespaces should be given as dictionaries. Completer instances should be used as the completion mechanism of readline via the set_completer() call: readline.set_completer(Completer(my_namespace).complete) """ if namespace and not isinstance(namespace, dict): raise TypeError('namespace must be a dictionary') # Don't bind to namespace quite yet, but flag whether the user wants a # specific namespace or to use __main__.__dict__. This will allow us # to bind to __main__.__dict__ at completion time, not now. if namespace is None: self.use_main_ns = 1 else: self.use_main_ns = 0 self.namespace = namespace def complete(self, text, state): """Return the next possible completion for 'text'. This is called successively with state == 0, 1, 2, ... until it returns None. The completion should begin with 'text'. """ if self.use_main_ns: self.namespace = __main__.__dict__ if state == 0: if "." in text: self.matches = self.attr_matches(text) else: self.matches = self.global_matches(text) try: return self.matches[state] except IndexError: return None def _callable_postfix(self, val, word): if callable(val): word = word + "(" return word def global_matches(self, text): """Compute matches when text is a simple name. Return a list of all keywords, built-in functions and names currently defined in self.namespace that match. """ import keyword matches = [] n = len(text) for word in keyword.kwlist: if word[:n] == text: matches.append(word) for nspace in [builtins.__dict__, self.namespace]: for word, val in nspace.items(): if word[:n] == text and word != "__builtins__": matches.append(self._callable_postfix(val, word)) return matches def attr_matches(self, text): """Compute matches when text contains a dot. Assuming the text is of the form NAME.NAME....[NAME], and is evaluable in self.namespace, it will be evaluated and its attributes (as revealed by dir()) are used as possible completions. (For class instances, class members are also considered.) WARNING: this can still invoke arbitrary C code, if an object with a __getattr__ hook is evaluated. """ import re m = re.match(r"(\w+(\.\w+)*)\.(\w*)", text) if not m: return [] expr, attr = m.group(1, 3) try: thisobject = eval(expr, self.namespace) except Exception: return [] # get the content of the object, except __builtins__ words = dir(thisobject) if "__builtins__" in words: words.remove("__builtins__") if hasattr(thisobject, '__class__'): words.append('__class__') words.extend(get_class_members(thisobject.__class__)) matches = [] n = len(attr) for word in words: if word[:n] == attr and hasattr(thisobject, word): val = getattr(thisobject, word) word = self._callable_postfix(val, "%s.%s" % (expr, word)) matches.append(word) return matches def get_class_members(klass): ret = dir(klass) if hasattr(klass,'__bases__'): for base in klass.__bases__: ret = ret + get_class_members(base) return ret try: import readline except ImportError: pass else: readline.set_completer(Completer().complete) # Release references early at shutdown (the readline module's # contents are quasi-immortal, and the completer function holds a # reference to globals). atexit.register(lambda: readline.set_completer(None))
# -*- encoding: utf-8 -*- ############################################################################## # # OpenERP, Open Source Management Solution # Copyright (C) 2004-TODAY OpenERP S.A. <http://www.openerp.com> # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## import survey import controllers import wizard
#!/usr/bin/env python # -*- coding: utf-8 -*- from runner.koan import * class AboutTrueAndFalse(Koan): def truth_value(self, condition): if condition: return 'true stuff' else: return 'false stuff' def test_true_is_treated_as_true(self): self.assertEqual("true stuff", self.truth_value(True)) def test_false_is_treated_as_false(self): self.assertEqual("false stuff", self.truth_value(False)) def test_none_is_treated_as_false(self): self.assertEqual("false stuff", self.truth_value(None)) def test_zero_is_treated_as_false(self): self.assertEqual("false stuff", self.truth_value(0)) def test_empty_collections_are_treated_as_false(self): self.assertEqual("false stuff", self.truth_value([])) self.assertEqual("false stuff", self.truth_value(())) self.assertEqual("false stuff", self.truth_value({})) self.assertEqual("false stuff", self.truth_value(set())) def test_blank_strings_are_treated_as_false(self): self.assertEqual("false stuff", self.truth_value("")) def test_everything_else_is_treated_as_true(self): self.assertEqual("true stuff", self.truth_value(1)) self.assertEqual("true stuff", self.truth_value(1,)) self.assertEqual( "true stuff", self.truth_value("Python is named after Monty Python")) self.assertEqual("true stuff", self.truth_value(' '))
import random import unittest from pyStock.models import ( Exchange, Stock, Account, Owner, ) from pyStock.models.money import Currency from analyzer.backtest.constant import ( SELL, BUY_TO_COVER, ) from analyzerstrategies.sma_strategy import SMAStrategy class TestSMAStrategy(unittest.TestCase): def setUp(self): pesos = Currency(name='Pesos', code='ARG') merval = Exchange(name='Merval', code='MERV', currency=pesos) owner = Owner(name='test user') self.account = Account(owner=owner) self.security = Stock(symbol='YPF', exchange=merval) self.tick = {'pattern': None, 'data': {'volume30d': '12165.08453826', 'timestamp': '1446070419', 'high': '305', 'ask': 302.7022, 'last': '302.632', 'bid': 301.0001, 'low': '294.51', 'volume': '437.07501250'}, 'type': 'message', 'security': self.security, 'channel': b'BTC'} def test_quotes_feeder(self): strategy = SMAStrategy(account=self.account, config=None, securities=[self.security], store=None) # not enough data to return action. self.assertTrue(strategy.update(self.tick) is None) tick = self.tick for i in range(0, 340): tick['data']['last'] = random.uniform(300, 350) action = strategy.update(tick) if action is not None: self.assertNotEquals(action, SELL) self.assertNotEquals(action, BUY_TO_COVER)
# Copyright 2013 OpenStack Foundation # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. ADMIN_EXTENSIONS = {} PUBLIC_EXTENSIONS = {} def register_admin_extension(url_prefix, extension_data): """Register extension with collection of admin extensions. Extensions register the information here that will show up in the /extensions page as a way to indicate that the extension is active. url_prefix: unique key for the extension that will appear in the urls generated by the extension. extension_data is a dictionary. The expected fields are: 'name': short, human readable name of the extension 'namespace': xml namespace 'alias': identifier for the extension 'updated': date the extension was last updated 'description': text description of the extension 'links': hyperlinks to documents describing the extension """ ADMIN_EXTENSIONS[url_prefix] = extension_data def register_public_extension(url_prefix, extension_data): """Same as register_admin_extension but for public extensions.""" PUBLIC_EXTENSIONS[url_prefix] = extension_data
"""Implementaton of :class:`GMPYRationalField` class. """ from sympy.polys.domains.rationalfield import RationalField from sympy.polys.domains.groundtypes import ( GMPYRationalType, SymPyRationalType, gmpy_numer, gmpy_denom, gmpy_factorial, ) from sympy.polys.polyerrors import CoercionFailed class GMPYRationalField(RationalField): """Rational field based on GMPY mpq class. """ dtype = GMPYRationalType zero = dtype(0) one = dtype(1) alias = 'QQ_gmpy' def __init__(self): pass def to_sympy(self, a): """Convert `a` to a SymPy object. """ return SymPyRationalType(int(gmpy_numer(a)), int(gmpy_denom(a))) def from_sympy(self, a): """Convert SymPy's Integer to `dtype`. """ if a.is_Rational: return GMPYRationalType(a.p, a.q) elif a.is_Float: from sympy.polys.domains import RR return GMPYRationalType(*RR.as_integer_ratio(a)) else: raise CoercionFailed("expected `Rational` object, got %s" % a) def from_ZZ_python(K1, a, K0): """Convert a Python `int` object to `dtype`. """ return GMPYRationalType(a) def from_QQ_python(K1, a, K0): """Convert a Python `Fraction` object to `dtype`. """ return GMPYRationalType(a.numerator, a.denominator) def from_ZZ_sympy(K1, a, K0): """Convert a SymPy `Integer` object to `dtype`. """ return GMPYRationalType(a.p) def from_QQ_sympy(K1, a, K0): """Convert a SymPy `Rational` object to `dtype`. """ return GMPYRationalType(a.p, a.q) def from_ZZ_gmpy(K1, a, K0): """Convert a GMPY `mpz` object to `dtype`. """ return GMPYRationalType(a) def from_QQ_gmpy(K1, a, K0): """Convert a GMPY `mpq` object to `dtype`. """ return a def from_RR_sympy(K1, a, K0): """Convert a SymPy `Float` object to `dtype`. """ return GMPYRationalType(*K0.as_integer_ratio(a)) def from_RR_mpmath(K1, a, K0): """Convert a mpmath `mpf` object to `dtype`. """ return GMPYRationalType(*K0.as_integer_ratio(a)) def exquo(self, a, b): """Exact quotient of `a` and `b`, implies `__div__`. """ return GMPYRationalType(a.qdiv(b)) def quo(self, a, b): """Quotient of `a` and `b`, implies `__div__`. """ return GMPYRationalType(a.qdiv(b)) def rem(self, a, b): """Remainder of `a` and `b`, implies nothing. """ return self.zero def div(self, a, b): """Division of `a` and `b`, implies `__div__`. """ return GMPYRationalType(a.qdiv(b)), self.zero def numer(self, a): """Returns numerator of `a`. """ return gmpy_numer(a) def denom(self, a): """Returns denominator of `a`. """ return gmpy_denom(a) def factorial(self, a): """Returns factorial of `a`. """ return GMPYRationalType(gmpy_factorial(int(a)))
""" Interactive D3 rendering of matplotlib images ============================================= Functions: General Use ---------------------- :func:`fig_to_html` convert a figure to an html string :func:`fig_to_dict` convert a figure to a dictionary representation :func:`show` launch a web server to view an d3/html figure representation :func:`save_html` save a figure to an html file :func:`save_json` save a JSON representation of a figure to file Functions: IPython Notebook --------------------------- :func:`display` display a figure in an IPython notebook :func:`enable_notebook` enable automatic D3 display of figures in the IPython notebook. :func:`disable_notebook` disable automatic D3 display of figures in the IPython """ __all__ = ["__version__", "fig_to_html", "fig_to_dict", "fig_to_d3", "display_d3", "display", "show_d3", "show", "save_html", "save_json", "enable_notebook", "disable_notebook", "plugins", "urls"] from .__about__ import __version__ from . import plugins from . import urls from ._display import *
#!/usr/bin/env python # Copyright (c) 2010 OpenStack Foundation # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. """Module for VNC Proxying.""" from oslo.config import cfg vnc_opts = [ cfg.StrOpt('novncproxy_base_url', default='http://127.0.0.1:6080/vnc_auto.html', help='Location of VNC console proxy, in the form ' '"http://127.0.0.1:6080/vnc_auto.html"'), cfg.StrOpt('xvpvncproxy_base_url', default='http://127.0.0.1:6081/console', help='Location of nova xvp VNC console proxy, in the form ' '"http://127.0.0.1:6081/console"'), cfg.StrOpt('vncserver_listen', default='127.0.0.1', help='IP address on which instance vncservers should listen'), cfg.StrOpt('vncserver_proxyclient_address', default='127.0.0.1', help='The address to which proxy clients ' '(like nova-xvpvncproxy) should connect'), cfg.BoolOpt('vnc_enabled', default=True, help='Enable VNC related features'), cfg.StrOpt('vnc_keymap', default='en-us', help='Keymap for VNC'), ] CONF = cfg.CONF CONF.register_opts(vnc_opts)
#!/usr/bin/python import os import sys THIS_NAME = "generate.py" # Note: these lists must be kept in sync with the lists in # Document-createElement-namespace.html, and this script must be run whenever # the lists are updated. (We could keep the lists in a shared JSON file, but # seems like too much effort.) FILES = ( ("empty", ""), ("minimal_html", "<!doctype html><title></title>"), ("xhtml", '<html xmlns="http://www.w3.org/1999/xhtml"></html>'), ("svg", '<svg xmlns="http://www.w3.org/2000/svg"></svg>'), ("mathml", '<mathml xmlns="http://www.w3.org/1998/Math/MathML"></mathml>'), ("bare_xhtml", "<html></html>"), ("bare_svg", "<svg></svg>"), ("bare_mathml", "<math></math>"), ("xhtml_ns_removed", """\ <html xmlns="http://www.w3.org/1999/xhtml"> <head><script> var newRoot = document.createElementNS(null, "html"); document.removeChild(document.documentElement); document.appendChild(newRoot); </script></head> </html> """), ("xhtml_ns_changed", """\ <html xmlns="http://www.w3.org/1999/xhtml"> <head><script> var newRoot = document.createElementNS("http://www.w3.org/2000/svg", "abc"); document.removeChild(document.documentElement); document.appendChild(newRoot); </script></head> </html> """), ) EXTENSIONS = ( "html", "xhtml", "xml", "svg", # Was not able to get server MIME type working properly :( #"mml", ) def __main__(): if len(sys.argv) > 1: print "No arguments expected, aborting" return if not os.access(THIS_NAME, os.F_OK): print "Must be run from the directory of " + THIS_NAME + ", aborting" return for name in os.listdir("."): if name == THIS_NAME: continue os.remove(name) manifest = open("MANIFEST", "w") for name, contents in FILES: for extension in EXTENSIONS: f = open(name + "." + extension, "w") f.write(contents) f.close() manifest.write("support " + name + "." + extension + "\n") manifest.close() __main__()
from __future__ import unicode_literals import warnings from django.apps import apps from django.db import models from django.db.utils import IntegrityError, OperationalError, ProgrammingError from django.utils.deprecation import RemovedInDjango110Warning from django.utils.encoding import force_text, python_2_unicode_compatible from django.utils.translation import ugettext_lazy as _ class ContentTypeManager(models.Manager): use_in_migrations = True # Cache to avoid re-looking up ContentType objects all over the place. # This cache is shared by all the get_for_* methods. _cache = {} def get_by_natural_key(self, app_label, model): try: ct = self.__class__._cache[self.db][(app_label, model)] except KeyError: ct = self.get(app_label=app_label, model=model) self._add_to_cache(self.db, ct) return ct def _get_opts(self, model, for_concrete_model): if for_concrete_model: model = model._meta.concrete_model elif model._deferred: model = model._meta.proxy_for_model return model._meta def _get_from_cache(self, opts): key = (opts.app_label, opts.model_name) return self.__class__._cache[self.db][key] def create(self, **kwargs): if 'name' in kwargs: del kwargs['name'] warnings.warn( "ContentType.name field doesn't exist any longer. Please remove it from your code.", RemovedInDjango110Warning, stacklevel=2) return super(ContentTypeManager, self).create(**kwargs) def get_for_model(self, model, for_concrete_model=True): """ Returns the ContentType object for a given model, creating the ContentType if necessary. Lookups are cached so that subsequent lookups for the same model don't hit the database. """ opts = self._get_opts(model, for_concrete_model) try: return self._get_from_cache(opts) except KeyError: pass # The ContentType entry was not found in the cache, therefore we # proceed to load or create it. try: try: # We start with get() and not get_or_create() in order to use # the db_for_read (see #20401). ct = self.get(app_label=opts.app_label, model=opts.model_name) except self.model.DoesNotExist: # Not found in the database; we proceed to create it. This time we # use get_or_create to take care of any race conditions. ct, created = self.get_or_create( app_label=opts.app_label, model=opts.model_name, ) except (OperationalError, ProgrammingError, IntegrityError): # It's possible to migrate a single app before contenttypes, # as it's not a required initial dependency (it's contrib!) # Have a nice error for this. raise RuntimeError( "Error creating new content types. Please make sure contenttypes " "is migrated before trying to migrate apps individually." ) self._add_to_cache(self.db, ct) return ct def get_for_models(self, *models, **kwargs): """ Given *models, returns a dictionary mapping {model: content_type}. """ for_concrete_models = kwargs.pop('for_concrete_models', True) # Final results results = {} # models that aren't already in the cache needed_app_labels = set() needed_models = set() needed_opts = set() for model in models: opts = self._get_opts(model, for_concrete_models) try: ct = self._get_from_cache(opts) except KeyError: needed_app_labels.add(opts.app_label) needed_models.add(opts.model_name) needed_opts.add(opts) else: results[model] = ct if needed_opts: cts = self.filter( app_label__in=needed_app_labels, model__in=needed_models ) for ct in cts: model = ct.model_class() if model._meta in needed_opts: results[model] = ct needed_opts.remove(model._meta) self._add_to_cache(self.db, ct) for opts in needed_opts: # These weren't in the cache, or the DB, create them. ct = self.create( app_label=opts.app_label, model=opts.model_name, ) self._add_to_cache(self.db, ct) results[ct.model_class()] = ct return results def get_for_id(self, id): """ Lookup a ContentType by ID. Uses the same shared cache as get_for_model (though ContentTypes are obviously not created on-the-fly by get_by_id). """ try: ct = self.__class__._cache[self.db][id] except KeyError: # This could raise a DoesNotExist; that's correct behavior and will # make sure that only correct ctypes get stored in the cache dict. ct = self.get(pk=id) self._add_to_cache(self.db, ct) return ct def clear_cache(self): """ Clear out the content-type cache. This needs to happen during database flushes to prevent caching of "stale" content type IDs (see django.contrib.contenttypes.management.update_contenttypes for where this gets called). """ self.__class__._cache.clear() def _add_to_cache(self, using, ct): """Insert a ContentType into the cache.""" # Note it's possible for ContentType objects to be stale; model_class() will return None. # Hence, there is no reliance on model._meta.app_label here, just using the model fields instead. key = (ct.app_label, ct.model) self.__class__._cache.setdefault(using, {})[key] = ct self.__class__._cache.setdefault(using, {})[ct.id] = ct @python_2_unicode_compatible class ContentType(models.Model): app_label = models.CharField(max_length=100) model = models.CharField(_('python model class name'), max_length=100) objects = ContentTypeManager() class Meta: verbose_name = _('content type') verbose_name_plural = _('content types') db_table = 'django_content_type' unique_together = (('app_label', 'model'),) def __str__(self): return self.name @property def name(self): model = self.model_class() if not model: return self.model return force_text(model._meta.verbose_name) def model_class(self): "Returns the Python model class for this type of content." try: return apps.get_model(self.app_label, self.model) except LookupError: return None def get_object_for_this_type(self, **kwargs): """ Returns an object of this type for the keyword arguments given. Basically, this is a proxy around this object_type's get_object() model method. The ObjectNotExist exception, if thrown, will not be caught, so code that calls this method should catch it. """ return self.model_class()._base_manager.using(self._state.db).get(**kwargs) def get_all_objects_for_this_type(self, **kwargs): """ Returns all objects of this type for the keyword arguments given. """ return self.model_class()._base_manager.using(self._state.db).filter(**kwargs) def natural_key(self): return (self.app_label, self.model)
class Solution: def areConnected(self, n: int, threshold: int, queries: List[List[int]]) -> List[bool]: cities=[0]*(n+1) group={} nextGroupId=1 def union(source, to): if source==to: return for c in group[source]: cities[c]=to group[to].extend(group[source]) del group[source] for base in range(threshold+1, n): currentGroupId=nextGroupId nextGroupId+=1 group[currentGroupId]=[] for member in range(base, n+1, base): if cities[member]==0: cities[member]=currentGroupId group[currentGroupId].append(member) else: union(cities[member], currentGroupId) answer=[False]*len(queries) for i in range(len(queries)): u,v=queries[i] if cities[u]==cities[v] and cities[u]!=0: answer[i]=True return answer
""" Python 'utf-16-be' Codec Written by Marc-Andre Lemburg (mal@lemburg.com). (c) Copyright CNRI, All Rights Reserved. NO WARRANTY. """ import codecs ### Codec APIs encode = codecs.utf_16_be_encode def decode(input, errors='strict'): return codecs.utf_16_be_decode(input, errors, True) class IncrementalEncoder(codecs.IncrementalEncoder): def encode(self, input, final=False): return codecs.utf_16_be_encode(input, self.errors)[0] class IncrementalDecoder(codecs.BufferedIncrementalDecoder): _buffer_decode = codecs.utf_16_be_decode class StreamWriter(codecs.StreamWriter): encode = codecs.utf_16_be_encode class StreamReader(codecs.StreamReader): decode = codecs.utf_16_be_decode ### encodings module API def getregentry(): return codecs.CodecInfo( name='utf-16-be', encode=encode, decode=decode, incrementalencoder=IncrementalEncoder, incrementaldecoder=IncrementalDecoder, streamreader=StreamReader, streamwriter=StreamWriter, )
import os class Cert(object): def __init__(self, name, buff): self.name = name self.len = len(buff) self.buff = buff pass def __str__(self): out_str = ['\0']*32 for i in range(len(self.name)): out_str[i] = self.name[i] out_str = "".join(out_str) out_str += str(chr(self.len & 0xFF)) out_str += str(chr((self.len & 0xFF00) >> 8)) out_str += self.buff return out_str pass def main(): cert_list = [] file_list = os.listdir(os.getcwd()) cert_file_list = [] for _file in file_list: pos = _file.find(".key_1024") if pos != -1: cert_file_list.append(_file[:pos]) pos = _file.find(".cer") if pos!= -1: cert_file_list.append(_file[:pos]) for cert_file in cert_file_list: if cert_file == 'private_key': with open(cert_file+".key_1024", 'rb') as f: buff = f.read() cert_list.append(Cert(cert_file, buff)) if cert_file == 'certificate': with open(cert_file+".cer", 'rb') as f: buff = f.read() cert_list.append(Cert(cert_file, buff)) with open('esp_cert_private_key.bin', 'wb+') as f: for _cert in cert_list: f.write("%s" % _cert) pass if __name__ == '__main__': main()
import os import sys import random import unittest sys.path.insert(1, os.path.abspath(os.path.join(__file__, "../.."))) import base_test repo_root = os.path.abspath(os.path.join(__file__, "../../..")) sys.path.insert(1, os.path.join(repo_root, "tools", "webdriver")) from webdriver import exceptions class SendKeysTest(base_test.WebDriverBaseTest): def setUp(self): self.driver.get(self.webserver.where_is("user_input/res/text-form.html")) def test_send_simple_string(self): element = self.driver.find_element_by_id("Text1") element.send_keys("lorem ipsum") self.assertEquals(self.driver.find_element_by_id("text").get_text(), u"lorem ipsum") def test_send_return(self): element = self.driver.find_element_by_id("Text1") returnkey = unichr(int("E006", 16)) element.send_keys([returnkey]) self.assertEquals(u"" + self.driver.get_current_url(), u"" + self.webserver.where_is("user_input/res/text-form-landing.html?e=mc2")) def test_send_backspace(self): element = self.driver.find_element_by_id("Text1") element.send_keys("world ") element.send_keys("wide ") element.send_keys("web ") element.send_keys("consortium") backspace= unichr(int("E003", 16)) for i in range(0, 11): element.send_keys([backspace]) self.assertEquals(self.driver.find_element_by_id("text").get_text(), u"world wide web") def test_send_tab(self): element1 = self.driver.find_element_by_id("Text1") element2 = self.driver.find_element_by_id("Text2") element1.send_keys("typing here") tab= unichr(int("E004", 16)) element1.send_keys([tab]) output = self.driver.find_element_by_id("output") tab_pressed = output.get_attribute("checked") self.assertEquals(tab_pressed, u"true") def test_send_shift(self): element = self.driver.find_element_by_id("Text1") element.send_keys("low ") shift= unichr(int("E008", 16)) element.send_keys([shift , "u", "p", shift]) self.assertEquals(self.driver.find_element_by_id("text").get_text(), u"low UP") def test_send_arrow_keys(self): element = self.driver.find_element_by_id("Text1") element.send_keys("internet") backspace= unichr(int("E003", 16)) left= unichr(int("E012", 16)) right= unichr(int("E014", 16)) for i in range(0, 4): element.send_keys([left]) element.send_keys([backspace]) element.send_keys([right]) element.send_keys("a") self.assertEquals(self.driver.find_element_by_id("text").get_text(), u"intranet") def test_select_text_with_shift(self): element = self.driver.find_element_by_id("Text1") element.send_keys("WebDriver") backspace= unichr(int("E003", 16)) shift= unichr(int("E008", 16)) left= unichr(int("E012", 16)) element.send_keys([shift, left, left, left, left, left, left, backspace]) self.assertEquals(self.driver.find_element_by_id("text").get_text(), u"Web") if __name__ == "__main__": unittest.main()
#!/usr/bin/python # # Copyright (c) 2008--2013 Red Hat, Inc. # # This software is licensed to you under the GNU General Public License, # version 2 (GPLv2). There is NO WARRANTY for this software, express or # implied, including the implied warranties of MERCHANTABILITY or FITNESS # FOR A PARTICULAR PURPOSE. You should have received a copy of GPLv2 # along with this software; if not, see # http://www.gnu.org/licenses/old-licenses/gpl-2.0.txt. # # Red Hat trademarks are not licensed under GPLv2. No permission is # granted to use or replicate Red Hat trademarks that are incorporated # in this software or its documentation. ## # rhnDefines.py - Constants used throughout the Spacewalk Proxy. #----------------------------------------------------------------------------- # """Constants used by the Spacewalk Proxy""" # HTTP Headers HEADER_ACTUAL_URI = 'X-RHN-ActualURI' HEADER_EFFECTIVE_URI = 'X-RHN-EffectiveURI' HEADER_CHECKSUM = 'X-RHN-Checksum' HEADER_LOCATION = 'Location' HEADER_CONTENT_LENGTH = 'Content-Length' HEADER_RHN_REDIRECT = 'X-RHN-Redirect' HEADER_RHN_ORIG_LOC = 'X-RHN-OriginalLocation' # HTTP Schemes SCHEME_HTTP = 'http' SCHEME_HTTPS = 'https' # These help us match URIs when kickstarting through a Proxy. URI_PREFIX_KS = '/ty/' URI_PREFIX_KS_CHECKSUM = '/ty-cksm/' # Component Constants COMPONENT_BROKER = 'proxy.broker' COMPONENT_REDIRECT = 'proxy.redirect'
#!/usr/bin/env python # # A script to compare the --debug=memoizer output found int # two different files. import sys,string def memoize_output(fname): mout = {} lines=filter(lambda words: len(words) == 5 and words[1] == 'hits' and words[3] == 'misses', map(string.split, open(fname,'r').readlines())) for line in lines: mout[line[-1]] = ( int(line[0]), int(line[2]) ) return mout def memoize_cmp(filea, fileb): ma = memoize_output(filea) mb = memoize_output(fileb) print 'All output: %s / %s [delta]'%(filea, fileb) print '----------HITS---------- ---------MISSES---------' cfmt='%7d/%-7d [%d]' ma_o = [] mb_o = [] mab = [] for k in ma.keys(): if k in mb.keys(): if k not in mab: mab.append(k) else: ma_o.append(k) for k in mb.keys(): if k in ma.keys(): if k not in mab: mab.append(k) else: mb_o.append(k) mab.sort() ma_o.sort() mb_o.sort() for k in mab: hits = cfmt%(ma[k][0], mb[k][0], mb[k][0]-ma[k][0]) miss = cfmt%(ma[k][1], mb[k][1], mb[k][1]-ma[k][1]) print '%-24s %-24s %s'%(hits, miss, k) for k in ma_o: hits = '%7d/ --'%(ma[k][0]) miss = '%7d/ --'%(ma[k][1]) print '%-24s %-24s %s'%(hits, miss, k) for k in mb_o: hits = ' -- /%-7d'%(mb[k][0]) miss = ' -- /%-7d'%(mb[k][1]) print '%-24s %-24s %s'%(hits, miss, k) print '-'*(24+24+1+20) if __name__ == "__main__": if len(sys.argv) != 3: print """Usage: %s file1 file2 Compares --debug=memomize output from file1 against file2."""%sys.argv[0] sys.exit(1) memoize_cmp(sys.argv[1], sys.argv[2]) sys.exit(0)
"""Email backend that writes messages to a file.""" import datetime import os from django.conf import settings from django.core.exceptions import ImproperlyConfigured from django.core.mail.backends.console import EmailBackend as ConsoleEmailBackend from django.utils import six class EmailBackend(ConsoleEmailBackend): def __init__(self, *args, **kwargs): self._fname = None if 'file_path' in kwargs: self.file_path = kwargs.pop('file_path') else: self.file_path = getattr(settings, 'EMAIL_FILE_PATH', None) # Make sure self.file_path is a string. if not isinstance(self.file_path, six.string_types): raise ImproperlyConfigured('Path for saving emails is invalid: %r' % self.file_path) self.file_path = os.path.abspath(self.file_path) # Make sure that self.file_path is an directory if it exists. if os.path.exists(self.file_path) and not os.path.isdir(self.file_path): raise ImproperlyConfigured( 'Path for saving email messages exists, but is not a directory: %s' % self.file_path ) # Try to create it, if it not exists. elif not os.path.exists(self.file_path): try: os.makedirs(self.file_path) except OSError as err: raise ImproperlyConfigured( 'Could not create directory for saving email messages: %s (%s)' % (self.file_path, err) ) # Make sure that self.file_path is writable. if not os.access(self.file_path, os.W_OK): raise ImproperlyConfigured('Could not write to directory: %s' % self.file_path) # Finally, call super(). # Since we're using the console-based backend as a base, # force the stream to be None, so we don't default to stdout kwargs['stream'] = None super(EmailBackend, self).__init__(*args, **kwargs) def write_message(self, message): self.stream.write(message.message().as_bytes() + b'\n') self.stream.write(b'-' * 79) self.stream.write(b'\n') def _get_filename(self): """Return a unique file name.""" if self._fname is None: timestamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S") fname = "%s-%s.log" % (timestamp, abs(id(self))) self._fname = os.path.join(self.file_path, fname) return self._fname def open(self): if self.stream is None: self.stream = open(self._get_filename(), 'ab') return True return False def close(self): try: if self.stream is not None: self.stream.close() finally: self.stream = None
# ---------------------------------------------------------------------------- # pyglet # Copyright (c) 2006-2008 Alex Holkner # All rights reserved. # # Redistribution and use in source and binary forms, with or without # modification, are permitted provided that the following conditions # are met: # # * Redistributions of source code must retain the above copyright # notice, this list of conditions and the following disclaimer. # * Redistributions in binary form must reproduce the above copyright # notice, this list of conditions and the following disclaimer in # the documentation and/or other materials provided with the # distribution. # * Neither the name of pyglet nor the names of its # contributors may be used to endorse or promote products # derived from this software without specific prior written # permission. # # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS # "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT # LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS # FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE # COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, # INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, # BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; # LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER # CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT # LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN # ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE # POSSIBILITY OF SUCH DAMAGE. # ---------------------------------------------------------------------------- ''' ''' __docformat__ = 'restructuredtext' __version__ = '$Id: lib_glx.py 597 2007-02-03 16:13:07Z Alex.Holkner $' import ctypes from ctypes import * import pyglet from pyglet.gl.lib import missing_function, decorate_function from pyglet.compat import asbytes __all__ = ['link_GL', 'link_GLU', 'link_WGL'] _debug_trace = pyglet.options['debug_trace'] gl_lib = ctypes.windll.opengl32 glu_lib = ctypes.windll.glu32 wgl_lib = gl_lib if _debug_trace: from pyglet.lib import _TraceLibrary gl_lib = _TraceLibrary(gl_lib) glu_lib = _TraceLibrary(glu_lib) wgl_lib = _TraceLibrary(wgl_lib) try: wglGetProcAddress = wgl_lib.wglGetProcAddress wglGetProcAddress.restype = CFUNCTYPE(POINTER(c_int)) wglGetProcAddress.argtypes = [c_char_p] _have_get_proc_address = True except AttributeError: _have_get_proc_address = False class WGLFunctionProxy(object): __slots__ = ['name', 'requires', 'suggestions', 'ftype', 'func'] def __init__(self, name, ftype, requires, suggestions): assert _have_get_proc_address self.name = name self.ftype = ftype self.requires = requires self.suggestions = suggestions self.func = None def __call__(self, *args, **kwargs): if self.func: return self.func(*args, **kwargs) from pyglet.gl import current_context if not current_context: raise Exception( 'Call to function "%s" before GL context created' % self.name) address = wglGetProcAddress(asbytes(self.name)) if cast(address, POINTER(c_int)): # check cast because address is func self.func = cast(address, self.ftype) decorate_function(self.func, self.name) else: self.func = missing_function( self.name, self.requires, self.suggestions) result = self.func(*args, **kwargs) return result def link_GL(name, restype, argtypes, requires=None, suggestions=None): try: func = getattr(gl_lib, name) func.restype = restype func.argtypes = argtypes decorate_function(func, name) return func except AttributeError: # Not in opengl32.dll. Try and get a pointer from WGL. try: fargs = (restype,) + tuple(argtypes) ftype = ctypes.WINFUNCTYPE(*fargs) if _have_get_proc_address: from pyglet.gl import gl_info if gl_info.have_context(): address = wglGetProcAddress(name) if address: func = cast(address, ftype) decorate_function(func, name) return func else: # Insert proxy until we have a context return WGLFunctionProxy(name, ftype, requires, suggestions) except: pass return missing_function(name, requires, suggestions) def link_GLU(name, restype, argtypes, requires=None, suggestions=None): try: func = getattr(glu_lib, name) func.restype = restype func.argtypes = argtypes decorate_function(func, name) return func except AttributeError: # Not in glu32.dll. Try and get a pointer from WGL. try: fargs = (restype,) + tuple(argtypes) ftype = ctypes.WINFUNCTYPE(*fargs) if _have_get_proc_address: from pyglet.gl import gl_info if gl_info.have_context(): address = wglGetProcAddress(name) if address: func = cast(address, ftype) decorate_function(func, name) return func else: # Insert proxy until we have a context return WGLFunctionProxy(name, ftype, requires, suggestions) except: pass return missing_function(name, requires, suggestions) link_WGL = link_GL
# Copyright 2012 Google Inc. All Rights Reserved. # # Permission is hereby granted, free of charge, to any person obtaining a # copy of this software and associated documentation files (the # "Software"), to deal in the Software without restriction, including # without limitation the rights to use, copy, modify, merge, publish, dis- # tribute, sublicense, and/or sell copies of the Software, and to permit # persons to whom the Software is furnished to do so, subject to the fol- # lowing conditions: # # The above copyright notice and this permission notice shall be included # in all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS # OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL- # ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT # SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, # WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS # IN THE SOFTWARE. import time class BucketListingRef(object): """ Container that holds a reference to one result from a bucket listing, allowing polymorphic iteration over wildcard-iterated URIs, Keys, or Prefixes. At a minimum, every reference contains a StorageUri. If the reference came from a bucket listing (as opposed to a manually instantiated ref that might populate only the StorageUri), it will additionally contain either a Key or a Prefix, depending on whether it was a reference to an object or was just a prefix of a path (i.e., bucket subdirectory). The latter happens when the bucket was listed using delimiter='/'. Note that Keys are shallow-populated, based on the contents extracted from parsing a bucket listing. This includes name, length, and other fields (basically, the info listed by gsutil ls -l), but does not include information like ACL and location (which require separate server requests, which is why there's a separate gsutil ls -L option to get this more detailed info). """ def __init__(self, uri, key=None, prefix=None, headers=None): """Instantiate BucketListingRef from uri and (if available) key or prefix. Args: uri: StorageUri for the object (required). key: Key for the object, or None if not available. prefix: Prefix for the subdir, or None if not available. headers: Dictionary containing optional HTTP headers to pass to boto (which happens when GetKey() is called on an BucketListingRef which has no constructor-populated Key), or None if not available. At most one of key and prefix can be populated. """ assert key is None or prefix is None self.uri = uri self.key = key self.prefix = prefix self.headers = headers or {} def GetUri(self): """Get URI form of listed URI. Returns: StorageUri. """ return self.uri def GetUriString(self): """Get string URI form of listed URI. Returns: String. """ return self.uri.uri def NamesBucket(self): """Determines if this BucketListingRef names a bucket. Returns: bool indicator. """ return self.key is None and self.prefix is None and self.uri.names_bucket() def IsLatest(self): """Determines if this BucketListingRef names the latest version of an object. Returns: bool indicator. """ return hasattr(self.uri, 'is_latest') and self.uri.is_latest def GetRStrippedUriString(self): """Get string URI form of listed URI, stripped of any right trailing delims, and without version string. Returns: String. """ return self.uri.versionless_uri.rstrip('/') def HasKey(self): """Return bool indicator of whether this BucketListingRef has a Key.""" return bool(self.key) def HasPrefix(self): """Return bool indicator of whether this BucketListingRef has a Prefix.""" return bool(self.prefix) def GetKey(self): """Get Key form of listed URI. Returns: Subclass of boto.s3.key.Key. Raises: BucketListingRefException: for bucket-only uri. """ # For gsutil ls -l gs://bucket self.key will be populated from (boto) # parsing the bucket listing. But as noted and handled below there are # cases where self.key isn't populated. if not self.key: if not self.uri.names_object(): raise BucketListingRefException( 'Attempt to call GetKey() on Key-less BucketListingRef (uri=%s) ' % self.uri) # This case happens when we do gsutil ls -l on a object name-ful # StorageUri with no object-name wildcard. Since the ls command # implementation only reads bucket info we need to read the object # for this case. self.key = self.uri.get_key(validate=False, headers=self.headers) # When we retrieve the object this way its last_modified timestamp # is formatted in RFC 1123 format, which is different from when we # retrieve from the bucket listing (which uses ISO 8601 format), so # convert so we consistently return ISO 8601 format. tuple_time = (time.strptime(self.key.last_modified, '%a, %d %b %Y %H:%M:%S %Z')) self.key.last_modified = time.strftime('%Y-%m-%dT%H:%M:%S', tuple_time) return self.key def GetPrefix(self): """Get Prefix form of listed URI. Returns: boto.s3.prefix.Prefix. Raises: BucketListingRefException: if this object has no Prefix. """ if not self.prefix: raise BucketListingRefException( 'Attempt to call GetPrefix() on Prefix-less BucketListingRef ' '(uri=%s)' % self.uri) return self.prefix def __repr__(self): """Returns string representation of BucketListingRef.""" return 'BucketListingRef(%s, HasKey=%s, HasPrefix=%s)' % ( self.uri, self.HasKey(), self.HasPrefix()) class BucketListingRefException(StandardError): """Exception thrown for invalid BucketListingRef requests.""" def __init__(self, reason): StandardError.__init__(self) self.reason = reason def __repr__(self): return 'BucketListingRefException: %s' % self.reason def __str__(self): return 'BucketListingRefException: %s' % self.reason
# Copyright 2016 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== """Export a TensorFlow model. See: go/tf-exporter """ from __future__ import absolute_import from __future__ import division from __future__ import print_function import os import re import six from google.protobuf.any_pb2 import Any from tensorflow.contrib.session_bundle import constants from tensorflow.contrib.session_bundle import gc from tensorflow.contrib.session_bundle import manifest_pb2 from tensorflow.core.framework import graph_pb2 from tensorflow.python.framework import dtypes from tensorflow.python.framework import ops from tensorflow.python.platform import gfile from tensorflow.python.platform import tf_logging as logging from tensorflow.python.training import saver as tf_saver from tensorflow.python.training import training_util from tensorflow.python.util import compat from tensorflow.python.util.deprecation import deprecated @deprecated("2017-06-30", "Please use SavedModel instead.") def gfile_copy_callback(files_to_copy, export_dir_path): """Callback to copy files using `gfile.Copy` to an export directory. This method is used as the default `assets_callback` in `Exporter.init` to copy assets from the `assets_collection`. It can also be invoked directly to copy additional supplementary files into the export directory (in which case it is not a callback). Args: files_to_copy: A dictionary that maps original file paths to desired basename in the export directory. export_dir_path: Directory to copy the files to. """ logging.info("Write assets into: %s using gfile_copy.", export_dir_path) gfile.MakeDirs(export_dir_path) for source_filepath, basename in files_to_copy.items(): new_path = os.path.join( compat.as_bytes(export_dir_path), compat.as_bytes(basename)) logging.info("Copying asset %s to path %s.", source_filepath, new_path) if gfile.Exists(new_path): # Guard against being restarted while copying assets, and the file # existing and being in an unknown state. # TODO(b/28676216): Do some file checks before deleting. logging.info("Removing file %s.", new_path) gfile.Remove(new_path) gfile.Copy(source_filepath, new_path) @deprecated("2017-06-30", "Please use SavedModel instead.") def regression_signature(input_tensor, output_tensor): """Creates a regression signature. Args: input_tensor: Tensor specifying the input to a graph. output_tensor: Tensor specifying the output of a graph. Returns: A Signature message. """ signature = manifest_pb2.Signature() signature.regression_signature.input.tensor_name = input_tensor.name signature.regression_signature.output.tensor_name = output_tensor.name return signature @deprecated("2017-06-30", "Please use SavedModel instead.") def classification_signature(input_tensor, classes_tensor=None, scores_tensor=None): """Creates a classification signature. Args: input_tensor: Tensor specifying the input to a graph. classes_tensor: Tensor specifying the output classes of a graph. scores_tensor: Tensor specifying the scores of the output classes. Returns: A Signature message. """ signature = manifest_pb2.Signature() signature.classification_signature.input.tensor_name = input_tensor.name if classes_tensor is not None: signature.classification_signature.classes.tensor_name = classes_tensor.name if scores_tensor is not None: signature.classification_signature.scores.tensor_name = scores_tensor.name return signature @deprecated("2017-06-30", "Please use SavedModel instead.") def generic_signature(name_tensor_map): """Creates a generic signature of name to Tensor name. Args: name_tensor_map: Map from logical name to Tensor. Returns: A Signature message. """ signature = manifest_pb2.Signature() for name, tensor in six.iteritems(name_tensor_map): signature.generic_signature.map[name].tensor_name = tensor.name return signature class Exporter(object): """Exporter helps package a TensorFlow model for serving. Args: saver: Saver object. """ def __init__(self, saver): # Makes a copy of the saver-def and disables garbage-collection, since the # exporter enforces garbage-collection independently. Specifically, since # the exporter performs atomic copies of the saver output, it is required # that garbage-collection via the underlying saver be disabled. saver_def = saver.as_saver_def() saver_def.ClearField("max_to_keep") self._saver = tf_saver.Saver(saver_def=saver_def) self._has_init = False self._assets_to_copy = {} @deprecated("2017-06-30", "Please use SavedModel instead.") def init(self, graph_def=None, init_op=None, clear_devices=False, default_graph_signature=None, named_graph_signatures=None, assets_collection=None, assets_callback=gfile_copy_callback): """Initialization. Args: graph_def: A GraphDef message of the graph to be used in inference. GraphDef of default graph is used when None. init_op: Op to be used in initialization. clear_devices: If device info of the graph should be cleared upon export. default_graph_signature: Default signature of the graph. named_graph_signatures: Map of named input/output signatures of the graph. assets_collection: A collection of constant asset filepath tensors. If set the assets will be exported into the asset directory. assets_callback: callback with two argument called during export with the list of files to copy and the asset path. Raises: RuntimeError: if init is called more than once. TypeError: if init_op is not an Operation or None. ValueError: if asset file path tensors are not non-empty constant string scalar tensors. """ # Avoid Dangerous default value [] if named_graph_signatures is None: named_graph_signatures = {} assets = [] if assets_collection: for asset_tensor in assets_collection: asset_filepath = self._file_path_value(asset_tensor) if not asset_filepath: raise ValueError("invalid asset filepath tensor %s" % asset_tensor) basename = os.path.basename(asset_filepath) assets.append((basename, asset_tensor)) self._assets_to_copy[asset_filepath] = basename if self._has_init: raise RuntimeError("init should be called only once") self._has_init = True if graph_def or clear_devices: copy = graph_pb2.GraphDef() if graph_def: copy.CopyFrom(graph_def) else: copy.CopyFrom(ops.get_default_graph().as_graph_def()) if clear_devices: for node in copy.node: node.device = "" graph_any_buf = Any() graph_any_buf.Pack(copy) ops.add_to_collection(constants.GRAPH_KEY, graph_any_buf) if init_op: if not isinstance(init_op, ops.Operation): raise TypeError("init_op needs to be an Operation: %s" % init_op) ops.add_to_collection(constants.INIT_OP_KEY, init_op) signatures_proto = manifest_pb2.Signatures() if default_graph_signature: signatures_proto.default_signature.CopyFrom(default_graph_signature) for signature_name, signature in six.iteritems(named_graph_signatures): signatures_proto.named_signatures[signature_name].CopyFrom(signature) signatures_any_buf = Any() signatures_any_buf.Pack(signatures_proto) ops.add_to_collection(constants.SIGNATURES_KEY, signatures_any_buf) for filename, tensor in assets: asset = manifest_pb2.AssetFile() asset.filename = filename asset.tensor_binding.tensor_name = tensor.name asset_any_buf = Any() asset_any_buf.Pack(asset) ops.add_to_collection(constants.ASSETS_KEY, asset_any_buf) self._assets_callback = assets_callback @deprecated("2017-06-30", "Please use SavedModel instead.") def export(self, export_dir_base, global_step_tensor, sess=None, exports_to_keep=None): """Exports the model. Args: export_dir_base: A string path to the base export dir. global_step_tensor: An Tensor or tensor name providing the global step counter to append to the export directory path and set in the manifest version. sess: A Session to use to save the parameters. exports_to_keep: a gc.Path filter function used to determine the set of exports to keep. If set to None, all versions will be kept. Returns: The string path to the exported directory. Raises: RuntimeError: if init is not called. RuntimeError: if the export would overwrite an existing directory. """ if not self._has_init: raise RuntimeError("init must be called first") # Export dir must not end with / or it will break exports to keep. Strip /. if export_dir_base.endswith("/"): export_dir_base = export_dir_base[:-1] global_step = training_util.global_step(sess, global_step_tensor) export_dir = os.path.join( compat.as_bytes(export_dir_base), compat.as_bytes(constants.VERSION_FORMAT_SPECIFIER % global_step)) # Prevent overwriting on existing exports which could lead to bad/corrupt # storage and loading of models. This is an important check that must be # done before any output files or directories are created. if gfile.Exists(export_dir): raise RuntimeError("Overwriting exports can cause corruption and are " "not allowed. Duplicate export dir: %s" % export_dir) # Output to a temporary directory which is atomically renamed to the final # directory when complete. tmp_export_dir = compat.as_text(export_dir) + "-tmp" gfile.MakeDirs(tmp_export_dir) self._saver.save(sess, os.path.join( compat.as_text(tmp_export_dir), compat.as_text(constants.EXPORT_BASE_NAME)), meta_graph_suffix=constants.EXPORT_SUFFIX_NAME) # Run the asset callback. if self._assets_callback and self._assets_to_copy: assets_dir = os.path.join( compat.as_bytes(tmp_export_dir), compat.as_bytes(constants.ASSETS_DIRECTORY)) gfile.MakeDirs(assets_dir) self._assets_callback(self._assets_to_copy, assets_dir) # TODO(b/27794910): Delete *checkpoint* file before rename. gfile.Rename(tmp_export_dir, export_dir) if exports_to_keep: # create a simple parser that pulls the export_version from the directory. def parser(path): match = re.match("^" + export_dir_base + "/(\\d{8})$", path.path) if not match: return None return path._replace(export_version=int(match.group(1))) paths_to_delete = gc.negation(exports_to_keep) for p in paths_to_delete(gc.get_paths(export_dir_base, parser=parser)): gfile.DeleteRecursively(p.path) return export_dir def _file_path_value(self, path_tensor): """Returns the filepath value stored in constant `path_tensor`.""" if not isinstance(path_tensor, ops.Tensor): raise TypeError("tensor is not a Tensor") if path_tensor.op.type != "Const": raise TypeError("Only constants tensor are supported") if path_tensor.dtype != dtypes.string: raise TypeError("File paths should be string") str_value = path_tensor.op.get_attr("value").string_val if len(str_value) != 1: raise TypeError("Only scalar tensors are supported") return str_value[0]
#!/usr/bin/env python from __future__ import print_function import unicodecsv as csv import argparse import panphon import Levenshtein import munkres import panphon.distance from functools import partial def levenshtein_dist(_, a, b): return Levenshtein.distance(a, b) def dogol_leven_dist(_, a, b): return Levenshtein.distance(dist.map_to_dogol_prime(a), dist.map_to_dogol_prime(b)) def feature_hamming_dist(dist, a, b): return dist.feature_edit_distance(a, b) def feature_weighted_dist(dist, a, b): return dist.weighted_feature_edit_distance(a, b) def construct_cost_matrix(words_a, words_b, dist): def matrix_row(word_a, words_b): return [dist(word_a, word_b) for (word_b, _) in words_b] return [matrix_row(word_a, words_b) for (word_a, _) in words_a] def score(indices): pairs, errors = 0, 0 for row, column in indices: pairs += 1 if row != column: errors += 1 return pairs, errors def main(wordlist1, wordlist2, dist_funcs): with open(wordlist1, 'rb') as file_a, open(wordlist2, 'rb') as file_b: reader_a = csv.reader(file_a, encoding='utf-8') reader_b = csv.reader(file_b, encoding='utf-8') print('Reading word lists...') words = zip([(w, g) for (g, w) in reader_a], [(w, g) for (g, w) in reader_b]) words_a, words_b = zip(*[(a, b) for (a, b) in words if a and b]) print('Constructing cost matrix...') matrix = construct_cost_matrix(words_a, words_b, dist_funcs) m = munkres.Munkres() print('Computing matrix using Hungarian Algorithm...') indices = m.compute(matrix) print(score(indices)) print('Done.') if __name__ == '__main__': parser = argparse.ArgumentParser(usage='Align two lists of "cognates" using a specified distance metric.') parser.add_argument('wordlists', nargs=2, help='Filenames of two wordlists in corresponding order.') parser.add_argument('-d', '--dist', default='hamming', help='Distance metric (e.g. Hamming).') args = parser.parse_args() dists = {'levenshtein': levenshtein_dist, 'dogol-leven': dogol_leven_dist, 'hamming': feature_hamming_dist, 'weighted': feature_weighted_dist} dist = panphon.distance.Distance() dist_funcs = partial(dists[args.dist], dist) main(args.wordlists[0], args.wordlists[1], dist_funcs)
# -*- encoding: utf-8 -*- from __future__ import unicode_literals import re from datetime import date, datetime from decimal import Decimal from django import template from django.conf import settings from django.template import defaultfilters from django.utils.encoding import force_text from django.utils.formats import number_format from django.utils.translation import pgettext, ungettext, ugettext as _ from django.utils.timezone import is_aware, utc register = template.Library() @register.filter(is_safe=True) def ordinal(value): """ Converts an integer to its ordinal as a string. 1 is '1st', 2 is '2nd', 3 is '3rd', etc. Works for any integer. """ try: value = int(value) except (TypeError, ValueError): return value suffixes = (_('th'), _('st'), _('nd'), _('rd'), _('th'), _('th'), _('th'), _('th'), _('th'), _('th')) if value % 100 in (11, 12, 13): # special case return "%d%s" % (value, suffixes[0]) return "%d%s" % (value, suffixes[value % 10]) @register.filter(is_safe=True) def intcomma(value, use_l10n=True): """ Converts an integer to a string containing commas every three digits. For example, 3000 becomes '3,000' and 45000 becomes '45,000'. """ if settings.USE_L10N and use_l10n: try: if not isinstance(value, (float, Decimal)): value = int(value) except (TypeError, ValueError): return intcomma(value, False) else: return number_format(value, force_grouping=True) orig = force_text(value) new = re.sub("^(-?\d+)(\d{3})", '\g<1>,\g<2>', orig) if orig == new: return new else: return intcomma(new, use_l10n) # A tuple of standard large number to their converters intword_converters = ( (6, lambda number: ( ungettext('%(value).1f million', '%(value).1f million', number), ungettext('%(value)s million', '%(value)s million', number), )), (9, lambda number: ( ungettext('%(value).1f billion', '%(value).1f billion', number), ungettext('%(value)s billion', '%(value)s billion', number), )), (12, lambda number: ( ungettext('%(value).1f trillion', '%(value).1f trillion', number), ungettext('%(value)s trillion', '%(value)s trillion', number), )), (15, lambda number: ( ungettext('%(value).1f quadrillion', '%(value).1f quadrillion', number), ungettext('%(value)s quadrillion', '%(value)s quadrillion', number), )), (18, lambda number: ( ungettext('%(value).1f quintillion', '%(value).1f quintillion', number), ungettext('%(value)s quintillion', '%(value)s quintillion', number), )), (21, lambda number: ( ungettext('%(value).1f sextillion', '%(value).1f sextillion', number), ungettext('%(value)s sextillion', '%(value)s sextillion', number), )), (24, lambda number: ( ungettext('%(value).1f septillion', '%(value).1f septillion', number), ungettext('%(value)s septillion', '%(value)s septillion', number), )), (27, lambda number: ( ungettext('%(value).1f octillion', '%(value).1f octillion', number), ungettext('%(value)s octillion', '%(value)s octillion', number), )), (30, lambda number: ( ungettext('%(value).1f nonillion', '%(value).1f nonillion', number), ungettext('%(value)s nonillion', '%(value)s nonillion', number), )), (33, lambda number: ( ungettext('%(value).1f decillion', '%(value).1f decillion', number), ungettext('%(value)s decillion', '%(value)s decillion', number), )), (100, lambda number: ( ungettext('%(value).1f googol', '%(value).1f googol', number), ungettext('%(value)s googol', '%(value)s googol', number), )), ) @register.filter(is_safe=False) def intword(value): """ Converts a large integer to a friendly text representation. Works best for numbers over 1 million. For example, 1000000 becomes '1.0 million', 1200000 becomes '1.2 million' and '1200000000' becomes '1.2 billion'. """ try: value = int(value) except (TypeError, ValueError): return value if value < 1000000: return value def _check_for_i18n(value, float_formatted, string_formatted): """ Use the i18n enabled defaultfilters.floatformat if possible """ if settings.USE_L10N: value = defaultfilters.floatformat(value, 1) template = string_formatted else: template = float_formatted return template % {'value': value} for exponent, converters in intword_converters: large_number = 10 ** exponent if value < large_number * 1000: new_value = value / float(large_number) return _check_for_i18n(new_value, *converters(new_value)) return value @register.filter(is_safe=True) def apnumber(value): """ For numbers 1-9, returns the number spelled out. Otherwise, returns the number. This follows Associated Press style. """ try: value = int(value) except (TypeError, ValueError): return value if not 0 < value < 10: return value return (_('one'), _('two'), _('three'), _('four'), _('five'), _('six'), _('seven'), _('eight'), _('nine'))[value-1] # Perform the comparison in the default time zone when USE_TZ = True # (unless a specific time zone has been applied with the |timezone filter). @register.filter(expects_localtime=True) def naturalday(value, arg=None): """ For date values that are tomorrow, today or yesterday compared to present day returns representing string. Otherwise, returns a string formatted according to settings.DATE_FORMAT. """ try: tzinfo = getattr(value, 'tzinfo', None) value = date(value.year, value.month, value.day) except AttributeError: # Passed value wasn't a date object return value except ValueError: # Date arguments out of range return value today = datetime.now(tzinfo).date() delta = value - today if delta.days == 0: return _('today') elif delta.days == 1: return _('tomorrow') elif delta.days == -1: return _('yesterday') return defaultfilters.date(value, arg) # This filter doesn't require expects_localtime=True because it deals properly # with both naive and aware datetimes. Therefore avoid the cost of conversion. @register.filter def naturaltime(value): """ For date and time values shows how many seconds, minutes or hours ago compared to current timestamp returns representing string. """ if not isinstance(value, date): # datetime is a subclass of date return value now = datetime.now(utc if is_aware(value) else None) if value < now: delta = now - value if delta.days != 0: return pgettext( 'naturaltime', '%(delta)s ago' ) % {'delta': defaultfilters.timesince(value, now)} elif delta.seconds == 0: return _('now') elif delta.seconds < 60: return ungettext( # Translators: please keep a non-breaking space (U+00A0) # between count and time unit. 'a second ago', '%(count)s seconds ago', delta.seconds ) % {'count': delta.seconds} elif delta.seconds // 60 < 60: count = delta.seconds // 60 return ungettext( # Translators: please keep a non-breaking space (U+00A0) # between count and time unit. 'a minute ago', '%(count)s minutes ago', count ) % {'count': count} else: count = delta.seconds // 60 // 60 return ungettext( # Translators: please keep a non-breaking space (U+00A0) # between count and time unit. 'an hour ago', '%(count)s hours ago', count ) % {'count': count} else: delta = value - now if delta.days != 0: return pgettext( 'naturaltime', '%(delta)s from now' ) % {'delta': defaultfilters.timeuntil(value, now)} elif delta.seconds == 0: return _('now') elif delta.seconds < 60: return ungettext( # Translators: please keep a non-breaking space (U+00A0) # between count and time unit. 'a second from now', '%(count)s seconds from now', delta.seconds ) % {'count': delta.seconds} elif delta.seconds // 60 < 60: count = delta.seconds // 60 return ungettext( # Translators: please keep a non-breaking space (U+00A0) # between count and time unit. 'a minute from now', '%(count)s minutes from now', count ) % {'count': count} else: count = delta.seconds // 60 // 60 return ungettext( # Translators: please keep a non-breaking space (U+00A0) # between count and time unit. 'an hour from now', '%(count)s hours from now', count ) % {'count': count}
from ctypes import c_void_p from django.contrib.gis.geos.error import GEOSException class GEOSBase(object): """ Base object for GEOS objects that has a pointer access property that controls access to the underlying C pointer. """ # Initially the pointer is NULL. _ptr = None # Default allowed pointer type. ptr_type = c_void_p # Pointer access property. def _get_ptr(self): # Raise an exception if the pointer isn't valid don't # want to be passing NULL pointers to routines -- # that's very bad. if self._ptr: return self._ptr else: raise GEOSException('NULL GEOS %s pointer encountered.' % self.__class__.__name__) def _set_ptr(self, ptr): # Only allow the pointer to be set with pointers of the # compatible type or None (NULL). if ptr is None or isinstance(ptr, self.ptr_type): self._ptr = ptr else: raise TypeError('Incompatible pointer type') # Property for controlling access to the GEOS object pointers. Using # this raises an exception when the pointer is NULL, thus preventing # the C library from attempting to access an invalid memory location. ptr = property(_get_ptr, _set_ptr)
"""Test .dist-info style distributions. """ import os import shutil import tempfile import pytest import pkg_resources from .textwrap import DALS class TestDistInfo: def test_distinfo(self): dists = dict( (d.project_name, d) for d in pkg_resources.find_distributions(self.tmpdir) ) assert len(dists) == 2, dists unversioned = dists['UnversionedDistribution'] versioned = dists['VersionedDistribution'] assert versioned.version == '2.718' # from filename assert unversioned.version == '0.3' # from METADATA @pytest.mark.importorskip('ast') def test_conditional_dependencies(self): specs = 'splort==4', 'quux>=1.1' requires = list(map(pkg_resources.Requirement.parse, specs)) for d in pkg_resources.find_distributions(self.tmpdir): assert d.requires() == requires[:1] assert d.requires(extras=('baz',)) == requires assert d.extras == ['baz'] metadata_template = DALS(""" Metadata-Version: 1.2 Name: {name} {version} Requires-Dist: splort (==4) Provides-Extra: baz Requires-Dist: quux (>=1.1); extra == 'baz' """) def setup_method(self, method): self.tmpdir = tempfile.mkdtemp() dist_info_name = 'VersionedDistribution-2.718.dist-info' versioned = os.path.join(self.tmpdir, dist_info_name) os.mkdir(versioned) with open(os.path.join(versioned, 'METADATA'), 'w+') as metadata_file: metadata = self.metadata_template.format( name='VersionedDistribution', version='', ).replace('\n\n', '\n') metadata_file.write(metadata) dist_info_name = 'UnversionedDistribution.dist-info' unversioned = os.path.join(self.tmpdir, dist_info_name) os.mkdir(unversioned) with open(os.path.join(unversioned, 'METADATA'), 'w+') as metadata_file: metadata = self.metadata_template.format( name='UnversionedDistribution', version='Version: 0.3', ) metadata_file.write(metadata) def teardown_method(self, method): shutil.rmtree(self.tmpdir)
#!/usr/bin/python ''' Copyright 2013 Google Inc. Use of this source code is governed by a BSD-style license that can be found in the LICENSE file. ''' ''' Rewrites a JSON file to use Python's standard JSON pretty-print format, so that subsequent runs of rebaseline.py will generate useful diffs (only the actual checksum differences will show up as diffs, not obscured by format differences). Should not modify the JSON contents in any meaningful way. ''' # System-level imports import argparse import os import sys # Imports from within Skia # # We need to add the 'gm' directory, so that we can import gm_json.py within # that directory. That script allows us to parse the actual-results.json file # written out by the GM tool. # Make sure that the 'gm' dir is in the PYTHONPATH, but add it at the *end* # so any dirs that are already in the PYTHONPATH will be preferred. # # This assumes that the 'gm' directory has been checked out as a sibling of # the 'tools' directory containing this script, which will be the case if # 'trunk' was checked out as a single unit. GM_DIRECTORY = os.path.realpath( os.path.join(os.path.dirname(os.path.dirname(__file__)), 'gm')) if GM_DIRECTORY not in sys.path: sys.path.append(GM_DIRECTORY) import gm_json def Reformat(filename): print 'Reformatting file %s...' % filename gm_json.WriteToFile(gm_json.LoadFromFile(filename), filename) def _Main(): parser = argparse.ArgumentParser(description='Reformat JSON files in-place.') parser.add_argument('filenames', metavar='FILENAME', nargs='+', help='file to reformat') args = parser.parse_args() for filename in args.filenames: Reformat(filename) sys.exit(0) if __name__ == '__main__': _Main()
# -*- coding: utf-8 -*- from __future__ import unicode_literals from .common import InfoExtractor from ..compat import ( compat_parse_qs, compat_urllib_request, ) from ..utils import ( ExtractorError, ) class ScreencastIE(InfoExtractor): _VALID_URL = r'https?://www\.screencast\.com/t/(?P<id>[a-zA-Z0-9]+)' _TESTS = [{ 'url': 'http://www.screencast.com/t/3ZEjQXlT', 'md5': '917df1c13798a3e96211dd1561fded83', 'info_dict': { 'id': '3ZEjQXlT', 'ext': 'm4v', 'title': 'Color Measurement with Ocean Optics Spectrometers', 'description': 'md5:240369cde69d8bed61349a199c5fb153', 'thumbnail': 're:^https?://.*\.(?:gif|jpg)$', } }, { 'url': 'http://www.screencast.com/t/V2uXehPJa1ZI', 'md5': 'e8e4b375a7660a9e7e35c33973410d34', 'info_dict': { 'id': 'V2uXehPJa1ZI', 'ext': 'mov', 'title': 'The Amadeus Spectrometer', 'description': 're:^In this video, our friends at.*To learn more about Amadeus, visit', 'thumbnail': 're:^https?://.*\.(?:gif|jpg)$', } }, { 'url': 'http://www.screencast.com/t/aAB3iowa', 'md5': 'dedb2734ed00c9755761ccaee88527cd', 'info_dict': { 'id': 'aAB3iowa', 'ext': 'mp4', 'title': 'Google Earth Export', 'description': 'Provides a demo of a CommunityViz export to Google Earth, one of the 3D viewing options.', 'thumbnail': 're:^https?://.*\.(?:gif|jpg)$', } }, { 'url': 'http://www.screencast.com/t/X3ddTrYh', 'md5': '669ee55ff9c51988b4ebc0877cc8b159', 'info_dict': { 'id': 'X3ddTrYh', 'ext': 'wmv', 'title': 'Toolkit 6 User Group Webinar (2014-03-04) - Default Judgment and First Impression', 'description': 'md5:7b9f393bc92af02326a5c5889639eab0', 'thumbnail': 're:^https?://.*\.(?:gif|jpg)$', } }, ] def _real_extract(self, url): video_id = self._match_id(url) webpage = self._download_webpage(url, video_id) video_url = self._html_search_regex( r'<embed name="Video".*?src="([^"]+)"', webpage, 'QuickTime embed', default=None) if video_url is None: flash_vars_s = self._html_search_regex( r'<param name="flashVars" value="([^"]+)"', webpage, 'flash vars', default=None) if not flash_vars_s: flash_vars_s = self._html_search_regex( r'<param name="initParams" value="([^"]+)"', webpage, 'flash vars', default=None) if flash_vars_s: flash_vars_s = flash_vars_s.replace(',', '&') if flash_vars_s: flash_vars = compat_parse_qs(flash_vars_s) video_url_raw = compat_urllib_request.quote( flash_vars['content'][0]) video_url = video_url_raw.replace('http%3A', 'http:') if video_url is None: video_meta = self._html_search_meta( 'og:video', webpage, default=None) if video_meta: video_url = self._search_regex( r'src=(.*?)(?:$|&)', video_meta, 'meta tag video URL', default=None) if video_url is None: raise ExtractorError('Cannot find video') title = self._og_search_title(webpage, default=None) if title is None: title = self._html_search_regex( [r'<b>Title:</b> ([^<]*)</div>', r'class="tabSeperator">></span><span class="tabText">(.*?)<'], webpage, 'title') thumbnail = self._og_search_thumbnail(webpage) description = self._og_search_description(webpage, default=None) if description is None: description = self._html_search_meta('description', webpage) return { 'id': video_id, 'url': video_url, 'title': title, 'description': description, 'thumbnail': thumbnail, }
# -*- coding: utf-8 -*- ############################################################################## # # OpenERP, Open Source Management Solution # Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>). # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## import hr_timesheet_report # vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
#!/usr/bin/env python """ Example of the very basic, minimal framework for a wxPython application This version adds a single button """ import wx import os #-------------------------------------------------------------- # This is how you pre-establish a file filter so that the dialog # only shows the extension(s) you want it to. wildcard = "Python source (*.py)|*.py|" \ "Compiled Python (*.pyc)|*.pyc|" \ "SPAM files (*.spam)|*.spam|" \ "Egg file (*.egg)|*.egg|" \ "All files (*.*)|*.*" #-------------------------------------------------------------- class AppLogic(object): """ A class to hold the application Application Logic. You generally don't want the real logic of the app mixed in with the GUI In a real app, this would be a substantial collection of modules, classes, etc... """ def file_open(self, filename="default_name"): """This method opens a file""" print "Open a file: " print "I'd be opening file: %s now"%filename def file_close(self): """This method closes a file""" print "Close a file: " print "I'd be closing a file now" class TestFrame(wx.Frame): def __init__(self, app_logic, *args, **kwargs): kwargs.setdefault('title', "Simple test App") wx.Frame.__init__(self, *args, **kwargs) self.app_logic = app_logic # Build up the menu bar: menuBar = wx.MenuBar() fileMenu = wx.Menu() saveasMenuItem = fileMenu.Append(wx.ID_ANY, "&Save As", "Create a new file") self.Bind(wx.EVT_MENU, self.onSaveAs, saveasMenuItem ) openMenuItem = fileMenu.Append(wx.ID_ANY, "&Open", "Open an existing file" ) self.Bind(wx.EVT_MENU, self.onOpen, openMenuItem) closeMenuItem = fileMenu.Append(wx.ID_ANY, "&Close", "Close a file" ) self.Bind(wx.EVT_MENU, self.onClose, closeMenuItem) exitMenuItem = fileMenu.Append(wx.ID_EXIT, "Exit", "Exit the application") self.Bind(wx.EVT_MENU, self.onExit, exitMenuItem) menuBar.Append(fileMenu, "&File") helpMenu = wx.Menu() helpMenuItem = helpMenu.Append(wx.ID_HELP, "Help", "Get help") menuBar.Append(helpMenu, "&Help") self.SetMenuBar(menuBar) ## add just a single button: self.theButton = wx.Button(self, label="Push Me") self.theButton.Bind(wx.EVT_BUTTON, self.onButton) self.theButton.Bind(wx.EVT_RIGHT_DOWN, self.onRight) def onButton(self, evt=None): print "You pushed the button!" evt.Skip() def onRight(self, evt=None): print "right click!" evt.Skip() def onClose(self, evt=None): print "close menu selected" self.file_close() def onExit(self, evt=None): print "Exit the program here" print "The event passed to onExit is type ", type(evt), self.Close() def onSaveAs ( self, evt=None ): """This method saves the file with a new name""" # Create the dialog. In this case the current directory is forced as the starting # directory for the dialog, and no default file name is forced. This can easilly # be changed in your program. This is an 'save' dialog. # # Unlike the 'open dialog' example found elsewhere, this example does NOT # force the current working directory to change if the user chooses a different # directory than the one initially set. dlg = wx.FileDialog(self, message="Save file as ...", defaultDir=os.getcwd(), defaultFile="", wildcard=wildcard, style=wx.SAVE ) # This sets the default filter that the user will initially see. Otherwise, # the first filter in the list will be used by default. dlg.SetFilterIndex(2) # Show the dialog and retrieve the user response. If it is the OK response, # process the data. if dlg.ShowModal() == wx.ID_OK: path = dlg.GetPath() print "In onSaveAs, the path is %s" % path # Normally, at this point you would save your data using the file and path # data that the user provided to you, but since we didn't actually start # with any data to work with, that would be difficult. # # The code to do so would be similar to this, assuming 'data' contains # the data you want to save: # # fp = file(path, 'w') # Create file anew # fp.write(data) # fp.close() # # You might want to add some error checking :-) else : print "The file dialog was canceled before anything was selected" # Note that the current working dir didn't change. This is good since # that's the way we set it up. # Destroy the dialog. Don't do this until you are done with it! # BAD things can happen otherwise! dlg.Destroy() def onOpen(self, evt=None): """This method opens an existing file""" print "Open a file: " # Create the dialog. In this case the current directory is forced as the starting # directory for the dialog, and no default file name is forced. This can easilly # be changed in your program. This is an 'open' dialog, and allows multitple # file selections as well. # # Finally, if the directory is changed in the process of getting files, this # dialog is set up to change the current working directory to the path chosen. dlg = wx.FileDialog( self, message="Choose a file", defaultDir=os.getcwd(), defaultFile="", wildcard=wildcard, style=wx.OPEN | wx.CHANGE_DIR ) # Show the dialog and retrieve the user response. If it is the OK response, # process the data. if dlg.ShowModal() == wx.ID_OK: # This returns a Python list of files that were selected. path = dlg.GetPath() print "I'd be opening file in onOpen ", path self.app_logic.file_open( path ) else : print "The file dialog was canceled before anything was selected" # Destroy the dialog. Don't do this until you are done with it! # BAD things can happen otherwise! dlg.Destroy() def file_close(self): """This method closes a file""" print "Close a file: " print "I'd be closing a file now" class TestApp(wx.App): def OnInit(self): """ App initilization goes here -- not much to do, in this case """ app_logic = AppLogic() f = TestFrame(app_logic, parent=None) f.Show() return True if __name__ == "__main__": app = TestApp(False) app.MainLoop()
""" This module contains all of the GEOS ctypes function prototypes. Each prototype handles the interaction between the GEOS library and Python via ctypes. """ # Coordinate sequence routines. from django.contrib.gis.geos.prototypes.coordseq import (create_cs, get_cs, cs_clone, cs_getordinate, cs_setordinate, cs_getx, cs_gety, cs_getz, cs_setx, cs_sety, cs_setz, cs_getsize, cs_getdims) # Geometry routines. from django.contrib.gis.geos.prototypes.geom import (from_hex, from_wkb, from_wkt, create_point, create_linestring, create_linearring, create_polygon, create_collection, destroy_geom, get_extring, get_intring, get_nrings, get_geomn, geom_clone, geos_normalize, geos_type, geos_typeid, geos_get_srid, geos_set_srid, get_dims, get_num_coords, get_num_geoms, to_hex, to_wkb, to_wkt) # Miscellaneous routines. from django.contrib.gis.geos.prototypes.misc import * # Predicates from django.contrib.gis.geos.prototypes.predicates import (geos_hasz, geos_isempty, geos_isring, geos_issimple, geos_isvalid, geos_contains, geos_crosses, geos_disjoint, geos_equals, geos_equalsexact, geos_intersects, geos_intersects, geos_overlaps, geos_relatepattern, geos_touches, geos_within) # Topology routines from django.contrib.gis.geos.prototypes.topology import *
# A Python port of the MS knowledge base article Q157234 # "How to deal with localized and renamed user and group names" # http://support.microsoft.com/default.aspx?kbid=157234 import sys from win32net import NetUserModalsGet from win32security import LookupAccountSid import pywintypes from ntsecuritycon import * def LookupAliasFromRid(TargetComputer, Rid): # Sid is the same regardless of machine, since the well-known # BUILTIN domain is referenced. sid = pywintypes.SID() sid.Initialize(SECURITY_NT_AUTHORITY, 2) for i, r in enumerate((SECURITY_BUILTIN_DOMAIN_RID, Rid)): sid.SetSubAuthority(i, r) name, domain, typ = LookupAccountSid(TargetComputer, sid) return name def LookupUserGroupFromRid(TargetComputer, Rid): # get the account domain Sid on the target machine # note: if you were looking up multiple sids based on the same # account domain, only need to call this once. umi2 = NetUserModalsGet(TargetComputer, 2) domain_sid = umi2['domain_id'] SubAuthorityCount = domain_sid.GetSubAuthorityCount() # create and init new sid with acct domain Sid + acct Rid sid = pywintypes.SID() sid.Initialize(domain_sid.GetSidIdentifierAuthority(), SubAuthorityCount+1) # copy existing subauthorities from account domain Sid into # new Sid for i in range(SubAuthorityCount): sid.SetSubAuthority(i, domain_sid.GetSubAuthority(i)) # append Rid to new Sid sid.SetSubAuthority(SubAuthorityCount, Rid) name, domain, typ = LookupAccountSid(TargetComputer, sid) return name def main(): if len(sys.argv) == 2: targetComputer = sys.argv[1] else: targetComputer = None name = LookupUserGroupFromRid(targetComputer, DOMAIN_USER_RID_ADMIN) print "'Administrator' user name = %s" % (name,) name = LookupAliasFromRid(targetComputer, DOMAIN_ALIAS_RID_ADMINS) print "'Administrators' local group/alias name = %s" % (name,) if __name__=='__main__': main()
#!/usr/bin/env python from xml.sax.saxutils import escape, quoteattr from param import * from emit import Emit # Emit APM documentation in an machine readable XML format class XmlEmit(Emit): def __init__(self): wiki_fname = 'apm.pdef.xml' self.f = open(wiki_fname, mode='w') preamble = '''<?xml version="1.0" encoding="utf-8"?> <!-- Dynamically generated list of documented parameters (generated by param_parse.py) --> <paramfile> <vehicles> ''' self.f.write(preamble) def close(self): self.f.write('</libraries>') self.f.write('''</paramfile>\n''') self.f.close def emit_comment(self, s): self.f.write("<!-- " + s + " -->") def start_libraries(self): self.f.write('</vehicles>') self.f.write('<libraries>') def emit(self, g, f): t = '''<parameters name=%s>\n''' % quoteattr(g.name) # i.e. ArduPlane for param in g.params: # Begin our parameter node if hasattr(param, 'DisplayName'): t += '<param humanName=%s name=%s' % (quoteattr(param.DisplayName),quoteattr(param.name)) # i.e. ArduPlane (ArduPlane:FOOPARM) else: t += '<param name=%s' % quoteattr(param.name) if hasattr(param, 'Description'): t += ' documentation=%s' % quoteattr(param.Description) # i.e. parameter docs if hasattr(param, 'User'): t += ' user=%s' % quoteattr(param.User) # i.e. Standard or Advanced t += ">\n" # Add values as chidren of this node for field in param.__dict__.keys(): if field not in ['name', 'DisplayName', 'Description', 'User'] and field in known_param_fields: if field == 'Values' and Emit.prog_values_field.match(param.__dict__[field]): t+= "<values>\n" values = (param.__dict__[field]).split(',') for value in values: v = value.split(':') t+='''<value code=%s>%s</value>\n''' % (quoteattr(v[0]), escape(v[1])) # i.e. numeric value, string label t += "</values>\n" else: t += '''<field name=%s>%s</field>\n''' % (quoteattr(field), escape(param.__dict__[field])) # i.e. Range: 0 10 t += '''</param>\n''' t += '''</parameters>\n''' #print t self.f.write(t)
# # Licensed to the Apache Software Foundation (ASF) under one # or more contributor license agreements. See the NOTICE file # distributed with this work for additional information # regarding copyright ownership. The ASF licenses this file # to you under the Apache License, Version 2.0 (the # "License"); you may not use this file except in compliance # with the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, # software distributed under the License is distributed on an # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY # KIND, either express or implied. See the License for the # specific language governing permissions and limitations # under the License. import unittest from unittest import mock import botocore.exceptions import pytest from parameterized import parameterized from airflow.exceptions import AirflowException from airflow.providers.amazon.aws.hooks.batch_client import AwsBatchClientHook # Use dummy AWS credentials AWS_REGION = "eu-west-1" AWS_ACCESS_KEY_ID = "airflow_dummy_key" AWS_SECRET_ACCESS_KEY = "airflow_dummy_secret" JOB_ID = "8ba9d676-4108-4474-9dca-8bbac1da9b19" class TestAwsBatchClient(unittest.TestCase): MAX_RETRIES = 2 STATUS_RETRIES = 3 @mock.patch.dict("os.environ", AWS_DEFAULT_REGION=AWS_REGION) @mock.patch.dict("os.environ", AWS_ACCESS_KEY_ID=AWS_ACCESS_KEY_ID) @mock.patch.dict("os.environ", AWS_SECRET_ACCESS_KEY=AWS_SECRET_ACCESS_KEY) @mock.patch("airflow.providers.amazon.aws.hooks.batch_client.AwsBaseHook.get_client_type") def setUp(self, get_client_type_mock): self.get_client_type_mock = get_client_type_mock self.batch_client = AwsBatchClientHook( max_retries=self.MAX_RETRIES, status_retries=self.STATUS_RETRIES, aws_conn_id='airflow_test', region_name=AWS_REGION, ) self.client_mock = get_client_type_mock.return_value assert self.batch_client.client == self.client_mock # setup client property # don't pause in these unit tests self.mock_delay = mock.Mock(return_value=None) self.batch_client.delay = self.mock_delay self.mock_exponential_delay = mock.Mock(return_value=0) self.batch_client.exponential_delay = self.mock_exponential_delay def test_init(self): assert self.batch_client.max_retries == self.MAX_RETRIES assert self.batch_client.status_retries == self.STATUS_RETRIES assert self.batch_client.region_name == AWS_REGION assert self.batch_client.aws_conn_id == 'airflow_test' assert self.batch_client.client == self.client_mock self.get_client_type_mock.assert_called_once_with("batch", region_name=AWS_REGION) def test_wait_for_job_with_success(self): self.client_mock.describe_jobs.return_value = {"jobs": [{"jobId": JOB_ID, "status": "SUCCEEDED"}]} with mock.patch.object( self.batch_client, "poll_for_job_running", wraps=self.batch_client.poll_for_job_running, ) as job_running: self.batch_client.wait_for_job(JOB_ID) job_running.assert_called_once_with(JOB_ID, None) with mock.patch.object( self.batch_client, "poll_for_job_complete", wraps=self.batch_client.poll_for_job_complete, ) as job_complete: self.batch_client.wait_for_job(JOB_ID) job_complete.assert_called_once_with(JOB_ID, None) assert self.client_mock.describe_jobs.call_count == 4 def test_wait_for_job_with_failure(self): self.client_mock.describe_jobs.return_value = {"jobs": [{"jobId": JOB_ID, "status": "FAILED"}]} with mock.patch.object( self.batch_client, "poll_for_job_running", wraps=self.batch_client.poll_for_job_running, ) as job_running: self.batch_client.wait_for_job(JOB_ID) job_running.assert_called_once_with(JOB_ID, None) with mock.patch.object( self.batch_client, "poll_for_job_complete", wraps=self.batch_client.poll_for_job_complete, ) as job_complete: self.batch_client.wait_for_job(JOB_ID) job_complete.assert_called_once_with(JOB_ID, None) assert self.client_mock.describe_jobs.call_count == 4 def test_poll_job_running_for_status_running(self): self.client_mock.describe_jobs.return_value = {"jobs": [{"jobId": JOB_ID, "status": "RUNNING"}]} self.batch_client.poll_for_job_running(JOB_ID) self.client_mock.describe_jobs.assert_called_once_with(jobs=[JOB_ID]) def test_poll_job_complete_for_status_success(self): self.client_mock.describe_jobs.return_value = {"jobs": [{"jobId": JOB_ID, "status": "SUCCEEDED"}]} self.batch_client.poll_for_job_complete(JOB_ID) self.client_mock.describe_jobs.assert_called_once_with(jobs=[JOB_ID]) def test_poll_job_complete_raises_for_max_retries(self): self.client_mock.describe_jobs.return_value = {"jobs": [{"jobId": JOB_ID, "status": "RUNNING"}]} with pytest.raises(AirflowException) as ctx: self.batch_client.poll_for_job_complete(JOB_ID) msg = f"AWS Batch job ({JOB_ID}) status checks exceed max_retries" assert msg in str(ctx.value) self.client_mock.describe_jobs.assert_called_with(jobs=[JOB_ID]) assert self.client_mock.describe_jobs.call_count == self.MAX_RETRIES + 1 def test_poll_job_status_hit_api_throttle(self): self.client_mock.describe_jobs.side_effect = botocore.exceptions.ClientError( error_response={"Error": {"Code": "TooManyRequestsException"}}, operation_name="get job description", ) with pytest.raises(AirflowException) as ctx: self.batch_client.poll_for_job_complete(JOB_ID) msg = f"AWS Batch job ({JOB_ID}) description error" assert msg in str(ctx.value) # It should retry when this client error occurs self.client_mock.describe_jobs.assert_called_with(jobs=[JOB_ID]) assert self.client_mock.describe_jobs.call_count == self.STATUS_RETRIES def test_poll_job_status_with_client_error(self): self.client_mock.describe_jobs.side_effect = botocore.exceptions.ClientError( error_response={"Error": {"Code": "InvalidClientTokenId"}}, operation_name="get job description", ) with pytest.raises(AirflowException) as ctx: self.batch_client.poll_for_job_complete(JOB_ID) msg = f"AWS Batch job ({JOB_ID}) description error" assert msg in str(ctx.value) # It will not retry when this client error occurs self.client_mock.describe_jobs.assert_called_once_with(jobs=[JOB_ID]) def test_check_job_success(self): self.client_mock.describe_jobs.return_value = {"jobs": [{"jobId": JOB_ID, "status": "SUCCEEDED"}]} status = self.batch_client.check_job_success(JOB_ID) assert status self.client_mock.describe_jobs.assert_called_once_with(jobs=[JOB_ID]) def test_check_job_success_raises_failed(self): self.client_mock.describe_jobs.return_value = { "jobs": [ { "jobId": JOB_ID, "status": "FAILED", "statusReason": "This is an error reason", "attempts": [{"exitCode": 1}], } ] } with pytest.raises(AirflowException) as ctx: self.batch_client.check_job_success(JOB_ID) self.client_mock.describe_jobs.assert_called_once_with(jobs=[JOB_ID]) msg = f"AWS Batch job ({JOB_ID}) failed" assert msg in str(ctx.value) def test_check_job_success_raises_failed_for_multiple_attempts(self): self.client_mock.describe_jobs.return_value = { "jobs": [ { "jobId": JOB_ID, "status": "FAILED", "statusReason": "This is an error reason", "attempts": [{"exitCode": 1}, {"exitCode": 10}], } ] } with pytest.raises(AirflowException) as ctx: self.batch_client.check_job_success(JOB_ID) self.client_mock.describe_jobs.assert_called_once_with(jobs=[JOB_ID]) msg = f"AWS Batch job ({JOB_ID}) failed" assert msg in str(ctx.value) def test_check_job_success_raises_incomplete(self): self.client_mock.describe_jobs.return_value = {"jobs": [{"jobId": JOB_ID, "status": "RUNNABLE"}]} with pytest.raises(AirflowException) as ctx: self.batch_client.check_job_success(JOB_ID) self.client_mock.describe_jobs.assert_called_once_with(jobs=[JOB_ID]) msg = f"AWS Batch job ({JOB_ID}) is not complete" assert msg in str(ctx.value) def test_check_job_success_raises_unknown_status(self): status = "STRANGE" self.client_mock.describe_jobs.return_value = {"jobs": [{"jobId": JOB_ID, "status": status}]} with pytest.raises(AirflowException) as ctx: self.batch_client.check_job_success(JOB_ID) self.client_mock.describe_jobs.assert_called_once_with(jobs=[JOB_ID]) msg = f"AWS Batch job ({JOB_ID}) has unknown status" assert msg in str(ctx.value) assert status in str(ctx.value) def test_check_job_success_raises_without_jobs(self): self.client_mock.describe_jobs.return_value = {"jobs": []} with pytest.raises(AirflowException) as ctx: self.batch_client.check_job_success(JOB_ID) self.client_mock.describe_jobs.assert_called_once_with(jobs=[JOB_ID]) msg = f"AWS Batch job ({JOB_ID}) description error" assert msg in str(ctx.value) def test_terminate_job(self): self.client_mock.terminate_job.return_value = {} reason = "Task killed by the user" response = self.batch_client.terminate_job(JOB_ID, reason) self.client_mock.terminate_job.assert_called_once_with(jobId=JOB_ID, reason=reason) assert response == {} class TestAwsBatchClientDelays(unittest.TestCase): @mock.patch.dict("os.environ", AWS_DEFAULT_REGION=AWS_REGION) @mock.patch.dict("os.environ", AWS_ACCESS_KEY_ID=AWS_ACCESS_KEY_ID) @mock.patch.dict("os.environ", AWS_SECRET_ACCESS_KEY=AWS_SECRET_ACCESS_KEY) def setUp(self): self.batch_client = AwsBatchClientHook(aws_conn_id='airflow_test', region_name=AWS_REGION) def test_init(self): assert self.batch_client.max_retries == self.batch_client.MAX_RETRIES assert self.batch_client.status_retries == self.batch_client.STATUS_RETRIES assert self.batch_client.region_name == AWS_REGION assert self.batch_client.aws_conn_id == 'airflow_test' def test_add_jitter(self): minima = 0 width = 5 result = self.batch_client.add_jitter(0, width=width, minima=minima) assert result >= minima assert result <= width @mock.patch("airflow.providers.amazon.aws.hooks.batch_client.uniform") @mock.patch("airflow.providers.amazon.aws.hooks.batch_client.sleep") def test_delay_defaults(self, mock_sleep, mock_uniform): assert AwsBatchClientHook.DEFAULT_DELAY_MIN == 1 assert AwsBatchClientHook.DEFAULT_DELAY_MAX == 10 mock_uniform.return_value = 0 self.batch_client.delay() mock_uniform.assert_called_once_with( AwsBatchClientHook.DEFAULT_DELAY_MIN, AwsBatchClientHook.DEFAULT_DELAY_MAX ) mock_sleep.assert_called_once_with(0) @mock.patch("airflow.providers.amazon.aws.hooks.batch_client.uniform") @mock.patch("airflow.providers.amazon.aws.hooks.batch_client.sleep") def test_delay_with_zero(self, mock_sleep, mock_uniform): self.batch_client.delay(0) mock_uniform.assert_called_once_with(0, 1) # in add_jitter mock_sleep.assert_called_once_with(mock_uniform.return_value) @mock.patch("airflow.providers.amazon.aws.hooks.batch_client.uniform") @mock.patch("airflow.providers.amazon.aws.hooks.batch_client.sleep") def test_delay_with_int(self, mock_sleep, mock_uniform): self.batch_client.delay(5) mock_uniform.assert_called_once_with(4, 6) # in add_jitter mock_sleep.assert_called_once_with(mock_uniform.return_value) @mock.patch("airflow.providers.amazon.aws.hooks.batch_client.uniform") @mock.patch("airflow.providers.amazon.aws.hooks.batch_client.sleep") def test_delay_with_float(self, mock_sleep, mock_uniform): self.batch_client.delay(5.0) mock_uniform.assert_called_once_with(4.0, 6.0) # in add_jitter mock_sleep.assert_called_once_with(mock_uniform.return_value) @parameterized.expand( [ (0, 0, 1), (1, 0, 2), (2, 0, 3), (3, 1, 5), (4, 2, 7), (5, 3, 11), (6, 4, 14), (7, 6, 19), (8, 8, 25), (9, 10, 31), (45, 200, 600), # > 40 tries invokes maximum delay allowed ] ) def test_exponential_delay(self, tries, lower, upper): result = self.batch_client.exponential_delay(tries) assert result >= lower assert result <= upper
# Copyright 2012 The Chromium Authors. All rights reserved. # Use of this source code is governed by a BSD-style license that can be # found in the LICENSE file. from telemetry.page import action_runner as action_runner_module from telemetry.page import test_expectations class TestNotSupportedOnPlatformError(Exception): """PageTest Exception raised when a required feature is unavailable. The feature required to run the test could be part of the platform, hardware configuration, or browser. """ class MultiTabTestAppCrashError(Exception): """PageTest Exception raised after browser or tab crash for multi-tab tests. Used to abort the test rather than try to recover from an unknown state. """ class Failure(Exception): """PageTest Exception raised when an undesired but designed-for problem.""" class MeasurementFailure(Failure): """PageTest Exception raised when an undesired but designed-for problem.""" class PageTest(object): """A class styled on unittest.TestCase for creating page-specific tests. Test should override ValidateAndMeasurePage to perform test validation and page measurement as necessary. class BodyChildElementMeasurement(PageTest): def ValidateAndMeasurePage(self, page, tab, results): body_child_count = tab.EvaluateJavaScript( 'document.body.children.length') results.AddValue(scalar.ScalarValue( page, 'body_children', 'count', body_child_count)) Args: discard_first_run: Discard the first run of this page. This is usually used with page_repeat and pageset_repeat options. """ def __init__(self, needs_browser_restart_after_each_page=False, discard_first_result=False, clear_cache_before_each_run=False): super(PageTest, self).__init__() self.options = None self._needs_browser_restart_after_each_page = ( needs_browser_restart_after_each_page) self._discard_first_result = discard_first_result self._clear_cache_before_each_run = clear_cache_before_each_run self._close_tabs_before_run = True @property def is_multi_tab_test(self): """Returns True if the test opens multiple tabs. If the test overrides TabForPage, it is deemed a multi-tab test. Multi-tab tests do not retry after tab or browser crashes, whereas, single-tab tests too. That is because the state of multi-tab tests (e.g., how many tabs are open, etc.) is unknown after crashes. """ return self.TabForPage.__func__ is not PageTest.TabForPage.__func__ @property def discard_first_result(self): """When set to True, the first run of the test is discarded. This is useful for cases where it's desirable to have some test resource cached so the first run of the test can warm things up. """ return self._discard_first_result @discard_first_result.setter def discard_first_result(self, discard): self._discard_first_result = discard @property def clear_cache_before_each_run(self): """When set to True, the browser's disk and memory cache will be cleared before each run.""" return self._clear_cache_before_each_run @property def close_tabs_before_run(self): """When set to True, all tabs are closed before running the test for the first time.""" return self._close_tabs_before_run @close_tabs_before_run.setter def close_tabs_before_run(self, close_tabs): self._close_tabs_before_run = close_tabs def RestartBrowserBeforeEachPage(self): """ Should the browser be restarted for the page? This returns true if the test needs to unconditionally restart the browser for each page. It may be called before the browser is started. """ return self._needs_browser_restart_after_each_page def StopBrowserAfterPage(self, browser, page): # pylint: disable=W0613 """Should the browser be stopped after the page is run? This is called after a page is run to decide whether the browser needs to be stopped to clean up its state. If it is stopped, then it will be restarted to run the next page. A test that overrides this can look at both the page and the browser to decide whether it needs to stop the browser. """ return False def CustomizeBrowserOptions(self, options): """Override to add test-specific options to the BrowserOptions object""" def CustomizeBrowserOptionsForSinglePage(self, page, options): """Set options specific to the test and the given page. This will be called with the current page when the browser is (re)started. Changing options at this point only makes sense if the browser is being restarted for each page. Note that if page has a startup_url, the browser will always be restarted for each run. """ if page.startup_url: options.browser_options.startup_url = page.startup_url def WillStartBrowser(self, platform): """Override to manipulate the browser environment before it launches.""" def DidStartBrowser(self, browser): """Override to customize the browser right after it has launched.""" def SetOptions(self, options): """Sets the BrowserFinderOptions instance to use.""" self.options = options def WillNavigateToPage(self, page, tab): """Override to do operations before the page is navigated, notably Telemetry will already have performed the following operations on the browser before calling this function: * Ensure only one tab is open. * Call WaitForDocumentReadyStateToComplete on the tab.""" def DidNavigateToPage(self, page, tab): """Override to do operations right after the page is navigated and after all waiting for completion has occurred.""" def CleanUpAfterPage(self, page, tab): """Called after the test run method was run, even if it failed.""" def CreateExpectations(self, page_set): # pylint: disable=W0613 """Override to make this test generate its own expectations instead of any that may have been defined in the page set.""" return test_expectations.TestExpectations() def TabForPage(self, page, browser): # pylint: disable=W0613 """Override to select a different tab for the page. For instance, to create a new tab for every page, return browser.tabs.New().""" return browser.tabs[0] def ValidateAndMeasurePage(self, page, tab, results): """Override to check test assertions and perform measurement. When adding measurement results, call results.AddValue(...) for each result. Raise an exception or add a failure.FailureValue on failure. page_test.py also provides several base exception classes to use. Prefer metric value names that are in accordance with python variable style. e.g., metric_name. The name 'url' must not be used. Put together: def ValidateAndMeasurePage(self, page, tab, results): res = tab.EvaluateJavaScript('2+2') if res != 4: raise Exception('Oh, wow.') results.AddValue(scalar.ScalarValue( page, 'two_plus_two', 'count', res)) Args: page: A telemetry.page.Page instance. tab: A telemetry.core.Tab instance. results: A telemetry.results.PageTestResults instance. """ raise NotImplementedError def RunPage(self, page, tab, results): # Run actions. action_runner = action_runner_module.ActionRunner( tab, skip_waits=page.skip_waits) page.RunPageInteractions(action_runner) self.ValidateAndMeasurePage(page, tab, results) def RunNavigateSteps(self, page, tab): """Navigates the tab to the page URL attribute. Runs the 'navigate_steps' page attribute as a compound action. """ action_runner = action_runner_module.ActionRunner( tab, skip_waits=page.skip_waits) page.RunNavigateSteps(action_runner)
# coding: utf-8 # This Source Code Form is subject to the terms of the Mozilla Public # License, v. 2.0. If a copy of the MPL was not distributed with this # file, You can obtain one at http://mozilla.org/MPL/2.0/. from django import forms from lib.l10n_utils.dotlang import _, _lazy from bedrock.mozorg.forms import (DateInput, EmailInput, HoneyPotWidget, NumberInput, TelInput, TimeInput, URLInput) SPEAKER_REQUEST_FILE_SIZE_LIMIT = 5242880 # 5MB class SpeakerRequestForm(forms.Form): # event fields sr_event_name = forms.CharField( max_length=255, required=True, error_messages={ 'required': _lazy(u'Please enter a name for the event.'), }, widget=forms.TextInput( attrs={ 'class': 'required', 'required': 'required', 'aria-required': 'true', } ), ) sr_event_url = forms.URLField( max_length=2000, required=True, error_messages={ 'required': _lazy(u'Please enter a URL.'), 'invalid': _lazy(u'Please enter a valid URL.'), }, widget=URLInput( attrs={ 'class': 'required', 'required': 'required', 'aria-required': 'true', 'placeholder': _lazy(u'http://www.my-event.com'), } ), ) sr_event_date = forms.CharField( required=True, error_messages={ 'required': _lazy(u'Please provide a date.'), }, widget=DateInput( attrs={ 'class': 'required', 'required': 'required', 'aria-required': 'true', } ), ) sr_event_time = forms.CharField( required=True, error_messages={ 'required': _lazy(u'Please provide a time.'), }, widget=TimeInput( attrs={ 'class': 'required', 'required': 'required', 'aria-required': 'true', } ), ) sr_guest_speaker1 = forms.CharField( max_length=200, required=False, ) sr_guest_speaker2 = forms.CharField( max_length=200, required=False, ) # contact fields sr_contact_name = forms.CharField( max_length=200, required=True, widget=forms.TextInput( attrs={ 'required': 'required', 'class': 'required', 'aria-required': 'true', } ), ) sr_contact_title = forms.CharField( max_length=200, required=False, ) sr_contact_company = forms.CharField( max_length=200, required=False, ) sr_contact_phone = forms.CharField( max_length=50, required=False, widget=TelInput(), ) sr_contact_email = forms.EmailField( max_length=254, # max length allowed for emails required=True, error_messages={ 'invalid': _lazy(u'Please enter a valid email address'), }, widget=EmailInput( attrs={ 'required': 'required', 'class': 'required', 'aria-required': 'true', } ), ) sr_contact_company_url = forms.URLField( max_length=2000, required=False, widget=forms.TextInput( attrs={ 'placeholder': _lazy(u'http://www.my-company.com'), } ), ) # event details fields sr_event_venue = forms.CharField( max_length=400, required=False, ) sr_event_theme = forms.CharField( max_length=200, required=False, ) sr_event_goal = forms.CharField( max_length=300, required=False, ) sr_event_format = forms.CharField( max_length=200, required=False, ) sr_event_audience_size = forms.IntegerField( required=False, widget=NumberInput( attrs={ 'min': 1, 'placeholder': 25, } ), ) sr_event_audience_demographics = forms.CharField( max_length=500, required=False, widget=forms.Textarea(), ) sr_event_speakers_confirmed = forms.CharField( max_length=500, required=False, widget=forms.Textarea(), ) sr_event_speakers_invited = forms.CharField( max_length=500, required=False, widget=forms.Textarea(), ) sr_event_speakers_past = forms.CharField( max_length=1000, required=False, widget=forms.Textarea(), ) sr_event_media_coverage = forms.CharField( max_length=500, required=False, widget=forms.Textarea(), ) sr_event_sponsors = forms.CharField( max_length=500, required=False, widget=forms.Textarea(), ) sr_event_confirmation_deadline = forms.DateField( required=False, widget=DateInput(), ) # presentation details fields sr_presentation_type = forms.MultipleChoiceField( required=False, choices=( ('keynote', _lazy(u'Keynote')), ('presentation', _lazy(u'Presentation')), ('fireside chat', _lazy(u'Fireside Chat')), ('panel', _lazy(u'Panel')), ('other', _lazy(u'Other')), ), widget=forms.CheckboxSelectMultiple(), ) sr_presentation_panelists = forms.CharField( max_length=500, required=False, widget=forms.Textarea(), ) sr_presentation_topic = forms.CharField( required=False, max_length=255, ) sr_presentation_length = forms.IntegerField( required=False, widget=NumberInput( attrs={ 'min': 0.5, 'step': 0.5, 'placeholder': 2.5, } ) ) # additional info fields sr_attachment = forms.FileField( required=False, ) # honeypot office_fax = forms.CharField(widget=HoneyPotWidget, required=False) def clean_sr_attachment(self): cleaned_data = super(SpeakerRequestForm, self).clean() attachment = cleaned_data.get("sr_attachment") if attachment: if attachment._size > SPEAKER_REQUEST_FILE_SIZE_LIMIT: raise forms.ValidationError( _("Attachment must not exceed 5MB")) return attachment def clean_office_fax(self): cleaned_data = super(SpeakerRequestForm, self).clean() honeypot = cleaned_data.pop('office_fax', None) if honeypot: raise forms.ValidationError( _('Your submission could not be processed'))
# -*- coding: utf-8 -*- import datetime from south.db import db from south.v2 import SchemaMigration from django.db import models class Migration(SchemaMigration): def forwards(self, orm): # Adding model 'PaidCourseRegistrationAnnotation' db.create_table('shoppingcart_paidcourseregistrationannotation', ( ('id', self.gf('django.db.models.fields.AutoField')(primary_key=True)), ('course_id', self.gf('django.db.models.fields.CharField')(unique=True, max_length=128, db_index=True)), ('annotation', self.gf('django.db.models.fields.TextField')(null=True)), )) db.send_create_signal('shoppingcart', ['PaidCourseRegistrationAnnotation']) # Adding field 'OrderItem.report_comments' db.add_column('shoppingcart_orderitem', 'report_comments', self.gf('django.db.models.fields.TextField')(default=''), keep_default=False) def backwards(self, orm): # Deleting model 'PaidCourseRegistrationAnnotation' db.delete_table('shoppingcart_paidcourseregistrationannotation') # Deleting field 'OrderItem.report_comments' db.delete_column('shoppingcart_orderitem', 'report_comments') models = { 'auth.group': { 'Meta': {'object_name': 'Group'}, 'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}), 'name': ('django.db.models.fields.CharField', [], {'unique': 'True', 'max_length': '80'}), 'permissions': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['auth.Permission']", 'symmetrical': 'False', 'blank': 'True'}) }, 'auth.permission': { 'Meta': {'ordering': "('content_type__app_label', 'content_type__model', 'codename')", 'unique_together': "(('content_type', 'codename'),)", 'object_name': 'Permission'}, 'codename': ('django.db.models.fields.CharField', [], {'max_length': '100'}), 'content_type': ('django.db.models.fields.related.ForeignKey', [], {'to': "orm['contenttypes.ContentType']"}), 'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}), 'name': ('django.db.models.fields.CharField', [], {'max_length': '50'}) }, 'auth.user': { 'Meta': {'object_name': 'User'}, 'date_joined': ('django.db.models.fields.DateTimeField', [], {'default': 'datetime.datetime.now'}), 'email': ('django.db.models.fields.EmailField', [], {'max_length': '75', 'blank': 'True'}), 'first_name': ('django.db.models.fields.CharField', [], {'max_length': '30', 'blank': 'True'}), 'groups': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['auth.Group']", 'symmetrical': 'False', 'blank': 'True'}), 'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}), 'is_active': ('django.db.models.fields.BooleanField', [], {'default': 'True'}), 'is_staff': ('django.db.models.fields.BooleanField', [], {'default': 'False'}), 'is_superuser': ('django.db.models.fields.BooleanField', [], {'default': 'False'}), 'last_login': ('django.db.models.fields.DateTimeField', [], {'default': 'datetime.datetime.now'}), 'last_name': ('django.db.models.fields.CharField', [], {'max_length': '30', 'blank': 'True'}), 'password': ('django.db.models.fields.CharField', [], {'max_length': '128'}), 'user_permissions': ('django.db.models.fields.related.ManyToManyField', [], {'to': "orm['auth.Permission']", 'symmetrical': 'False', 'blank': 'True'}), 'username': ('django.db.models.fields.CharField', [], {'unique': 'True', 'max_length': '30'}) }, 'contenttypes.contenttype': { 'Meta': {'ordering': "('name',)", 'unique_together': "(('app_label', 'model'),)", 'object_name': 'ContentType', 'db_table': "'django_content_type'"}, 'app_label': ('django.db.models.fields.CharField', [], {'max_length': '100'}), 'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}), 'model': ('django.db.models.fields.CharField', [], {'max_length': '100'}), 'name': ('django.db.models.fields.CharField', [], {'max_length': '100'}) }, 'shoppingcart.certificateitem': { 'Meta': {'object_name': 'CertificateItem', '_ormbases': ['shoppingcart.OrderItem']}, 'course_enrollment': ('django.db.models.fields.related.ForeignKey', [], {'to': "orm['student.CourseEnrollment']"}), 'course_id': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}), 'mode': ('django.db.models.fields.SlugField', [], {'max_length': '50'}), 'orderitem_ptr': ('django.db.models.fields.related.OneToOneField', [], {'to': "orm['shoppingcart.OrderItem']", 'unique': 'True', 'primary_key': 'True'}) }, 'shoppingcart.order': { 'Meta': {'object_name': 'Order'}, 'bill_to_cardtype': ('django.db.models.fields.CharField', [], {'max_length': '32', 'blank': 'True'}), 'bill_to_ccnum': ('django.db.models.fields.CharField', [], {'max_length': '8', 'blank': 'True'}), 'bill_to_city': ('django.db.models.fields.CharField', [], {'max_length': '64', 'blank': 'True'}), 'bill_to_country': ('django.db.models.fields.CharField', [], {'max_length': '64', 'blank': 'True'}), 'bill_to_first': ('django.db.models.fields.CharField', [], {'max_length': '64', 'blank': 'True'}), 'bill_to_last': ('django.db.models.fields.CharField', [], {'max_length': '64', 'blank': 'True'}), 'bill_to_postalcode': ('django.db.models.fields.CharField', [], {'max_length': '16', 'blank': 'True'}), 'bill_to_state': ('django.db.models.fields.CharField', [], {'max_length': '8', 'blank': 'True'}), 'bill_to_street1': ('django.db.models.fields.CharField', [], {'max_length': '128', 'blank': 'True'}), 'bill_to_street2': ('django.db.models.fields.CharField', [], {'max_length': '128', 'blank': 'True'}), 'currency': ('django.db.models.fields.CharField', [], {'default': "'usd'", 'max_length': '8'}), 'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}), 'processor_reply_dump': ('django.db.models.fields.TextField', [], {'blank': 'True'}), 'purchase_time': ('django.db.models.fields.DateTimeField', [], {'null': 'True', 'blank': 'True'}), 'status': ('django.db.models.fields.CharField', [], {'default': "'cart'", 'max_length': '32'}), 'user': ('django.db.models.fields.related.ForeignKey', [], {'to': "orm['auth.User']"}) }, 'shoppingcart.orderitem': { 'Meta': {'object_name': 'OrderItem'}, 'currency': ('django.db.models.fields.CharField', [], {'default': "'usd'", 'max_length': '8'}), 'fulfilled_time': ('django.db.models.fields.DateTimeField', [], {'null': 'True'}), 'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}), 'line_desc': ('django.db.models.fields.CharField', [], {'default': "'Misc. Item'", 'max_length': '1024'}), 'order': ('django.db.models.fields.related.ForeignKey', [], {'to': "orm['shoppingcart.Order']"}), 'qty': ('django.db.models.fields.IntegerField', [], {'default': '1'}), 'report_comments': ('django.db.models.fields.TextField', [], {'default': "''"}), 'status': ('django.db.models.fields.CharField', [], {'default': "'cart'", 'max_length': '32'}), 'unit_cost': ('django.db.models.fields.DecimalField', [], {'default': '0.0', 'max_digits': '30', 'decimal_places': '2'}), 'user': ('django.db.models.fields.related.ForeignKey', [], {'to': "orm['auth.User']"}) }, 'shoppingcart.paidcourseregistration': { 'Meta': {'object_name': 'PaidCourseRegistration', '_ormbases': ['shoppingcart.OrderItem']}, 'course_id': ('django.db.models.fields.CharField', [], {'max_length': '128', 'db_index': 'True'}), 'mode': ('django.db.models.fields.SlugField', [], {'default': "'honor'", 'max_length': '50'}), 'orderitem_ptr': ('django.db.models.fields.related.OneToOneField', [], {'to': "orm['shoppingcart.OrderItem']", 'unique': 'True', 'primary_key': 'True'}) }, 'shoppingcart.paidcourseregistrationannotation': { 'Meta': {'object_name': 'PaidCourseRegistrationAnnotation'}, 'annotation': ('django.db.models.fields.TextField', [], {'null': 'True'}), 'course_id': ('django.db.models.fields.CharField', [], {'unique': 'True', 'max_length': '128', 'db_index': 'True'}), 'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}) }, 'student.courseenrollment': { 'Meta': {'ordering': "('user', 'course_id')", 'unique_together': "(('user', 'course_id'),)", 'object_name': 'CourseEnrollment'}, 'course_id': ('django.db.models.fields.CharField', [], {'max_length': '255', 'db_index': 'True'}), 'created': ('django.db.models.fields.DateTimeField', [], {'auto_now_add': 'True', 'null': 'True', 'db_index': 'True', 'blank': 'True'}), 'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}), 'is_active': ('django.db.models.fields.BooleanField', [], {'default': 'True'}), 'mode': ('django.db.models.fields.CharField', [], {'default': "'honor'", 'max_length': '100'}), 'user': ('django.db.models.fields.related.ForeignKey', [], {'to': "orm['auth.User']"}) } } complete_apps = ['shoppingcart']
# This file is distributed under the same license as the Django package. # # The *_FORMAT strings use the Django date format syntax, # see http://docs.djangoproject.com/en/dev/ref/templates/builtins/#date DATE_FORMAT = 'j. F Y' TIME_FORMAT = 'H:i' DATETIME_FORMAT = 'j. F Y H:i' YEAR_MONTH_FORMAT = 'F Y' MONTH_DAY_FORMAT = 'j. F' SHORT_DATE_FORMAT = 'd.m.Y' SHORT_DATETIME_FORMAT = 'd.m.Y H:i' FIRST_DAY_OF_WEEK = 1 # Monday # The *_INPUT_FORMATS strings use the Python strftime format syntax, # see http://docs.python.org/library/datetime.html#strftime-strptime-behavior # Kept ISO formats as they are in first position DATE_INPUT_FORMATS = [ '%Y-%m-%d', '%d.%m.%Y', '%d.%m.%y', # '2006-10-25', '25.10.2006', '25.10.06' # '%d. %b %Y', '%d %b %Y', # '25. okt 2006', '25 okt 2006' # '%d. %b. %Y', '%d %b. %Y', # '25. okt. 2006', '25 okt. 2006' # '%d. %B %Y', '%d %B %Y', # '25. oktober 2006', '25 oktober 2006' ] DATETIME_INPUT_FORMATS = [ '%Y-%m-%d %H:%M:%S', # '2006-10-25 14:30:59' '%Y-%m-%d %H:%M:%S.%f', # '2006-10-25 14:30:59.000200' '%Y-%m-%d %H:%M', # '2006-10-25 14:30' '%Y-%m-%d', # '2006-10-25' '%d.%m.%Y %H:%M:%S', # '25.10.2006 14:30:59' '%d.%m.%Y %H:%M:%S.%f', # '25.10.2006 14:30:59.000200' '%d.%m.%Y %H:%M', # '25.10.2006 14:30' '%d.%m.%Y', # '25.10.2006' '%d.%m.%y %H:%M:%S', # '25.10.06 14:30:59' '%d.%m.%y %H:%M:%S.%f', # '25.10.06 14:30:59.000200' '%d.%m.%y %H:%M', # '25.10.06 14:30' '%d.%m.%y', # '25.10.06' ] DECIMAL_SEPARATOR = ',' THOUSAND_SEPARATOR = '\xa0' # non-breaking space NUMBER_GROUPING = 3
from __future__ import absolute_import, division, print_function import logging # concurrent.futures is optional try: from concurrent.futures import ThreadPoolExecutor except ImportError: ThreadPoolExecutor = None log = logging.getLogger(__name__) class Emitter(object): threading = False threading_workers = 2 __constructed = False __name = None __callbacks = None __threading_pool = None def __ensure_constructed(self): if self.__constructed: return self.__callbacks = {} self.__constructed = True if self.threading: if ThreadPoolExecutor is None: raise Exception('concurrent.futures is required for threading') self.__threading_pool = ThreadPoolExecutor(max_workers=self.threading_workers) def __log(self, message, *args, **kwargs): if self.__name is None: self.__name = '%s.%s' % ( self.__module__, self.__class__.__name__ ) log.debug( ('[%s]:' % self.__name.ljust(34)) + str(message), *args, **kwargs ) def __wrap(self, callback, *args, **kwargs): def wrap(func): callback(func=func, *args, **kwargs) return func return wrap def on(self, events, func=None, on_bound=None): if not func: # assume decorator, wrap return self.__wrap(self.on, events, on_bound=on_bound) if not isinstance(events, (list, tuple)): events = [events] self.__log('on(events: %s, func: %s)', repr(events), repr(func)) self.__ensure_constructed() for event in events: if event not in self.__callbacks: self.__callbacks[event] = [] # Bind callback to event self.__callbacks[event].append(func) # Call 'on_bound' callback if on_bound: self.__call(on_bound, kwargs={ 'func': func }) return self def once(self, event, func=None): if not func: # assume decorator, wrap return self.__wrap(self.once, event) self.__log('once(event: %s, func: %s)', repr(event), repr(func)) def once_callback(*args, **kwargs): self.off(event, once_callback) func(*args, **kwargs) self.on(event, once_callback) return self def off(self, event=None, func=None): self.__log('off(event: %s, func: %s)', repr(event), repr(func)) self.__ensure_constructed() if event and event not in self.__callbacks: return self if func and func not in self.__callbacks[event]: return self if event and func: self.__callbacks[event].remove(func) elif event: self.__callbacks[event] = [] elif func: raise ValueError('"event" is required if "func" is specified') else: self.__callbacks = {} return self def emit(self, event, *args, **kwargs): suppress = kwargs.pop('__suppress', False) if not suppress: self.__log('emit(event: %s, args: %s, kwargs: %s)', repr(event), repr_trim(args), repr_trim(kwargs)) self.__ensure_constructed() if event not in self.__callbacks: return for callback in list(self.__callbacks[event]): self.__call(callback, args, kwargs, event) return self def emit_on(self, event, *args, **kwargs): func = kwargs.pop('func', None) if not func: # assume decorator, wrap return self.__wrap(self.emit_on, event, *args, **kwargs) self.__log('emit_on(event: %s, func: %s, args: %s, kwargs: %s)', repr(event), repr(func), repr(args), repr(kwargs)) # Bind func from wrapper self.on(event, func) # Emit event (calling 'func') self.emit(event, *args, **kwargs) def pipe(self, events, other): if type(events) is not list: events = [events] self.__log('pipe(events: %s, other: %s)', repr(events), repr(other)) self.__ensure_constructed() for event in events: self.on(event, PipeHandler(event, other.emit)) return self def __call(self, callback, args=None, kwargs=None, event=None): args = args or () kwargs = kwargs or {} if self.threading: return self.__call_async(callback, args, kwargs, event) return self.__call_sync(callback, args, kwargs, event) @classmethod def __call_sync(cls, callback, args=None, kwargs=None, event=None): try: callback(*args, **kwargs) return True except Exception as ex: log.warn('[%s] Exception raised in: %s - %s' % (event, cls.__function_name(callback), ex), exc_info=True) return False def __call_async(self, callback, args=None, kwargs=None, event=None): self.__threading_pool.submit(self.__call_sync, callback, args, kwargs, event) @staticmethod def __function_name(func): fragments = [] # Try append class name cls = getattr(func, 'im_class', None) if cls and hasattr(cls, '__name__'): fragments.append(cls.__name__) # Append function name fragments.append(func.__name__) return '.'.join(fragments) class PipeHandler(object): def __init__(self, event, callback): self.event = event self.callback = callback def __call__(self, *args, **kwargs): self.callback(self.event, *args, **kwargs) def on(emitter, event, func=None): emitter.on(event, func) return { 'destroy': lambda: emitter.off(event, func) } def once(emitter, event, func=None): return emitter.once(event, func) def off(emitter, event, func=None): return emitter.off(event, func) def emit(emitter, event, *args, **kwargs): return emitter.emit(event, *args, **kwargs) def repr_trim(value, length=1000): value = repr(value) if len(value) < length: return value return '<%s - %s characters>' % (type(value).__name__, len(value))
# -*- coding: utf-8 -*- # ########################## Copyrights and license ############################ # # # Copyright 2012 Vincent Jacques <vincent@vincent-jacques.net> # # Copyright 2012 Zearin <zearin@gonk.net> # # Copyright 2013 AKFish <akfish@gmail.com> # # Copyright 2013 Vincent Jacques <vincent@vincent-jacques.net> # # # # This file is part of PyGithub. http://jacquev6.github.com/PyGithub/ # # # # PyGithub is free software: you can redistribute it and/or modify it under # # the terms of the GNU Lesser General Public License as published by the Free # # Software Foundation, either version 3 of the License, or (at your option) # # any later version. # # # # PyGithub is distributed in the hope that it will be useful, but WITHOUT ANY # # WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS # # FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more # # details. # # # # You should have received a copy of the GNU Lesser General Public License # # along with PyGithub. If not, see <http://www.gnu.org/licenses/>. # # # # ############################################################################## import base64 import sys import github.GithubObject import github.Repository atLeastPython3 = sys.hexversion >= 0x03000000 class ContentFile(github.GithubObject.CompletableGithubObject): """ This class represents ContentFiles as returned for example by http://developer.github.com/v3/todo """ @property def content(self): """ :type: string """ self._completeIfNotSet(self._content) return self._content.value @property def decoded_content(self): assert self.encoding == "base64", "unsupported encoding: %s" % self.encoding if atLeastPython3: content = bytearray(self.content, "utf-8") # pragma no cover (covered by tests with Python 3.2) else: content = self.content return base64.b64decode(content) @property def encoding(self): """ :type: string """ self._completeIfNotSet(self._encoding) return self._encoding.value @property def git_url(self): """ :type: string """ self._completeIfNotSet(self._git_url) return self._git_url.value @property def html_url(self): """ :type: string """ self._completeIfNotSet(self._html_url) return self._html_url.value @property def name(self): """ :type: string """ self._completeIfNotSet(self._name) return self._name.value @property def path(self): """ :type: string """ self._completeIfNotSet(self._path) return self._path.value @property def repository(self): """ :type: :class:`github.Repository.Repository` """ if self._repository is github.GithubObject.NotSet: # The repository was not set automatically, so it must be looked up by url. repo_url = "/".join(self.url.split("/")[:6]) # pragma no cover (Should be covered) self._repository = github.GithubObject._ValuedAttribute(github.Repository.Repository(self._requester, self._headers, {'url': repo_url}, completed=False)) # pragma no cover (Should be covered) return self._repository.value @property def sha(self): """ :type: string """ self._completeIfNotSet(self._sha) return self._sha.value @property def size(self): """ :type: integer """ self._completeIfNotSet(self._size) return self._size.value @property def type(self): """ :type: string """ self._completeIfNotSet(self._type) return self._type.value @property def url(self): """ :type: string """ self._completeIfNotSet(self._url) return self._url.value def _initAttributes(self): self._content = github.GithubObject.NotSet self._encoding = github.GithubObject.NotSet self._git_url = github.GithubObject.NotSet self._html_url = github.GithubObject.NotSet self._name = github.GithubObject.NotSet self._path = github.GithubObject.NotSet self._repository = github.GithubObject.NotSet self._sha = github.GithubObject.NotSet self._size = github.GithubObject.NotSet self._type = github.GithubObject.NotSet def _useAttributes(self, attributes): if "content" in attributes: # pragma no branch self._content = self._makeStringAttribute(attributes["content"]) if "encoding" in attributes: # pragma no branch self._encoding = self._makeStringAttribute(attributes["encoding"]) if "git_url" in attributes: # pragma no branch self._git_url = self._makeStringAttribute(attributes["git_url"]) if "html_url" in attributes: # pragma no branch self._html_url = self._makeStringAttribute(attributes["html_url"]) if "name" in attributes: # pragma no branch self._name = self._makeStringAttribute(attributes["name"]) if "path" in attributes: # pragma no branch self._path = self._makeStringAttribute(attributes["path"]) if "repository" in attributes: # pragma no branch self._repository = self._makeClassAttribute(github.Repository.Repository, attributes["repository"]) if "sha" in attributes: # pragma no branch self._sha = self._makeStringAttribute(attributes["sha"]) if "size" in attributes: # pragma no branch self._size = self._makeIntAttribute(attributes["size"]) if "type" in attributes: # pragma no branch self._type = self._makeStringAttribute(attributes["type"]) if "url" in attributes: # pragma no branch self._url = self._makeStringAttribute(attributes["url"])
#!/usr/bin/env python # Copyright (c) 2012 Google Inc. All rights reserved. # Use of this source code is governed by a BSD-style license that can be # found in the LICENSE file. """Make the format of a vcproj really pretty. This script normalize and sort an xml. It also fetches all the properties inside linked vsprops and include them explicitly in the vcproj. It outputs the resulting xml to stdout. """ __author__ = 'nsylvain (Nicolas Sylvain)' import os import sys from xml.dom.minidom import parse from xml.dom.minidom import Node REPLACEMENTS = dict() ARGUMENTS = None class CmpTuple(object): """Compare function between 2 tuple.""" def __call__(self, x, y): return cmp(x[0], y[0]) class CmpNode(object): """Compare function between 2 xml nodes.""" def __call__(self, x, y): def get_string(node): node_string = "node" node_string += node.nodeName if node.nodeValue: node_string += node.nodeValue if node.attributes: # We first sort by name, if present. node_string += node.getAttribute("Name") all_nodes = [] for (name, value) in node.attributes.items(): all_nodes.append((name, value)) all_nodes.sort(CmpTuple()) for (name, value) in all_nodes: node_string += name node_string += value return node_string return cmp(get_string(x), get_string(y)) def PrettyPrintNode(node, indent=0): if node.nodeType == Node.TEXT_NODE: if node.data.strip(): print '%s%s' % (' '*indent, node.data.strip()) return if node.childNodes: node.normalize() # Get the number of attributes attr_count = 0 if node.attributes: attr_count = node.attributes.length # Print the main tag if attr_count == 0: print '%s<%s>' % (' '*indent, node.nodeName) else: print '%s<%s' % (' '*indent, node.nodeName) all_attributes = [] for (name, value) in node.attributes.items(): all_attributes.append((name, value)) all_attributes.sort(CmpTuple()) for (name, value) in all_attributes: print '%s %s="%s"' % (' '*indent, name, value) print '%s>' % (' '*indent) if node.nodeValue: print '%s %s' % (' '*indent, node.nodeValue) for sub_node in node.childNodes: PrettyPrintNode(sub_node, indent=indent+2) print '%s</%s>' % (' '*indent, node.nodeName) def FlattenFilter(node): """Returns a list of all the node and sub nodes.""" node_list = [] if (node.attributes and node.getAttribute('Name') == '_excluded_files'): # We don't add the "_excluded_files" filter. return [] for current in node.childNodes: if current.nodeName == 'Filter': node_list.extend(FlattenFilter(current)) else: node_list.append(current) return node_list def FixFilenames(filenames, current_directory): new_list = [] for filename in filenames: if filename: for key in REPLACEMENTS: filename = filename.replace(key, REPLACEMENTS[key]) os.chdir(current_directory) filename = filename.strip('"\' ') if filename.startswith('$'): new_list.append(filename) else: new_list.append(os.path.abspath(filename)) return new_list def AbsoluteNode(node): """Makes all the properties we know about in this node absolute.""" if node.attributes: for (name, value) in node.attributes.items(): if name in ['InheritedPropertySheets', 'RelativePath', 'AdditionalIncludeDirectories', 'IntermediateDirectory', 'OutputDirectory', 'AdditionalLibraryDirectories']: # We want to fix up these paths path_list = value.split(';') new_list = FixFilenames(path_list, os.path.dirname(ARGUMENTS[1])) node.setAttribute(name, ';'.join(new_list)) if not value: node.removeAttribute(name) def CleanupVcproj(node): """For each sub node, we call recursively this function.""" for sub_node in node.childNodes: AbsoluteNode(sub_node) CleanupVcproj(sub_node) # Normalize the node, and remove all extranous whitespaces. for sub_node in node.childNodes: if sub_node.nodeType == Node.TEXT_NODE: sub_node.data = sub_node.data.replace("\r", "") sub_node.data = sub_node.data.replace("\n", "") sub_node.data = sub_node.data.rstrip() # Fix all the semicolon separated attributes to be sorted, and we also # remove the dups. if node.attributes: for (name, value) in node.attributes.items(): sorted_list = sorted(value.split(';')) unique_list = [] for i in sorted_list: if not unique_list.count(i): unique_list.append(i) node.setAttribute(name, ';'.join(unique_list)) if not value: node.removeAttribute(name) if node.childNodes: node.normalize() # For each node, take a copy, and remove it from the list. node_array = [] while node.childNodes and node.childNodes[0]: # Take a copy of the node and remove it from the list. current = node.childNodes[0] node.removeChild(current) # If the child is a filter, we want to append all its children # to this same list. if current.nodeName == 'Filter': node_array.extend(FlattenFilter(current)) else: node_array.append(current) # Sort the list. node_array.sort(CmpNode()) # Insert the nodes in the correct order. for new_node in node_array: # But don't append empty tool node. if new_node.nodeName == 'Tool': if new_node.attributes and new_node.attributes.length == 1: # This one was empty. continue if new_node.nodeName == 'UserMacro': continue node.appendChild(new_node) def GetConfiguationNodes(vcproj): #TODO(nsylvain): Find a better way to navigate the xml. nodes = [] for node in vcproj.childNodes: if node.nodeName == "Configurations": for sub_node in node.childNodes: if sub_node.nodeName == "Configuration": nodes.append(sub_node) return nodes def GetChildrenVsprops(filename): dom = parse(filename) if dom.documentElement.attributes: vsprops = dom.documentElement.getAttribute('InheritedPropertySheets') return FixFilenames(vsprops.split(';'), os.path.dirname(filename)) return [] def SeekToNode(node1, child2): # A text node does not have properties. if child2.nodeType == Node.TEXT_NODE: return None # Get the name of the current node. current_name = child2.getAttribute("Name") if not current_name: # There is no name. We don't know how to merge. return None # Look through all the nodes to find a match. for sub_node in node1.childNodes: if sub_node.nodeName == child2.nodeName: name = sub_node.getAttribute("Name") if name == current_name: return sub_node # No match. We give up. return None def MergeAttributes(node1, node2): # No attributes to merge? if not node2.attributes: return for (name, value2) in node2.attributes.items(): # Don't merge the 'Name' attribute. if name == 'Name': continue value1 = node1.getAttribute(name) if value1: # The attribute exist in the main node. If it's equal, we leave it # untouched, otherwise we concatenate it. if value1 != value2: node1.setAttribute(name, ';'.join([value1, value2])) else: # The attribute does nto exist in the main node. We append this one. node1.setAttribute(name, value2) # If the attribute was a property sheet attributes, we remove it, since # they are useless. if name == 'InheritedPropertySheets': node1.removeAttribute(name) def MergeProperties(node1, node2): MergeAttributes(node1, node2) for child2 in node2.childNodes: child1 = SeekToNode(node1, child2) if child1: MergeProperties(child1, child2) else: node1.appendChild(child2.cloneNode(True)) def main(argv): """Main function of this vcproj prettifier.""" global ARGUMENTS ARGUMENTS = argv # check if we have exactly 1 parameter. if len(argv) < 2: print ('Usage: %s "c:\\path\\to\\vcproj.vcproj" [key1=value1] ' '[key2=value2]' % argv[0]) return 1 # Parse the keys for i in range(2, len(argv)): (key, value) = argv[i].split('=') REPLACEMENTS[key] = value # Open the vcproj and parse the xml. dom = parse(argv[1]) # First thing we need to do is find the Configuration Node and merge them # with the vsprops they include. for configuration_node in GetConfiguationNodes(dom.documentElement): # Get the property sheets associated with this configuration. vsprops = configuration_node.getAttribute('InheritedPropertySheets') # Fix the filenames to be absolute. vsprops_list = FixFilenames(vsprops.strip().split(';'), os.path.dirname(argv[1])) # Extend the list of vsprops with all vsprops contained in the current # vsprops. for current_vsprops in vsprops_list: vsprops_list.extend(GetChildrenVsprops(current_vsprops)) # Now that we have all the vsprops, we need to merge them. for current_vsprops in vsprops_list: MergeProperties(configuration_node, parse(current_vsprops).documentElement) # Now that everything is merged, we need to cleanup the xml. CleanupVcproj(dom.documentElement) # Finally, we use the prett xml function to print the vcproj back to the # user. #print dom.toprettyxml(newl="\n") PrettyPrintNode(dom.documentElement) return 0 if __name__ == '__main__': sys.exit(main(sys.argv))
# -*- coding: utf-8 -*- import re from module.network.RequestFactory import getURL from module.plugins.Hoster import Hoster def getInfo(urls): ids = "" names = "" p = re.compile(RapidshareCom.__pattern__) for url in urls: r = p.search(url) if r.group("name"): ids += "," + r.group("id") names += "," + r.group("name") elif r.group("name_new"): ids += "," + r.group("id_new") names += "," + r.group("name_new") url = "http://api.rapidshare.com/cgi-bin/rsapi.cgi?sub=checkfiles&files=%s&filenames=%s" % (ids[1:], names[1:]) api = getURL(url) result = [] i = 0 for res in api.split(): tmp = res.split(",") if tmp[4] in ("0", "4", "5"): status = 1 elif tmp[4] == "1": status = 2 else: status = 3 result.append((tmp[1], tmp[2], status, urls[i])) i += 1 yield result class RapidshareCom(Hoster): __name__ = "RapidshareCom" __type__ = "hoster" __version__ = "1.40" __pattern__ = r'https?://(?:www\.)?rapidshare\.com/(?:files/(?P<id>\d+)/(?P<name>[^?]+)|#!download\|(?:\w+)\|(?P<id_new>\d+)\|(?P<name_new>[^|]+))' __description__ = """Rapidshare.com hoster plugin""" __license__ = "GPLv3" __authors__ = [("spoob", "spoob@pyload.org"), ("RaNaN", "ranan@pyload.org"), ("mkaay", "mkaay@mkaay.de")] def setup(self): self.no_download = True self.api_data = None self.offset = 0 self.dl_dict = {} self.id = None self.name = None self.chunkLimit = -1 if self.premium else 1 self.multiDL = self.resumeDownload = self.premium def process(self, pyfile): self.url = pyfile.url self.prepare() def prepare(self): m = re.match(self.__pattern__, self.url) if m.group("name"): self.id = m.group("id") self.name = m.group("name") else: self.id = m.group("id_new") self.name = m.group("name_new") self.download_api_data() if self.api_data['status'] == "1": self.pyfile.name = self.get_file_name() if self.premium: self.handlePremium() else: self.handleFree() elif self.api_data['status'] == "2": self.logInfo(_("Rapidshare: Traffic Share (direct download)")) self.pyfile.name = self.get_file_name() self.download(self.pyfile.url, get={"directstart": 1}) elif self.api_data['status'] in ("0", "4", "5"): self.offline() elif self.api_data['status'] == "3": self.tempOffline() else: self.error(_("Unknown response code")) def handleFree(self): while self.no_download: self.dl_dict = self.freeWait() #tmp = "#!download|%(server)s|%(id)s|%(name)s|%(size)s" download = "http://%(host)s/cgi-bin/rsapi.cgi?sub=download&editparentlocation=0&bin=1&fileid=%(id)s&filename=%(name)s&dlauth=%(auth)s" % self.dl_dict self.logDebug("RS API Request: %s" % download) self.download(download, ref=False) check = self.checkDownload({"ip": "You need RapidPro to download more files from your IP address", "auth": "Download auth invalid"}) if check == "ip": self.setWait(60) self.logInfo(_("Already downloading from this ip address, waiting 60 seconds")) self.wait() self.handleFree() elif check == "auth": self.logInfo(_("Invalid Auth Code, download will be restarted")) self.offset += 5 self.handleFree() def handlePremium(self): info = self.account.getAccountInfo(self.user, True) self.logDebug("Use Premium Account") url = self.api_data['mirror'] self.download(url, get={"directstart": 1}) def download_api_data(self, force=False): """ http://images.rapidshare.com/apidoc.txt """ if self.api_data and not force: return api_url_base = "http://api.rapidshare.com/cgi-bin/rsapi.cgi" api_param_file = {"sub": "checkfiles", "incmd5": "1", "files": self.id, "filenames": self.name} html = self.load(api_url_base, cookies=False, get=api_param_file).strip() self.logDebug("RS INFO API: %s" % html) if html.startswith("ERROR"): return fields = html.split(",") # status codes: # 0=File not found # 1=File OK (Anonymous downloading) # 3=Server down # 4=File marked as illegal # 5=Anonymous file locked, because it has more than 10 downloads already # 50+n=File OK (TrafficShare direct download type "n" without any logging.) # 100+n=File OK (TrafficShare direct download type "n" with logging. # Read our privacy policy to see what is logged.) self.api_data = {"fileid": fields[0], "filename": fields[1], "size": int(fields[2]), "serverid": fields[3], "status": fields[4], "shorthost": fields[5], "checksum": fields[6].strip().lower()} if int(self.api_data['status']) > 100: self.api_data['status'] = str(int(self.api_data['status']) - 100) elif int(self.api_data['status']) > 50: self.api_data['status'] = str(int(self.api_data['status']) - 50) self.api_data['mirror'] = "http://rs%(serverid)s%(shorthost)s.rapidshare.com/files/%(fileid)s/%(filename)s" % self.api_data def freeWait(self): """downloads html with the important information """ self.no_download = True id = self.id name = self.name prepare = "https://api.rapidshare.com/cgi-bin/rsapi.cgi?sub=download&fileid=%(id)s&filename=%(name)s&try=1&cbf=RSAPIDispatcher&cbid=1" % { "name": name, "id": id} self.logDebug("RS API Request: %s" % prepare) result = self.load(prepare, ref=False) self.logDebug("RS API Result: %s" % result) between_wait = re.search("You need to wait (\d+) seconds", result) if "You need RapidPro to download more files from your IP address" in result: self.setWait(60) self.logInfo(_("Already downloading from this ip address, waiting 60 seconds")) self.wait() elif ("Too many users downloading from this server right now" in result or "All free download slots are full" in result): self.setWait(120) self.logInfo(_("RapidShareCom: No free slots")) self.wait() elif "This file is too big to download it for free" in result: self.fail(_("You need a premium account for this file")) elif "Filename invalid." in result: self.fail(_("Filename reported invalid")) elif between_wait: self.setWait(int(between_wait.group(1)), True) self.wait() else: self.no_download = False tmp, info = result.split(":") data = info.split(",") dl_dict = {"id": id, "name": name, "host": data[0], "auth": data[1], "server": self.api_data['serverid'], "size": self.api_data['size']} self.setWait(int(data[2]) + 2 + self.offset) self.wait() return dl_dict def get_file_name(self): if self.api_data['filename']: return self.api_data['filename'] return self.url.split("/")[-1]
# -*- coding: utf-8 -*- from south.utils import datetime_utils as datetime from south.db import db from south.v2 import SchemaMigration from django.db import models class Migration(SchemaMigration): def forwards(self, orm): # Adding M2M table for field authors on 'Book' m2m_table_name = db.shorten_name(u'library_book_authors') db.create_table(m2m_table_name, ( ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)), ('book', models.ForeignKey(orm[u'library.book'], null=False)), ('author', models.ForeignKey(orm[u'library.author'], null=False)) )) db.create_unique(m2m_table_name, ['book_id', 'author_id']) # Adding field 'Author.uuid' db.add_column(u'library_author', 'uuid', self.gf('django.db.models.fields.CharField')(default='None', max_length=36, db_index=True), keep_default=False) def backwards(self, orm): # Removing M2M table for field authors on 'Book' db.delete_table(db.shorten_name(u'library_book_authors')) # Deleting field 'Author.uuid' db.delete_column(u'library_author', 'uuid') models = { u'library.author': { 'Meta': {'object_name': 'Author'}, 'firstname': ('django.db.models.fields.CharField', [], {'max_length': '50'}), u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}), 'lastname': ('django.db.models.fields.CharField', [], {'max_length': '50'}), 'uuid': ('django.db.models.fields.CharField', [], {'default': 'None', 'max_length': '36', 'db_index': 'True'}) }, u'library.book': { 'Meta': {'object_name': 'Book'}, 'authors': ('django.db.models.fields.related.ManyToManyField', [], {'to': u"orm['library.Author']", 'symmetrical': 'False'}), 'description': ('django.db.models.fields.TextField', [], {'null': 'True'}), u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}), 'isbn10': ('django.db.models.fields.CharField', [], {'max_length': '10', 'null': 'True'}), 'isbn13': ('django.db.models.fields.CharField', [], {'max_length': '13', 'null': 'True'}), 'publish_date': ('django.db.models.fields.DateField', [], {'null': 'True'}), 'publisher': ('django.db.models.fields.CharField', [], {'max_length': '100', 'null': 'True'}), 'title': ('django.db.models.fields.CharField', [], {'max_length': '200'}), 'uuid': ('django.db.models.fields.CharField', [], {'default': 'None', 'max_length': '36', 'db_index': 'True'}) } } complete_apps = ['library']
from __future__ import absolute_import import datetime import os import sys import socket from socket import error as SocketError, timeout as SocketTimeout import warnings from .packages import six try: # Python 3 from http.client import HTTPConnection as _HTTPConnection from http.client import HTTPException # noqa: unused in this module except ImportError: from httplib import HTTPConnection as _HTTPConnection from httplib import HTTPException # noqa: unused in this module try: # Compiled with SSL? import ssl BaseSSLError = ssl.SSLError except (ImportError, AttributeError): # Platform-specific: No SSL. ssl = None class BaseSSLError(BaseException): pass try: # Python 3: # Not a no-op, we're adding this to the namespace so it can be imported. ConnectionError = ConnectionError except NameError: # Python 2: class ConnectionError(Exception): pass from .exceptions import ( NewConnectionError, ConnectTimeoutError, SubjectAltNameWarning, SystemTimeWarning, ) from .packages.ssl_match_hostname import match_hostname from .util.ssl_ import ( resolve_cert_reqs, resolve_ssl_version, ssl_wrap_socket, assert_fingerprint, ) from .util import connection port_by_scheme = { 'http': 80, 'https': 443, } RECENT_DATE = datetime.date(2014, 1, 1) class DummyConnection(object): """Used to detect a failed ConnectionCls import.""" pass class HTTPConnection(_HTTPConnection, object): """ Based on httplib.HTTPConnection but provides an extra constructor backwards-compatibility layer between older and newer Pythons. Additional keyword parameters are used to configure attributes of the connection. Accepted parameters include: - ``strict``: See the documentation on :class:`urllib3.connectionpool.HTTPConnectionPool` - ``source_address``: Set the source address for the current connection. .. note:: This is ignored for Python 2.6. It is only applied for 2.7 and 3.x - ``socket_options``: Set specific options on the underlying socket. If not specified, then defaults are loaded from ``HTTPConnection.default_socket_options`` which includes disabling Nagle's algorithm (sets TCP_NODELAY to 1) unless the connection is behind a proxy. For example, if you wish to enable TCP Keep Alive in addition to the defaults, you might pass:: HTTPConnection.default_socket_options + [ (socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1), ] Or you may want to disable the defaults by passing an empty list (e.g., ``[]``). """ default_port = port_by_scheme['http'] #: Disable Nagle's algorithm by default. #: ``[(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)]`` default_socket_options = [(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)] #: Whether this connection verifies the host's certificate. is_verified = False def __init__(self, *args, **kw): if six.PY3: # Python 3 kw.pop('strict', None) # Pre-set source_address in case we have an older Python like 2.6. self.source_address = kw.get('source_address') if sys.version_info < (2, 7): # Python 2.6 # _HTTPConnection on Python 2.6 will balk at this keyword arg, but # not newer versions. We can still use it when creating a # connection though, so we pop it *after* we have saved it as # self.source_address. kw.pop('source_address', None) #: The socket options provided by the user. If no options are #: provided, we use the default options. self.socket_options = kw.pop('socket_options', self.default_socket_options) # Superclass also sets self.source_address in Python 2.7+. _HTTPConnection.__init__(self, *args, **kw) def _new_conn(self): """ Establish a socket connection and set nodelay settings on it. :return: New socket connection. """ extra_kw = {} if self.source_address: extra_kw['source_address'] = self.source_address if self.socket_options: extra_kw['socket_options'] = self.socket_options try: conn = connection.create_connection( (self.host, self.port), self.timeout, **extra_kw) except SocketTimeout as e: raise ConnectTimeoutError( self, "Connection to %s timed out. (connect timeout=%s)" % (self.host, self.timeout)) except SocketError as e: raise NewConnectionError( self, "Failed to establish a new connection: %s" % e) return conn def _prepare_conn(self, conn): self.sock = conn # the _tunnel_host attribute was added in python 2.6.3 (via # http://hg.python.org/cpython/rev/0f57b30a152f) so pythons 2.6(0-2) do # not have them. if getattr(self, '_tunnel_host', None): # TODO: Fix tunnel so it doesn't depend on self.sock state. self._tunnel() # Mark this connection as not reusable self.auto_open = 0 def connect(self): conn = self._new_conn() self._prepare_conn(conn) class HTTPSConnection(HTTPConnection): default_port = port_by_scheme['https'] def __init__(self, host, port=None, key_file=None, cert_file=None, strict=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT, **kw): HTTPConnection.__init__(self, host, port, strict=strict, timeout=timeout, **kw) self.key_file = key_file self.cert_file = cert_file # Required property for Google AppEngine 1.9.0 which otherwise causes # HTTPS requests to go out as HTTP. (See Issue #356) self._protocol = 'https' def connect(self): conn = self._new_conn() self._prepare_conn(conn) self.sock = ssl.wrap_socket(conn, self.key_file, self.cert_file) class VerifiedHTTPSConnection(HTTPSConnection): """ Based on httplib.HTTPSConnection but wraps the socket with SSL certification. """ cert_reqs = None ca_certs = None ca_cert_dir = None ssl_version = None assert_fingerprint = None def set_cert(self, key_file=None, cert_file=None, cert_reqs=None, ca_certs=None, assert_hostname=None, assert_fingerprint=None, ca_cert_dir=None): if (ca_certs or ca_cert_dir) and cert_reqs is None: cert_reqs = 'CERT_REQUIRED' self.key_file = key_file self.cert_file = cert_file self.cert_reqs = cert_reqs self.assert_hostname = assert_hostname self.assert_fingerprint = assert_fingerprint self.ca_certs = ca_certs and os.path.expanduser(ca_certs) self.ca_cert_dir = ca_cert_dir and os.path.expanduser(ca_cert_dir) def connect(self): # Add certificate verification conn = self._new_conn() resolved_cert_reqs = resolve_cert_reqs(self.cert_reqs) resolved_ssl_version = resolve_ssl_version(self.ssl_version) hostname = self.host if getattr(self, '_tunnel_host', None): # _tunnel_host was added in Python 2.6.3 # (See: http://hg.python.org/cpython/rev/0f57b30a152f) self.sock = conn # Calls self._set_hostport(), so self.host is # self._tunnel_host below. self._tunnel() # Mark this connection as not reusable self.auto_open = 0 # Override the host with the one we're requesting data from. hostname = self._tunnel_host is_time_off = datetime.date.today() < RECENT_DATE if is_time_off: warnings.warn(( 'System time is way off (before {0}). This will probably ' 'lead to SSL verification errors').format(RECENT_DATE), SystemTimeWarning ) # Wrap socket using verification with the root certs in # trusted_root_certs self.sock = ssl_wrap_socket(conn, self.key_file, self.cert_file, cert_reqs=resolved_cert_reqs, ca_certs=self.ca_certs, ca_cert_dir=self.ca_cert_dir, server_hostname=hostname, ssl_version=resolved_ssl_version) if self.assert_fingerprint: assert_fingerprint(self.sock.getpeercert(binary_form=True), self.assert_fingerprint) elif resolved_cert_reqs != ssl.CERT_NONE \ and self.assert_hostname is not False: cert = self.sock.getpeercert() if not cert.get('subjectAltName', ()): warnings.warn(( 'Certificate for {0} has no `subjectAltName`, falling back to check for a ' '`commonName` for now. This feature is being removed by major browsers and ' 'deprecated by RFC 2818. (See https://github.com/shazow/urllib3/issues/497 ' 'for details.)'.format(hostname)), SubjectAltNameWarning ) # In case the hostname is an IPv6 address, strip the square # brackets from it before using it to validate. This is because # a certificate with an IPv6 address in it won't have square # brackets around that address. Sadly, match_hostname won't do this # for us: it expects the plain host part without any extra work # that might have been done to make it palatable to httplib. asserted_hostname = self.assert_hostname or hostname asserted_hostname = asserted_hostname.strip('[]') match_hostname(cert, asserted_hostname) self.is_verified = (resolved_cert_reqs == ssl.CERT_REQUIRED or self.assert_fingerprint is not None) if ssl: # Make a copy for testing. UnverifiedHTTPSConnection = HTTPSConnection HTTPSConnection = VerifiedHTTPSConnection else: HTTPSConnection = DummyConnection
import socket import sys from redis.connection import (Connection, SYM_STAR, SYM_DOLLAR, SYM_EMPTY, SYM_CRLF, b) from redis._compat import imap from base import Benchmark class StringJoiningConnection(Connection): def send_packed_command(self, command): "Send an already packed command to the Redis server" if not self._sock: self.connect() try: self._sock.sendall(command) except socket.error: e = sys.exc_info()[1] self.disconnect() if len(e.args) == 1: _errno, errmsg = 'UNKNOWN', e.args[0] else: _errno, errmsg = e.args raise ConnectionError("Error %s while writing to socket. %s." % (_errno, errmsg)) except: self.disconnect() raise def pack_command(self, *args): "Pack a series of arguments into a value Redis command" args_output = SYM_EMPTY.join([ SYM_EMPTY.join((SYM_DOLLAR, b(str(len(k))), SYM_CRLF, k, SYM_CRLF)) for k in imap(self.encode, args)]) output = SYM_EMPTY.join( (SYM_STAR, b(str(len(args))), SYM_CRLF, args_output)) return output class ListJoiningConnection(Connection): def send_packed_command(self, command): if not self._sock: self.connect() try: if isinstance(command, str): command = [command] for item in command: self._sock.sendall(item) except socket.error: e = sys.exc_info()[1] self.disconnect() if len(e.args) == 1: _errno, errmsg = 'UNKNOWN', e.args[0] else: _errno, errmsg = e.args raise ConnectionError("Error %s while writing to socket. %s." % (_errno, errmsg)) except: self.disconnect() raise def pack_command(self, *args): output = [] buff = SYM_EMPTY.join( (SYM_STAR, b(str(len(args))), SYM_CRLF)) for k in imap(self.encode, args): if len(buff) > 6000 or len(k) > 6000: buff = SYM_EMPTY.join( (buff, SYM_DOLLAR, b(str(len(k))), SYM_CRLF)) output.append(buff) output.append(k) buff = SYM_CRLF else: buff = SYM_EMPTY.join((buff, SYM_DOLLAR, b(str(len(k))), SYM_CRLF, k, SYM_CRLF)) output.append(buff) return output class CommandPackerBenchmark(Benchmark): ARGUMENTS = ( { 'name': 'connection_class', 'values': [StringJoiningConnection, ListJoiningConnection] }, { 'name': 'value_size', 'values': [10, 100, 1000, 10000, 100000, 1000000, 10000000, 100000000] }, ) def setup(self, connection_class, value_size): self.get_client(connection_class=connection_class) def run(self, connection_class, value_size): r = self.get_client() x = 'a' * value_size r.set('benchmark', x) if __name__ == '__main__': CommandPackerBenchmark().run_benchmark()
# -*- coding: utf-8 -*- from sqlalchemy.exc import IntegrityError from flask import g, Response, redirect, flash from flask.ext.lastuser import signal_user_session_refreshed from coaster.views import get_next_url from baseframe import csrf from .. import app, lastuser from ..signals import signal_login, signal_logout from ..models import db, UserActiveAt @app.route('/500') def error500(): raise Exception("Something b0rked") @app.route('/login') @lastuser.login_handler def login(): return {'scope': 'id email/* phone/* organizations/* teams/* notice/*'} @app.route('/logout') @lastuser.logout_handler def logout(): flash(u"You are now logged out", category='info') signal_logout.send(app, user=g.user) return get_next_url() @app.route('/login/redirect') @lastuser.auth_handler def lastuserauth(): signal_login.send(app, user=g.user) db.session.commit() return redirect(get_next_url()) @csrf.exempt @app.route('/login/notify', methods=['POST']) @lastuser.notification_handler def lastusernotify(user): db.session.commit() @lastuser.auth_error_handler def lastuser_error(error, error_description=None, error_uri=None): if error == 'access_denied': flash("You denied the request to login", category='error') return redirect(get_next_url()) return Response(u"Error: %s\n" u"Description: %s\n" u"URI: %s" % (error, error_description, error_uri), mimetype="text/plain") @signal_user_session_refreshed.connect def track_user(user): db.session.add(UserActiveAt(user=user, board=g.board)) try: db.session.commit() except IntegrityError: # Small but not impossible chance we got two parallel signals db.session.rollback()
# Copyright (c) 2001-2004 Twisted Matrix Laboratories. # See LICENSE for details. """ Test cases for twisted.hook module. """ from twisted.python import hook from twisted.trial import unittest class BaseClass: """ dummy class to help in testing. """ def __init__(self): """ dummy initializer """ self.calledBasePre = 0 self.calledBasePost = 0 self.calledBase = 0 def func(self, a, b): """ dummy method """ assert a == 1 assert b == 2 self.calledBase = self.calledBase + 1 class SubClass(BaseClass): """ another dummy class """ def __init__(self): """ another dummy initializer """ BaseClass.__init__(self) self.calledSubPre = 0 self.calledSubPost = 0 self.calledSub = 0 def func(self, a, b): """ another dummy function """ assert a == 1 assert b == 2 BaseClass.func(self, a, b) self.calledSub = self.calledSub + 1 _clean_BaseClass = BaseClass.__dict__.copy() _clean_SubClass = SubClass.__dict__.copy() def basePre(base, a, b): """ a pre-hook for the base class """ base.calledBasePre = base.calledBasePre + 1 def basePost(base, a, b): """ a post-hook for the base class """ base.calledBasePost = base.calledBasePost + 1 def subPre(sub, a, b): """ a pre-hook for the subclass """ sub.calledSubPre = sub.calledSubPre + 1 def subPost(sub, a, b): """ a post-hook for the subclass """ sub.calledSubPost = sub.calledSubPost + 1 class HookTestCase(unittest.TestCase): """ test case to make sure hooks are called """ def setUp(self): """Make sure we have clean versions of our classes.""" BaseClass.__dict__.clear() BaseClass.__dict__.update(_clean_BaseClass) SubClass.__dict__.clear() SubClass.__dict__.update(_clean_SubClass) def testBaseHook(self): """make sure that the base class's hook is called reliably """ base = BaseClass() self.assertEquals(base.calledBase, 0) self.assertEquals(base.calledBasePre, 0) base.func(1,2) self.assertEquals(base.calledBase, 1) self.assertEquals(base.calledBasePre, 0) hook.addPre(BaseClass, "func", basePre) base.func(1, b=2) self.assertEquals(base.calledBase, 2) self.assertEquals(base.calledBasePre, 1) hook.addPost(BaseClass, "func", basePost) base.func(1, b=2) self.assertEquals(base.calledBasePost, 1) self.assertEquals(base.calledBase, 3) self.assertEquals(base.calledBasePre, 2) hook.removePre(BaseClass, "func", basePre) hook.removePost(BaseClass, "func", basePost) base.func(1, b=2) self.assertEquals(base.calledBasePost, 1) self.assertEquals(base.calledBase, 4) self.assertEquals(base.calledBasePre, 2) def testSubHook(self): """test interactions between base-class hooks and subclass hooks """ sub = SubClass() self.assertEquals(sub.calledSub, 0) self.assertEquals(sub.calledBase, 0) sub.func(1, b=2) self.assertEquals(sub.calledSub, 1) self.assertEquals(sub.calledBase, 1) hook.addPre(SubClass, 'func', subPre) self.assertEquals(sub.calledSub, 1) self.assertEquals(sub.calledBase, 1) self.assertEquals(sub.calledSubPre, 0) self.assertEquals(sub.calledBasePre, 0) sub.func(1, b=2) self.assertEquals(sub.calledSub, 2) self.assertEquals(sub.calledBase, 2) self.assertEquals(sub.calledSubPre, 1) self.assertEquals(sub.calledBasePre, 0) # let the pain begin hook.addPre(BaseClass, 'func', basePre) BaseClass.func(sub, 1, b=2) # sub.func(1, b=2) self.assertEquals(sub.calledBase, 3) self.assertEquals(sub.calledBasePre, 1, str(sub.calledBasePre)) sub.func(1, b=2) self.assertEquals(sub.calledBasePre, 2) self.assertEquals(sub.calledBase, 4) self.assertEquals(sub.calledSubPre, 2) self.assertEquals(sub.calledSub, 3) testCases = [HookTestCase]
# ---------------------------------------------------------------------- # Numenta Platform for Intelligent Computing (NuPIC) # Copyright (C) 2015, Numenta, Inc. Unless you have an agreement # with Numenta, Inc., for a separate license for this software code, the # following terms and conditions apply: # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero Public License version 3 as # published by the Free Software Foundation. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. # See the GNU Affero Public License for more details. # # You should have received a copy of the GNU Affero Public License # along with this program. If not, see http://www.gnu.org/licenses. # # http://numenta.org/licenses/ # ---------------------------------------------------------------------- MODEL_PARAMS = { # Type of model that the rest of these parameters apply to. 'model': "HTMPrediction", # Version that specifies the format of the config. 'version': 1, # Intermediate variables used to compute fields in modelParams and also # referenced from the control section. 'aggregationInfo': {'days': 0, 'fields': [('consumption', 'sum')], 'hours': 1, 'microseconds': 0, 'milliseconds': 0, 'minutes': 0, 'months': 0, 'seconds': 0, 'weeks': 0, 'years': 0}, 'predictAheadTime': None, # Model parameter dictionary. 'modelParams': { # The type of inference that this model will perform 'inferenceType': 'TemporalAnomaly', 'sensorParams': { # Sensor diagnostic output verbosity control; # if > 0: sensor region will print out on screen what it's sensing # at each step 0: silent; >=1: some info; >=2: more info; # >=3: even more info (see compute() in py/regions/RecordSensor.py) 'verbosity' : 0, # Include the encoders we use 'encoders': { u'consumption': { 'fieldname': u'consumption', 'resolution': 0.88, 'seed': 1, 'name': u'consumption', 'type': 'RandomDistributedScalarEncoder', }, 'timestamp_timeOfDay': { 'fieldname': u'timestamp', 'name': u'timestamp_timeOfDay', 'timeOfDay': (21, 1), 'type': 'DateEncoder'}, 'timestamp_weekend': { 'fieldname': u'timestamp', 'name': u'timestamp_weekend', 'type': 'DateEncoder', 'weekend': 21} }, # A dictionary specifying the period for automatically-generated # resets from a RecordSensor; # # None = disable automatically-generated resets (also disabled if # all of the specified values evaluate to 0). # Valid keys is the desired combination of the following: # days, hours, minutes, seconds, milliseconds, microseconds, weeks # # Example for 1.5 days: sensorAutoReset = dict(days=1,hours=12), # # (value generated from SENSOR_AUTO_RESET) 'sensorAutoReset' : None, }, 'spEnable': True, 'spParams': { # SP diagnostic output verbosity control; # 0: silent; >=1: some info; >=2: more info; 'spVerbosity' : 0, # Spatial Pooler implementation selector. # Options: 'py', 'cpp' (speed optimized, new) 'spatialImp' : 'cpp', 'globalInhibition': 1, # Number of cell columns in the cortical region (same number for # SP and TM) # (see also tpNCellsPerCol) 'columnCount': 2048, 'inputWidth': 0, # SP inhibition control (absolute value); # Maximum number of active columns in the SP region's output (when # there are more, the weaker ones are suppressed) 'numActiveColumnsPerInhArea': 40, 'seed': 1956, # potentialPct # What percent of the columns's receptive field is available # for potential synapses. 'potentialPct': 0.85, # The default connected threshold. Any synapse whose # permanence value is above the connected threshold is # a "connected synapse", meaning it can contribute to the # cell's firing. Typical value is 0.10. 'synPermConnected': 0.1, 'synPermActiveInc': 0.04, 'synPermInactiveDec': 0.005, }, # Controls whether TM is enabled or disabled; # TM is necessary for making temporal predictions, such as predicting # the next inputs. Without TM, the model is only capable of # reconstructing missing sensor inputs (via SP). 'tmEnable' : True, 'tmParams': { # TM diagnostic output verbosity control; # 0: silent; [1..6]: increasing levels of verbosity # (see verbosity in nupic/trunk/py/nupic/research/backtracking_tm.py and backtracking_tm_cpp.py) 'verbosity': 0, # Number of cell columns in the cortical region (same number for # SP and TM) # (see also tpNCellsPerCol) 'columnCount': 2048, # The number of cells (i.e., states), allocated per column. 'cellsPerColumn': 32, 'inputWidth': 2048, 'seed': 1960, # Temporal Pooler implementation selector (see _getTPClass in # CLARegion.py). 'temporalImp': 'cpp', # New Synapse formation count # NOTE: If None, use spNumActivePerInhArea # # TODO: need better explanation 'newSynapseCount': 20, # Maximum number of synapses per segment # > 0 for fixed-size CLA # -1 for non-fixed-size CLA # # TODO: for Ron: once the appropriate value is placed in TM # constructor, see if we should eliminate this parameter from # description.py. 'maxSynapsesPerSegment': 32, # Maximum number of segments per cell # > 0 for fixed-size CLA # -1 for non-fixed-size CLA # # TODO: for Ron: once the appropriate value is placed in TM # constructor, see if we should eliminate this parameter from # description.py. 'maxSegmentsPerCell': 128, # Initial Permanence # TODO: need better explanation 'initialPerm': 0.21, # Permanence Increment 'permanenceInc': 0.1, # Permanence Decrement # If set to None, will automatically default to tpPermanenceInc # value. 'permanenceDec' : 0.1, 'globalDecay': 0.0, 'maxAge': 0, # Minimum number of active synapses for a segment to be considered # during search for the best-matching segments. # None=use default # Replaces: tpMinThreshold 'minThreshold': 12, # Segment activation threshold. # A segment is active if it has >= tpSegmentActivationThreshold # connected synapses that are active due to infActiveState # None=use default # Replaces: tpActivationThreshold 'activationThreshold': 16, 'outputType': 'normal', # "Pay Attention Mode" length. This tells the TM how many new # elements to append to the end of a learned sequence at a time. # Smaller values are better for datasets with short sequences, # higher values are better for datasets with long sequences. 'pamLength': 1, }, 'clParams': { 'regionName' : 'SDRClassifierRegion', # Classifier diagnostic output verbosity control; # 0: silent; [1..6]: increasing levels of verbosity 'verbosity' : 0, # This controls how fast the classifier learns/forgets. Higher values # make it adapt faster and forget older patterns faster. 'alpha': 0.0001, # This is set after the call to updateConfigFromSubConfig and is # computed from the aggregationInfo and predictAheadTime. 'steps': '1,5', 'implementation': 'py', }, 'anomalyParams': { u'anomalyCacheRecords': None, u'autoDetectThreshold': None, u'autoDetectWaitRecords': 2184}, 'trainSPNetOnlyIfRequested': False, }, }
# Copyright (C) 2011-2014 Free Software Foundation, Inc. # This program is free software; you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation; either version 3 of the License, or # (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. """Configure GDB using the ELinOS environment.""" import os import glob import gdb def warn(msg): print "warning: %s" % msg def get_elinos_environment(): """Return the ELinOS environment. If the ELinOS environment is properly set up, return a dictionary which contains: * The path to the ELinOS project at key 'project'; * The path to the ELinOS CDK at key 'cdk'; * The ELinOS target name at key 'target' (Eg. 'i486-linux'); * A list of Xenomai install prefixes (which could be empty, if the ELinOS project does not include Xenomai) at key 'xenomai'. If one of these cannot be found, print a warning; the corresponding value in the returned dictionary will be None. """ result = {} for key in ("project", "cdk", "target"): var = "ELINOS_" + key.upper() if var in os.environ: result[key] = os.environ[var] else: warn("%s not set" % var) result[key] = None if result["project"] is not None: result["xenomai"] = glob.glob(result["project"] + "/xenomai-[0-9.]*") else: result["xenomai"] = [] return result def elinos_init(): """Initialize debugger environment for ELinOS. Let the debugger know where to find the ELinOS libraries on host. This assumes that an ELinOS environment is properly set up. If some environment variables are missing, warn about which library may be missing. """ elinos_env = get_elinos_environment() solib_dirs = [] # System libraries if None in (elinos_env[key] for key in ("cdk", "target")): warn("ELinOS system libraries will not be loaded") else: solib_prefix = "%s/%s" % (elinos_env["cdk"], elinos_env["target"]) solib_dirs += ["%s/%s" % (solib_prefix, "lib")] gdb.execute("set solib-absolute-prefix %s" % solib_prefix) # Xenomai libraries. Those are optional, so have a lighter warning # if they cannot be located. if elinos_env["project"] is None: warn("Xenomai libraries may not be loaded") else: for dir in elinos_env['xenomai']: solib_dirs += ["%s/%s" % (dir, "xenomai-build/usr/realtime/lib")] if len(solib_dirs) != 0: gdb.execute("set solib-search-path %s" % ":".join(solib_dirs)) if __name__ == "__main__": elinos_init()
# Copyright (c) 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved # # Permission is hereby granted, free of charge, to any person obtaining a # copy of this software and associated documentation files (the # "Software"), to deal in the Software without restriction, including # without limitation the rights to use, copy, modify, merge, publish, dis- # tribute, sublicense, and/or sell copies of the Software, and to permit # persons to whom the Software is furnished to do so, subject to the fol- # lowing conditions: # # The above copyright notice and this permission notice shall be included # in all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS # OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL- # ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT # SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, # WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS # IN THE SOFTWARE. # class DirectConnectClientException(Exception): pass class DirectConnectServerException(Exception): pass
# -*- coding: utf-8 -*- # Copyright (c) 2018 Marcus Watkins <marwatk@marcuswatkins.net> # GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt) from ansible.compat.tests.mock import patch from ansible.modules.source_control import gitlab_deploy_key from ansible.module_utils._text import to_bytes from ansible.module_utils import basic import pytest import json from units.modules.utils import set_module_args fake_server_state = [ { "id": 1, "title": "Public key", "key": 'ssh-rsa long/+base64//+string==', "created_at": "2013-10-02T10:12:29Z", "can_push": False }, ] class FakeReader: def __init__(self, object): self.content = json.dumps(object, sort_keys=True) def read(self): return self.content class AnsibleExitJson(Exception): """Exception class to be raised by module.exit_json and caught by the test case""" pass class AnsibleFailJson(Exception): """Exception class to be raised by module.fail_json and caught by the test case""" pass def exit_json(*args, **kwargs): """function to patch over exit_json; package return data into an exception""" if 'changed' not in kwargs: kwargs['changed'] = False raise AnsibleExitJson(kwargs) def fail_json(*args, **kwargs): """function to patch over fail_json; package return data into an exception""" kwargs['failed'] = True raise AnsibleFailJson(kwargs) @pytest.fixture def fetch_url_mock(mocker): return mocker.patch('ansible.module_utils.gitlab.fetch_url') @pytest.fixture def module_mock(mocker): return mocker.patch.multiple(basic.AnsibleModule, exit_json=exit_json, fail_json=fail_json) def test_access_token_output(capfd, fetch_url_mock, module_mock): fetch_url_mock.return_value = [FakeReader(fake_server_state), {'status': 200}] set_module_args({ 'api_url': 'https://gitlab.example.com/api', 'access_token': 'test-access-token', 'project': '10', 'key': 'ssh-key foobar', 'title': 'a title', 'state': 'absent' }) with pytest.raises(AnsibleExitJson) as result: gitlab_deploy_key.main() first_call = fetch_url_mock.call_args_list[0][1] assert first_call['url'] == 'https://gitlab.example.com/api/v4/projects/10/deploy_keys' assert first_call['headers']['Authorization'] == 'Bearer test-access-token' assert 'Private-Token' not in first_call['headers'] assert first_call['method'] == 'GET' def test_private_token_output(capfd, fetch_url_mock, module_mock): fetch_url_mock.return_value = [FakeReader(fake_server_state), {'status': 200}] set_module_args({ 'api_url': 'https://gitlab.example.com/api', 'private_token': 'test-private-token', 'project': 'foo/bar', 'key': 'ssh-key foobar', 'title': 'a title', 'state': 'absent' }) with pytest.raises(AnsibleExitJson) as result: gitlab_deploy_key.main() first_call = fetch_url_mock.call_args_list[0][1] assert first_call['url'] == 'https://gitlab.example.com/api/v4/projects/foo%2Fbar/deploy_keys' assert first_call['headers']['Private-Token'] == 'test-private-token' assert 'Authorization' not in first_call['headers'] assert first_call['method'] == 'GET' def test_bad_http_first_response(capfd, fetch_url_mock, module_mock): fetch_url_mock.side_effect = [[FakeReader("Permission denied"), {'status': 403}], [FakeReader("Permission denied"), {'status': 403}]] set_module_args({ 'api_url': 'https://gitlab.example.com/api', 'access_token': 'test-access-token', 'project': '10', 'key': 'ssh-key foobar', 'title': 'a title', 'state': 'absent' }) with pytest.raises(AnsibleFailJson): gitlab_deploy_key.main() def test_bad_http_second_response(capfd, fetch_url_mock, module_mock): fetch_url_mock.side_effect = [[FakeReader(fake_server_state), {'status': 200}], [FakeReader("Permission denied"), {'status': 403}]] set_module_args({ 'api_url': 'https://gitlab.example.com/api', 'access_token': 'test-access-token', 'project': '10', 'key': 'ssh-key foobar', 'title': 'a title', 'state': 'present' }) with pytest.raises(AnsibleFailJson): gitlab_deploy_key.main() def test_delete_non_existing(capfd, fetch_url_mock, module_mock): fetch_url_mock.return_value = [FakeReader(fake_server_state), {'status': 200}] set_module_args({ 'api_url': 'https://gitlab.example.com/api', 'access_token': 'test-access-token', 'project': '10', 'key': 'ssh-key foobar', 'title': 'a title', 'state': 'absent' }) with pytest.raises(AnsibleExitJson) as result: gitlab_deploy_key.main() assert result.value.args[0]['changed'] is False def test_delete_existing(capfd, fetch_url_mock, module_mock): fetch_url_mock.return_value = [FakeReader(fake_server_state), {'status': 200}] set_module_args({ 'api_url': 'https://gitlab.example.com/api', 'access_token': 'test-access-token', 'project': '10', 'key': 'ssh-rsa long/+base64//+string==', 'title': 'a title', 'state': 'absent' }) with pytest.raises(AnsibleExitJson) as result: gitlab_deploy_key.main() second_call = fetch_url_mock.call_args_list[1][1] assert second_call['url'] == 'https://gitlab.example.com/api/v4/projects/10/deploy_keys/1' assert second_call['method'] == 'DELETE' assert result.value.args[0]['changed'] is True def test_add_new(capfd, fetch_url_mock, module_mock): fetch_url_mock.return_value = [FakeReader(fake_server_state), {'status': 200}] set_module_args({ 'api_url': 'https://gitlab.example.com/api', 'access_token': 'test-access-token', 'project': '10', 'key': 'ssh-key foobar', 'title': 'a title', 'state': 'present' }) with pytest.raises(AnsibleExitJson) as result: gitlab_deploy_key.main() second_call = fetch_url_mock.call_args_list[1][1] assert second_call['url'] == 'https://gitlab.example.com/api/v4/projects/10/deploy_keys' assert second_call['method'] == 'POST' assert second_call['data'] == '{"can_push": false, "key": "ssh-key foobar", "title": "a title"}' assert result.value.args[0]['changed'] is True def test_update_existing(capfd, fetch_url_mock, module_mock): fetch_url_mock.return_value = [FakeReader(fake_server_state), {'status': 200}] set_module_args({ 'api_url': 'https://gitlab.example.com/api', 'access_token': 'test-access-token', 'project': '10', 'title': 'Public key', 'key': 'ssh-rsa long/+base64//+string==', 'can_push': 'yes', 'state': 'present' }) with pytest.raises(AnsibleExitJson) as result: gitlab_deploy_key.main() second_call = fetch_url_mock.call_args_list[1][1] assert second_call['url'] == 'https://gitlab.example.com/api/v4/projects/10/deploy_keys/1' assert second_call['method'] == 'PUT' assert second_call['data'] == ('{"can_push": true, "key": "ssh-rsa long/+base64//+string==", "title": "Public key"}') assert result.value.args[0]['changed'] is True def test_unchanged_existing(capfd, fetch_url_mock, module_mock): fetch_url_mock.return_value = [FakeReader(fake_server_state), {'status': 200}] set_module_args({ 'api_url': 'https://gitlab.example.com/api', 'access_token': 'test-access-token', 'project': '10', 'title': 'Public key', 'key': 'ssh-rsa long/+base64//+string==', 'can_push': 'no', 'state': 'present' }) with pytest.raises(AnsibleExitJson) as result: gitlab_deploy_key.main() assert result.value.args[0]['changed'] is False assert fetch_url_mock.call_count == 1
import contextlib import sys import unittest import warnings import pkg_resources try: import mock _mock_error = None except ImportError as e: _mock_error = e def _check_mock_available(): if _mock_error is not None: raise RuntimeError( 'mock is not available: Reason: {}'.format(_mock_error)) def with_requires(*requirements): """Run a test case only when given requirements are satisfied. .. admonition:: Example This test case runs only when `numpy>=1.10` is installed. >>> import unittest >>> from chainer import testing >>> class Test(unittest.TestCase): ... @testing.with_requires('numpy>=1.10') ... def test_for_numpy_1_10(self): ... pass Args: requirements: A list of string representing requirement condition to run a given test case. """ ws = pkg_resources.WorkingSet() try: ws.require(*requirements) skip = False except pkg_resources.ResolutionError: skip = True msg = 'requires: {}'.format(','.join(requirements)) return unittest.skipIf(skip, msg) def without_requires(*requirements): """Run a test case only when given requirements are not satisfied. .. admonition:: Example This test case runs only when `numpy>=1.10` is not installed. >>> from chainer import testing ... class Test(unittest.TestCase): ... @testing.without_requires('numpy>=1.10') ... def test_without_numpy_1_10(self): ... pass Args: requirements: A list of string representing requirement condition to run a given test case. """ ws = pkg_resources.WorkingSet() try: ws.require(*requirements) skip = True except pkg_resources.ResolutionError: skip = False msg = 'requires: {}'.format(','.join(requirements)) return unittest.skipIf(skip, msg) @contextlib.contextmanager def assert_warns(expected): with warnings.catch_warnings(record=True) as w: warnings.simplefilter('always') yield # Python 2 does not raise warnings multiple times from the same stack # frame. if sys.version_info >= (3, 0): if not any(isinstance(m.message, expected) for m in w): try: exc_name = expected.__name__ except AttributeError: exc_name = str(expected) raise AssertionError('%s not triggerred' % exc_name) def _import_object_from_name(fullname): comps = fullname.split('.') obj = sys.modules.get(comps[0]) if obj is None: raise RuntimeError('Can\'t import {}'.format(comps[0])) for i, comp in enumerate(comps[1:]): obj = getattr(obj, comp) if obj is None: raise RuntimeError( 'Can\'t find object {}'.format('.'.join(comps[:i + 1]))) return obj def patch(target, *args, **kwargs): """A wrapper of mock.patch which appends wraps argument. .. note:: Unbound methods are not supported as ``wraps`` argument. Args: target(str): Full name of target object. wraps: Wrapping object which will be passed to ``mock.patch`` as ``wraps`` argument. If omitted, the object specified by ``target`` is used. *args: Passed to ``mock.patch``. **kwargs: Passed to ``mock.patch``. """ _check_mock_available() try: wraps = kwargs.pop('wraps') except KeyError: wraps = _import_object_from_name(target) return mock.patch(target, *args, wraps=wraps, **kwargs)
# -*- coding: utf-8 -*- from __future__ import unicode_literals import os import unittest from unittest import skipUnless from django.conf import settings from django.contrib.gis.geos import HAS_GEOS from django.contrib.gis.geoip import HAS_GEOIP from django.utils import six if HAS_GEOIP: from . import GeoIP, GeoIPException if HAS_GEOS: from ..geos import GEOSGeometry # Note: Requires use of both the GeoIP country and city datasets. # The GEOIP_DATA path should be the only setting set (the directory # should contain links or the actual database files 'GeoIP.dat' and # 'GeoLiteCity.dat'. @skipUnless(HAS_GEOIP and getattr(settings, "GEOIP_PATH", None), "GeoIP is required along with the GEOIP_PATH setting.") class GeoIPTest(unittest.TestCase): def test01_init(self): "Testing GeoIP initialization." g1 = GeoIP() # Everything inferred from GeoIP path path = settings.GEOIP_PATH g2 = GeoIP(path, 0) # Passing in data path explicitly. g3 = GeoIP.open(path, 0) # MaxMind Python API syntax. for g in (g1, g2, g3): self.assertEqual(True, bool(g._country)) self.assertEqual(True, bool(g._city)) # Only passing in the location of one database. city = os.path.join(path, 'GeoLiteCity.dat') cntry = os.path.join(path, 'GeoIP.dat') g4 = GeoIP(city, country='') self.assertEqual(None, g4._country) g5 = GeoIP(cntry, city='') self.assertEqual(None, g5._city) # Improper parameters. bad_params = (23, 'foo', 15.23) for bad in bad_params: self.assertRaises(GeoIPException, GeoIP, cache=bad) if isinstance(bad, six.string_types): e = GeoIPException else: e = TypeError self.assertRaises(e, GeoIP, bad, 0) def test02_bad_query(self): "Testing GeoIP query parameter checking." cntry_g = GeoIP(city='<foo>') # No city database available, these calls should fail. self.assertRaises(GeoIPException, cntry_g.city, 'google.com') self.assertRaises(GeoIPException, cntry_g.coords, 'yahoo.com') # Non-string query should raise TypeError self.assertRaises(TypeError, cntry_g.country_code, 17) self.assertRaises(TypeError, cntry_g.country_name, GeoIP) def test03_country(self): "Testing GeoIP country querying methods." g = GeoIP(city='<foo>') fqdn = 'www.google.com' addr = '12.215.42.19' for query in (fqdn, addr): for func in (g.country_code, g.country_code_by_addr, g.country_code_by_name): self.assertEqual('US', func(query)) for func in (g.country_name, g.country_name_by_addr, g.country_name_by_name): self.assertEqual('United States', func(query)) self.assertEqual({'country_code': 'US', 'country_name': 'United States'}, g.country(query)) @skipUnless(HAS_GEOS, "Geos is required") def test04_city(self): "Testing GeoIP city querying methods." g = GeoIP(country='<foo>') addr = '128.249.1.1' fqdn = 'tmc.edu' for query in (fqdn, addr): # Country queries should still work. for func in (g.country_code, g.country_code_by_addr, g.country_code_by_name): self.assertEqual('US', func(query)) for func in (g.country_name, g.country_name_by_addr, g.country_name_by_name): self.assertEqual('United States', func(query)) self.assertEqual({'country_code': 'US', 'country_name': 'United States'}, g.country(query)) # City information dictionary. d = g.city(query) self.assertEqual('USA', d['country_code3']) self.assertEqual('Houston', d['city']) self.assertEqual('TX', d['region']) self.assertEqual(713, d['area_code']) geom = g.geos(query) self.assertIsInstance(geom, GEOSGeometry) lon, lat = (-95.4010, 29.7079) lat_lon = g.lat_lon(query) lat_lon = (lat_lon[1], lat_lon[0]) for tup in (geom.tuple, g.coords(query), g.lon_lat(query), lat_lon): self.assertAlmostEqual(lon, tup[0], 4) self.assertAlmostEqual(lat, tup[1], 4) def test05_unicode_response(self): "Testing that GeoIP strings are properly encoded, see #16553." g = GeoIP() d = g.city("www.osnabrueck.de") self.assertEqual('Osnabrck', d['city']) d = g.country('200.7.49.81') self.assertEqual('Curaao', d['country_name'])
#!/usr/bin/env python """ test """ INTERP = 128 TXGAIN = 30 CONSTANT = 0.10 from gnuradio import gr, gr_unittest import usrp_options from optparse import OptionParser from gnuradio.eng_option import eng_option from pick_bitrate import pick_tx_bitrate def main(): gr.enable_realtime_scheduling() tb = gr.top_block () src = gr.file_source(gr.sizeof_gr_complex, "transmit-data.dat", True) parser = OptionParser(option_class=eng_option, conflict_handler="resolve") (options, args) = parser.parse_args () d = {'verbose': True, 'discontinuous': False, 'samples_per_symbol': 2, 'usrpx': None, 'interp': INTERP, 'fusb_block_size': 0, 'megabytes': 1.0, 'rx_freq': 2.475e9, 'size': 1500, 'show_tx_gain_range': False, 'log': False, 'tx_subdev_spec': None, 'fusb_nblocks': 0, 'lo_offset': None, 'tx_gain': TXGAIN, 'which': 0, 'modulation': 'gmsk', 'excess_bw': 0.34999999999999998, 'bt': 0.34999999999999998, 'interface': 'eth0', 'freq': None, 'bitrate': 100000.0, 'from_file': None, 'tx_freq': 2475000000.0, 'mac_addr': '', 'tx_amplitude': 0.1, 'gray_code': True} for i, j in d.items(): setattr(options, i, j) u = usrp_options.create_usrp_sink(options) dac_rate = u.dac_rate() if options.verbose: print 'USRP Sink:', u (_bitrate, _samples_per_symbol, _interp) = \ pick_tx_bitrate(options.bitrate, 2, \ options.samples_per_symbol, options.interp, dac_rate, \ u.get_interp_rates()) u.set_interp(_interp) u.set_auto_tr(True) if not u.set_center_freq(options.tx_freq): print "Failed to set Rx frequency to %s" % (eng_notation.num_to_str(options.tx_freq)) raise ValueError, eng_notation.num_to_str(options.tx_freq) m = gr.multiply_const_cc(CONSTANT) tb.connect(src, m, u) tb.run() if __name__ == '__main__': try: main() except KeyboardInterrupt: print "Bye"
from __future__ import unicode_literals import os import unittest import warnings from django.test import SimpleTestCase from django.test.utils import reset_warning_registry from django.utils import six from django.utils.deprecation import RenameMethodsBase from django.utils.encoding import force_text class RenameManagerMethods(RenameMethodsBase): renamed_methods = ( ('old', 'new', DeprecationWarning), ) class RenameMethodsTests(SimpleTestCase): """ Tests the `RenameMethodsBase` type introduced to rename `get_query_set` to `get_queryset` across the code base following #15363. """ def test_class_definition_warnings(self): """ Ensure a warning is raised upon class definition to suggest renaming the faulty method. """ reset_warning_registry() with warnings.catch_warnings(record=True) as recorded: warnings.simplefilter('always') class Manager(six.with_metaclass(RenameManagerMethods)): def old(self): pass self.assertEqual(len(recorded), 1) msg = str(recorded[0].message) self.assertEqual(msg, '`Manager.old` method should be renamed `new`.') def test_get_new_defined(self): """ Ensure `old` complains and not `new` when only `new` is defined. """ with warnings.catch_warnings(record=True) as recorded: warnings.simplefilter('ignore') class Manager(six.with_metaclass(RenameManagerMethods)): def new(self): pass warnings.simplefilter('always') manager = Manager() manager.new() self.assertEqual(len(recorded), 0) manager.old() self.assertEqual(len(recorded), 1) msg = str(recorded.pop().message) self.assertEqual(msg, '`Manager.old` is deprecated, use `new` instead.') def test_get_old_defined(self): """ Ensure `old` complains when only `old` is defined. """ with warnings.catch_warnings(record=True) as recorded: warnings.simplefilter('ignore') class Manager(six.with_metaclass(RenameManagerMethods)): def old(self): pass warnings.simplefilter('always') manager = Manager() manager.new() self.assertEqual(len(recorded), 0) manager.old() self.assertEqual(len(recorded), 1) msg = str(recorded.pop().message) self.assertEqual(msg, '`Manager.old` is deprecated, use `new` instead.') def test_deprecated_subclass_renamed(self): """ Ensure the correct warnings are raised when a class that didn't rename `old` subclass one that did. """ with warnings.catch_warnings(record=True) as recorded: warnings.simplefilter('ignore') class Renamed(six.with_metaclass(RenameManagerMethods)): def new(self): pass class Deprecated(Renamed): def old(self): super(Deprecated, self).old() warnings.simplefilter('always') deprecated = Deprecated() deprecated.new() self.assertEqual(len(recorded), 1) msg = str(recorded.pop().message) self.assertEqual(msg, '`Renamed.old` is deprecated, use `new` instead.') recorded[:] = [] deprecated.old() self.assertEqual(len(recorded), 2) msgs = [str(warning.message) for warning in recorded] self.assertEqual(msgs, [ '`Deprecated.old` is deprecated, use `new` instead.', '`Renamed.old` is deprecated, use `new` instead.', ]) def test_renamed_subclass_deprecated(self): """ Ensure the correct warnings are raised when a class that renamed `old` subclass one that didn't. """ with warnings.catch_warnings(record=True) as recorded: warnings.simplefilter('ignore') class Deprecated(six.with_metaclass(RenameManagerMethods)): def old(self): pass class Renamed(Deprecated): def new(self): super(Renamed, self).new() warnings.simplefilter('always') renamed = Renamed() renamed.new() self.assertEqual(len(recorded), 0) renamed.old() self.assertEqual(len(recorded), 1) msg = str(recorded.pop().message) self.assertEqual(msg, '`Renamed.old` is deprecated, use `new` instead.') def test_deprecated_subclass_renamed_and_mixins(self): """ Ensure the correct warnings are raised when a subclass inherit from a class that renamed `old` and mixins that may or may not have renamed `new`. """ with warnings.catch_warnings(record=True) as recorded: warnings.simplefilter('ignore') class Renamed(six.with_metaclass(RenameManagerMethods)): def new(self): pass class RenamedMixin(object): def new(self): super(RenamedMixin, self).new() class DeprecatedMixin(object): def old(self): super(DeprecatedMixin, self).old() class Deprecated(DeprecatedMixin, RenamedMixin, Renamed): pass warnings.simplefilter('always') deprecated = Deprecated() deprecated.new() self.assertEqual(len(recorded), 1) msg = str(recorded.pop().message) self.assertEqual(msg, '`RenamedMixin.old` is deprecated, use `new` instead.') deprecated.old() self.assertEqual(len(recorded), 2) msgs = [str(warning.message) for warning in recorded] self.assertEqual(msgs, [ '`DeprecatedMixin.old` is deprecated, use `new` instead.', '`RenamedMixin.old` is deprecated, use `new` instead.', ]) class DeprecatingSimpleTestCaseUrls(unittest.TestCase): def test_deprecation(self): """ Ensure the correct warning is raised when SimpleTestCase.urls is used. """ class TempTestCase(SimpleTestCase): urls = 'tests.urls' def test(self): pass with warnings.catch_warnings(record=True) as recorded: warnings.filterwarnings('always') suite = unittest.TestLoader().loadTestsFromTestCase(TempTestCase) with open(os.devnull, 'w') as devnull: unittest.TextTestRunner(stream=devnull, verbosity=2).run(suite) msg = force_text(recorded.pop().message) self.assertEqual(msg, "SimpleTestCase.urls is deprecated and will be removed in " "Django 1.10. Use @override_settings(ROOT_URLCONF=...) " "in TempTestCase instead.")
# -*- coding: utf-8 -*- # credentials.py # Copyright (C) 2013 LEAP # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. """ Credentials utilities """ from PySide import QtCore, QtGui WEAK_PASSWORDS = ("123456", "qweasd", "qwerty", "password") USERNAME_REGEX = r"^[a-z][a-z\d_\-\.]+[a-z\d]$" USERNAME_VALIDATOR = QtGui.QRegExpValidator(QtCore.QRegExp(USERNAME_REGEX)) def username_checks(username): # translation helper _tr = QtCore.QObject().tr message = None if message is None and len(username) < 2: message = _tr("Username must have at least 2 characters") valid = USERNAME_VALIDATOR.validate(username, 0) valid_username = valid[0] == QtGui.QValidator.State.Acceptable if message is None and not valid_username: message = _tr("That username is not allowed. Try another.") return message is None, message def password_checks(username, password, password2): """ Performs basic password checks to avoid really easy passwords. :param username: username provided at the registrarion form :type username: str :param password: password from the registration form :type password: str :param password2: second password from the registration form :type password: str :returns: (True, None, None) if all the checks pass, (False, message, field name) otherwise :rtype: tuple(bool, str, str) """ # translation helper _tr = QtCore.QObject().tr message = None field = None if message is None and password != password2: message = _tr("Passwords don't match") field = 'new_password_confirmation' if message is None and not password: message = _tr("Password is empty") field = 'new_password' if message is None and len(password) < 8: message = _tr("Password is too short") field = 'new_password' if message is None and password in WEAK_PASSWORDS: message = _tr("Password is too easy") field = 'new_password' if message is None and username == password: message = _tr("Password can't be the same as username") field = 'new_password' return message is None, message, field
''' Created on 28 okt 2011 @author: jev ''' from tradingWithPython import estimateBeta, Spread, returns, Portfolio, readBiggerScreener from tradingWithPython.lib import yahooFinance from pandas import DataFrame, Series import numpy as np import matplotlib.pyplot as plt import os symbols = ['SPY','IWM'] y = yahooFinance.HistData('temp.csv') y.startDate = (2007,1,1) df = y.loadSymbols(symbols,forceDownload=False) #df = y.downloadData(symbols) res = readBiggerScreener('CointPairs.csv') #---check with spread scanner #sp = DataFrame(index=symbols) # #sp['last'] = df.ix[-1,:] #sp['targetCapital'] = Series({'SPY':100,'IWM':-100}) #sp['targetShares'] = sp['targetCapital']/sp['last'] #print sp #The dollar-neutral ratio is about 1 * IWM - 1.7 * IWM. You will get the spread = zero (or probably very near zero) #s = Spread(symbols, histClose = df) #print s #s.value.plot() #print 'beta (returns)', estimateBeta(df[symbols[0]],df[symbols[1]],algo='returns') #print 'beta (log)', estimateBeta(df[symbols[0]],df[symbols[1]],algo='log') #print 'beta (standard)', estimateBeta(df[symbols[0]],df[symbols[1]],algo='standard') #p = Portfolio(df) #p.setShares([1, -1.7]) #p.value.plot() quote = yahooFinance.getQuote(symbols) print quote s = Spread(symbols,histClose=df, estimateBeta = False) s.setLast(quote['last']) s.setShares(Series({'SPY':1,'IWM':-1.7})) print s #s.value.plot() #s.plot() fig = figure(2) s.plot()
# pylint: disable=g-bad-file-header # Copyright 2015 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== """Utilities to remove unneeded nodes from a GraphDefs.""" from __future__ import absolute_import from __future__ import division from __future__ import print_function import copy from google.protobuf import text_format from tensorflow.core.framework import attr_value_pb2 from tensorflow.core.framework import graph_pb2 from tensorflow.core.framework import node_def_pb2 from tensorflow.python.framework import graph_util from tensorflow.python.platform import gfile def strip_unused(input_graph_def, input_node_names, output_node_names, placeholder_type_enum): """Removes unused nodes from a GraphDef. Args: input_graph_def: A graph with nodes we want to prune. input_node_names: A list of the nodes we use as inputs. output_node_names: A list of the output nodes. placeholder_type_enum: The AttrValue enum for the placeholder data type, or a list that specifies one value per input node name. Returns: A GraphDef with all unnecessary ops removed. """ # Here we replace the nodes we're going to override as inputs with # placeholders so that any unused nodes that are inputs to them are # automatically stripped out by extract_sub_graph(). inputs_replaced_graph_def = graph_pb2.GraphDef() for node in input_graph_def.node: if node.name in input_node_names: placeholder_node = node_def_pb2.NodeDef() placeholder_node.op = "Placeholder" placeholder_node.name = node.name if isinstance(placeholder_type_enum, list): input_node_index = input_node_names.index(node.name) placeholder_node.attr["dtype"].CopyFrom( attr_value_pb2.AttrValue(type=placeholder_type_enum[ input_node_index])) else: placeholder_node.attr["dtype"].CopyFrom( attr_value_pb2.AttrValue(type=placeholder_type_enum)) if "_output_shapes" in node.attr: placeholder_node.attr["_output_shapes"].CopyFrom(node.attr[ "_output_shapes"]) inputs_replaced_graph_def.node.extend([placeholder_node]) else: inputs_replaced_graph_def.node.extend([copy.deepcopy(node)]) output_graph_def = graph_util.extract_sub_graph(inputs_replaced_graph_def, output_node_names) return output_graph_def def strip_unused_from_files(input_graph, input_binary, output_graph, output_binary, input_node_names, output_node_names, placeholder_type_enum): """Removes unused nodes from a graph file.""" if not gfile.Exists(input_graph): print("Input graph file '" + input_graph + "' does not exist!") return -1 if not output_node_names: print("You need to supply the name of a node to --output_node_names.") return -1 input_graph_def = graph_pb2.GraphDef() mode = "rb" if input_binary else "r" with gfile.FastGFile(input_graph, mode) as f: if input_binary: input_graph_def.ParseFromString(f.read()) else: text_format.Merge(f.read(), input_graph_def) output_graph_def = strip_unused(input_graph_def, input_node_names.split(","), output_node_names.split(","), placeholder_type_enum) if output_binary: with gfile.GFile(output_graph, "wb") as f: f.write(output_graph_def.SerializeToString()) else: with gfile.GFile(output_graph, "w") as f: f.write(text_format.MessageToString(output_graph_def)) print("%d ops in the final graph." % len(output_graph_def.node))
from django.contrib import messages from django import http from django.core.urlresolvers import reverse from django.utils.translation import ugettext_lazy as _ from datacash.facade import Facade from oscar.apps.checkout import views, exceptions from oscar.apps.payment.forms import BankcardForm from oscar.apps.payment.models import SourceType from oscar.apps.order.models import BillingAddress from .forms import BillingAddressForm # Customise the core PaymentDetailsView to integrate Datacash class PaymentDetailsView(views.PaymentDetailsView): def check_payment_data_is_captured(self, request): if request.method != "POST": raise exceptions.FailedPreCondition( url=reverse('checkout:payment-details'), message=_("Please enter your payment details")) def get_context_data(self, **kwargs): ctx = super(PaymentDetailsView, self).get_context_data(**kwargs) # Ensure newly instantiated instances of the bankcard and billing # address forms are passed to the template context (when they aren't # already specified). if 'bankcard_form' not in kwargs: ctx['bankcard_form'] = BankcardForm() if 'billing_address_form' not in kwargs: ctx['billing_address_form'] = self.get_billing_address_form( ctx['shipping_address'] ) elif kwargs['billing_address_form'].is_valid(): # On the preview view, we extract the billing address into the # template context so we can show it to the customer. ctx['billing_address'] = kwargs[ 'billing_address_form'].save(commit=False) return ctx def get_billing_address_form(self, shipping_address): """ Return an instantiated billing address form """ addr = self.get_default_billing_address() if not addr: return BillingAddressForm(shipping_address=shipping_address) billing_addr = BillingAddress() addr.populate_alternative_model(billing_addr) return BillingAddressForm(shipping_address=shipping_address, instance=billing_addr) def handle_payment_details_submission(self, request): # Validate the submitted forms bankcard_form = BankcardForm(request.POST) shipping_address = self.get_shipping_address( self.request.basket) address_form = BillingAddressForm(shipping_address, request.POST) if address_form.is_valid() and bankcard_form.is_valid(): # If both forms are valid, we render the preview view with the # forms hidden within the page. This seems odd but means we don't # have to store sensitive details on the server. return self.render_preview( request, bankcard_form=bankcard_form, billing_address_form=address_form) # Forms are invalid - show them to the customer along with the # validation errors. return self.render_payment_details( request, bankcard_form=bankcard_form, billing_address_form=address_form) def handle_place_order_submission(self, request): bankcard_form = BankcardForm(request.POST) shipping_address = self.get_shipping_address( self.request.basket) address_form = BillingAddressForm(shipping_address, request.POST) if address_form.is_valid() and bankcard_form.is_valid(): # Forms still valid, let's submit an order submission = self.build_submission( order_kwargs={ 'billing_address': address_form.save(commit=False), }, payment_kwargs={ 'bankcard_form': bankcard_form, 'billing_address_form': address_form } ) return self.submit(**submission) # Must be DOM tampering as these forms were valid and were rendered in # a hidden element. Hence, we don't need to be that friendly with our # error message. messages.error(request, _("Invalid submission")) return http.HttpResponseRedirect( reverse('checkout:payment-details')) def handle_payment(self, order_number, total, **kwargs): # Make request to DataCash - if there any problems (eg bankcard # not valid / request refused by bank) then an exception would be # raised and handled by the parent PaymentDetail view) facade = Facade() bankcard = kwargs['bankcard_form'].bankcard datacash_ref = facade.pre_authorise( order_number, total.incl_tax, bankcard) # Request was successful - record the "payment source". As this # request was a 'pre-auth', we set the 'amount_allocated' - if we had # performed an 'auth' request, then we would set 'amount_debited'. source_type, _ = SourceType.objects.get_or_create(name='Datacash') source = source_type.sources.model( source_type=source_type, currency=total.currency, amount_allocated=total.incl_tax, reference=datacash_ref) self.add_payment_source(source) # Also record payment event self.add_payment_event( 'pre-auth', total.incl_tax, reference=datacash_ref)
# Copyright: (c) 2018 Ansible Project # GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt) # FUTURE: this could be swapped out for our bundled version of distro to move more complete platform # logic to the targets, so long as we maintain Py2.6 compat and don't need to do any kind of script assembly from __future__ import (absolute_import, division, print_function) __metaclass__ = type import json import platform import io import os def read_utf8_file(path, encoding='utf-8'): if not os.access(path, os.R_OK): return None with io.open(path, 'r', encoding=encoding) as fd: content = fd.read() return content def get_platform_info(): result = dict(platform_dist_result=[]) if hasattr(platform, 'dist'): result['platform_dist_result'] = platform.dist() osrelease_content = read_utf8_file('/etc/os-release') # try to fall back to /usr/lib/os-release if not osrelease_content: osrelease_content = read_utf8_file('/usr/lib/os-release') result['osrelease_content'] = osrelease_content return result def main(): info = get_platform_info() print(json.dumps(info)) if __name__ == '__main__': main()
""" Base class for Scrapy spiders See documentation in docs/topics/spiders.rst """ from scrapy import log from scrapy.http import Request from scrapy.utils.trackref import object_ref from scrapy.utils.url import url_is_from_spider from scrapy.utils.deprecate import create_deprecated_class class Spider(object_ref): """Base class for scrapy spiders. All spiders must inherit from this class. """ name = None def __init__(self, name=None, **kwargs): if name is not None: self.name = name elif not getattr(self, 'name', None): raise ValueError("%s must have a name" % type(self).__name__) self.__dict__.update(kwargs) if not hasattr(self, 'start_urls'): self.start_urls = [] def log(self, message, level=log.DEBUG, **kw): """Log the given messages at the given log level. Always use this method to send log messages from your spider """ log.msg(message, spider=self, level=level, **kw) def set_crawler(self, crawler): assert not hasattr(self, '_crawler'), "Spider already bounded to %s" % crawler self._crawler = crawler @property def crawler(self): assert hasattr(self, '_crawler'), "Spider not bounded to any crawler" return self._crawler @property def settings(self): return self.crawler.settings def start_requests(self): for url in self.start_urls: yield self.make_requests_from_url(url) def make_requests_from_url(self, url): return Request(url, dont_filter=True) def parse(self, response): raise NotImplementedError @classmethod def handles_request(cls, request): return url_is_from_spider(request.url, cls) def __str__(self): return "<%s %r at 0x%0x>" % (type(self).__name__, self.name, id(self)) __repr__ = __str__ BaseSpider = create_deprecated_class('BaseSpider', Spider) class ObsoleteClass(object): def __init__(self, message): self.message = message def __getattr__(self, name): raise AttributeError(self.message) spiders = ObsoleteClass(""" "from scrapy.spider import spiders" no longer works - use "from scrapy.project import crawler" and then access crawler.spiders attribute" """)
#!/usr/bin/env python ''' these tables are generated from the STM32 datasheets for the STM32F103x8 ''' # additional build information for ChibiOS build = { "CHIBIOS_STARTUP_MK" : "os/common/startup/ARMCMx/compilers/GCC/mk/startup_stm32f1xx.mk", "CHIBIOS_PLATFORM_MK" : "os/hal/ports/STM32/STM32F1xx/platform.mk", "CHPRINTF_USE_FLOAT" : 'no', "USE_FPU" : 'no' } pincount = { 'A': 16, 'B': 16, 'C': 16, 'D': 16, 'E': 16 } # MCU parameters mcu = { # location of MCU serial number 'UDID_START' : 0x1FFFF7E8, 'RAM_MAP' : [ (0x20000000, 20, 1), # main memory, DMA safe ], 'EXPECTED_CLOCK' : 72000000 } ADC1_map = { # format is PIN : ADC1_CHAN "PA0" : 0, "PA1" : 1, "PA2" : 2, "PA3" : 3, "PA4" : 4, "PA5" : 5, "PA6" : 6, "PA7" : 7, "PB0" : 8, "PB1" : 9, "PC0" : 10, "PC1" : 11, "PC2" : 12, "PC3" : 13, "PC4" : 14, "PC5" : 15, } DMA_Map = { # format is (DMA_TABLE, StreamNum, Channel) "ADC1" : [(1,1,0)], "TIM1_CH1" : [(1,2,0)], "TIM1_CH3" : [(1,6,0)], "TIM1_CH4" : [(1,4,0)], "TIM1_UP" : [(1,5,0)], "TIM2_CH1" : [(1,5,0)], "TIM2_CH2" : [(1,7,0)], "TIM2_CH3" : [(1,1,0)], "TIM2_CH4" : [(1,7,0)], "TIM2_UP" : [(1,2,0)], "TIM3_CH1" : [(1,6,0)], "TIM3_CH3" : [(1,2,0)], "TIM3_CH4" : [(1,3,0)], "TIM3_UP" : [(1,3,0)], "TIM4_CH1" : [(1,1,0)], "TIM4_CH2" : [(1,4,0)], "TIM4_CH3" : [(1,5,0)], "TIM4_UP" : [(1,7,0)], "TIM5_CH1" : [(2,5,0)], "TIM5_CH2" : [(2,4,0)], "TIM5_CH3" : [(2,2,0)], "TIM5_CH4" : [(2,1,0)], "TIM5_UP" : [(2,2,0)], "TIM8_CH1" : [(2,3,0)], "TIM8_CH2" : [(2,5,0)], "TIM8_CH3" : [(2,1,0)], "TIM8_CH4" : [(2,2,0)], "TIM8_UP" : [(2,1,0)], "TIM6_UP" : [(2,3,0)], "TIM7_UP" : [(2,4,0)], "I2C1_RX" : [(1,7,0)], "I2C1_TX" : [(1,6,0)], "I2C2_RX" : [(1,5,0)], "I2C2_TX" : [(1,4,0)], "SPI1_RX" : [(1,2,0)], "SPI1_TX" : [(1,3,0)], "SPI2_RX" : [(1,4,0)], "SPI2_TX" : [(1,5,0)], "SPI3_RX" : [(2,1,0)], "SPI3_TX" : [(2,2,0)], "UART4_RX" : [(2,3,0)], "UART4_TX" : [(2,5,0)], "USART1_RX" : [(1,5,0)], "USART1_TX" : [(1,4,0)], "USART2_RX" : [(1,6,0)], "USART2_TX" : [(1,7,0)], "USART3_RX" : [(1,3,0)], "USART3_TX" : [(1,2,0)], }
# -*- coding: utf-8 -*- # This code is part of Ansible, but is an independent component. # This particular file snippet, and this file snippet only, is BSD licensed. # Modules you write using this snippet, which is embedded dynamically by Ansible # still belong to the author of the module, and may assign their own license # to the complete work. # # Copyright (c), Simon Dodsley <simon@purestorage.com>,2017 # All rights reserved. # # Redistribution and use in source and binary forms, with or without modification, # are permitted provided that the following conditions are met: # # * Redistributions of source code must retain the above copyright # notice, this list of conditions and the following disclaimer. # * Redistributions in binary form must reproduce the above copyright notice, # this list of conditions and the following disclaimer in the documentation # and/or other materials provided with the distribution. # # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND # ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED # WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. # IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, # INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, # PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT # LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE # USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. HAS_PURESTORAGE = True try: from purestorage import purestorage except ImportError: HAS_PURESTORAGE = False from functools import wraps from os import environ from os import path import platform VERSION = 1.0 USER_AGENT_BASE = 'Ansible' def get_system(module): """Return System Object or Fail""" user_agent = '%(base)s %(class)s/%(version)s (%(platform)s)' % { 'base': USER_AGENT_BASE, 'class': __name__, 'version': VERSION, 'platform': platform.platform() } array_name = module.params['fa_url'] api = module.params['api_token'] if array_name and api: system = purestorage.FlashArray(array_name, api_token=api, user_agent=user_agent) elif environ.get('PUREFA_URL') and environ.get('PUREFA_API'): system = purestorage.FlashArray(environ.get('PUREFA_URL'), api_token=(environ.get('PUREFA_API')), user_agent=user_agent) else: module.fail_json(msg="You must set PUREFA_URL and PUREFA_API environment variables or the fa_url and api_token module arguments") try: system.get() except Exception: module.fail_json(msg="Pure Storage FlashArray authentication failed. Check your credentials") return system def purefa_argument_spec(): """Return standard base dictionary used for the argument_spec argument in AnsibleModule""" return dict( fa_url=dict(), api_token=dict(no_log=True), )
# -*- coding: utf-8 -*- # Copyright 2014-16 Akretion - Alexis de Lattre <alexis.delattre@akretion.com> # Copyright 2014 Serv. Tecnol. Avanzados - Pedro M. Baeza # License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl.html). from odoo import models, fields, api class AccountInvoice(models.Model): _inherit = 'account.invoice' payment_mode_id = fields.Many2one( comodel_name='account.payment.mode', string="Payment Mode", ondelete='restrict', readonly=True, states={'draft': [('readonly', False)]}) bank_account_required = fields.Boolean( related='payment_mode_id.payment_method_id.bank_account_required', readonly=True) partner_bank_id = fields.Many2one(ondelete='restrict') @api.onchange('partner_id', 'company_id') def _onchange_partner_id(self): res = super(AccountInvoice, self)._onchange_partner_id() if self.partner_id: if self.type == 'in_invoice': pay_mode = self.partner_id.supplier_payment_mode_id self.payment_mode_id = pay_mode if ( pay_mode and pay_mode.payment_type == 'outbound' and pay_mode.payment_method_id.bank_account_required and self.commercial_partner_id.bank_ids): self.partner_bank_id =\ self.commercial_partner_id.bank_ids[0] elif self.type == 'out_invoice': # No bank account assignation is done here as this is only # needed for printing purposes and it can conflict with # SEPA direct debit payments. Current report prints it. self.payment_mode_id = self.partner_id.customer_payment_mode_id else: self.payment_mode_id = False if self.type == 'in_invoice': self.partner_bank_id = False return res @api.model def create(self, vals): """Fill the payment_mode_id from the partner if none is provided on creation, using same method as upstream.""" onchanges = { '_onchange_partner_id': ['payment_mode_id'], } for onchange_method, changed_fields in onchanges.items(): if any(f not in vals for f in changed_fields): invoice = self.new(vals) getattr(invoice, onchange_method)() for field in changed_fields: if field not in vals and invoice[field]: vals[field] = invoice._fields[field].convert_to_write( invoice[field], invoice, ) return super(AccountInvoice, self).create(vals) @api.onchange('payment_mode_id') def payment_mode_id_change(self): if ( self.payment_mode_id and self.payment_mode_id.payment_type == 'outbound' and not self.payment_mode_id.payment_method_id. bank_account_required): self.partner_bank_id = False elif not self.payment_mode_id: self.partner_bank_id = False @api.model def line_get_convert(self, line, part): """Copy payment mode from invoice to account move line""" res = super(AccountInvoice, self).line_get_convert(line, part) if line.get('type') == 'dest' and line.get('invoice_id'): invoice = self.browse(line['invoice_id']) res['payment_mode_id'] = invoice.payment_mode_id.id or False return res # I think copying payment mode from invoice to refund by default # is a good idea because the most common way of "paying" a refund is to # deduct it on the payment of the next invoice (and OCA/bank-payment # allows to have negative payment lines since March 2016) @api.model def _prepare_refund( self, invoice, date_invoice=None, date=None, description=None, journal_id=None): vals = super(AccountInvoice, self)._prepare_refund( invoice, date_invoice=date_invoice, date=date, description=description, journal_id=journal_id) vals['payment_mode_id'] = invoice.payment_mode_id.id if invoice.type == 'in_invoice': vals['partner_bank_id'] = invoice.partner_bank_id.id return vals def partner_banks_to_show(self): self.ensure_one() if self.partner_bank_id: return self.partner_bank_id if self.payment_mode_id.show_bank_account_from_journal: if self.payment_mode_id.bank_account_link == 'fixed': return self.payment_mode_id.fixed_journal_id.bank_account_id else: return self.payment_mode_id.variable_journal_ids.mapped( 'bank_account_id') if self.payment_mode_id.payment_method_id.code == \ 'sepa_direct_debit': # pragma: no cover return (self.mandate_id.partner_bank_id or self.partner_id.valid_mandate_id.partner_bank_id) # Return this as empty recordset return self.partner_bank_id
# (c) 2016, Saran Ahluwalia <ahlusar.ahluwalia@gmail.com> # # This file is part of Ansible # # Ansible is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Ansible is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Ansible. If not, see <http://www.gnu.org/licenses/>. from __future__ import (absolute_import, division, print_function) __metaclass__ = type from ansible.errors import AnsibleActionFail from ansible.compat.tests import unittest from ansible.compat.tests.mock import patch, MagicMock, Mock from ansible.plugins.action.raw import ActionModule from ansible.playbook.task import Task class TestCopyResultExclude(unittest.TestCase): def setUp(self): pass def tearDown(self): pass # The current behavior of the raw aciton in regards to executable is currently in question; # the test_raw_executable_is_not_empty_string verifies the current behavior (whether it is desireed or not. # Please refer to the following for context: # Issue: https://github.com/ansible/ansible/issues/16054 # PR: https://github.com/ansible/ansible/pull/16085 def test_raw_executable_is_not_empty_string(self): play_context = Mock() task = MagicMock(Task) task.async_val = False connection = Mock() task.args = {'_raw_params': 'Args1'} play_context.check_mode = False self.mock_am = ActionModule(task, connection, play_context, loader=None, templar=None, shared_loader_obj=None) self.mock_am._low_level_execute_command = Mock(return_value={}) self.mock_am.display = Mock() self.mock_am.run() self.mock_am._low_level_execute_command.assert_called_with('Args1', executable=False) def test_raw_check_mode_is_True(self): play_context = Mock() task = MagicMock(Task) task.async_val = False connection = Mock() task.args = {'_raw_params': 'Args1'} play_context.check_mode = True try: self.mock_am = ActionModule(task, connection, play_context, loader=None, templar=None, shared_loader_obj=None) except AnsibleActionFail: pass def test_raw_test_environment_is_None(self): play_context = Mock() task = MagicMock(Task) task.async_val = False connection = Mock() task.args = {'_raw_params': 'Args1'} task.environment = None play_context.check_mode = False self.mock_am = ActionModule(task, connection, play_context, loader=None, templar=None, shared_loader_obj=None) self.mock_am._low_level_execute_command = Mock(return_value={}) self.mock_am.display = Mock() self.assertEqual(task.environment, None) def test_raw_task_vars_is_not_None(self): play_context = Mock() task = MagicMock(Task) task.async_val = False connection = Mock() task.args = {'_raw_params': 'Args1'} task.environment = None play_context.check_mode = False self.mock_am = ActionModule(task, connection, play_context, loader=None, templar=None, shared_loader_obj=None) self.mock_am._low_level_execute_command = Mock(return_value={}) self.mock_am.display = Mock() self.mock_am.run(task_vars={'a': 'b'}) self.assertEqual(task.environment, None)
import ast import ConfigParser import glob import grp import importlib import multiprocessing import os import sys from drop_privileges import drop_privileges from jobhandler import JobCtl from pwd import getpwnam class SchedCtl(object): def __init__(self, sched, config, logging): self.sched = sched self.config = config self.logging = logging # create an object to link to the job control class. only really used by this class to import # jobs in the $RATKINGROOT/etc/jobs.d directory self.job_control_instance = JobCtl(self.sched, self.config, self.logging) def check_sched(self): """Checks to see if scheduler is running""" if self.sched.running is True: return True, "Scheduler is running." else: return False, "Scheduler is stopped." def import_jobs(self): """read jobs from persistent directory, specified in the config file, under option job_dir""" for infile in glob.glob( os.path.join(self.config.get('main', 'job_dir'), '*.conf') ): self.logging.info("Trying to import jobfile: %s", infile) try: self.job_control_instance.add_job(infile, 'initial_startup', 'initial_startup') except RatkingException as error: print "RatkingException: Error adding job, jobfile: %s. " % infile pass except ConfigParser.ParsingError as error: self.logging.error("ConfigParser.ParsingError: %s. ", error) pass def initialize(self): """Starts the scheduler for the first time. Only to be used in ratkingd daemon""" self.sched.start() return True def start_sched(self, user): """Start the AP Scheduler. Return 'True' if success.""" if user != 'root': return False, "Only root can stop/start scheduling." if self.sched.running is True: return False, "Scheduler already running." else: try: self.sched.start() except exceptions.AttributeError as e: raise RatkingException("Error starting scheduling: %s" % e) return True, "Scheduler started." def stop_sched(self, user): """Stop the AP Scheduler. Return 'True' if success.""" if user != 'root': return False, "Only root can stop/start scheduling." if self.sched.running is False: return False, "Scheduler is not running." else: try: self.sched.shutdown() except exceptions.AttributeError as e: raise RatkingException("Error stopping scheduling: %s" % e) self.sched.shutdown() return True, "Ratkingd job scheduling has been stopped." class RatkingException(Exception): def __init__(self, message): self.message = message def __str__(self): return repr(self.message)
# -*- coding: utf-8 -*- # Copyright(C) 2009-2011 Romain Bignon, Florent Fourcot # # This file is part of weboob. # # weboob is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # weboob is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with weboob. If not, see <http://www.gnu.org/licenses/>. from decimal import Decimal from weboob.tools.capabilities.bank.transactions import FrenchTransaction from weboob.tools.captcha.virtkeyboard import VirtKeyboardError from weboob.capabilities.bank import Recipient, AccountNotFound, Transfer from weboob.tools.browser import BasePage, BrokenPageError from weboob.tools.mech import ClientForm from .login import INGVirtKeyboard from logging import error __all__ = ['TransferPage'] class TransferPage(BasePage): def on_loaded(self): pass def get_recipients(self): # First, internals recipients table = self.document.xpath('//table[@id="transfer_form:receiptAccount"]') for tr in table[0].xpath('tbody/tr'): tds = tr.xpath('td') id = tds[0].xpath('input')[0].attrib['value'] name = tds[0].xpath('label')[0].text name += u" " + tds[1].xpath('label')[0].text.replace('\n', '') name += u" " + tds[2].xpath('label')[0].text.replace('\n', '') recipient = Recipient() recipient.id = id recipient.label = name recipient._type = "int" yield recipient # Second, externals recipients select = self.document.xpath('//select[@id="transfer_form:externalAccounts"]') if len(select) > 0: recipients = select[0].xpath('option') recipients.pop(0) for option in recipients: recipient = Recipient() recipient.id = option.attrib['value'] recipient.label = option.text recipient._type = "ext" yield recipient def ischecked(self, account): id = account.id # remove prefix (CC-, LA-, ...) if "-" in id: id = id.split('-')[1] option = self.document.xpath('//input[@value="%s"]' % id) if len(option) == 0: raise AccountNotFound() else: option = option[0] try: if option.attrib["checked"] == "checked": return True else: return False except: return False def transfer(self, recipient, amount, reason): self.browser.select_form("transfer_form") self.browser.set_all_readonly(False) for a in self.browser.controls[:]: #for label in a.get_labels(): if "transfer_form:_link_hidden_" in str(a) or "transfer_form:j_idcl" in str(a): self.browser.controls.remove(a) if "transfer_form:valide" in str(a): self.browser.controls.remove(a) self.browser.controls.append(ClientForm.TextControl('text', 'AJAXREQUEST', {'value': "_viewRoot"})) self.browser.controls.append(ClientForm.TextControl('text', 'AJAX:EVENTS_COUNT', {'value': "1"})) self.browser['transfer_form:transferMotive'] = reason self.browser.controls.append(ClientForm.TextControl('text', 'transfer_form:valide', {'value': "transfer_form:valide"})) self.browser['transfer_form:validateDoTransfer'] = "needed" self.browser['transfer_form:transferAmount'] = str(amount) if recipient._type == "int": self.browser['transfer_recipient_radio'] = [recipient.id] else: self.browser['transfer_form:externalAccounts'] = [recipient.id] self.browser.submit() def buildonclick(self, recipient, account): javax = self.document.xpath('//input[@id="javax.faces.ViewState"]')[0].attrib['value'] if recipient._type == "ext": select = self.document.xpath('//select[@id="transfer_form:externalAccounts"]')[0] onclick = select.attrib['onchange'] params = onclick.split(',')[3].split('{')[1] idparam = params.split("'")[1] param = params.split("'")[3] request = self.browser.buildurl('', ("AJAXREQUEST", "transfer_form:transfer_radios_form"), ("transfer_form:generalMessages", ""), ("transfer_issuer_radio", account.id[3:]), ("transfer_form:externalAccounts", recipient.id), ("transfer_date", 0), ("transfer_form:transferAmount", ""), ("transfer_form:transferMotive", ""), ("transfer_form:validateDoTransfer", "needed"), ("transfer_form", "transfer_form"), ("autoScrol", ""), ("javax.faces.ViewState", javax), (idparam, param)) request = request[1:] # remove the "?" return request elif recipient._type == "int": for input in self.document.xpath('//input[@value=%s]' % recipient.id): if input.attrib['name'] == "transfer_recipient_radio": onclick = input.attrib['onclick'] break # Get something like transfer_form:issueAccount:0:click params = onclick.split(',')[3].split('{')[1] idparam = params.split("'")[1] param = params.split("'")[3] request = self.browser.buildurl('', ("AJAXREQUEST", "transfer_form:transfer_radios_form"), ('transfer_issuer_radio', account.id[3:]), ("transfer_recipient_radio", recipient.id), ("transfer_form:externalAccounts", "na"), ("transfer_date", 0), ("transfer_form:transferAmount", ""), ("transfer_form:transferMotive", ""), ("transfer_form:validateDoTransfer", "needed"), ("transfer_form", "transfer_form"), ("autoScroll", ""), ("javax.faces.ViewState", javax), (idparam, param)) request = request[1:] return request class TransferConfirmPage(BasePage): def on_loaded(self): pass def confirm(self, password): try: vk = INGVirtKeyboard(self) except VirtKeyboardError, err: error("Error: %s" % err) return realpasswd = "" span = self.document.find('//span[@id="digitpadtransfer"]') i = 0 for font in span.getiterator('font'): if font.attrib.get('class') == "vide": realpasswd += password[i] i += 1 confirmform = None for form in self.document.xpath('//form'): try: if form.attrib['name'][0:4] == "j_id": confirmform = form break except: continue if confirmform is None: raise BrokenPageError('Unable to find confirm form') formname = confirmform.attrib['name'] self.browser.logger.debug('We are looking for : ' + realpasswd) self.browser.select_form(formname) self.browser.set_all_readonly(False) for a in self.browser.controls[:]: if "_link_hidden_" in str(a) or "j_idcl" in str(a): self.browser.controls.remove(a) coordinates = vk.get_string_code(realpasswd) self.browser.logger.debug("Coordonates: " + coordinates) self.browser.controls.append(ClientForm.TextControl('text', 'AJAXREQUEST', {'value': '_viewRoot'})) self.browser.controls.append(ClientForm.TextControl( 'text', '%s:mrgtransfer' % formname, {'value': '%s:mrgtransfer' % formname})) self.browser['%s:mrltransfer' % formname] = coordinates self.browser.submit(nologin=True) def recap(self): if len(self.document.xpath('//p[@class="alert alert-success"]')) == 0: raise BrokenPageError('Unable to find confirmation') div = self.document.find( '//div[@class="encadre transfert-validation"]') transfer = Transfer(0) transfer.amount = Decimal(FrenchTransaction.clean_amount( div.xpath('.//label[@id="confirmtransferAmount"]')[0].text)) transfer.origin = div.xpath( './/span[@id="confirmfromAccount"]')[0].text transfer.recipient = div.xpath( './/span[@id="confirmtoAccount"]')[0].text transfer.reason = unicode( div.xpath('.//span[@id="confirmtransferMotive"]')[0].text) return transfer
import os import socket import atexit import re import functools from setuptools.extern.six.moves import urllib, http_client, map, filter from pkg_resources import ResolutionError, ExtractionError try: import ssl except ImportError: ssl = None __all__ = [ 'VerifyingHTTPSHandler', 'find_ca_bundle', 'is_available', 'cert_paths', 'opener_for' ] cert_paths = """ /etc/pki/tls/certs/ca-bundle.crt /etc/ssl/certs/ca-certificates.crt /usr/share/ssl/certs/ca-bundle.crt /usr/local/share/certs/ca-root.crt /etc/ssl/cert.pem /System/Library/OpenSSL/certs/cert.pem /usr/local/share/certs/ca-root-nss.crt /etc/ssl/ca-bundle.pem """.strip().split() try: HTTPSHandler = urllib.request.HTTPSHandler HTTPSConnection = http_client.HTTPSConnection except AttributeError: HTTPSHandler = HTTPSConnection = object is_available = ssl is not None and object not in (HTTPSHandler, HTTPSConnection) try: from ssl import CertificateError, match_hostname except ImportError: try: from backports.ssl_match_hostname import CertificateError from backports.ssl_match_hostname import match_hostname except ImportError: CertificateError = None match_hostname = None if not CertificateError: class CertificateError(ValueError): pass if not match_hostname: def _dnsname_match(dn, hostname, max_wildcards=1): """Matching according to RFC 6125, section 6.4.3 http://tools.ietf.org/html/rfc6125#section-6.4.3 """ pats = [] if not dn: return False # Ported from python3-syntax: # leftmost, *remainder = dn.split(r'.') parts = dn.split(r'.') leftmost = parts[0] remainder = parts[1:] wildcards = leftmost.count('*') if wildcards > max_wildcards: # Issue #17980: avoid denials of service by refusing more # than one wildcard per fragment. A survey of established # policy among SSL implementations showed it to be a # reasonable choice. raise CertificateError( "too many wildcards in certificate DNS name: " + repr(dn)) # speed up common case w/o wildcards if not wildcards: return dn.lower() == hostname.lower() # RFC 6125, section 6.4.3, subitem 1. # The client SHOULD NOT attempt to match a presented identifier in which # the wildcard character comprises a label other than the left-most label. if leftmost == '*': # When '*' is a fragment by itself, it matches a non-empty dotless # fragment. pats.append('[^.]+') elif leftmost.startswith('xn--') or hostname.startswith('xn--'): # RFC 6125, section 6.4.3, subitem 3. # The client SHOULD NOT attempt to match a presented identifier # where the wildcard character is embedded within an A-label or # U-label of an internationalized domain name. pats.append(re.escape(leftmost)) else: # Otherwise, '*' matches any dotless string, e.g. www* pats.append(re.escape(leftmost).replace(r'\*', '[^.]*')) # add the remaining fragments, ignore any wildcards for frag in remainder: pats.append(re.escape(frag)) pat = re.compile(r'\A' + r'\.'.join(pats) + r'\Z', re.IGNORECASE) return pat.match(hostname) def match_hostname(cert, hostname): """Verify that *cert* (in decoded format as returned by SSLSocket.getpeercert()) matches the *hostname*. RFC 2818 and RFC 6125 rules are followed, but IP addresses are not accepted for *hostname*. CertificateError is raised on failure. On success, the function returns nothing. """ if not cert: raise ValueError("empty or no certificate") dnsnames = [] san = cert.get('subjectAltName', ()) for key, value in san: if key == 'DNS': if _dnsname_match(value, hostname): return dnsnames.append(value) if not dnsnames: # The subject is only checked when there is no dNSName entry # in subjectAltName for sub in cert.get('subject', ()): for key, value in sub: # XXX according to RFC 2818, the most specific Common Name # must be used. if key == 'commonName': if _dnsname_match(value, hostname): return dnsnames.append(value) if len(dnsnames) > 1: raise CertificateError("hostname %r " "doesn't match either of %s" % (hostname, ', '.join(map(repr, dnsnames)))) elif len(dnsnames) == 1: raise CertificateError("hostname %r " "doesn't match %r" % (hostname, dnsnames[0])) else: raise CertificateError("no appropriate commonName or " "subjectAltName fields were found") class VerifyingHTTPSHandler(HTTPSHandler): """Simple verifying handler: no auth, subclasses, timeouts, etc.""" def __init__(self, ca_bundle): self.ca_bundle = ca_bundle HTTPSHandler.__init__(self) def https_open(self, req): return self.do_open( lambda host, **kw: VerifyingHTTPSConn(host, self.ca_bundle, **kw), req ) class VerifyingHTTPSConn(HTTPSConnection): """Simple verifying connection: no auth, subclasses, timeouts, etc.""" def __init__(self, host, ca_bundle, **kw): HTTPSConnection.__init__(self, host, **kw) self.ca_bundle = ca_bundle def connect(self): sock = socket.create_connection( (self.host, self.port), getattr(self, 'source_address', None) ) # Handle the socket if a (proxy) tunnel is present if hasattr(self, '_tunnel') and getattr(self, '_tunnel_host', None): self.sock = sock self._tunnel() # http://bugs.python.org/issue7776: Python>=3.4.1 and >=2.7.7 # change self.host to mean the proxy server host when tunneling is # being used. Adapt, since we are interested in the destination # host for the match_hostname() comparison. actual_host = self._tunnel_host else: actual_host = self.host if hasattr(ssl, 'create_default_context'): ctx = ssl.create_default_context(cafile=self.ca_bundle) self.sock = ctx.wrap_socket(sock, server_hostname=actual_host) else: # This is for python < 2.7.9 and < 3.4? self.sock = ssl.wrap_socket( sock, cert_reqs=ssl.CERT_REQUIRED, ca_certs=self.ca_bundle ) try: match_hostname(self.sock.getpeercert(), actual_host) except CertificateError: self.sock.shutdown(socket.SHUT_RDWR) self.sock.close() raise def opener_for(ca_bundle=None): """Get a urlopen() replacement that uses ca_bundle for verification""" return urllib.request.build_opener( VerifyingHTTPSHandler(ca_bundle or find_ca_bundle()) ).open # from jaraco.functools def once(func): @functools.wraps(func) def wrapper(*args, **kwargs): if not hasattr(func, 'always_returns'): func.always_returns = func(*args, **kwargs) return func.always_returns return wrapper @once def get_win_certfile(): try: import wincertstore except ImportError: return None class CertFile(wincertstore.CertFile): def __init__(self): super(CertFile, self).__init__() atexit.register(self.close) def close(self): try: super(CertFile, self).close() except OSError: pass _wincerts = CertFile() _wincerts.addstore('CA') _wincerts.addstore('ROOT') return _wincerts.name def find_ca_bundle(): """Return an existing CA bundle path, or None""" extant_cert_paths = filter(os.path.isfile, cert_paths) return ( get_win_certfile() or next(extant_cert_paths, None) or _certifi_where() ) def _certifi_where(): try: return __import__('certifi').where() except (ImportError, ResolutionError, ExtractionError): pass
# -*- coding: utf-8 -*- from south.utils import datetime_utils as datetime from south.db import db from south.v2 import SchemaMigration from django.db import models class Migration(SchemaMigration): def forwards(self, orm): # Adding model 'TagKeyphrase' db.create_table(u'auxiliary_tagkeyphrase', ( (u'id', self.gf('django.db.models.fields.AutoField')(primary_key=True)), ('tag', self.gf('django.db.models.fields.related.ForeignKey')(to=orm['tagging.Tag'])), ('phrase', self.gf('django.db.models.fields.CharField')(max_length=100)), )) db.send_create_signal(u'auxiliary', ['TagKeyphrase']) def backwards(self, orm): # Deleting model 'TagKeyphrase' db.delete_table(u'auxiliary_tagkeyphrase') models = { u'auth.group': { 'Meta': {'object_name': 'Group'}, u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}), 'name': ('django.db.models.fields.CharField', [], {'unique': 'True', 'max_length': '80'}), 'permissions': ('django.db.models.fields.related.ManyToManyField', [], {'to': u"orm['auth.Permission']", 'symmetrical': 'False', 'blank': 'True'}) }, u'auth.permission': { 'Meta': {'ordering': "(u'content_type__app_label', u'content_type__model', u'codename')", 'unique_together': "((u'content_type', u'codename'),)", 'object_name': 'Permission'}, 'codename': ('django.db.models.fields.CharField', [], {'max_length': '100'}), 'content_type': ('django.db.models.fields.related.ForeignKey', [], {'to': u"orm['contenttypes.ContentType']"}), u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}), 'name': ('django.db.models.fields.CharField', [], {'max_length': '50'}) }, u'auth.user': { 'Meta': {'object_name': 'User'}, 'date_joined': ('django.db.models.fields.DateTimeField', [], {'default': 'datetime.datetime.now'}), 'email': ('django.db.models.fields.EmailField', [], {'max_length': '75', 'blank': 'True'}), 'first_name': ('django.db.models.fields.CharField', [], {'max_length': '30', 'blank': 'True'}), 'groups': ('django.db.models.fields.related.ManyToManyField', [], {'symmetrical': 'False', 'related_name': "u'user_set'", 'blank': 'True', 'to': u"orm['auth.Group']"}), u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}), 'is_active': ('django.db.models.fields.BooleanField', [], {'default': 'True'}), 'is_staff': ('django.db.models.fields.BooleanField', [], {'default': 'False'}), 'is_superuser': ('django.db.models.fields.BooleanField', [], {'default': 'False'}), 'last_login': ('django.db.models.fields.DateTimeField', [], {'default': 'datetime.datetime.now'}), 'last_name': ('django.db.models.fields.CharField', [], {'max_length': '30', 'blank': 'True'}), 'password': ('django.db.models.fields.CharField', [], {'max_length': '128'}), 'user_permissions': ('django.db.models.fields.related.ManyToManyField', [], {'symmetrical': 'False', 'related_name': "u'user_set'", 'blank': 'True', 'to': u"orm['auth.Permission']"}), 'username': ('django.db.models.fields.CharField', [], {'unique': 'True', 'max_length': '30'}) }, u'auxiliary.feedback': { 'Meta': {'object_name': 'Feedback'}, 'content': ('django.db.models.fields.TextField', [], {}), u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}), 'ip_address': ('django.db.models.fields.IPAddressField', [], {'max_length': '15', 'null': 'True', 'blank': 'True'}), 'suggested_at': ('django.db.models.fields.DateTimeField', [], {'auto_now_add': 'True', 'null': 'True', 'blank': 'True'}), 'suggested_by': ('django.db.models.fields.related.ForeignKey', [], {'blank': 'True', 'related_name': "'feedback'", 'null': 'True', 'to': u"orm['auth.User']"}), 'url': ('django.db.models.fields.TextField', [], {}), 'user_agent': ('django.db.models.fields.TextField', [], {'null': 'True', 'blank': 'True'}) }, u'auxiliary.tagkeyphrase': { 'Meta': {'object_name': 'TagKeyphrase'}, u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}), 'phrase': ('django.db.models.fields.CharField', [], {'max_length': '100'}), 'tag': ('django.db.models.fields.related.ForeignKey', [], {'to': u"orm['tagging.Tag']"}) }, u'auxiliary.tagsuggestion': { 'Meta': {'object_name': 'TagSuggestion'}, 'content_type': ('django.db.models.fields.related.ForeignKey', [], {'to': u"orm['contenttypes.ContentType']"}), u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}), 'name': ('django.db.models.fields.TextField', [], {'unique': 'True'}), 'object_id': ('django.db.models.fields.PositiveIntegerField', [], {'db_index': 'True'}), 'suggested_by': ('django.db.models.fields.related.ForeignKey', [], {'blank': 'True', 'related_name': "'tagsuggestion'", 'null': 'True', 'to': u"orm['auth.User']"}) }, u'auxiliary.tagsynonym': { 'Meta': {'object_name': 'TagSynonym'}, u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}), 'synonym_tag': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'synonym_synonym_tag'", 'unique': 'True', 'to': u"orm['tagging.Tag']"}), 'tag': ('django.db.models.fields.related.ForeignKey', [], {'related_name': "'synonym_proper_tag'", 'to': u"orm['tagging.Tag']"}) }, u'auxiliary.tidbit': { 'Meta': {'object_name': 'Tidbit'}, 'button_link': ('django.db.models.fields.CharField', [], {'max_length': '255'}), 'button_text': ('django.db.models.fields.CharField', [], {'max_length': '100'}), 'content': ('tinymce.models.HTMLField', [], {}), 'icon': ('django.db.models.fields.CharField', [], {'max_length': '15'}), u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}), 'is_active': ('django.db.models.fields.BooleanField', [], {'default': 'True'}), 'ordering': ('django.db.models.fields.IntegerField', [], {'default': '20', 'db_index': 'True'}), 'photo': ('django.db.models.fields.files.ImageField', [], {'max_length': '200', 'null': 'True', 'blank': 'True'}), 'suggested_by': ('django.db.models.fields.related.ForeignKey', [], {'blank': 'True', 'related_name': "'tidbits'", 'null': 'True', 'to': u"orm['auth.User']"}), 'title': ('django.db.models.fields.CharField', [], {'default': "u'Did you know ?'", 'max_length': '40'}) }, u'contenttypes.contenttype': { 'Meta': {'ordering': "('name',)", 'unique_together': "(('app_label', 'model'),)", 'object_name': 'ContentType', 'db_table': "'django_content_type'"}, 'app_label': ('django.db.models.fields.CharField', [], {'max_length': '100'}), u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}), 'model': ('django.db.models.fields.CharField', [], {'max_length': '100'}), 'name': ('django.db.models.fields.CharField', [], {'max_length': '100'}) }, u'tagging.tag': { 'Meta': {'ordering': "('name',)", 'object_name': 'Tag'}, u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}), 'name': ('django.db.models.fields.CharField', [], {'unique': 'True', 'max_length': '50', 'db_index': 'True'}) } } complete_apps = ['auxiliary']
# Licensed under the Apache License: http://www.apache.org/licenses/LICENSE-2.0 # For details: https://bitbucket.org/ned/coveragepy/src/default/NOTICE.txt """Mixin classes to help make good tests.""" import atexit import collections import os import random import shutil import sys import tempfile import textwrap from coverage.backunittest import TestCase from coverage.backward import StringIO, to_bytes class Tee(object): """A file-like that writes to all the file-likes it has.""" def __init__(self, *files): """Make a Tee that writes to all the files in `files.`""" self._files = files if hasattr(files[0], "encoding"): self.encoding = files[0].encoding def write(self, data): """Write `data` to all the files.""" for f in self._files: f.write(data) def flush(self): """Flush the data on all the files.""" for f in self._files: f.flush() if 0: # Use this if you need to use a debugger, though it makes some tests # fail, I'm not sure why... def __getattr__(self, name): return getattr(self._files[0], name) class ModuleAwareMixin(TestCase): """A test case mixin that isolates changes to sys.modules.""" def setUp(self): super(ModuleAwareMixin, self).setUp() # Record sys.modules here so we can restore it in cleanup_modules. self.old_modules = dict(sys.modules) self.addCleanup(self.cleanup_modules) def cleanup_modules(self): """Remove any new modules imported during the test run. This lets us import the same source files for more than one test. """ for m in [m for m in sys.modules if m not in self.old_modules]: del sys.modules[m] class SysPathAwareMixin(TestCase): """A test case mixin that isolates changes to sys.path.""" def setUp(self): super(SysPathAwareMixin, self).setUp() self.old_syspath = sys.path[:] self.addCleanup(self.cleanup_syspath) def cleanup_syspath(self): """Restore the original sys.path.""" sys.path = self.old_syspath class EnvironmentAwareMixin(TestCase): """A test case mixin that isolates changes to the environment.""" def setUp(self): super(EnvironmentAwareMixin, self).setUp() # Record environment variables that we changed with set_environ. self.environ_undos = {} self.addCleanup(self.cleanup_environ) def set_environ(self, name, value): """Set an environment variable `name` to be `value`. The environment variable is set, and record is kept that it was set, so that `cleanup_environ` can restore its original value. """ if name not in self.environ_undos: self.environ_undos[name] = os.environ.get(name) os.environ[name] = value def cleanup_environ(self): """Undo all the changes made by `set_environ`.""" for name, value in self.environ_undos.items(): if value is None: del os.environ[name] else: os.environ[name] = value class StdStreamCapturingMixin(TestCase): """A test case mixin that captures stdout and stderr.""" def setUp(self): super(StdStreamCapturingMixin, self).setUp() # Capture stdout and stderr so we can examine them in tests. # nose keeps stdout from littering the screen, so we can safely Tee it, # but it doesn't capture stderr, so we don't want to Tee stderr to the # real stderr, since it will interfere with our nice field of dots. self.old_stdout = sys.stdout self.captured_stdout = StringIO() sys.stdout = Tee(sys.stdout, self.captured_stdout) self.old_stderr = sys.stderr self.captured_stderr = StringIO() sys.stderr = self.captured_stderr self.addCleanup(self.cleanup_std_streams) def cleanup_std_streams(self): """Restore stdout and stderr.""" sys.stdout = self.old_stdout sys.stderr = self.old_stderr def stdout(self): """Return the data written to stdout during the test.""" return self.captured_stdout.getvalue() def stderr(self): """Return the data written to stderr during the test.""" return self.captured_stderr.getvalue() class TempDirMixin(SysPathAwareMixin, ModuleAwareMixin, TestCase): """A test case mixin that creates a temp directory and files in it. Includes SysPathAwareMixin and ModuleAwareMixin, because making and using temp dirs like this will also need that kind of isolation. """ # Our own setting: most of these tests run in their own temp directory. # Set this to False in your subclass if you don't want a temp directory # created. run_in_temp_dir = True # Set this if you aren't creating any files with make_file, but still want # the temp directory. This will stop the test behavior checker from # complaining. no_files_in_temp_dir = False def setUp(self): super(TempDirMixin, self).setUp() if self.run_in_temp_dir: # Create a temporary directory. self.temp_dir = self.make_temp_dir("test_cover") self.chdir(self.temp_dir) # Modules should be importable from this temp directory. We don't # use '' because we make lots of different temp directories and # nose's caching importer can get confused. The full path prevents # problems. sys.path.insert(0, os.getcwd()) class_behavior = self.class_behavior() class_behavior.tests += 1 class_behavior.temp_dir = self.run_in_temp_dir class_behavior.no_files_ok = self.no_files_in_temp_dir self.addCleanup(self.check_behavior) def make_temp_dir(self, slug="test_cover"): """Make a temp directory that is cleaned up when the test is done.""" name = "%s_%08d" % (slug, random.randint(0, 99999999)) temp_dir = os.path.join(tempfile.gettempdir(), name) os.makedirs(temp_dir) self.addCleanup(shutil.rmtree, temp_dir) return temp_dir def chdir(self, new_dir): """Change directory, and change back when the test is done.""" old_dir = os.getcwd() os.chdir(new_dir) self.addCleanup(os.chdir, old_dir) def check_behavior(self): """Check that we did the right things.""" class_behavior = self.class_behavior() if class_behavior.test_method_made_any_files: class_behavior.tests_making_files += 1 def make_file(self, filename, text="", newline=None): """Create a file for testing. `filename` is the relative path to the file, including directories if desired, which will be created if need be. `text` is the content to create in the file, a native string (bytes in Python 2, unicode in Python 3). If `newline` is provided, it is a string that will be used as the line endings in the created file, otherwise the line endings are as provided in `text`. Returns `filename`. """ # Tests that call `make_file` should be run in a temp environment. assert self.run_in_temp_dir self.class_behavior().test_method_made_any_files = True text = textwrap.dedent(text) if newline: text = text.replace("\n", newline) # Make sure the directories are available. dirs, _ = os.path.split(filename) if dirs and not os.path.exists(dirs): os.makedirs(dirs) # Create the file. with open(filename, 'wb') as f: f.write(to_bytes(text)) return filename # We run some tests in temporary directories, because they may need to make # files for the tests. But this is expensive, so we can change per-class # whether a temp dir is used or not. It's easy to forget to set that # option properly, so we track information about what the tests did, and # then report at the end of the process on test classes that were set # wrong. class ClassBehavior(object): """A value object to store per-class.""" def __init__(self): self.tests = 0 self.skipped = 0 self.temp_dir = True self.no_files_ok = False self.tests_making_files = 0 self.test_method_made_any_files = False # Map from class to info about how it ran. class_behaviors = collections.defaultdict(ClassBehavior) @classmethod def report_on_class_behavior(cls): """Called at process exit to report on class behavior.""" for test_class, behavior in cls.class_behaviors.items(): bad = "" if behavior.tests <= behavior.skipped: bad = "" elif behavior.temp_dir and behavior.tests_making_files == 0: if not behavior.no_files_ok: bad = "Inefficient" elif not behavior.temp_dir and behavior.tests_making_files > 0: bad = "Unsafe" if bad: if behavior.temp_dir: where = "in a temp directory" else: where = "without a temp directory" print( "%s: %s ran %d tests, %d made files %s" % ( bad, test_class.__name__, behavior.tests, behavior.tests_making_files, where, ) ) def class_behavior(self): """Get the ClassBehavior instance for this test.""" return self.class_behaviors[self.__class__] # When the process ends, find out about bad classes. atexit.register(TempDirMixin.report_on_class_behavior)
# Copyright 2017 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== """Inception Resnet v2 Faster R-CNN implementation. See "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning" by Szegedy et al. (https://arxiv.org/abs/1602.07261) as well as "Speed/accuracy trade-offs for modern convolutional object detectors" by Huang et al. (https://arxiv.org/abs/1611.10012) """ import tensorflow as tf from object_detection.meta_architectures import faster_rcnn_meta_arch from nets import inception_resnet_v2 slim = tf.contrib.slim class FasterRCNNInceptionResnetV2FeatureExtractor( faster_rcnn_meta_arch.FasterRCNNFeatureExtractor): """Faster R-CNN with Inception Resnet v2 feature extractor implementation.""" def __init__(self, is_training, first_stage_features_stride, batch_norm_trainable=False, reuse_weights=None, weight_decay=0.0): """Constructor. Args: is_training: See base class. first_stage_features_stride: See base class. batch_norm_trainable: See base class. reuse_weights: See base class. weight_decay: See base class. Raises: ValueError: If `first_stage_features_stride` is not 8 or 16. """ if first_stage_features_stride != 8 and first_stage_features_stride != 16: raise ValueError('`first_stage_features_stride` must be 8 or 16.') super(FasterRCNNInceptionResnetV2FeatureExtractor, self).__init__( is_training, first_stage_features_stride, batch_norm_trainable, reuse_weights, weight_decay) def preprocess(self, resized_inputs): """Faster R-CNN with Inception Resnet v2 preprocessing. Maps pixel values to the range [-1, 1]. Args: resized_inputs: A [batch, height_in, width_in, channels] float32 tensor representing a batch of images with values between 0 and 255.0. Returns: preprocessed_inputs: A [batch, height_out, width_out, channels] float32 tensor representing a batch of images. """ return (2.0 / 255.0) * resized_inputs - 1.0 def _extract_proposal_features(self, preprocessed_inputs, scope): """Extracts first stage RPN features. Extracts features using the first half of the Inception Resnet v2 network. We construct the network in `align_feature_maps=True` mode, which means that all VALID paddings in the network are changed to SAME padding so that the feature maps are aligned. Args: preprocessed_inputs: A [batch, height, width, channels] float32 tensor representing a batch of images. scope: A scope name. Returns: rpn_feature_map: A tensor with shape [batch, height, width, depth] Raises: InvalidArgumentError: If the spatial size of `preprocessed_inputs` (height or width) is less than 33. ValueError: If the created network is missing the required activation. """ if len(preprocessed_inputs.get_shape().as_list()) != 4: raise ValueError('`preprocessed_inputs` must be 4 dimensional, got a ' 'tensor of shape %s' % preprocessed_inputs.get_shape()) with slim.arg_scope(inception_resnet_v2.inception_resnet_v2_arg_scope( weight_decay=self._weight_decay)): # Forces is_training to False to disable batch norm update. with slim.arg_scope([slim.batch_norm], is_training=self._train_batch_norm): with tf.variable_scope('InceptionResnetV2', reuse=self._reuse_weights) as scope: rpn_feature_map, _ = ( inception_resnet_v2.inception_resnet_v2_base( preprocessed_inputs, final_endpoint='PreAuxLogits', scope=scope, output_stride=self._first_stage_features_stride, align_feature_maps=True)) return rpn_feature_map def _extract_box_classifier_features(self, proposal_feature_maps, scope): """Extracts second stage box classifier features. This function reconstructs the "second half" of the Inception ResNet v2 network after the part defined in `_extract_proposal_features`. Args: proposal_feature_maps: A 4-D float tensor with shape [batch_size * self.max_num_proposals, crop_height, crop_width, depth] representing the feature map cropped to each proposal. scope: A scope name. Returns: proposal_classifier_features: A 4-D float tensor with shape [batch_size * self.max_num_proposals, height, width, depth] representing box classifier features for each proposal. """ with tf.variable_scope('InceptionResnetV2', reuse=self._reuse_weights): with slim.arg_scope(inception_resnet_v2.inception_resnet_v2_arg_scope( weight_decay=self._weight_decay)): # Forces is_training to False to disable batch norm update. with slim.arg_scope([slim.batch_norm], is_training=self._train_batch_norm): with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], stride=1, padding='SAME'): with tf.variable_scope('Mixed_7a'): with tf.variable_scope('Branch_0'): tower_conv = slim.conv2d(proposal_feature_maps, 256, 1, scope='Conv2d_0a_1x1') tower_conv_1 = slim.conv2d( tower_conv, 384, 3, stride=2, padding='VALID', scope='Conv2d_1a_3x3') with tf.variable_scope('Branch_1'): tower_conv1 = slim.conv2d( proposal_feature_maps, 256, 1, scope='Conv2d_0a_1x1') tower_conv1_1 = slim.conv2d( tower_conv1, 288, 3, stride=2, padding='VALID', scope='Conv2d_1a_3x3') with tf.variable_scope('Branch_2'): tower_conv2 = slim.conv2d( proposal_feature_maps, 256, 1, scope='Conv2d_0a_1x1') tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3, scope='Conv2d_0b_3x3') tower_conv2_2 = slim.conv2d( tower_conv2_1, 320, 3, stride=2, padding='VALID', scope='Conv2d_1a_3x3') with tf.variable_scope('Branch_3'): tower_pool = slim.max_pool2d( proposal_feature_maps, 3, stride=2, padding='VALID', scope='MaxPool_1a_3x3') net = tf.concat( [tower_conv_1, tower_conv1_1, tower_conv2_2, tower_pool], 3) net = slim.repeat(net, 9, inception_resnet_v2.block8, scale=0.20) net = inception_resnet_v2.block8(net, activation_fn=None) proposal_classifier_features = slim.conv2d( net, 1536, 1, scope='Conv2d_7b_1x1') return proposal_classifier_features def restore_from_classification_checkpoint_fn( self, first_stage_feature_extractor_scope, second_stage_feature_extractor_scope): """Returns a map of variables to load from a foreign checkpoint. Note that this overrides the default implementation in faster_rcnn_meta_arch.FasterRCNNFeatureExtractor which does not work for InceptionResnetV2 checkpoints. TODO(jonathanhuang,rathodv): revisit whether it's possible to force the `Repeat` namescope as created in `_extract_box_classifier_features` to start counting at 2 (e.g. `Repeat_2`) so that the default restore_fn can be used. Args: first_stage_feature_extractor_scope: A scope name for the first stage feature extractor. second_stage_feature_extractor_scope: A scope name for the second stage feature extractor. Returns: A dict mapping variable names (to load from a checkpoint) to variables in the model graph. """ variables_to_restore = {} for variable in tf.global_variables(): if variable.op.name.startswith( first_stage_feature_extractor_scope): var_name = variable.op.name.replace( first_stage_feature_extractor_scope + '/', '') variables_to_restore[var_name] = variable if variable.op.name.startswith( second_stage_feature_extractor_scope): var_name = variable.op.name.replace( second_stage_feature_extractor_scope + '/InceptionResnetV2/Repeat', 'InceptionResnetV2/Repeat_2') var_name = var_name.replace( second_stage_feature_extractor_scope + '/', '') variables_to_restore[var_name] = variable return variables_to_restore
#!/usr/bin/env python # coding: ISO8859-1 # # Copyright (c) 2013, Preferred Infrastructure, Inc. # All rights reserved. # # Redistribution and use in source and binary forms, with or without # modification, are permitted provided that the following conditions are met: # # * Redistributions of source code must retain the above copyright notice, # this list of conditions and the following disclaimer. # # * Redistributions in binary form must reproduce the above copyright # notice, this list of conditions and the following disclaimer in the # documentation and/or other materials provided with the distribution. # # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE # ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE # LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE # POSSIBILITY OF SUCH DAMAGE. """ maf - a waf extension for automation of parameterized computational experiments """ # NOTE: coding ISO8859-1 is necessary for attaching maflib at the end of this # file. import os import os.path import shutil import subprocess import sys import tarfile import waflib.Context import waflib.Logs TAR_NAME = 'maflib.tar' NEW_LINE = '#XXX'.encode() CARRIAGE_RETURN = '#YYY'.encode() ARCHIVE_BEGIN = '#==>\n'.encode() ARCHIVE_END = '#<==\n'.encode() class _Cleaner: def __init__(self, directory): self._cwd = os.getcwd() self._directory = directory def __enter__(self): self.clean() def __exit__(self, exc_type, exc_value, traceback): os.chdir(self._cwd) if exc_type: self.clean() return False def clean(self): try: path = os.path.join(self._directory, 'maflib') shutil.rmtree(path) except OSError: pass def _read_archive(filename): if filename.endswith('.pyc'): filename = filename[:-1] with open(filename, 'rb') as f: while True: line = f.readline() if not line: raise Exception('archive not found') if line == ARCHIVE_BEGIN: content = f.readline() if not content or f.readline() != ARCHIVE_END: raise Exception('corrupt archive') break return content[1:-1].replace(NEW_LINE, '\n'.encode()).replace( CARRIAGE_RETURN, '\r'.encode()) def unpack_maflib(directory): with _Cleaner(directory) as c: content = _read_archive(__file__) os.makedirs(os.path.join(directory, 'maflib')) os.chdir(directory) bz2_name = TAR_NAME + '.bz2' with open(bz2_name, 'wb') as f: f.write(content) try: t = tarfile.open(bz2_name) except: try: os.system('bunzip2 ' + bz2_name) t = tarfile.open(TAR_NAME) except: raise Exception('Cannot extract maflib. Check that python bz2 module or bunzip2 command is available.') try: t.extractall() finally: t.close() try: os.remove(bz2_name) os.remove(TAR_NAME) except: pass maflib_path = os.path.abspath(os.getcwd()) return maflib_path def test_maflib(directory): try: os.stat(os.path.join(directory, 'maflib')) return os.path.abspath(directory) except OSError: return None def find_maflib(): path = waflib.Context.waf_dir if not test_maflib(path): unpack_maflib(path) return path find_maflib() import maflib.core
# (c) 2016, James Cammarata <jimi@sngx.net> # # This file is part of Ansible # # Ansible is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Ansible is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Ansible. If not, see <http://www.gnu.org/licenses/>. # Make coding more python3-ish from __future__ import (absolute_import, division, print_function) __metaclass__ = type from ansible.compat.tests import unittest from ansible.compat.tests.mock import patch, MagicMock from ansible.executor.task_result import TaskResult class TestTaskResult(unittest.TestCase): def test_task_result_basic(self): mock_host = MagicMock() mock_task = MagicMock() # test loading a result with a dict tr = TaskResult(mock_host, mock_task, dict()) # test loading a result with a JSON string with patch('ansible.parsing.dataloader.DataLoader.load') as p: tr = TaskResult(mock_host, mock_task, '{}') def test_task_result_is_changed(self): mock_host = MagicMock() mock_task = MagicMock() # test with no changed in result tr = TaskResult(mock_host, mock_task, dict()) self.assertFalse(tr.is_changed()) # test with changed in the result tr = TaskResult(mock_host, mock_task, dict(changed=True)) self.assertTrue(tr.is_changed()) # test with multiple results but none changed mock_task.loop = 'foo' tr = TaskResult(mock_host, mock_task, dict(results=[dict(foo='bar'), dict(bam='baz'), True])) self.assertFalse(tr.is_changed()) # test with multiple results and one changed mock_task.loop = 'foo' tr = TaskResult(mock_host, mock_task, dict(results=[dict(changed=False), dict(changed=True), dict(some_key=False)])) self.assertTrue(tr.is_changed()) def test_task_result_is_skipped(self): mock_host = MagicMock() mock_task = MagicMock() # test with no skipped in result tr = TaskResult(mock_host, mock_task, dict()) self.assertFalse(tr.is_skipped()) # test with skipped in the result tr = TaskResult(mock_host, mock_task, dict(skipped=True)) self.assertTrue(tr.is_skipped()) # test with multiple results but none skipped mock_task.loop = 'foo' tr = TaskResult(mock_host, mock_task, dict(results=[dict(foo='bar'), dict(bam='baz'), True])) self.assertFalse(tr.is_skipped()) # test with multiple results and one skipped mock_task.loop = 'foo' tr = TaskResult(mock_host, mock_task, dict(results=[dict(skipped=False), dict(skipped=True), dict(some_key=False)])) self.assertFalse(tr.is_skipped()) # test with multiple results and all skipped mock_task.loop = 'foo' tr = TaskResult(mock_host, mock_task, dict(results=[dict(skipped=True), dict(skipped=True), dict(skipped=True)])) self.assertTrue(tr.is_skipped()) # test with multiple squashed results (list of strings) # first with the main result having skipped=False mock_task.loop = 'foo' tr = TaskResult(mock_host, mock_task, dict(results=["a", "b", "c"], skipped=False)) self.assertFalse(tr.is_skipped()) # then with the main result having skipped=True tr = TaskResult(mock_host, mock_task, dict(results=["a", "b", "c"], skipped=True)) self.assertTrue(tr.is_skipped()) def test_task_result_is_unreachable(self): mock_host = MagicMock() mock_task = MagicMock() # test with no unreachable in result tr = TaskResult(mock_host, mock_task, dict()) self.assertFalse(tr.is_unreachable()) # test with unreachable in the result tr = TaskResult(mock_host, mock_task, dict(unreachable=True)) self.assertTrue(tr.is_unreachable()) # test with multiple results but none unreachable mock_task.loop = 'foo' tr = TaskResult(mock_host, mock_task, dict(results=[dict(foo='bar'), dict(bam='baz'), True])) self.assertFalse(tr.is_unreachable()) # test with multiple results and one unreachable mock_task.loop = 'foo' tr = TaskResult(mock_host, mock_task, dict(results=[dict(unreachable=False), dict(unreachable=True), dict(some_key=False)])) self.assertTrue(tr.is_unreachable()) def test_task_result_is_failed(self): mock_host = MagicMock() mock_task = MagicMock() # test with no failed in result tr = TaskResult(mock_host, mock_task, dict()) self.assertFalse(tr.is_failed()) # test failed result with rc values tr = TaskResult(mock_host, mock_task, dict(rc=0)) self.assertFalse(tr.is_failed()) tr = TaskResult(mock_host, mock_task, dict(rc=1)) self.assertTrue(tr.is_failed()) # test with failed in result tr = TaskResult(mock_host, mock_task, dict(failed=True)) self.assertTrue(tr.is_failed()) # test with failed_when in result tr = TaskResult(mock_host, mock_task, dict(failed_when_result=True)) self.assertTrue(tr.is_failed())
from __future__ import absolute_import from bokeh.io import save from bokeh.models import Plot, Range1d, LinearAxis, Circle, Column, ColumnDataSource import pytest pytestmark = pytest.mark.integration HEIGHT = 600 WIDTH = 600 @pytest.mark.screenshot def test_the_default_titles_settings_and_ensure_outside_any_axes(output_file_url, selenium, screenshot): # Testing title rendering of background and border is covered in the # label test. The title added to plot as the primary title # should always be outside axes and other side renderers. source = ColumnDataSource(data=dict(x=[1, 2], y=[1, 2])) def make_plot(location, title_align, two_axes=True): plot = Plot( plot_width=400, plot_height=200, x_range=Range1d(0, 2), y_range=Range1d(0, 2), toolbar_location=None, title_location=location, ) plot.title.text = "Title %s - %s" % (location, title_align) plot.title.align = title_align plot.add_glyph(source, Circle(x='x', y='y', radius=0.4)) plot.add_layout(LinearAxis(), location) if two_axes: plot.add_layout(LinearAxis(), location) return plot layout = Column( make_plot('above', 'left', two_axes=False), # This is a workaround top doesn't like two axes make_plot('right', 'right'), make_plot('below', 'center'), make_plot('left', 'left') ) # Save the plot and start the test save(layout) selenium.get(output_file_url) # Take screenshot screenshot.assert_is_valid()
#!/usr/bin/env python # -*- coding: utf-8 -*- from runner.koan import * class AboutTuples(Koan): def test_creating_a_tuple(self): count_of_three = (1, 2, 5) self.assertEqual(__, count_of_three[2]) def test_tuples_are_immutable_so_item_assignment_is_not_possible(self): count_of_three = (1, 2, 5) try: count_of_three[2] = "three" except TypeError as ex: self.assertMatch(__, ex[0]) def test_tuples_are_immutable_so_appending_is_not_possible(self): count_of_three = (1, 2, 5) try: count_of_three.append("boom") except Exception as ex: self.assertEqual(AttributeError, type(ex)) # Note, assertMatch() uses regular expression pattern matching, # so you don't have to copy the whole message. self.assertMatch(__, ex[0]) # Tuples are less flexible than lists, but faster. def test_tuples_can_only_be_changed_through_replacement(self): count_of_three = (1, 2, 5) list_count = list(count_of_three) list_count.append("boom") count_of_three = tuple(list_count) self.assertEqual(__, count_of_three) def test_tuples_of_one_look_peculiar(self): self.assertEqual(__, (1).__class__) self.assertEqual(__, (1,).__class__) self.assertEqual(__, ("Hello comma!", )) def test_tuple_constructor_can_be_surprising(self): self.assertEqual(__, tuple("Surprise!")) def test_creating_empty_tuples(self): self.assertEqual(__, ()) self.assertEqual(__, tuple()) # Sometimes less confusing def test_tuples_can_be_embedded(self): lat = (37, 14, 6, 'N') lon = (115, 48, 40, 'W') place = ('Area 51', lat, lon) self.assertEqual(__, place) def test_tuples_are_good_for_representing_records(self): locations = [ ("Illuminati HQ", (38, 52, 15.56, 'N'), (77, 3, 21.46, 'W')), ("Stargate B", (41, 10, 43.92, 'N'), (1, 49, 34.29, 'W')), ] locations.append( ("Cthulhu", (26, 40, 1, 'N'), (70, 45, 7, 'W')) ) self.assertEqual(__, locations[2][0]) self.assertEqual(__, locations[0][1][2])
import threading class FtpCoord: shutdown = None lock = None stats = {} def __init__(self): self.shutdown = False self.lock = threading.Lock() def kill(self): print("raising shutdown") self.shutdown = True def need_to_stop(self): return self.shutdown def update_stats(self, filepath, size, status, elapsed): with self.lock: self.stats[filepath] = {'size':size, 'status' : status, 'elapsed' :elapsed} def show_stats(self): xferred = 0 resumed = 0 failed = 0 already = 0 elapsed = 0 size = 0 with self.lock: for k, v in self.stats.items(): if v['status'] == 'xferred': xferred += 1 elif v['status'] == 'resumed': resumed += 1 elif v['status'] == 'failed': print(k) failed += 1 elif v['status'] == 'already': already += 1 elapsed += v['elapsed'] size += v['size'] print("xferred: " + str(xferred)) print("resumed: " + str(resumed)) print("failed: " + str(failed)) print("already: " + str(already)) print("elapsed: " + str(elapsed)) print("size: " + str(size)) if size > 0 and elapsed > 0: print("bandwith: " + str((size/elapsed)/1024) + " KiB/s")
__author__ = 'xiaoxiaol' __author__ = 'xiaoxiaol' # run standardize swc to make sure swc files have one single root, and sorted, and has the valide type id ( 1~4) import matplotlib.pyplot as plt import seaborn as sb import os import os.path as path import numpy as np import pandas as pd import platform import sys import glob if (platform.system() == "Linux"): WORK_PATH = "/local1/xiaoxiaol/work" else: WORK_PATH = "/Users/xiaoxiaoliu/work" p = WORK_PATH + '/src/morphology_analysis' sys.path.append(p) import bigneuron.recon_prescreening as rp import bigneuron.plot_distances as plt_dist import blast_neuron.blast_neuron_comp as bn ### main data_DIR = "/data/mat/xiaoxiaol/data/big_neuron/silver/0401_gold163_all_soma_sort" output_dir = data_DIR #run_consensus(data_DIR, output_dir) os.system("rm "+data_DIR+"/qsub2/*.qsub") os.system("rm "+data_DIR+"/qsub2/*.o*") for item in os.listdir(data_DIR): folder_name = os.path.join(data_DIR, item) if os.path.isdir(folder_name): print folder_name imagefile = glob.glob(folder_name+'/*.v3dpbd') imagefile.extend(glob.glob(folder_name+'/*.v3draw')) files =glob.glob(folder_name+'/*.strict.swc') if len(files)>0 and len(imagefile)>0: gs_swc_file =files[0] if not os.path.exists(gs_swc_file+".out.swc"): bn.estimate_radius(input_image=imagefile[0], input_swc_path=gs_swc_file,bg_th=40, GEN_QSUB = 0, qsub_script_dir= output_dir+"/qsub2", id=None)
""" ======================================= Clustering text documents using k-means ======================================= This is an example showing how the scikit-learn can be used to cluster documents by topics using a bag-of-words approach. This example uses a scipy.sparse matrix to store the features instead of standard numpy arrays. Two feature extraction methods can be used in this example: - TfidfVectorizer uses a in-memory vocabulary (a python dict) to map the most frequent words to features indices and hence compute a word occurrence frequency (sparse) matrix. The word frequencies are then reweighted using the Inverse Document Frequency (IDF) vector collected feature-wise over the corpus. - HashingVectorizer hashes word occurrences to a fixed dimensional space, possibly with collisions. The word count vectors are then normalized to each have l2-norm equal to one (projected to the euclidean unit-ball) which seems to be important for k-means to work in high dimensional space. HashingVectorizer does not provide IDF weighting as this is a stateless model (the fit method does nothing). When IDF weighting is needed it can be added by pipelining its output to a TfidfTransformer instance. Two algorithms are demoed: ordinary k-means and its more scalable cousin minibatch k-means. Additionally, latent sematic analysis can also be used to reduce dimensionality and discover latent patterns in the data. It can be noted that k-means (and minibatch k-means) are very sensitive to feature scaling and that in this case the IDF weighting helps improve the quality of the clustering by quite a lot as measured against the "ground truth" provided by the class label assignments of the 20 newsgroups dataset. This improvement is not visible in the Silhouette Coefficient which is small for both as this measure seem to suffer from the phenomenon called "Concentration of Measure" or "Curse of Dimensionality" for high dimensional datasets such as text data. Other measures such as V-measure and Adjusted Rand Index are information theoretic based evaluation scores: as they are only based on cluster assignments rather than distances, hence not affected by the curse of dimensionality. Note: as k-means is optimizing a non-convex objective function, it will likely end up in a local optimum. Several runs with independent random init might be necessary to get a good convergence. """ # Author: Peter Prettenhofer <peter.prettenhofer@gmail.com> # Lars Buitinck <L.J.Buitinck@uva.nl> # License: BSD 3 clause from __future__ import print_function from sklearn.datasets import fetch_20newsgroups from sklearn.decomposition import TruncatedSVD from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.feature_extraction.text import HashingVectorizer from sklearn.feature_extraction.text import TfidfTransformer from sklearn.pipeline import make_pipeline from sklearn.preprocessing import Normalizer from sklearn import metrics from sklearn.cluster import KMeans, MiniBatchKMeans import logging from optparse import OptionParser import sys from time import time import numpy as np # Display progress logs on stdout logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s') # parse commandline arguments op = OptionParser() op.add_option("--lsa", dest="n_components", type="int", help="Preprocess documents with latent semantic analysis.") op.add_option("--no-minibatch", action="store_false", dest="minibatch", default=True, help="Use ordinary k-means algorithm (in batch mode).") op.add_option("--no-idf", action="store_false", dest="use_idf", default=True, help="Disable Inverse Document Frequency feature weighting.") op.add_option("--use-hashing", action="store_true", default=False, help="Use a hashing feature vectorizer") op.add_option("--n-features", type=int, default=10000, help="Maximum number of features (dimensions)" " to extract from text.") op.add_option("--verbose", action="store_true", dest="verbose", default=False, help="Print progress reports inside k-means algorithm.") print(__doc__) op.print_help() (opts, args) = op.parse_args() if len(args) > 0: op.error("this script takes no arguments.") sys.exit(1) ############################################################################### # Load some categories from the training set categories = [ 'alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space', ] # Uncomment the following to do the analysis on all the categories #categories = None print("Loading 20 newsgroups dataset for categories:") print(categories) dataset = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42) print("%d documents" % len(dataset.data)) print("%d categories" % len(dataset.target_names)) print() labels = dataset.target true_k = np.unique(labels).shape[0] print("Extracting features from the training dataset using a sparse vectorizer") t0 = time() if opts.use_hashing: if opts.use_idf: # Perform an IDF normalization on the output of HashingVectorizer hasher = HashingVectorizer(n_features=opts.n_features, stop_words='english', non_negative=True, norm=None, binary=False) vectorizer = make_pipeline(hasher, TfidfTransformer()) else: vectorizer = HashingVectorizer(n_features=opts.n_features, stop_words='english', non_negative=False, norm='l2', binary=False) else: vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features, min_df=2, stop_words='english', use_idf=opts.use_idf) X = vectorizer.fit_transform(dataset.data) print("done in %fs" % (time() - t0)) print("n_samples: %d, n_features: %d" % X.shape) print() if opts.n_components: print("Performing dimensionality reduction using LSA") t0 = time() # Vectorizer results are normalized, which makes KMeans behave as # spherical k-means for better results. Since LSA/SVD results are # not normalized, we have to redo the normalization. svd = TruncatedSVD(opts.n_components) normalizer = Normalizer(copy=False) lsa = make_pipeline(svd, normalizer) X = lsa.fit_transform(X) print("done in %fs" % (time() - t0)) explained_variance = svd.explained_variance_ratio_.sum() print("Explained variance of the SVD step: {}%".format( int(explained_variance * 100))) print() ############################################################################### # Do the actual clustering if opts.minibatch: km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1, init_size=1000, batch_size=1000, verbose=opts.verbose) else: km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1, verbose=opts.verbose) print("Clustering sparse data with %s" % km) t0 = time() km.fit(X) print("done in %0.3fs" % (time() - t0)) print() print("Homogeneity: %0.3f" % metrics.homogeneity_score(labels, km.labels_)) print("Completeness: %0.3f" % metrics.completeness_score(labels, km.labels_)) print("V-measure: %0.3f" % metrics.v_measure_score(labels, km.labels_)) print("Adjusted Rand-Index: %.3f" % metrics.adjusted_rand_score(labels, km.labels_)) print("Silhouette Coefficient: %0.3f" % metrics.silhouette_score(X, km.labels_, sample_size=1000)) print() if not opts.use_hashing: print("Top terms per cluster:") if opts.n_components: original_space_centroids = svd.inverse_transform(km.cluster_centers_) order_centroids = original_space_centroids.argsort()[:, ::-1] else: order_centroids = km.cluster_centers_.argsort()[:, ::-1] terms = vectorizer.get_feature_names() for i in range(true_k): print("Cluster %d:" % i, end='') for ind in order_centroids[i, :10]: print(' %s' % terms[ind], end='') print()
import sys import pickle import os import getopt from time import ctime import numpy as np usage = ''' USAGE: python xlsearch_train.py -l [path to xlsearch library] -p [parameter file] -o [output file]''' (pairs, args) = getopt.getopt(sys.argv[1:], 'l:p:o:') cmd_arg = dict() for i in range(len(pairs)): cmd_arg[pairs[i][0]] = pairs[i][1] if len(cmd_arg) != 3: print usage sys.exit(1) lib_path = cmd_arg['-l'] param_file = cmd_arg['-p'] output_file = cmd_arg['-o'] sys.path.append(lib_path) from utility import * from index import EnumIndexBuilder from fastareader import FastaReader print 'XLSearch, version 1.0' print 'Copyright of School of Informatics and Computing, Indiana University' print 'Current time %s' % ctime() print 'Training logistic regression models using authetic true-true PSMs...' print '\nReading paramters from: %s...' % param_file [param, mass] = read_param(param_file) param['ntermxlink'] = False param['neutral_loss']['h2o_loss']['aa'] = set('DEST') param['neutral_loss']['nh3_loss']['aa'] = set('KNQR') param['neutral_loss']['h2o_gain']['aa'] = set() mass['C'] = 103.009184 print 'Reading parameters done!' print '\nReading MSMS spectra files from directory: %s...' % param['ms_data'] spec_dict = read_spec(param['ms_data'], param, mass) pickle.dump(spec_dict, file('spectra.pickle', 'w')) print 'Total number of spectra: %d' % len(spec_dict) print 'Reading MSMS spectra files done!' print '\nDeisotoping MSMS spectra...' spec_dict = pickle.load(file('spectra.pickle')) deisotoped = dict() titles = spec_dict.keys() for i in range(len(titles)): title = titles[i] (one, align) = spec_dict[title].deisotope(mass, 4, 0.02) deisotoped[title] = one pickle.dump(deisotoped, file('deisotoped.pickle', 'w')) deisotoped = pickle.load(file('deisotoped.pickle')) spec_dict = deisotoped print 'Deisotoping MSMS spectra done!' print 'Current time %s' % ctime() print '\nBuilding index for all possible inter-peptide cross-links...' index = EnumIndexBuilder(param['target_database'], spec_dict, mass, param) pickle.dump(index, file('index.pickle', 'w')) index = pickle.load(file('index.pickle')) print 'Building index done!' print 'Current time %s' % ctime() print '\nComputing features for candidate PSMs for query spectra...' results = [] titles = [] for title in index.search_index.keys(): if len(index.search_index[title]) != 0: titles.append(title) length = len(titles) for i in range(0, length): print '%d / %d' % (i, length) sys.stdout.flush() title = titles[i] result = get_matches_per_spec(mass, param, index, title) result = [title, result] results.append(result) print 'Computing features done!\n' print 'Current time: %s' % ctime() pickle.dump(results, file('results.pickle', 'w')) results = pickle.load(file('results.pickle')) print 'Extracting authentic true-true PSMs...' true_true = get_true_true(results, index, param, mass) pickle.dump(true_true, file('TT.pickle', 'w')) print 'Extracting authentic true-true PSMs done!' print 'Extracting true-false PSMs based on true-true PSMs as seeds...' true_false = get_true_false(true_true, param, mass) pickle.dump(true_false, file('TF.pickle', 'w')) print 'Extracting true-false PSMs done!' print 'Extracting false-false PSMs based on true-true PSMs as seeds...' false_false = get_false_false(true_true, param, mass) pickle.dump(false_false, file('FF.pickle', 'w')) print 'Extracting false-false PSMs done!' print 'Computing feature matrix for true-true, true-false, false-false PSMs...' X_true_true = get_feature_matrix(true_true) X_true_false = get_feature_matrix(true_false) X_false_false = get_feature_matrix(false_false) X_TT_TF = np.concatenate((X_true_true, X_true_false), axis = 0) y_TT_TF = [] y_TT_TF.extend([1.0] * len(true_true)) y_TT_TF.extend([0.0] * len(true_false)) y_TT_TF = np.asarray(y_TT_TF) y_TT_TF = y_TT_TF.T X_TF_FF = np.concatenate((X_true_false, X_false_false), axis = 0) y_TF_FF = [] y_TF_FF.extend([1.0] * len(true_false)) y_TF_FF.extend([0.0] * len(false_false)) y_TF_FF = np.asarray(y_TF_FF) y_TF_FF = y_TF_FF.T print 'Computing features done!' from sklearn import linear_model log_reg = linear_model.LogisticRegression() log_reg.fit(X_TT_TF, y_TT_TF) model_TT_TF = [] model_TT_TF.extend(log_reg.intercept_.tolist()) model_TT_TF.extend(log_reg.coef_.tolist()) log_reg = linear_model.LogisticRegression() log_reg.fit(X_TF_FF, y_TF_FF) model_TF_FF = [] model_TF_FF.extend(log_reg.intercept_.tolist()) model_TF_FF.extend(log_reg.coef_.tolist()) f = open(output_file, 'w') f.write('# Classifier I (TT-TF) coefficients') for i in range(len(model_TT_TF)): f.write('CI%02d\t') f.write('%.60f\n' % model_TT_TF[i]) f.write('# Classifier II (TF-FF) coefficients') for i in range(len(model_TF_FF)): f.write('CII%02d\t') f.write('%.60f\n' % model_TF_FF[i]) f.write('nTT\t%d\n' % len(true_true)) f.write('nTF\t%d\n' % len(true_false)) f.write('nFF\t%d\n' % len(false_false)) f.close() print 'XLSearch train mode finished!'
# Copyright 2014 The Chromium Authors. All rights reserved. # Use of this source code is governed by a BSD-style license that can be # found in the LICENSE file. from telemetry.page import page as page_module from telemetry.page import page_set as page_set_module class ToughTextureUploadCasesPage(page_module.Page): def __init__(self, url, page_set): super( ToughTextureUploadCasesPage, self).__init__( url=url, page_set=page_set) def RunSmoothness(self, action_runner): interaction = action_runner.BeginGestureInteraction( 'ScrollAction', is_smooth=True) action_runner.ScrollPage() interaction.End() class ToughTextureUploadCasesPageSet(page_set_module.PageSet): """ Description: A collection of texture upload performance tests """ def __init__(self): super(ToughTextureUploadCasesPageSet, self).__init__() urls_list = [ 'file://tough_texture_upload_cases/background_color_animation.html', # pylint: disable=C0301 'file://tough_texture_upload_cases/background_color_animation_and_transform_animation.html', # pylint: disable=C0301 'file://tough_texture_upload_cases/background_color_animation_with_gradient.html', # pylint: disable=C0301 'file://tough_texture_upload_cases/background_color_animation_with_gradient_and_transform_animation.html'] for url in urls_list: self.AddPage(ToughTextureUploadCasesPage(url, self))
# -*- coding: utf-8 -*- ''' -------------------------------------------------------------------------------------- project_conf.py -------------------------------------------------------------------------------------- Configuration settings that detail EC2 instances. Note that we are not using the built-in env from fabric.api -- there are no official recommendations on best practice. See: http://lists.gnu.org/archive/html/fab-user/2013-11/msg00006.html ''' import os import os.path import pwd fabconf = {} # Do not edit fabconf['FAB_CONFIG_PATH'] = os.path.dirname(__file__) fabconf['FAB_HOSTS_FILE'] = fabconf.get('FAB_CONFIG_PATH') + '/hosts.txt' # Project name fabconf['PROJECT_NAME'] = os.environ.get('PROJECT_NAME', 'chime') fabconf['GIT_BRANCH'] = 'master' # Username for connecting to EC2 instaces - Do not edit unless you have a reason to fabconf['SERVER_USERNAME'] = 'ubuntu' # Don't edit. Full path of the ssh key you use to connect to EC2 instances fabconf['SSH_PRIVATE_KEY_PATH'] = os.environ.get('SSH_PRIVATE_KEY_PATH') # Where to install apps fabconf['APPS_DIR'] = "/home/{user}/web".format(user=fabconf.get('SERVER_USERNAME')) # Where your project will installed: /<APPS_DIR>/<PROJECT_NAME> fabconf['PROJECT_PATH'] = '{apps}/{project}'.format( apps=fabconf.get('APPS_DIR'), project=fabconf.get('PROJECT_NAME') ) # Space-delimited list of app domains fabconf['DOMAINS'] = os.environ.get('DOMAINS') # Name tag for your server instance on EC2 # Use recommendation from https://docs.python.org/2/library/os.html#os.getlogin # to get around ioctl error thrown by os.getlogin() in a cron job. fabconf['INSTANCE_NAME_TAG'] = os.environ.get('INSTANCE_NAME_TAG', 'ChimeCMS Autotest') fabconf['INSTANCE_CREATED_BY'] = '{}-{}'.format(pwd.getpwuid(os.getuid())[0], os.uname()[1]) # EC2 key. fabconf['AWS_ACCESS_KEY'] = os.environ['AWS_ACCESS_KEY'] # EC2 secret. fabconf['AWS_SECRET_KEY'] = os.environ['AWS_SECRET_KEY'] #EC2 region. Defaults to us-east-1 fabconf['EC2_REGION'] = os.environ.get('EC2_REGION', 'us-east-1') # AMI name. Either pass in a comma-delimited list of values. # Defaults to Ubuntu 14.04 fabconf['EC2_AMIS'] = os.environ.get('EC2_AMIS', 'ami-6725ea0c').split(',') # Name of the keypair you use in EC2. fabconf['EC2_KEY_PAIR'] = os.environ.get('EC2_KEY_PAIR', 'cfa-chime-keypair') # Name of the security group. fabconf['AWS_SECURITY_GROUPS'] = os.environ.get('AWS_SECURITY_GROUPS', 'default') # API Name of instance type. Defaults to t2.micro fabconf['EC2_INSTANCE_TYPE'] = os.environ.get('EC2_INSTANCE_TYPE', 't2.small') # Assorted other config (described in AcceptanceTests.md) used here to fail fast fabconf['TESTING_EMAIL'] = os.environ['TESTING_EMAIL'] fabconf['TESTING_PASSWORD'] = os.environ['TESTING_PASSWORD']
# -*- coding: utf-8 -*- # # This file is a part of Sirano. # # Copyright (C) 2015 HES-SO // HEIA-FR # Copyright (C) 2015 Loic Gremaud <loic.gremaud@grelinfo.ch> # # This program is free software; you can redistribute it and/or # modify it under the terms of the GNU General Public License # as published by the Free Software Foundation; either version 2 # of the License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program; if not, write to the Free Software # Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA. from sirano.action import Action class RTPPayloadAction(Action): """Anonymize the RTP payload content field""" name = "raw-payload" def __init__(self, app): super(RTPPayloadAction, self).__init__(app) def anonymize(self, value): value_len = len(value) text = "ANONYMIZED BY SIRANO " text_len = len(text) s = '' for i in range(value_len): s += text[i % text_len] return s def discover(self, value): pass
#!/usr/bin/python # # Copyright 2014 Google Inc. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. """This example gets custom targeting values for the given predefined custom targeting key. To create custom targeting values, run create_custom_targeting_keys_and_values.py. To determine which custom targeting keys exist, run get_all_custom_targeting_keys_and_values.py.""" __author__ = ('Nicholas Chen', 'Joseph DiLallo') # Import appropriate classes from the client library. from googleads import dfp CUSTOM_TARGETING_KEY_ID = 'INSERT_CUSTOM_TARGETING_KEY_ID_HERE' def main(client, key_id): # Initialize appropriate service. custom_targeting_service = client.GetService( 'CustomTargetingService', version='v201311') values = [{ 'key': 'keyId', 'value': { 'xsi_type': 'NumberValue', 'value': key_id } }] query = 'WHERE customTargetingKeyId = :keyId' statement = dfp.FilterStatement(query, values) # Get custom targeting values by statement. while True: response = custom_targeting_service.getCustomTargetingValuesByStatement( statement.ToStatement()) if 'results' in response: # Display results. for value in response['results']: print ('Custom targeting value with id \'%s\', name \'%s\', and display' ' name \'%s\' was found.' % (value['id'], value['name'], value['displayName'])) statement.offset += dfp.SUGGESTED_PAGE_LIMIT else: break print '\nNumber of results found: %s' % response['totalResultSetSize'] if __name__ == '__main__': # Initialize client object. dfp_client = dfp.DfpClient.LoadFromStorage() main(dfp_client, CUSTOM_TARGETING_KEY_ID)
# -*- coding: utf-8 -*- ############################################################################## # # OpenERP, Open Source Management Solution # Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>). # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## from openerp.osv import osv from openerp.tools.translate import _ class crm_phonecall2meeting(osv.osv_memory): """ Phonecall to Meeting """ _name = 'crm.phonecall2meeting' _description = 'Phonecall To Meeting' def action_cancel(self, cr, uid, ids, context=None): """ Closes Phonecall to Meeting form @param self: The object pointer @param cr: the current row, from the database cursor, @param uid: the current users ID for security checks, @param ids: List of Phonecall to Meeting IDs @param context: A standard dictionary for contextual values """ return {'type':'ir.actions.act_window_close'} def action_make_meeting(self, cr, uid, ids, context=None): """ This opens Meeting's calendar view to schedule meeting on current Phonecall @return : Dictionary value for created Meeting view """ res = {} phonecall_id = context and context.get('active_id', False) or False if phonecall_id: phonecall = self.pool.get('crm.phonecall').browse(cr, uid, phonecall_id, context) res = self.pool.get('ir.actions.act_window').for_xml_id(cr, uid, 'base_calendar', 'action_crm_meeting', context) res['context'] = { 'default_phonecall_id': phonecall.id, 'default_partner_id': phonecall.partner_id and phonecall.partner_id.id or False, 'default_user_id': uid, 'default_email_from': phonecall.email_from, 'default_state': 'open', 'default_name': phonecall.name, } return res crm_phonecall2meeting() # vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
#!/usr/bin/env python # Copyright (c) 2011 The Chromium Authors. All rights reserved. # Use of this source code is governed by a BSD-style license that can be # found in the LICENSE file. import re import pyauto_functional # Must be imported before pyauto import pyauto import test_utils class SearchEnginesTest(pyauto.PyUITest): """TestCase for Search Engines.""" _localhost_prefix = 'http://localhost:1000/' def _GetSearchEngineWithKeyword(self, keyword): """Get search engine info and return an element that matches keyword. Args: keyword: Search engine keyword field. Returns: A search engine info dict or None. """ match_list = ([x for x in self.GetSearchEngineInfo() if x['keyword'] == keyword]) if match_list: return match_list[0] return None def Debug(self): """Test method for experimentation. This method will not run automatically. """ while True: raw_input('Interact with the browser and hit <enter>') self.pprint(self.GetSearchEngineInfo()) def testDiscoverSearchEngine(self): """Test that chrome discovers youtube search engine after searching.""" # Take a snapshot of current search engine info. info = self.GetSearchEngineInfo() youtube = self._GetSearchEngineWithKeyword('youtube.com') self.assertFalse(youtube) # Use omnibox to invoke search engine discovery. # Navigating using NavigateToURL does not currently invoke this logic. self.SetOmniboxText('http://www.youtube.com') self.OmniboxAcceptInput() def InfoUpdated(old_info): new_info = self.GetSearchEngineInfo() if len(new_info) > len(old_info): return True return False self.WaitUntil(lambda: InfoUpdated(info)) youtube = self._GetSearchEngineWithKeyword('youtube.com') self.assertTrue(youtube) self.assertTrue(re.search('youtube', youtube['short_name'], re.IGNORECASE)) self.assertFalse(youtube['in_default_list']) self.assertFalse(youtube['is_default']) def testDeleteSearchEngine(self): """Test adding then deleting a search engine.""" self.AddSearchEngine(title='foo', keyword='foo.com', url='http://foo/?q=%s') foo = self._GetSearchEngineWithKeyword('foo.com') self.assertTrue(foo) self.DeleteSearchEngine('foo.com') foo = self._GetSearchEngineWithKeyword('foo.com') self.assertFalse(foo) def testMakeSearchEngineDefault(self): """Test adding then making a search engine default.""" self.AddSearchEngine( title='foo', keyword='foo.com', url=self._localhost_prefix + '?q=%s') foo = self._GetSearchEngineWithKeyword('foo.com') self.assertTrue(foo) self.assertFalse(foo['is_default']) self.MakeSearchEngineDefault('foo.com') foo = self._GetSearchEngineWithKeyword('foo.com') self.assertTrue(foo) self.assertTrue(foo['is_default']) self.SetOmniboxText('foobar') self.OmniboxAcceptInput() self.assertEqual(self._localhost_prefix + '?q=foobar', self.GetActiveTabURL().spec()) def testDefaultSearchEngines(self): """Test that we have 3 default search options.""" info = self.GetSearchEngineInfo() self.assertEqual(len(info), 3) # Verify that each can be used as the default search provider. default_providers = ['google.com', 'yahoo.com', 'bing.com'] for keyword in default_providers: self.MakeSearchEngineDefault(keyword) search_engine = self._GetSearchEngineWithKeyword(keyword) self.assertTrue(search_engine['is_default']) self.SetOmniboxText('test search') self.OmniboxAcceptInput() self.assertTrue(re.search(keyword, self.GetActiveTabURL().spec())) if __name__ == '__main__': pyauto_functional.Main()
from __future__ import unicode_literals import os from django.contrib.staticfiles import finders from django.core.management.base import LabelCommand from django.utils.encoding import force_text class Command(LabelCommand): help = "Finds the absolute paths for the given static file(s)." label = 'static file' def add_arguments(self, parser): super(Command, self).add_arguments(parser) parser.add_argument('--first', action='store_false', dest='all', default=True, help="Only return the first match for each static file.") def handle_label(self, path, **options): verbosity = options['verbosity'] result = finders.find(path, all=options['all']) path = force_text(path) if verbosity >= 2: searched_locations = ("Looking in the following locations:\n %s" % "\n ".join(force_text(location) for location in finders.searched_locations)) else: searched_locations = '' if result: if not isinstance(result, (list, tuple)): result = [result] result = (force_text(os.path.realpath(path)) for path in result) if verbosity >= 1: file_list = '\n '.join(result) return ("Found '%s' here:\n %s\n%s" % (path, file_list, searched_locations)) else: return '\n'.join(result) else: message = ["No matching file found for '%s'." % path] if verbosity >= 2: message.append(searched_locations) if verbosity >= 1: self.stderr.write('\n'.join(message))
"""Process http://www.otsys.com/clue/ DB for use with python.""" import collections import os import sqlite3 import sys # Add parent directory to path. sys.path.append(os.path.join( os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'src')) from data import crossword from data import data from puzzle.puzzlepedia import prod_config prod_config.init() STOP_WORD = '~' # Appears after z. MAX_KEYWORDS = 50 MIN_USAGES = 5 VISITED = set() ALL_SEEN = collections.defaultdict(int) def _prune_keywords(keywords): top = sorted( keywords.items(), key=lambda i: i[1], reverse=True )[:MAX_KEYWORDS] results = {} for keyword, count in top: ALL_SEEN[keyword] += count results[keyword] = count return results conn = crossword.init('data/crossword.sqlite') c = conn.cursor() def _insert(solution, usages, keywords): try: crossword.add(c, solution, usages, _prune_keywords(keywords)) except (sqlite3.OperationalError, sqlite3.IntegrityError): conn.commit() conn.close() raise last_solution = None keywords = collections.defaultdict(int) usages = 0 for i, line in enumerate( data.open_project_path('data/clues.txt', errors='ignore')): solution, unused_int, unused_year, unused_source, clue = line.lower().split( None, 4) if solution > STOP_WORD: print(line) break if solution in VISITED: print('Skipping %s' % line) continue if last_solution and solution != last_solution: if usages >= MIN_USAGES and keywords: _insert(last_solution, usages, keywords) VISITED.add(last_solution) keywords.clear() usages = 0 usages += 1 for keyword in crossword.clue_keywords(clue): keywords[keyword] += 1 last_solution = solution _insert(last_solution, usages, keywords) conn.commit() conn.close() print(_prune_keywords(ALL_SEEN))
# helper module for test_runner.Test_TextTestRunner.test_warnings """ This module has a number of tests that raise different kinds of warnings. When the tests are run, the warnings are caught and their messages are printed to stdout. This module also accepts an arg that is then passed to unittest.main to affect the behavior of warnings. Test_TextTestRunner.test_warnings executes this script with different combinations of warnings args and -W flags and check that the output is correct. See #10535. """ import sys import unittest import warnings def warnfun(): warnings.warn('rw', RuntimeWarning) class TestWarnings(unittest.TestCase): # unittest warnings will be printed at most once per type (max one message # for the fail* methods, and one for the assert* methods) def test_assert(self): self.assertEquals(2+2, 4) self.assertEquals(2*2, 4) self.assertEquals(2**2, 4) def test_fail(self): self.failUnless(1) self.failUnless(True) def test_other_unittest(self): self.assertAlmostEqual(2+2, 4) self.assertNotAlmostEqual(4+4, 2) # these warnings are normally silenced, but they are printed in unittest def test_deprecation(self): warnings.warn('dw', DeprecationWarning) warnings.warn('dw', DeprecationWarning) warnings.warn('dw', DeprecationWarning) def test_import(self): warnings.warn('iw', ImportWarning) warnings.warn('iw', ImportWarning) warnings.warn('iw', ImportWarning) # user warnings should always be printed def test_warning(self): warnings.warn('uw') warnings.warn('uw') warnings.warn('uw') # these warnings come from the same place; they will be printed # only once by default or three times if the 'always' filter is used def test_function(self): warnfun() warnfun() warnfun() if __name__ == '__main__': with warnings.catch_warnings(record=True) as ws: # if an arg is provided pass it to unittest.main as 'warnings' if len(sys.argv) == 2: unittest.main(exit=False, warnings=sys.argv.pop()) else: unittest.main(exit=False) # print all the warning messages collected for w in ws: print(w.message)
# # Analogue of `multiprocessing.connection` which uses queues instead of sockets # # multiprocessing/dummy/connection.py # # Copyright (c) 2006-2008, R Oudkerk # All rights reserved. # # Redistribution and use in source and binary forms, with or without # modification, are permitted provided that the following conditions # are met: # # 1. Redistributions of source code must retain the above copyright # notice, this list of conditions and the following disclaimer. # 2. Redistributions in binary form must reproduce the above copyright # notice, this list of conditions and the following disclaimer in the # documentation and/or other materials provided with the distribution. # 3. Neither the name of author nor the names of any contributors may be # used to endorse or promote products derived from this software # without specific prior written permission. # # THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS "AS IS" AND # ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE # ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE # FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL # DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS # OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) # HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT # LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY # OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF # SUCH DAMAGE. # __all__ = [ 'Client', 'Listener', 'Pipe' ] from queue import Queue families = [None] class Listener(object): def __init__(self, address=None, family=None, backlog=1): self._backlog_queue = Queue(backlog) def accept(self): return Connection(*self._backlog_queue.get()) def close(self): self._backlog_queue = None address = property(lambda self: self._backlog_queue) def __enter__(self): return self def __exit__(self, exc_type, exc_value, exc_tb): self.close() def Client(address): _in, _out = Queue(), Queue() address.put((_out, _in)) return Connection(_in, _out) def Pipe(duplex=True): a, b = Queue(), Queue() return Connection(a, b), Connection(b, a) class Connection(object): def __init__(self, _in, _out): self._out = _out self._in = _in self.send = self.send_bytes = _out.put self.recv = self.recv_bytes = _in.get def poll(self, timeout=0.0): if self._in.qsize() > 0: return True if timeout <= 0.0: return False self._in.not_empty.acquire() self._in.not_empty.wait(timeout) self._in.not_empty.release() return self._in.qsize() > 0 def close(self): pass def __enter__(self): return self def __exit__(self, exc_type, exc_value, exc_tb): self.close()
# # ElementTree # $Id: ElementPath.py 3375 2008-02-13 08:05:08Z fredrik $ # # limited xpath support for element trees # # history: # 2003-05-23 fl created # 2003-05-28 fl added support for // etc # 2003-08-27 fl fixed parsing of periods in element names # 2007-09-10 fl new selection engine # 2007-09-12 fl fixed parent selector # 2007-09-13 fl added iterfind; changed findall to return a list # 2007-11-30 fl added namespaces support # 2009-10-30 fl added child element value filter # # Copyright (c) 2003-2009 by Fredrik Lundh. All rights reserved. # # fredrik@pythonware.com # http://www.pythonware.com # # -------------------------------------------------------------------- # The ElementTree toolkit is # # Copyright (c) 1999-2009 by Fredrik Lundh # # By obtaining, using, and/or copying this software and/or its # associated documentation, you agree that you have read, understood, # and will comply with the following terms and conditions: # # Permission to use, copy, modify, and distribute this software and # its associated documentation for any purpose and without fee is # hereby granted, provided that the above copyright notice appears in # all copies, and that both that copyright notice and this permission # notice appear in supporting documentation, and that the name of # Secret Labs AB or the author not be used in advertising or publicity # pertaining to distribution of the software without specific, written # prior permission. # # SECRET LABS AB AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD # TO THIS SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANT- # ABILITY AND FITNESS. IN NO EVENT SHALL SECRET LABS AB OR THE AUTHOR # BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY # DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, # WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS # ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE # OF THIS SOFTWARE. # -------------------------------------------------------------------- # Licensed to PSF under a Contributor Agreement. # See http://www.python.org/psf/license for licensing details. ## # Implementation module for XPath support. There's usually no reason # to import this module directly; the <b>ElementTree</b> does this for # you, if needed. ## import re xpath_tokenizer_re = re.compile( "(" "'[^']*'|\"[^\"]*\"|" "::|" "//?|" "\.\.|" "\(\)|" "[/.*:\[\]\(\)@=])|" "((?:\{[^}]+\})?[^/\[\]\(\)@=\s]+)|" "\s+" ) def xpath_tokenizer(pattern, namespaces=None): for token in xpath_tokenizer_re.findall(pattern): tag = token[1] if tag and tag[0] != "{" and ":" in tag: try: prefix, uri = tag.split(":", 1) if not namespaces: raise KeyError yield token[0], "{%s}%s" % (namespaces[prefix], uri) except KeyError: raise SyntaxError("prefix %r not found in prefix map" % prefix) else: yield token def get_parent_map(context): parent_map = context.parent_map if parent_map is None: context.parent_map = parent_map = {} for p in context.root.iter(): for e in p: parent_map[e] = p return parent_map def prepare_child(next, token): tag = token[1] def select(context, result): for elem in result: for e in elem: if e.tag == tag: yield e return select def prepare_star(next, token): def select(context, result): for elem in result: for e in elem: yield e return select def prepare_self(next, token): def select(context, result): for elem in result: yield elem return select def prepare_descendant(next, token): token = next() if token[0] == "*": tag = "*" elif not token[0]: tag = token[1] else: raise SyntaxError("invalid descendant") def select(context, result): for elem in result: for e in elem.iter(tag): if e is not elem: yield e return select def prepare_parent(next, token): def select(context, result): # FIXME: raise error if .. is applied at toplevel? parent_map = get_parent_map(context) result_map = {} for elem in result: if elem in parent_map: parent = parent_map[elem] if parent not in result_map: result_map[parent] = None yield parent return select def prepare_predicate(next, token): # FIXME: replace with real parser!!! refs: # http://effbot.org/zone/simple-iterator-parser.htm # http://javascript.crockford.com/tdop/tdop.html signature = [] predicate = [] while 1: token = next() if token[0] == "]": break if token[0] and token[0][:1] in "'\"": token = "'", token[0][1:-1] signature.append(token[0] or "-") predicate.append(token[1]) signature = "".join(signature) # use signature to determine predicate type if signature == "@-": # [@attribute] predicate key = predicate[1] def select(context, result): for elem in result: if elem.get(key) is not None: yield elem return select if signature == "@-='": # [@attribute='value'] key = predicate[1] value = predicate[-1] def select(context, result): for elem in result: if elem.get(key) == value: yield elem return select if signature == "-" and not re.match("\d+$", predicate[0]): # [tag] tag = predicate[0] def select(context, result): for elem in result: if elem.find(tag) is not None: yield elem return select if signature == "-='" and not re.match("\d+$", predicate[0]): # [tag='value'] tag = predicate[0] value = predicate[-1] def select(context, result): for elem in result: for e in elem.findall(tag): if "".join(e.itertext()) == value: yield elem break return select if signature == "-" or signature == "-()" or signature == "-()-": # [index] or [last()] or [last()-index] if signature == "-": index = int(predicate[0]) - 1 else: if predicate[0] != "last": raise SyntaxError("unsupported function") if signature == "-()-": try: index = int(predicate[2]) - 1 except ValueError: raise SyntaxError("unsupported expression") else: index = -1 def select(context, result): parent_map = get_parent_map(context) for elem in result: try: parent = parent_map[elem] # FIXME: what if the selector is "*" ? elems = list(parent.findall(elem.tag)) if elems[index] is elem: yield elem except (IndexError, KeyError): pass return select raise SyntaxError("invalid predicate") ops = { "": prepare_child, "*": prepare_star, ".": prepare_self, "..": prepare_parent, "//": prepare_descendant, "[": prepare_predicate, } _cache = {} class _SelectorContext: parent_map = None def __init__(self, root): self.root = root # -------------------------------------------------------------------- ## # Generate all matching objects. def iterfind(elem, path, namespaces=None): # compile selector pattern if path[-1:] == "/": path = path + "*" # implicit all (FIXME: keep this?) try: selector = _cache[path] except KeyError: if len(_cache) > 100: _cache.clear() if path[:1] == "/": raise SyntaxError("cannot use absolute path on element") next = iter(xpath_tokenizer(path, namespaces)).__next__ token = next() selector = [] while 1: try: selector.append(ops[token[0]](next, token)) except StopIteration: raise SyntaxError("invalid path") try: token = next() if token[0] == "/": token = next() except StopIteration: break _cache[path] = selector # execute selector pattern result = [elem] context = _SelectorContext(elem) for select in selector: result = select(context, result) return result ## # Find first matching object. def find(elem, path, namespaces=None): try: return next(iterfind(elem, path, namespaces)) except StopIteration: return None ## # Find all matching objects. def findall(elem, path, namespaces=None): return list(iterfind(elem, path, namespaces)) ## # Find text for first matching object. def findtext(elem, path, default=None, namespaces=None): try: elem = next(iterfind(elem, path, namespaces)) return elem.text or "" except StopIteration: return default
#!/usr/bin/env python '''Add an Item to Pocket''' __author__ = 'Felipe Borges' import sys sys.path.append("..") import getopt import pocket USAGE = '''Usage: save_to_pocket [options] url This script adds an Item to Pocket. Options: -h --help: print this help --consumer_key : the Pocket API consumer key --access_token : the user's Pocket Access Token ''' def print_usage_and_exit(): print USAGE sys.exit(2) def main(): try: shortflags = 'h' longflags = ['help', 'consumer_key=', 'access_token='] opts, args = getopt.gnu_getopt(sys.argv[1:], shortflags, longflags) except getopt.GetoptError: print_usage_and_exit() consumer_key = None access_token = None for o, a in opts: if o in ('-h', '--help'): print_usage_and_exit() if o in ('--consumer_key'): consumer_key = a if o in ('--access_token'): access_token = a url = ' '.join(args) if not url or not consumer_key or not access_token: print_usage_and_exit() api = pocket.Api(consumer_key = consumer_key, access_token = access_token) try: item = api.add(url) print 'Item \'%s\' added successfuly!' % item.normal_url except e: print e sys.exit(2) if __name__ == "__main__": main()
############################################################################## # # Copyright (c) 2008-2011 Alistek Ltd (http://www.alistek.com) All Rights Reserved. # General contacts <info@alistek.com> # # WARNING: This program as such is intended to be used by professional # programmers who take the whole responsability of assessing all potential # consequences resulting from its eventual inadequacies and bugs # End users who are looking for a ready-to-use solution with commercial # garantees and support are strongly adviced to contract a Free Software # Service Company # # This program is Free Software; you can redistribute it and/or # modify it under the terms of the GNU General Public License # as published by the Free Software Foundation; either version 3 # of the License, or (at your option) any later version. # # This module is GPLv3 or newer and incompatible # with OpenERP SA "AGPL + Private Use License"! # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program; if not, write to the Free Software # Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA. # ############################################################################## from code128 import get_code from code39 import create_c39 from EANBarCode import EanBarCode try: from cStringIO import StringIO except ImportError: from StringIO import StringIO def make_barcode(code, code_type='ean13', rotate=None, height=50, xw=1): if code: if code_type.lower()=='ean13': bar=EanBarCode() im = bar.getImage(code,height) elif code_type.lower()=='code128': im = get_code(code, xw, height) elif code_type.lower()=='code39': im = create_c39(height, xw, code) else: return StringIO(), 'image/png' tf = StringIO() try: if rotate!=None: im=im.rotate(int(rotate)) except Exception, e: pass im.save(tf, 'png') size_x = str(im.size[0]/96.0)+'in' size_y = str(im.size[1]/96.0)+'in' return tf, 'image/png', size_x, size_y
#!/usr/bin/env python # Copyright (c) 2009-2010 Stanford University # # Permission to use, copy, modify, and distribute this software for any # purpose with or without fee is hereby granted, provided that the above # copyright notice and this permission notice appear in all copies. # # THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR(S) DISCLAIM ALL WARRANTIES # WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF # MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL AUTHORS BE LIABLE FOR # ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES # WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN # ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF # OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE. """Repeatedly execute bank transfers. WARNING: This file does not create a new client instance per worker process. Your client library needs a mutex in shared memory around the networking code. See RAM-39. This is a stress test for RAMCloud. Run this program with --help for usage.""" import os import sys import random import time from optparse import OptionParser import multiprocessing from retries import ImmediateRetry as RetryStrategy import ramcloud import txramcloud from testutil import BreakException txramcloud.RetryStrategy = RetryStrategy class Stats(object): INCREMENTS = 0 ABORTS = 1 CRASHES = 2 NUM = 3 LABELS = ['increments', 'aborts', 'crashes'] @classmethod def to_str(cls, stats): pairs = ["'%s': %d" % (l, v) for (l, v) in zip(cls.LABELS, stats)] return '{%s}' % ', '.join(pairs) class CountdownHook(object): def __init__(self, count): self.count = count def __call__(self): if self.count == 0: raise BreakException else: self.count -= 1 class Test(object): def __init__(self, txrc, table, oids, stats, die, options, args): self.txrc = txrc self.table = table self.oids = oids self.global_stats = stats self.die = die self.options = options self.args = args def __call__(self): # Called by the child in its address space # self.global_stats, self.die are in shared memory # detach the child from the parent's TTY os.setsid() if self.options.crash: when = random.randint(0, self.options.crash) self.txrc.hook = CountdownHook(when) self.local_stats = [0] * Stats.NUM self.cache = {} for oid in self.oids: self.cache[oid] = None i = 1 try: while True: if i % 10**6 == 0: print "PID %d: continuing after %s" % (os.getpid(), Stats.to_str(self.local_stats)) accts = self.choose_accts() for retry in RetryStrategy(): if die.value: print "PID %d: done after %s" % (os.getpid(), Stats.to_str(self.local_stats)) return try: for oid in accts: if self.cache[oid] is None: blob, version = self.txrc.read(self.table, oid) value = int(blob) self.cache[oid] = (value, version) if not self.algo(accts): retry.later() except BreakException: print "PID %d: crash after %s" % (os.getpid(), Stats.to_str(self.local_stats)) self.local_stats[Stats.CRASHES] += 1 for oid in self.cache: self.cache[oid] = None when = random.randint(0, self.options.crash) self.txrc.hook = CountdownHook(when) # and keep going i += 1 finally: # update global stats for i, v in enumerate(self.local_stats): self.global_stats[i] += v def choose_accts(self): assert len(self.oids) >= 2 max_num_accts = len(self.oids) if (self.options.max_num_accts_per_tx and self.options.max_num_accts_per_tx < max_num_accts): max_num_accts = self.options.max_num_accts_per_tx num_accts = random.randint(2, max_num_accts) accts = list(oids) random.shuffle(accts) accts = accts[:num_accts] return accts def algo(self, accts): mt = txramcloud.MiniTransaction() new_values = {} for oid in accts: value, version = self.cache[oid] rr = ramcloud.RejectRules.exactly(version) if oid == accts[0]: value -= len(accts[1:]) else: value += 1 new_values[oid] = value mt[(self.table, oid)] = txramcloud.MTWrite(str(value), rr) try: result = self.txrc.mt_commit(mt) except txramcloud.TxRAMCloud.TransactionRejected, e: for ((table, oid), reason) in e.reasons.items(): self.cache[oid] = None self.local_stats[Stats.ABORTS] += 1 return False except txramcloud.TxRAMCloud.TransactionExpired, e: self.local_stats[Stats.ABORTS] += 1 return False else: for ((table, oid), version) in result.items(): self.cache[oid] = (new_values[oid], version) self.local_stats[Stats.INCREMENTS] += 1 return True if __name__ == '__main__': parser = OptionParser() parser.set_description(__doc__.split('\n\n', 1)[0]) parser.add_option("-p", "--num-processes", dest="num_procs", type="int", default=1, help="spawn NUM processes, defaults to 1", metavar="NUM") parser.add_option("-o", "--num-objects", dest="num_objects", type="int", default=2, help=("increment across NUM objects, defaults to 2"), metavar="NUM") parser.add_option("-m", "--max-tx", dest="max_num_accts_per_tx", type="int", default=0, help=("the maximum NUM of accounts to involve in a " + "single transaction, defaults to infinity"), metavar="NUM") parser.add_option("-c", "--crash", dest="crash", type="int", default=0, help=("crash randomly by the NUM-th RAMCloud " "operation, defaults to not crashing"), metavar="NUM") (options, args) = parser.parse_args() assert not args r = txramcloud.TxRAMCloud(7) r.connect() r.create_table("test") table = r.get_table_id("test") oids = range(options.num_objects) for oid in oids: r.create(table, oid, str(0)) stats = multiprocessing.Array('i', Stats.NUM) die = multiprocessing.Value('i', 0, lock=False) target = Test(r, table, oids, stats, die, options, args) procs = [] for i in range(options.num_procs): procs.append(multiprocessing.Process(target=target)) start = time.time() for p in procs: p.start() try: for p in procs: p.join() except KeyboardInterrupt: # a process can be joined multiple times die.value = 1 for p in procs: p.join() end = time.time() print "wall time: %0.02fs" % (end - start) print "stats:", Stats.to_str(stats[:]) sum = 0 for oid in oids: blob, version = r.read(table, oid) value = int(blob) sum += value print 'oid %d: value=%d, version=%d' % (oid, value, version) print 'sum: %d' % sum assert sum == 0
# # Licensed to the Apache Software Foundation (ASF) under one # or more contributor license agreements. See the NOTICE file # distributed with this work for additional information # regarding copyright ownership. The ASF licenses this file # to you under the Apache License, Version 2.0 (the # "License"); you may not use this file except in compliance # with the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, # software distributed under the License is distributed on an # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY # KIND, either express or implied. See the License for the # specific language governing permissions and limitations # under the License. # from thrift.protocol import TBinaryProtocol from thrift.transport import TTransport def serialize(thrift_object, protocol_factory=TBinaryProtocol.TBinaryProtocolFactory()): transport = TTransport.TMemoryBuffer() protocol = protocol_factory.getProtocol(transport) thrift_object.write(protocol) return transport.getvalue() def deserialize(base, buf, protocol_factory=TBinaryProtocol.TBinaryProtocolFactory()): transport = TTransport.TMemoryBuffer(buf) protocol = protocol_factory.getProtocol(transport) base.read(protocol) return base
# Copyright 2016 Tesora, Inc. # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. # from datetime import date from oslo_log import log as logging from trove.common import cfg from trove.common.i18n import _ from trove.common import stream_codecs from trove.common import utils from trove.guestagent.common import operating_system from trove.guestagent.module.drivers import module_driver LOG = logging.getLogger(__name__) CONF = cfg.CONF NR_ADD_LICENSE_CMD = ['nrsysmond-config', '--set', 'license_key=%s'] NR_SRV_CONTROL_CMD = ['/etc/init.d/newrelic-sysmond'] class NewRelicLicenseDriver(module_driver.ModuleDriver): """Module to set up the license for the NewRelic service.""" def get_description(self): return "New Relic License Module Driver" def get_updated(self): return date(2016, 4, 12) @module_driver.output( log_message=_('Installing New Relic license key'), success_message=_('New Relic license key installed'), fail_message=_('New Relic license key not installed')) def apply(self, name, datastore, ds_version, data_file, admin_module): license_key = None data = operating_system.read_file( data_file, codec=stream_codecs.KeyValueCodec()) for key, value in data.items(): if 'license_key' == key.lower(): license_key = value break if license_key: self._add_license_key(license_key) self._server_control('start') else: return False, "'license_key' not found in contents file" def _add_license_key(self, license_key): try: exec_args = {'timeout': 10, 'run_as_root': True, 'root_helper': 'sudo'} cmd = list(NR_ADD_LICENSE_CMD) cmd[-1] = cmd[-1] % license_key utils.execute_with_timeout(*cmd, **exec_args) except Exception: LOG.exception(_("Could not install license key '%s'") % license_key) raise def _server_control(self, command): try: exec_args = {'timeout': 10, 'run_as_root': True, 'root_helper': 'sudo'} cmd = list(NR_SRV_CONTROL_CMD) cmd.append(command) utils.execute_with_timeout(*cmd, **exec_args) except Exception: LOG.exception(_("Could not %s New Relic server") % command) raise @module_driver.output( log_message=_('Removing New Relic license key'), success_message=_('New Relic license key removed'), fail_message=_('New Relic license key not removed')) def remove(self, name, datastore, ds_version, data_file): self._add_license_key("bad_key") self._server_control('stop')
# Copyright 2009 Google Inc. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. """Tricks that depend on a certain DNS provider. Tricks that require inheritence by nameserver.py must go here, otherwise, see providers.py for externally available functions. """ __author__ = 'tstromberg@google.com (Thomas Stromberg)' class NameServerProvider(object): """Inherited by nameserver.""" # myresolver.info def GetMyResolverIpWithDuration(self): return self.GetIpFromNameWithDuration('self.myresolver.info.') def GetMyResolverHostNameWithDuration(self): return self.GetNameFromNameWithDuration('self.myresolver.info.') # OpenDNS def GetOpenDnsNodeWithDuration(self): return self.GetTxtRecordWithDuration('which.opendns.com.')[0:2] def GetOpenDnsInterceptionStateWithDuration(self): """Check if our packets are actually getting to the correct servers.""" (node_id, duration) = self.GetOpenDnsNodeWithDuration() if node_id and 'I am not an OpenDNS resolver' in node_id: return (True, duration) return (False, duration) # UltraDNS def GetUltraDnsNodeWithDuration(self): return self.GetNameFromNameWithDuration('whoareyou.ultradns.net.')
#!/usr/bin/env python3 # # kdf_pbkdf2.py - (kdf_backend for btrbk) # # Copyright (c) 2017 Axel Burri # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # # --------------------------------------------------------------------- # The official btrbk website is located at: # https://digint.ch/btrbk/ # # Author: # Axel Burri <axel@tty0.ch> # --------------------------------------------------------------------- import sys import os import getpass import hashlib def passprompt(): pprompt = lambda: (getpass.getpass("Passphrase: "), getpass.getpass("Retype passphrase: ")) p1, p2 = pprompt() while p1 != p2: print("No match, please try again", file=sys.stderr) p1, p2 = pprompt() return p1 if len(sys.argv) <= 1: print("Usage: {} <dklen>".format(sys.argv[0]), file=sys.stderr) sys.exit(1) hash_name = "sha256" iterations = 300000 dklen = int(sys.argv[1]) salt = os.urandom(16) password = passprompt().encode("utf-8") dk = hashlib.pbkdf2_hmac(hash_name=hash_name, password=password, salt=salt, iterations=iterations, dklen=dklen) salt_hex = "".join(["{:02x}".format(x) for x in salt]) dk_hex = "".join(["{:02x}".format(x) for x in dk]) print("KEY=" + dk_hex); print("algoritm=pbkdf2_hmac"); print("hash_name=" + hash_name); print("salt=" + salt_hex); print("iterations=" + str(iterations));
# Copyright 2015 Internap. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. import logging import traceback from netman import regex import re class SubShell(object): debug = False def __init__(self, ssh, enter, exit_cmd, validate=None): self.ssh = ssh self.enter = enter self.exit = exit_cmd self.validate = validate or (lambda x: None) def __enter__(self): if isinstance(self.enter, list): [self.validate(self.ssh.do(cmd)) for cmd in self.enter] else: self.validate(self.ssh.do(self.enter)) return self.ssh def __exit__(self, eType, eValue, eTrace): if self.debug and eType is not None: logging.error("Subshell exception {}: {}\n{}" .format(eType.__name__, eValue, "".join(traceback.format_tb(eTrace)))) self.ssh.do(self.exit) def no_output(exc, *args): def m(welcome_msg): if len(welcome_msg) > 0: raise exc(*args) return m def split_on_bang(data): current_chunk = [] for line in data: if re.match("^!.*", line): if len(current_chunk) > 0: yield current_chunk current_chunk = [] else: current_chunk.append(line) def split_on_dedent(data): current_chunk = [] for line in data: if re.match("^[^\s].*", line) and len(current_chunk) > 0: yield current_chunk current_chunk = [line] else: current_chunk.append(line) yield current_chunk class ResultChecker(object): def __init__(self, result=None): self.result = result def on_any_result(self, exception, *args, **kwargs): if self.result and len(self.result) > 0: raise exception(*args, **kwargs) return self def on_result_matching(self, matcher, exception, *args, **kwargs): if regex.match(matcher, "\n".join(self.result), flags=re.DOTALL): raise exception(*args, **kwargs) return self class PageReader(object): def __init__(self, read_while, and_press, unless_prompt): self.next_page_indicator = read_while self.continue_key = and_press self.prompt = unless_prompt def do(self, shell, command): result = shell.do(command, wait_for=(self.next_page_indicator, self.prompt), include_last_line=True) while len(result) > 0 and self.next_page_indicator in result[-1]: result = result[:-1] + shell.send_key(self.continue_key, wait_for=(self.next_page_indicator, self.prompt), include_last_line=True) return result[:-1]
# -*- coding: utf-8 -*- ############################################################################## # # OpenERP, Open Source Management Solution # Copyright (C) 2004-2011 OpenERP S.A (<http://www.openerp.com>). # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## from openerp import SUPERUSER_ID from openerp.osv import osv from openerp.tools.translate import _ class mail_mail(osv.Model): """ Update of mail_mail class, to add the signin URL to notifications. """ _inherit = 'mail.mail' def _get_partner_access_link(self, cr, uid, mail, partner=None, context=None): """ Generate URLs for links in mails: - partner is not an user: signup_url - partner is an user: fallback on classic URL """ if context is None: context = {} partner_obj = self.pool.get('res.partner') if partner and not partner.user_ids: contex_signup = dict(context, signup_valid=True) signup_url = partner_obj._get_signup_url_for_action(cr, SUPERUSER_ID, [partner.id], action='mail.action_mail_redirect', model=mail.model, res_id=mail.res_id, context=contex_signup)[partner.id] return ", <span class='oe_mail_footer_access'><small>%(access_msg)s <a style='color:inherit' href='%(portal_link)s'>%(portal_msg)s</a></small></span>" % { 'access_msg': _('access directly to'), 'portal_link': signup_url, 'portal_msg': '%s %s' % (context.get('model_name', ''), mail.record_name) if mail.record_name else _('your messages '), } else: return super(mail_mail, self)._get_partner_access_link(cr, uid, mail, partner=partner, context=context)
#!/usr/bin/env python # Copyright (c) 2012 Google Inc. All rights reserved. # Use of this source code is governed by a BSD-style license that can be # found in the LICENSE file. """ Verifies simple actions when using an explicit build target of 'all'. """ import glob import os import TestGyp test = TestGyp.TestGyp(workdir='workarea_all') test.run_gyp('actions.gyp', chdir='src') test.relocate('src', 'relocate/src') # Some gyp files use an action that mentions an output but never # writes it as a means to making the action run on every build. That # doesn't mesh well with ninja's semantics. TODO(evan): figure out # how to work always-run actions in to ninja. # Android also can't do this as it doesn't have order-only dependencies. if test.format in ['ninja', 'android']: test.build('actions.gyp', test.ALL, chdir='relocate/src') else: # Test that an "always run" action increases a counter on multiple # invocations, and that a dependent action updates in step. test.build('actions.gyp', test.ALL, chdir='relocate/src') test.must_match('relocate/src/subdir1/actions-out/action-counter.txt', '1') test.must_match('relocate/src/subdir1/actions-out/action-counter_2.txt', '1') test.build('actions.gyp', test.ALL, chdir='relocate/src') test.must_match('relocate/src/subdir1/actions-out/action-counter.txt', '2') test.must_match('relocate/src/subdir1/actions-out/action-counter_2.txt', '2') # The "always run" action only counts to 2, but the dependent target # will count forever if it's allowed to run. This verifies that the # dependent target only runs when the "always run" action generates # new output, not just because the "always run" ran. test.build('actions.gyp', test.ALL, chdir='relocate/src') test.must_match('relocate/src/subdir1/actions-out/action-counter.txt', '2') test.must_match('relocate/src/subdir1/actions-out/action-counter_2.txt', '2') expect = """\ Hello from program.c Hello from make-prog1.py Hello from make-prog2.py """ if test.format == 'xcode': chdir = 'relocate/src/subdir1' else: chdir = 'relocate/src' test.run_built_executable('program', chdir=chdir, stdout=expect) test.must_match('relocate/src/subdir2/file.out', "Hello from make-file.py\n") expect = "Hello from generate_main.py\n" if test.format == 'xcode': chdir = 'relocate/src/subdir3' else: chdir = 'relocate/src' test.run_built_executable('null_input', chdir=chdir, stdout=expect) # Clean out files which may have been created if test.ALL was run. def clean_dep_files(): for file in (glob.glob('relocate/src/dep_*.txt') + glob.glob('relocate/src/deps_all_done_*.txt')): if os.path.exists(file): os.remove(file) # Confirm our clean. clean_dep_files() test.must_not_exist('relocate/src/dep_1.txt') test.must_not_exist('relocate/src/deps_all_done_first_123.txt') # Make sure all deps finish before an action is run on a 'None' target. # If using the Make builder, add -j to make things more difficult. arguments = [] if test.format == 'make': arguments = ['-j'] test.build('actions.gyp', 'action_with_dependencies_123', chdir='relocate/src', arguments=arguments) test.must_exist('relocate/src/deps_all_done_first_123.txt') # Try again with a target that has deps in reverse. Output files from # previous tests deleted. Confirm this execution did NOT run the ALL # target which would mess up our dep tests. clean_dep_files() test.build('actions.gyp', 'action_with_dependencies_321', chdir='relocate/src', arguments=arguments) test.must_exist('relocate/src/deps_all_done_first_321.txt') test.must_not_exist('relocate/src/deps_all_done_first_123.txt') test.pass_test()
# Copyright 2011 OpenStack Foundation. # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from oslo.config import cfg from openstack_dashboard.openstack.common import jsonutils from openstack_dashboard.openstack.common import log as logging CONF = cfg.CONF def notify(_context, message): """Notifies the recipient of the desired event given the model. Log notifications using openstack's default logging system. """ priority = message.get('priority', CONF.default_notification_level) priority = priority.lower() logger = logging.getLogger( 'openstack_dashboard.openstack.common.notification.%s' % message['event_type']) getattr(logger, priority)(jsonutils.dumps(message))
# encoding: utf-8 from __future__ import unicode_literals import re import json import xml.etree.ElementTree from .common import InfoExtractor from ..compat import ( compat_parse_qs, compat_str, compat_urllib_parse, compat_urllib_parse_urlparse, compat_urllib_request, compat_urlparse, compat_xml_parse_error, ) from ..utils import ( determine_ext, ExtractorError, find_xpath_attr, fix_xml_ampersands, unescapeHTML, unsmuggle_url, ) class BrightcoveIE(InfoExtractor): _VALID_URL = r'(?:https?://.*brightcove\.com/(services|viewer).*?\?|brightcove:)(?P<query>.*)' _FEDERATED_URL_TEMPLATE = 'http://c.brightcove.com/services/viewer/htmlFederated?%s' _TESTS = [ { # From http://www.8tv.cat/8aldia/videos/xavier-sala-i-martin-aquesta-tarda-a-8-al-dia/ 'url': 'http://c.brightcove.com/services/viewer/htmlFederated?playerID=1654948606001&flashID=myExperience&%40videoPlayer=2371591881001', 'md5': '5423e113865d26e40624dce2e4b45d95', 'note': 'Test Brightcove downloads and detection in GenericIE', 'info_dict': { 'id': '2371591881001', 'ext': 'mp4', 'title': 'Xavier Sala i Martn: Un banc que no presta s un banc zombi que no serveix per a res', 'uploader': '8TV', 'description': 'md5:a950cc4285c43e44d763d036710cd9cd', } }, { # From http://medianetwork.oracle.com/video/player/1785452137001 'url': 'http://c.brightcove.com/services/viewer/htmlFederated?playerID=1217746023001&flashID=myPlayer&%40videoPlayer=1785452137001', 'info_dict': { 'id': '1785452137001', 'ext': 'flv', 'title': 'JVMLS 2012: Arrays 2.0 - Opportunities and Challenges', 'description': 'John Rose speaks at the JVM Language Summit, August 1, 2012.', 'uploader': 'Oracle', }, }, { # From http://mashable.com/2013/10/26/thermoelectric-bracelet-lets-you-control-your-body-temperature/ 'url': 'http://c.brightcove.com/services/viewer/federated_f9?&playerID=1265504713001&publisherID=AQ%7E%7E%2CAAABBzUwv1E%7E%2CxP-xFHVUstiMFlNYfvF4G9yFnNaqCw_9&videoID=2750934548001', 'info_dict': { 'id': '2750934548001', 'ext': 'mp4', 'title': 'This Bracelet Acts as a Personal Thermostat', 'description': 'md5:547b78c64f4112766ccf4e151c20b6a0', 'uploader': 'Mashable', }, }, { # test that the default referer works # from http://national.ballet.ca/interact/video/Lost_in_Motion_II/ 'url': 'http://link.brightcove.com/services/player/bcpid756015033001?bckey=AQ~~,AAAApYJi_Ck~,GxhXCegT1Dp39ilhXuxMJxasUhVNZiil&bctid=2878862109001', 'info_dict': { 'id': '2878862109001', 'ext': 'mp4', 'title': 'Lost in Motion II', 'description': 'md5:363109c02998fee92ec02211bd8000df', 'uploader': 'National Ballet of Canada', }, }, { # test flv videos served by akamaihd.net # From http://www.redbull.com/en/bike/stories/1331655643987/replay-uci-dh-world-cup-2014-from-fort-william 'url': 'http://c.brightcove.com/services/viewer/htmlFederated?%40videoPlayer=ref%3ABC2996102916001&linkBaseURL=http%3A%2F%2Fwww.redbull.com%2Fen%2Fbike%2Fvideos%2F1331655630249%2Freplay-uci-fort-william-2014-dh&playerKey=AQ%7E%7E%2CAAAApYJ7UqE%7E%2Cxqr_zXk0I-zzNndy8NlHogrCb5QdyZRf&playerID=1398061561001#__youtubedl_smuggle=%7B%22Referer%22%3A+%22http%3A%2F%2Fwww.redbull.com%2Fen%2Fbike%2Fstories%2F1331655643987%2Freplay-uci-dh-world-cup-2014-from-fort-william%22%7D', # The md5 checksum changes on each download 'info_dict': { 'id': '2996102916001', 'ext': 'flv', 'title': 'UCI MTB World Cup 2014: Fort William, UK - Downhill Finals', 'uploader': 'Red Bull TV', 'description': 'UCI MTB World Cup 2014: Fort William, UK - Downhill Finals', }, }, { # playlist test # from http://support.brightcove.com/en/video-cloud/docs/playlist-support-single-video-players 'url': 'http://c.brightcove.com/services/viewer/htmlFederated?playerID=3550052898001&playerKey=AQ%7E%7E%2CAAABmA9XpXk%7E%2C-Kp7jNgisre1fG5OdqpAFUTcs0lP_ZoL', 'info_dict': { 'title': 'Sealife', 'id': '3550319591001', }, 'playlist_mincount': 7, }, ] @classmethod def _build_brighcove_url(cls, object_str): """ Build a Brightcove url from a xml string containing <object class="BrightcoveExperience">{params}</object> """ # Fix up some stupid HTML, see https://github.com/rg3/youtube-dl/issues/1553 object_str = re.sub(r'(<param(?:\s+[a-zA-Z0-9_]+="[^"]*")*)>', lambda m: m.group(1) + '/>', object_str) # Fix up some stupid XML, see https://github.com/rg3/youtube-dl/issues/1608 object_str = object_str.replace('<--', '<!--') # remove namespace to simplify extraction object_str = re.sub(r'(<object[^>]*)(xmlns=".*?")', r'\1', object_str) object_str = fix_xml_ampersands(object_str) try: object_doc = xml.etree.ElementTree.fromstring(object_str.encode('utf-8')) except compat_xml_parse_error: return fv_el = find_xpath_attr(object_doc, './param', 'name', 'flashVars') if fv_el is not None: flashvars = dict( (k, v[0]) for k, v in compat_parse_qs(fv_el.attrib['value']).items()) else: flashvars = {} def find_param(name): if name in flashvars: return flashvars[name] node = find_xpath_attr(object_doc, './param', 'name', name) if node is not None: return node.attrib['value'] return None params = {} playerID = find_param('playerID') if playerID is None: raise ExtractorError('Cannot find player ID') params['playerID'] = playerID playerKey = find_param('playerKey') # Not all pages define this value if playerKey is not None: params['playerKey'] = playerKey # The three fields hold the id of the video videoPlayer = find_param('@videoPlayer') or find_param('videoId') or find_param('videoID') if videoPlayer is not None: params['@videoPlayer'] = videoPlayer linkBase = find_param('linkBaseURL') if linkBase is not None: params['linkBaseURL'] = linkBase return cls._make_brightcove_url(params) @classmethod def _build_brighcove_url_from_js(cls, object_js): # The layout of JS is as follows: # customBC.createVideo = function (width, height, playerID, playerKey, videoPlayer, VideoRandomID) { # // build Brightcove <object /> XML # } m = re.search( r'''(?x)customBC.\createVideo\( .*? # skipping width and height ["\'](?P<playerID>\d+)["\']\s*,\s* # playerID ["\'](?P<playerKey>AQ[^"\']{48})[^"\']*["\']\s*,\s* # playerKey begins with AQ and is 50 characters # in length, however it's appended to itself # in places, so truncate ["\'](?P<videoID>\d+)["\'] # @videoPlayer ''', object_js) if m: return cls._make_brightcove_url(m.groupdict()) @classmethod def _make_brightcove_url(cls, params): data = compat_urllib_parse.urlencode(params) return cls._FEDERATED_URL_TEMPLATE % data @classmethod def _extract_brightcove_url(cls, webpage): """Try to extract the brightcove url from the webpage, returns None if it can't be found """ urls = cls._extract_brightcove_urls(webpage) return urls[0] if urls else None @classmethod def _extract_brightcove_urls(cls, webpage): """Return a list of all Brightcove URLs from the webpage """ url_m = re.search( r'<meta\s+property=[\'"]og:video[\'"]\s+content=[\'"](https?://(?:secure|c)\.brightcove.com/[^\'"]+)[\'"]', webpage) if url_m: url = unescapeHTML(url_m.group(1)) # Some sites don't add it, we can't download with this url, for example: # http://www.ktvu.com/videos/news/raw-video-caltrain-releases-video-of-man-almost/vCTZdY/ if 'playerKey' in url or 'videoId' in url: return [url] matches = re.findall( r'''(?sx)<object (?: [^>]+?class=[\'"][^>]*?BrightcoveExperience.*?[\'"] | [^>]*?>\s*<param\s+name="movie"\s+value="https?://[^/]*brightcove\.com/ ).+?>\s*</object>''', webpage) if matches: return list(filter(None, [cls._build_brighcove_url(m) for m in matches])) return list(filter(None, [ cls._build_brighcove_url_from_js(custom_bc) for custom_bc in re.findall(r'(customBC\.createVideo\(.+?\);)', webpage)])) def _real_extract(self, url): url, smuggled_data = unsmuggle_url(url, {}) # Change the 'videoId' and others field to '@videoPlayer' url = re.sub(r'(?<=[?&])(videoI(d|D)|bctid)', '%40videoPlayer', url) # Change bckey (used by bcove.me urls) to playerKey url = re.sub(r'(?<=[?&])bckey', 'playerKey', url) mobj = re.match(self._VALID_URL, url) query_str = mobj.group('query') query = compat_urlparse.parse_qs(query_str) videoPlayer = query.get('@videoPlayer') if videoPlayer: # We set the original url as the default 'Referer' header referer = smuggled_data.get('Referer', url) return self._get_video_info( videoPlayer[0], query_str, query, referer=referer) elif 'playerKey' in query: player_key = query['playerKey'] return self._get_playlist_info(player_key[0]) else: raise ExtractorError( 'Cannot find playerKey= variable. Did you forget quotes in a shell invocation?', expected=True) def _get_video_info(self, video_id, query_str, query, referer=None): request_url = self._FEDERATED_URL_TEMPLATE % query_str req = compat_urllib_request.Request(request_url) linkBase = query.get('linkBaseURL') if linkBase is not None: referer = linkBase[0] if referer is not None: req.add_header('Referer', referer) webpage = self._download_webpage(req, video_id) error_msg = self._html_search_regex( r"<h1>We're sorry.</h1>([\s\n]*<p>.*?</p>)+", webpage, 'error message', default=None) if error_msg is not None: raise ExtractorError( 'brightcove said: %s' % error_msg, expected=True) self.report_extraction(video_id) info = self._search_regex(r'var experienceJSON = ({.*});', webpage, 'json') info = json.loads(info)['data'] video_info = info['programmedContent']['videoPlayer']['mediaDTO'] video_info['_youtubedl_adServerURL'] = info.get('adServerURL') return self._extract_video_info(video_info) def _get_playlist_info(self, player_key): info_url = 'http://c.brightcove.com/services/json/experience/runtime/?command=get_programming_for_experience&playerKey=%s' % player_key playlist_info = self._download_webpage( info_url, player_key, 'Downloading playlist information') json_data = json.loads(playlist_info) if 'videoList' not in json_data: raise ExtractorError('Empty playlist') playlist_info = json_data['videoList'] videos = [self._extract_video_info(video_info) for video_info in playlist_info['mediaCollectionDTO']['videoDTOs']] return self.playlist_result(videos, playlist_id='%s' % playlist_info['id'], playlist_title=playlist_info['mediaCollectionDTO']['displayName']) def _extract_video_info(self, video_info): info = { 'id': compat_str(video_info['id']), 'title': video_info['displayName'].strip(), 'description': video_info.get('shortDescription'), 'thumbnail': video_info.get('videoStillURL') or video_info.get('thumbnailURL'), 'uploader': video_info.get('publisherName'), } renditions = video_info.get('renditions') if renditions: formats = [] for rend in renditions: url = rend['defaultURL'] if not url: continue ext = None if rend['remote']: url_comp = compat_urllib_parse_urlparse(url) if url_comp.path.endswith('.m3u8'): formats.extend( self._extract_m3u8_formats(url, info['id'], 'mp4')) continue elif 'akamaihd.net' in url_comp.netloc: # This type of renditions are served through # akamaihd.net, but they don't use f4m manifests url = url.replace('control/', '') + '?&v=3.3.0&fp=13&r=FEEFJ&g=RTSJIMBMPFPB' ext = 'flv' if ext is None: ext = determine_ext(url) size = rend.get('size') formats.append({ 'url': url, 'ext': ext, 'height': rend.get('frameHeight'), 'width': rend.get('frameWidth'), 'filesize': size if size != 0 else None, }) self._sort_formats(formats) info['formats'] = formats elif video_info.get('FLVFullLengthURL') is not None: info.update({ 'url': video_info['FLVFullLengthURL'], }) if self._downloader.params.get('include_ads', False): adServerURL = video_info.get('_youtubedl_adServerURL') if adServerURL: ad_info = { '_type': 'url', 'url': adServerURL, } if 'url' in info: return { '_type': 'playlist', 'title': info['title'], 'entries': [ad_info, info], } else: return ad_info if 'url' not in info and not info.get('formats'): raise ExtractorError('Unable to extract video url for %s' % info['id']) return info
""" Optional fixer to transform set() calls to set literals. """ # Author: Benjamin Peterson from lib2to3 import fixer_base, pytree from lib2to3.fixer_util import token, syms class FixSetLiteral(fixer_base.BaseFix): BM_compatible = True explicit = True PATTERN = """power< 'set' trailer< '(' (atom=atom< '[' (items=listmaker< any ((',' any)* [',']) > | single=any) ']' > | atom< '(' items=testlist_gexp< any ((',' any)* [',']) > ')' > ) ')' > > """ def transform(self, node, results): single = results.get("single") if single: # Make a fake listmaker fake = pytree.Node(syms.listmaker, [single.clone()]) single.replace(fake) items = fake else: items = results["items"] # Build the contents of the literal literal = [pytree.Leaf(token.LBRACE, u"{")] literal.extend(n.clone() for n in items.children) literal.append(pytree.Leaf(token.RBRACE, u"}")) # Set the prefix of the right brace to that of the ')' or ']' literal[-1].prefix = items.next_sibling.prefix maker = pytree.Node(syms.dictsetmaker, literal) maker.prefix = node.prefix # If the original was a one tuple, we need to remove the extra comma. if len(maker.children) == 4: n = maker.children[2] n.remove() maker.children[-1].prefix = n.prefix # Finally, replace the set call with our shiny new literal. return maker
from __future__ import print_function, division # do not edit! added by PythonBreakpoints from pdb import set_trace as _breakpoint class Key(object): """A location of data or metadata within NILMTK. Attributes ---------- building : int meter : int utility : str """ def __init__(self, string=None, building=None, meter=None): """ Parameters ---------- string : str, optional e.g. 'building1/elec/meter1' building : int, optional meter : int, optional """ self.utility = None if string is None: self.building = building self.meter = meter else: split = string.strip('/').split('/') assert split[0].startswith('building'), "The first element must be 'building<I>', e.g. 'building1'; not '{}'.".format(split[0]) try: self.building = int(split[0].replace("building", "")) except ValueError as e: raise ValueError("'building' must be followed by an integer.\n{}" .format(e)) if len(split) > 1: self.utility = split[1] if len(split) == 3: assert split[2].startswith('meter') self.meter = int(split[-1].replace("meter", "")) else: self.meter = None self._check() def _check(self): assert isinstance(self.building, int) assert self.building >= 1 if self.meter is not None: assert isinstance(self.meter, int) assert self.meter >= 1 def __repr__(self): self._check() s = "/building{:d}".format(self.building) if self.meter is not None: s += "/elec/meter{:d}".format(self.meter) return s
# # Copyright (c) 2011 Red Hat, Inc. # # This software is licensed to you under the GNU Lesser General Public # License as published by the Free Software Foundation; either version # 2 of the License (LGPLv2) or (at your option) any later version. # There is NO WARRANTY for this software, express or implied, # including the implied warranties of MERCHANTABILITY, # NON-INFRINGEMENT, or FITNESS FOR A PARTICULAR PURPOSE. You should # have received a copy of LGPLv2 along with this software; if not, see # http://www.gnu.org/licenses/old-licenses/lgpl-2.0.txt. # # Jeff Ortel <jortel@redhat.com> # from gofer import NAME, Singleton from gofer.config import Config, Graph from gofer.config import REQUIRED, OPTIONAL, ANY, BOOL, NUMBER # # [management] # enabled # The manager is (enabled|disabled). # host # Host (name or IP) the manager listens on. # port # The port number the manager listens on. # # [logging] # <module> # Logging level # # [pam] # service # The default PAM service for authentication. Default:passwd # AGENT_SCHEMA = ( ('management', REQUIRED, ( ('enabled', OPTIONAL, BOOL), ('host', OPTIONAL, ANY), ('port', OPTIONAL, NUMBER), ) ), ('logging', REQUIRED, [] ), ('pam', REQUIRED, ( ('service', OPTIONAL, ANY), ) ), ) # # [main] # # enabled # Plugin enabled/disabled (0|1) # name # The (optional) plugin name. The basename of the descriptor is used when not specified. # plugin # The (optional) fully qualified module to be loaded from the PYTHON path. # threads # The (optional) number of threads for the RMI dispatcher. # accept # Accept forwarding from. A comma (,) separated list of plugin names (,=none|*=all). # forward # Forward to. A comma (,) separated list of plugin names (,=none|*=all). # # [messaging] # # uuid # The (optional) agent identity. This value also specifies the queue name. # url # The (optional) broker connection URL. # cacert # The (optional) SSL CA certificate used to validate the server certificate. # clientcert # The (optional) SSL client certificate. PEM encoded and contains both key and certificate. # host_validation # The (optional) flag indicates SSL host validation should be performed. # authenticator # The (optional) fully qualified Authenticator to be loaded from the PYTHON path. # # [model] # # managed # The (optional) level of broker model management. Default: 2. # - 0 = none # - 1 = declare and bind queue. # - 2 = declare and bind queue; drain and delete queue on explicit detach. # queue # The (optional) AMQP queue name. This has precedent over uuid. # Format: <exchange>/<queue> where *exchange* is optional. # expiration # The (optional) auto-deleted queue expiration (seconds). # PLUGIN_SCHEMA = ( ('main', REQUIRED, ( ('enabled', REQUIRED, BOOL), ('name', OPTIONAL, ANY), ('plugin', OPTIONAL, ANY), ('threads', OPTIONAL, NUMBER), ('accept', OPTIONAL, ANY), ('forward', OPTIONAL, ANY), ) ), ('messaging', REQUIRED, ( ('url', OPTIONAL, ANY), ('uuid', OPTIONAL, ANY), ('cacert', OPTIONAL, ANY), ('clientcert', OPTIONAL, ANY), ('clientkey', OPTIONAL, ANY), ('host_validation', OPTIONAL, BOOL), ('authenticator', OPTIONAL, ANY), ) ), ('model', OPTIONAL, ( ('managed', OPTIONAL, '(0|1|2)'), ('queue', OPTIONAL, ANY), ('expiration', OPTIONAL, NUMBER) ) ), ) AGENT_DEFAULTS = { 'management': { 'enabled': '0', 'host': 'localhost', 'port': '5650', }, 'logging': { }, 'pam': { 'service': 'passwd' } } PLUGIN_DEFAULTS = { 'main': { 'enabled': '0', 'threads': '1', 'accept': ',', 'forward': ',' }, 'messaging': { }, 'model': { 'managed': '2' } } class AgentConfig(Graph): """ The gofer agent configuration. :cvar PATH: The absolute path to the config directory. :type PATH: str """ __metaclass__ = Singleton PATH = '/etc/%s/agent.conf' % NAME def __init__(self, path=None): """ Read the configuration. """ conf = Config(AGENT_DEFAULTS, path or AgentConfig.PATH) conf.validate(AGENT_SCHEMA) Graph.__init__(self, conf)
# -*- coding: utf-8 -*- ############################################################################## # # Copyright (C) 2013 Daniel Reis # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## from openerp.osv import fields, orm class ProjectProject(orm.Model): _inherit = 'project.project' _columns = { 'use_analytic_account': fields.selection( [('no', 'No'), ('yes', 'Optional'), ('req', 'Required')], 'Use Analytic Account'), } _defaults = { 'use_analytic_account': 'no', } class ProjectTask(orm.Model): """ Add related ``Analytic Account`` and service ``Location``. A Location can be any Contact Partner of the AA's Partner. Other logic is possible, such as maintaining a specific list of service addresses for each Contract, but that's out of scope here - modules implementing these other possibilities are very welcome. """ _inherit = 'project.task' _columns = { 'analytic_account_id': fields.many2one( 'account.analytic.account', 'Contract/Analytic', domain="[('type','in',['normal','contract'])]"), 'location_id': fields.many2one( 'res.partner', 'Location', domain="[('parent_id','child_of',partner_id)]"), 'use_analytic_account': fields.related( 'project_id', 'use_analytic_account', type='char', string="Use Analytic Account"), 'project_code': fields.related( 'project_id', 'code', type='char', string="Project Code"), } def onchange_project(self, cr, uid, id, project_id, context=None): # on_change is necessary to populate fields on Create, before saving try: # try applying a parent's onchange, may it exist res = super(ProjectTask, self).onchange_project( cr, uid, id, project_id, context=context) or {} except AttributeError: res = {} if project_id: obj = self.pool.get('project.project').browse( cr, uid, project_id, context=context) res.setdefault('value', {}) res['value']['use_analytic_account'] = ( obj.use_analytic_account or 'no') return res def onchange_analytic(self, cr, uid, id, analytic_id, context=None): res = {} model = self.pool.get('account.analytic.account') obj = model.browse(cr, uid, analytic_id, context=context) if obj: # "contact_id" and "department_id" may be provided by other modules fldmap = [ # analytic_account field -> task field ('partner_id', 'partner_id'), ('contact_id', 'location_id'), ('department_id', 'department_id')] res['value'] = {dest: getattr(obj, orig).id for orig, dest in fldmap if hasattr(obj, orig) and getattr(obj, orig)} return res
from __future__ import unicode_literals import logging from django.conf import settings from django.contrib.gis import gdal from django.contrib.gis.geos import GEOSException, GEOSGeometry from django.forms.widgets import Widget from django.template import loader from django.utils import six, translation logger = logging.getLogger('django.contrib.gis') class BaseGeometryWidget(Widget): """ The base class for rich geometry widgets. Renders a map using the WKT of the geometry. """ geom_type = 'GEOMETRY' map_srid = 4326 map_width = 600 map_height = 400 display_raw = False supports_3d = False template_name = '' # set on subclasses def __init__(self, attrs=None): self.attrs = {} for key in ('geom_type', 'map_srid', 'map_width', 'map_height', 'display_raw'): self.attrs[key] = getattr(self, key) if attrs: self.attrs.update(attrs) def serialize(self, value): return value.wkt if value else '' def deserialize(self, value): try: return GEOSGeometry(value, self.map_srid) except (GEOSException, ValueError) as err: logger.error( "Error creating geometry from value '%s' (%s)" % ( value, err) ) return None def render(self, name, value, attrs=None): # If a string reaches here (via a validation error on another # field) then just reconstruct the Geometry. if isinstance(value, six.string_types): value = self.deserialize(value) if value: # Check that srid of value and map match if value.srid != self.map_srid: try: ogr = value.ogr ogr.transform(self.map_srid) value = ogr except gdal.GDALException as err: logger.error( "Error transforming geometry from srid '%s' to srid '%s' (%s)" % ( value.srid, self.map_srid, err) ) context = self.build_attrs( attrs, name=name, module='geodjango_%s' % name.replace('-', '_'), # JS-safe serialized=self.serialize(value), geom_type=gdal.OGRGeomType(self.attrs['geom_type']), STATIC_URL=settings.STATIC_URL, LANGUAGE_BIDI=translation.get_language_bidi(), ) return loader.render_to_string(self.template_name, context) class OpenLayersWidget(BaseGeometryWidget): template_name = 'gis/openlayers.html' class Media: js = ( 'http://openlayers.org/api/2.13/OpenLayers.js', 'gis/js/OLMapWidget.js', ) class OSMWidget(BaseGeometryWidget): """ An OpenLayers/OpenStreetMap-based widget. """ template_name = 'gis/openlayers-osm.html' default_lon = 5 default_lat = 47 class Media: js = ( 'http://openlayers.org/api/2.13/OpenLayers.js', 'http://www.openstreetmap.org/openlayers/OpenStreetMap.js', 'gis/js/OLMapWidget.js', ) def __init__(self, attrs=None): super(OSMWidget, self).__init__() for key in ('default_lon', 'default_lat'): self.attrs[key] = getattr(self, key) if attrs: self.attrs.update(attrs) @property def map_srid(self): # Use the official spherical mercator projection SRID when GDAL is # available; otherwise, fallback to 900913. if gdal.HAS_GDAL: return 3857 else: return 900913
# Copyright (c) 2012 Mitch Garnaat http://garnaat.org/ # Copyright (c) 2012 Amazon.com, Inc. or its affiliates. # All rights reserved. # # Permission is hereby granted, free of charge, to any person obtaining a # copy of this software and associated documentation files (the # "Software"), to deal in the Software without restriction, including # without limitation the rights to use, copy, modify, merge, publish, dis- # tribute, sublicense, and/or sell copies of the Software, and to permit # persons to whom the Software is furnished to do so, subject to the fol- # lowing conditions: # # The above copyright notice and this permission notice shall be included # in all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS # OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL- # ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT # SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, # WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS # IN THE SOFTWARE. """ Check that all of the certs on SQS endpoints validate. """ import unittest from tests.integration import ServiceCertVerificationTest import boto.s3 class S3CertVerificationTest(unittest.TestCase, ServiceCertVerificationTest): s3 = True regions = boto.s3.regions() def sample_service_call(self, conn): conn.get_all_buckets()
# Copyright 2011, Google Inc. # All rights reserved. # # Redistribution and use in source and binary forms, with or without # modification, are permitted provided that the following conditions are # met: # # * Redistributions of source code must retain the above copyright # notice, this list of conditions and the following disclaimer. # * Redistributions in binary form must reproduce the above # copyright notice, this list of conditions and the following disclaimer # in the documentation and/or other materials provided with the # distribution. # * Neither the name of Google Inc. nor the names of its # contributors may be used to endorse or promote products derived from # this software without specific prior written permission. # # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS # "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT # LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR # A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT # OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, # SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT # LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, # DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY # THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT # (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE # OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. """A simple load tester for WebSocket clients. A client program sends a message formatted as "<time> <count> <message>" to this handler. This handler starts sending total <count> WebSocket messages containing <message> every <time> seconds. <time> can be a floating point value. <count> must be an integer value. """ import time def web_socket_do_extra_handshake(request): pass # Always accept. def web_socket_transfer_data(request): line = request.ws_stream.receive_message() parts = line.split(' ') if len(parts) != 3: raise ValueError('Bad parameter format') wait = float(parts[0]) count = int(parts[1]) message = parts[2] for i in xrange(count): request.ws_stream.send_message(message) time.sleep(wait) # vi:sts=4 sw=4 et
# Copyright 2018 The TensorFlow Probability Authors. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================ """Testing the TFP Hypothesis strategies. (As opposed to using them to test other things). """ from __future__ import absolute_import from __future__ import division from __future__ import print_function from absl.testing import parameterized import hypothesis as hp from hypothesis import strategies as hps import numpy as np import tensorflow.compat.v2 as tf from tensorflow_probability.python.internal import hypothesis_testlib as tfp_hps from tensorflow_probability.python.internal import test_util @test_util.test_all_tf_execution_regimes class HypothesisTestlibTest(test_util.TestCase): @parameterized.parameters((support,) for support in tfp_hps.ALL_SUPPORTS) @hp.given(hps.data()) @tfp_hps.tfp_hp_settings() def testTensorsInSupportsAlwaysFinite(self, support, data): try: result_ = data.draw(tfp_hps.tensors_in_support(support)) except NotImplementedError: # Constraint class doesn't have a constrainer function at all, so this # test is moot. return result = self.evaluate(result_) self.assertTrue(np.all(np.isfinite(result))) if __name__ == '__main__': tf.test.main()
# -*- coding: utf-8 -*- # # This file is part of Invenio. # Copyright (C) 2009, 2010, 2011, 2012, 2013, 2014 CERN. # # Invenio is free software; you can redistribute it and/or # modify it under the terms of the GNU General Public License as # published by the Free Software Foundation; either version 2 of the # License, or (at your option) any later version. # # Invenio is distributed in the hope that it will be useful, but # WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU # General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Invenio; if not, write to the Free Software Foundation, Inc., # 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA. """Invenio Bibliographic Tasklet BibTask. This is a particular BibTask that execute tasklets, which can be any function dropped into ``<package>.tasklets`` where ``<package>`` is defined in ``PACKAGES``. """ from __future__ import print_function import sys from invenio.version import __version__ from invenio.legacy.bibsched.bibtask import ( task_init, write_message, task_set_option, task_get_option, task_update_progress) from invenio.utils.autodiscovery.helpers import get_callable_documentation from invenio.utils.autodiscovery.checkers import check_arguments_compatibility from invenio.modules.scheduler.registry import tasklets _TASKLETS = tasklets def cli_list_tasklets(): """Print the list of available tasklets and broken tasklets.""" print("""Available tasklets:""") for tasklet in _TASKLETS.values(): print(get_callable_documentation(tasklet)) sys.exit(0) def task_submit_elaborate_specific_parameter(key, value, dummy_opts, dummy_args): """Check meaning of given string key. Eventually use the value for check. Usually it fills some key in the options dict. It must return True if it has elaborated the key, False, if it doesn't know that key. Example: .. code-block:: python if key in ('-n', '--number'): task_set_option('number', value) return True return False """ if key in ('-T', '--tasklet'): task_set_option('tasklet', value) return True elif key in ('-a', '--argument'): arguments = task_get_option('arguments', {}) try: key, value = value.split('=', 1) except NameError: print('ERROR: an argument must be in the form ' 'param=value, not "%s"' % (value, ), file=sys.stderr) return False arguments[key] = value task_set_option('arguments', arguments) return True elif key in ('-l', '--list-tasklets'): cli_list_tasklets() return True return False def task_submit_check_options(): """Check if a tasklet has been specified and the parameters are good.""" tasklet = task_get_option('tasklet', None) arguments = task_get_option('arguments', {}) if not tasklet: print('ERROR: no tasklet specified', file=sys.stderr) return False elif tasklet not in _TASKLETS: print('ERROR: "%s" is not a valid tasklet. Use ' '--list-tasklets to obtain a list of the working tasklets.' % tasklet, file=sys.stderr) return False else: try: check_arguments_compatibility(_TASKLETS[tasklet], arguments) except ValueError as err: print('ERROR: wrong arguments (%s) specified for ' 'tasklet "%s": %s' % ( arguments, tasklet, err), file=sys.stderr) return False return True def task_run_core(): """Run the specific tasklet.""" tasklet = task_get_option('tasklet') arguments = task_get_option('arguments', {}) write_message('Starting tasklet "%s" (with arguments %s)' % ( tasklet, arguments)) task_update_progress('%s started' % tasklet) ret = _TASKLETS[tasklet](**arguments) task_update_progress('%s finished' % tasklet) write_message('Finished tasklet "%s" (with arguments %s)' % ( tasklet, arguments)) if ret is not None: return ret return True def main(): """Main body of bibtasklet.""" task_init( authorization_action='runbibtasklet', authorization_msg="BibTaskLet Task Submission", help_specific_usage="""\ -T, --tasklet Execute the specific tasklet -a, --argument Specify an argument to be passed to tasklet in the form param=value, e.g. --argument foo=bar -l, --list-tasklets List the existing tasklets """, version=__version__, specific_params=("T:a:l", ["tasklet=", "argument=", "list-tasklets"]), task_submit_elaborate_specific_parameter_fnc=( task_submit_elaborate_specific_parameter ), task_run_fnc=task_run_core, task_submit_check_options_fnc=task_submit_check_options)
from os import listdir, path from types import GeneratorType import six from pyinfra import logger, pseudo_inventory from pyinfra.api.inventory import Inventory from pyinfra_cli.util import exec_file # Hosts in an inventory can be just the hostname or a tuple (hostname, data) ALLOWED_HOST_TYPES = tuple( six.string_types + (tuple,), ) def _is_inventory_group(key, value): ''' Verify that a module-level variable (key = value) is a valid inventory group. ''' if ( key.startswith('_') or not isinstance(value, (list, tuple, GeneratorType)) ): return False # If the group is a tuple of (hosts, data), check the hosts if isinstance(value, tuple): value = value[0] # Expand any generators of hosts if isinstance(value, GeneratorType): value = list(value) return all( isinstance(item, ALLOWED_HOST_TYPES) for item in value ) def _get_group_data(deploy_dir): group_data = {} group_data_directory = path.join(deploy_dir, 'group_data') if path.exists(group_data_directory): files = listdir(group_data_directory) for file in files: if not file.endswith('.py'): continue group_data_file = path.join(group_data_directory, file) group_name = path.basename(file)[:-3] logger.debug('Looking for group data in: {0}'.format(group_data_file)) # Read the files locals into a dict attrs = exec_file(group_data_file, return_locals=True) keys = attrs.get('__all__', attrs.keys()) group_data[group_name] = { key: value for key, value in six.iteritems(attrs) if key in keys and not key.startswith('_') } return group_data def _get_groups_from_filename(inventory_filename): attrs = exec_file(inventory_filename, return_locals=True) return { key: value for key, value in six.iteritems(attrs) if _is_inventory_group(key, value) } def make_inventory( inventory_filename, deploy_dir=None, ssh_port=None, ssh_user=None, ssh_key=None, ssh_key_password=None, ssh_password=None, winrm_username=None, winrm_password=None, winrm_port=None, winrm_transport=None, ): ''' Builds a ``pyinfra.api.Inventory`` from the filesystem. If the file does not exist and doesn't contain a / attempts to use that as the only hostname. ''' if ssh_port is not None: ssh_port = int(ssh_port) file_groupname = None # If we're not a valid file we assume a list of comma separated hostnames if not path.exists(inventory_filename): groups = { 'all': inventory_filename.split(','), } else: groups = _get_groups_from_filename(inventory_filename) # Used to set all the hosts to an additional group - that of the filename # ie inventories/dev.py means all the hosts are in the dev group, if not present file_groupname = path.basename(inventory_filename).rsplit('.', 1)[0] all_data = {} if 'all' in groups: all_hosts = groups.pop('all') if isinstance(all_hosts, tuple): all_hosts, all_data = all_hosts # Build all out of the existing hosts if not defined else: all_hosts = [] for hosts in groups.values(): # Groups can be a list of hosts or tuple of (hosts, data) hosts = hosts[0] if isinstance(hosts, tuple) else hosts for host in hosts: # Hosts can be a hostname or tuple of (hostname, data) hostname = host[0] if isinstance(host, tuple) else host if hostname not in all_hosts: all_hosts.append(hostname) groups['all'] = (all_hosts, all_data) # Apply the filename group if not already defined if file_groupname and file_groupname not in groups: groups[file_groupname] = all_hosts # In pyinfra an inventory is a combination of (hostnames + data). However, in CLI # mode we want to be define this in separate files (inventory / group data). The # issue is we want inventory access within the group data files - but at this point # we're not ready to make an Inventory. So here we just create a fake one, and # attach it to pseudo_inventory while we import the data files. logger.debug('Creating fake inventory...') fake_groups = { # In API mode groups *must* be tuples of (hostnames, data) name: group if isinstance(group, tuple) else (group, {}) for name, group in six.iteritems(groups) } fake_inventory = Inventory((all_hosts, all_data), **fake_groups) pseudo_inventory.set(fake_inventory) # Get all group data (group_data/*.py) group_data = _get_group_data(deploy_dir) # Reset the pseudo inventory pseudo_inventory.reset() # For each group load up any data for name, hosts in six.iteritems(groups): data = {} if isinstance(hosts, tuple): hosts, data = hosts if name in group_data: data.update(group_data.pop(name)) # Attach to group object groups[name] = (hosts, data) # Loop back through any leftover group data and create an empty (for now) # group - this is because inventory @connectors can attach arbitrary groups # to hosts, so we need to support that. for name, data in six.iteritems(group_data): groups[name] = ([], data) return Inventory( groups.pop('all'), ssh_user=ssh_user, ssh_key=ssh_key, ssh_key_password=ssh_key_password, ssh_port=ssh_port, ssh_password=ssh_password, winrm_username=winrm_username, winrm_password=winrm_password, winrm_port=winrm_port, winrm_transport=winrm_transport, **groups ), file_groupname and file_groupname.lower()
from enigma import eDVBFrontendParametersSatellite, eDVBFrontendParametersTerrestrial, eDVBFrontendParametersCable, eDVBFrontendParameters, eDVBResourceManager, eTimer class Tuner: def __init__(self, frontend, ignore_rotor=False): self.frontend = frontend self.ignore_rotor = ignore_rotor # transponder = (frequency, symbolrate, polarisation, fec, inversion, orbpos, system, modulation, rolloff, pilot, tsid, onid) # 0 1 2 3 4 5 6 7 8 9 10 11 def tune(self, transponder): if self.frontend: print "[TuneTest] tuning to transponder with data", transponder parm = eDVBFrontendParametersSatellite() parm.frequency = transponder[0] * 1000 parm.symbol_rate = transponder[1] * 1000 parm.polarisation = transponder[2] parm.fec = transponder[3] parm.inversion = transponder[4] parm.orbital_position = transponder[5] parm.system = transponder[6] parm.modulation = transponder[7] parm.rolloff = transponder[8] parm.pilot = transponder[9] self.tuneSatObj(parm) def tuneSatObj(self, transponderObj): if self.frontend: feparm = eDVBFrontendParameters() feparm.setDVBS(transponderObj, self.ignore_rotor) self.lastparm = feparm self.frontend.tune(feparm) def tuneTerr(self, frequency, inversion=2, bandwidth = 7000000, fechigh = 6, feclow = 6, modulation = 2, transmission = 2, guard = 4, hierarchy = 4, system = 0, plpid = 0): if self.frontend: print "[TuneTest] tuning to transponder with data", [frequency, inversion, bandwidth, fechigh, feclow, modulation, transmission, guard, hierarchy, system, plpid] parm = eDVBFrontendParametersTerrestrial() parm.frequency = frequency parm.inversion = inversion parm.bandwidth = bandwidth parm.code_rate_HP = fechigh parm.code_rate_LP = feclow parm.modulation = modulation parm.transmission_mode = transmission parm.guard_interval = guard parm.hierarchy = hierarchy parm.system = system parm.plpid = plpid self.tuneTerrObj(parm) def tuneTerrObj(self, transponderObj): if self.frontend: feparm = eDVBFrontendParameters() feparm.setDVBT(transponderObj) self.lastparm = feparm self.frontend.tune(feparm) def tuneCab(self, transponder): if self.frontend: print "[TuneTest] tuning to transponder with data", transponder parm = eDVBFrontendParametersCable() parm.frequency = transponder[0] parm.symbol_rate = transponder[1] parm.modulation = transponder[2] parm.fec_inner = transponder[3] parm.inversion = transponder[4] #parm.system = transponder[5] self.tuneCabObj(parm) def tuneCabObj(self, transponderObj): if self.frontend: feparm = eDVBFrontendParameters() feparm.setDVBC(transponderObj) self.lastparm = feparm self.frontend.tune(feparm) def retune(self): if self.frontend: self.frontend.tune(self.lastparm) def getTransponderData(self): ret = { } if self.frontend: self.frontend.getTransponderData(ret, True) return ret # tunes a list of transponders and checks, if they lock and optionally checks the onid/tsid combination # 1) add transponders with addTransponder() # 2) call run(<checkPIDs = True>) # 3) finishedChecking() is called, when the run is finished class TuneTest: def __init__(self, feid, stopOnSuccess = -1, stopOnError = -1): self.stopOnSuccess = stopOnSuccess self.stopOnError = stopOnError self.feid = feid self.transponderlist = [] self.currTuned = None print "TuneTest for feid %d" % self.feid if not self.openFrontend(): self.oldref = self.session.nav.getCurrentlyPlayingServiceOrGroup() self.session.nav.stopService() # try to disable foreground service if not self.openFrontend(): if self.session.pipshown: # try to disable pip if hasattr(self.session, 'infobar'): if self.session.infobar.servicelist.dopipzap: self.session.infobar.servicelist.togglePipzap() if hasattr(self.session, 'pip'): del self.session.pip self.session.pipshown = False if not self.openFrontend(): self.frontend = None # in normal case this should not happen self.tuner = Tuner(self.frontend) self.timer = eTimer() self.timer.callback.append(self.updateStatus) def gotTsidOnid(self, tsid, onid): print "******** got tsid, onid:", tsid, onid if tsid is not -1 and onid is not -1: self.pidStatus = self.INTERNAL_PID_STATUS_SUCCESSFUL self.tsid = tsid self.onid = onid else: self.pidStatus = self.INTERNAL_PID_STATUS_FAILED self.tsid = -1 self.onid = -1 self.timer.start(100, True) def updateStatus(self): dict = {} self.frontend.getFrontendStatus(dict) stop = False print "status:", dict if dict["tuner_state"] == "TUNING": print "TUNING" self.timer.start(100, True) self.progressCallback((self.getProgressLength(), self.tuningtransponder, self.STATUS_TUNING, self.currTuned)) elif self.checkPIDs and self.pidStatus == self.INTERNAL_PID_STATUS_NOOP: print "2nd choice" if dict["tuner_state"] == "LOCKED": print "acquiring TSID/ONID" self.raw_channel.receivedTsidOnid.get().append(self.gotTsidOnid) self.raw_channel.requestTsidOnid() self.pidStatus = self.INTERNAL_PID_STATUS_WAITING else: self.pidStatus = self.INTERNAL_PID_STATUS_FAILED elif self.checkPIDs and self.pidStatus == self.INTERNAL_PID_STATUS_WAITING: print "waiting for pids" else: if dict["tuner_state"] == "LOSTLOCK" or dict["tuner_state"] == "FAILED": self.tuningtransponder = self.nextTransponder() self.failedTune.append([self.currTuned, self.oldTuned, "tune_failed", dict]) # last parameter is the frontend status) if self.stopOnError != -1 and self.stopOnError <= len(self.failedTune): stop = True elif dict["tuner_state"] == "LOCKED": pidsFailed = False if self.checkPIDs: if self.currTuned is not None: if self.tsid != self.currTuned[10] or self.onid != self.currTuned[11]: self.failedTune.append([self.currTuned, self.oldTuned, "pids_failed", {"real": (self.tsid, self.onid), "expected": (self.currTuned[10], self.currTuned[11])}, dict]) # last parameter is the frontend status pidsFailed = True else: self.successfullyTune.append([self.currTuned, self.oldTuned, dict]) # 3rd parameter is the frontend status if self.stopOnSuccess != -1 and self.stopOnSuccess <= len(self.successfullyTune): stop = True elif not self.checkPIDs or (self.checkPids and not pidsFailed): self.successfullyTune.append([self.currTuned, self.oldTuned, dict]) # 3rd parameter is the frontend status if self.stopOnSuccess != -1 and self.stopOnSuccess <= len(self.successfullyTune): stop = True self.tuningtransponder = self.nextTransponder() else: print "************* tuner_state:", dict["tuner_state"] self.progressCallback((self.getProgressLength(), self.tuningtransponder, self.STATUS_NOOP, self.currTuned)) if not stop: self.tune() if self.tuningtransponder < len(self.transponderlist) and not stop: if self.pidStatus != self.INTERNAL_PID_STATUS_WAITING: self.timer.start(100, True) print "restart timer" else: print "not restarting timers (waiting for pids)" else: self.progressCallback((self.getProgressLength(), len(self.transponderlist), self.STATUS_DONE, self.currTuned)) print "finishedChecking" self.finishedChecking() def firstTransponder(self): print "firstTransponder:" index = 0 if self.checkPIDs: print "checkPIDs-loop" # check for tsid != -1 and onid != -1 print "index:", index print "len(self.transponderlist):", len(self.transponderlist) while (index < len(self.transponderlist) and (self.transponderlist[index][10] == -1 or self.transponderlist[index][11] == -1)): index += 1 print "FirstTransponder final index:", index return index def nextTransponder(self): print "getting next transponder", self.tuningtransponder index = self.tuningtransponder + 1 if self.checkPIDs: print "checkPIDs-loop" # check for tsid != -1 and onid != -1 print "index:", index print "len(self.transponderlist):", len(self.transponderlist) while (index < len(self.transponderlist) and (self.transponderlist[index][10] == -1 or self.transponderlist[index][11] == -1)): index += 1 print "next transponder index:", index return index def finishedChecking(self): print "finished testing" print "successfull:", self.successfullyTune print "failed:", self.failedTune def openFrontend(self): res_mgr = eDVBResourceManager.getInstance() if res_mgr: self.raw_channel = res_mgr.allocateRawChannel(self.feid) if self.raw_channel: self.frontend = self.raw_channel.getFrontend() if self.frontend: return True else: print "getFrontend failed" else: print "getRawChannel failed" else: print "getResourceManager instance failed" return False def tune(self): print "tuning to", self.tuningtransponder if self.tuningtransponder < len(self.transponderlist): self.pidStatus = self.INTERNAL_PID_STATUS_NOOP self.oldTuned = self.currTuned self.currTuned = self.transponderlist[self.tuningtransponder] self.tuner.tune(self.transponderlist[self.tuningtransponder]) INTERNAL_PID_STATUS_NOOP = 0 INTERNAL_PID_STATUS_WAITING = 1 INTERNAL_PID_STATUS_SUCCESSFUL = 2 INTERNAL_PID_STATUS_FAILED = 3 def run(self, checkPIDs = False): self.checkPIDs = checkPIDs self.pidStatus = self.INTERNAL_PID_STATUS_NOOP self.failedTune = [] self.successfullyTune = [] self.tuningtransponder = self.firstTransponder() self.tune() self.progressCallback((self.getProgressLength(), self.tuningtransponder, self.STATUS_START, self.currTuned)) self.timer.start(100, True) # transponder = (frequency, symbolrate, polarisation, fec, inversion, orbpos, <system>, <modulation>, <rolloff>, <pilot>, <tsid>, <onid>) # 0 1 2 3 4 5 6 7 8 9 10 11 def addTransponder(self, transponder): self.transponderlist.append(transponder) def clearTransponder(self): self.transponderlist = [] def getProgressLength(self): count = 0 if self.stopOnError == -1: count = len(self.transponderlist) else: if count < self.stopOnError: count = self.stopOnError if self.stopOnSuccess == -1: count = len(self.transponderlist) else: if count < self.stopOnSuccess: count = self.stopOnSuccess return count STATUS_START = 0 STATUS_TUNING = 1 STATUS_DONE = 2 STATUS_NOOP = 3 # can be overwritten # progress = (range, value, status, transponder) def progressCallback(self, progress): pass
# -*- coding: utf-8 -*- """ flask.sessions ~~~~~~~~~~~~~~ Implements cookie based sessions based on itsdangerous. :copyright: (c) 2012 by Armin Ronacher. :license: BSD, see LICENSE for more details. """ import uuid import hashlib from base64 import b64encode, b64decode from datetime import datetime from werkzeug.http import http_date, parse_date from werkzeug.datastructures import CallbackDict from . import Markup, json from ._compat import iteritems, text_type from itsdangerous import URLSafeTimedSerializer, BadSignature def total_seconds(td): return td.days * 60 * 60 * 24 + td.seconds class SessionMixin(object): """Expands a basic dictionary with an accessors that are expected by Flask extensions and users for the session. """ def _get_permanent(self): return self.get('_permanent', False) def _set_permanent(self, value): self['_permanent'] = bool(value) #: this reflects the ``'_permanent'`` key in the dict. permanent = property(_get_permanent, _set_permanent) del _get_permanent, _set_permanent #: some session backends can tell you if a session is new, but that is #: not necessarily guaranteed. Use with caution. The default mixin #: implementation just hardcodes `False` in. new = False #: for some backends this will always be `True`, but some backends will #: default this to false and detect changes in the dictionary for as #: long as changes do not happen on mutable structures in the session. #: The default mixin implementation just hardcodes `True` in. modified = True class TaggedJSONSerializer(object): """A customized JSON serializer that supports a few extra types that we take for granted when serializing (tuples, markup objects, datetime). """ def dumps(self, value): def _tag(value): if isinstance(value, tuple): return {' t': [_tag(x) for x in value]} elif isinstance(value, uuid.UUID): return {' u': value.hex} elif isinstance(value, bytes): return {' b': b64encode(value).decode('ascii')} elif callable(getattr(value, '__html__', None)): return {' m': text_type(value.__html__())} elif isinstance(value, list): return [_tag(x) for x in value] elif isinstance(value, datetime): return {' d': http_date(value)} elif isinstance(value, dict): return dict((k, _tag(v)) for k, v in iteritems(value)) elif isinstance(value, str): try: return text_type(value) except UnicodeError: raise UnexpectedUnicodeError(u'A byte string with ' u'non-ASCII data was passed to the session system ' u'which can only store unicode strings. Consider ' u'base64 encoding your string (String was %r)' % value) return value return json.dumps(_tag(value), separators=(',', ':')) def loads(self, value): def object_hook(obj): if len(obj) != 1: return obj the_key, the_value = next(iteritems(obj)) if the_key == ' t': return tuple(the_value) elif the_key == ' u': return uuid.UUID(the_value) elif the_key == ' b': return b64decode(the_value) elif the_key == ' m': return Markup(the_value) elif the_key == ' d': return parse_date(the_value) return obj return json.loads(value, object_hook=object_hook) session_json_serializer = TaggedJSONSerializer() class SecureCookieSession(CallbackDict, SessionMixin): """Baseclass for sessions based on signed cookies.""" def __init__(self, initial=None): def on_update(self): self.modified = True CallbackDict.__init__(self, initial, on_update) self.modified = False class NullSession(SecureCookieSession): """Class used to generate nicer error messages if sessions are not available. Will still allow read-only access to the empty session but fail on setting. """ def _fail(self, *args, **kwargs): raise RuntimeError('the session is unavailable because no secret ' 'key was set. Set the secret_key on the ' 'application to something unique and secret.') __setitem__ = __delitem__ = clear = pop = popitem = \ update = setdefault = _fail del _fail class SessionInterface(object): """The basic interface you have to implement in order to replace the default session interface which uses werkzeug's securecookie implementation. The only methods you have to implement are :meth:`open_session` and :meth:`save_session`, the others have useful defaults which you don't need to change. The session object returned by the :meth:`open_session` method has to provide a dictionary like interface plus the properties and methods from the :class:`SessionMixin`. We recommend just subclassing a dict and adding that mixin:: class Session(dict, SessionMixin): pass If :meth:`open_session` returns `None` Flask will call into :meth:`make_null_session` to create a session that acts as replacement if the session support cannot work because some requirement is not fulfilled. The default :class:`NullSession` class that is created will complain that the secret key was not set. To replace the session interface on an application all you have to do is to assign :attr:`flask.Flask.session_interface`:: app = Flask(__name__) app.session_interface = MySessionInterface() .. versionadded:: 0.8 """ #: :meth:`make_null_session` will look here for the class that should #: be created when a null session is requested. Likewise the #: :meth:`is_null_session` method will perform a typecheck against #: this type. null_session_class = NullSession #: A flag that indicates if the session interface is pickle based. #: This can be used by flask extensions to make a decision in regards #: to how to deal with the session object. #: #: .. versionadded:: 0.10 pickle_based = False def make_null_session(self, app): """Creates a null session which acts as a replacement object if the real session support could not be loaded due to a configuration error. This mainly aids the user experience because the job of the null session is to still support lookup without complaining but modifications are answered with a helpful error message of what failed. This creates an instance of :attr:`null_session_class` by default. """ return self.null_session_class() def is_null_session(self, obj): """Checks if a given object is a null session. Null sessions are not asked to be saved. This checks if the object is an instance of :attr:`null_session_class` by default. """ return isinstance(obj, self.null_session_class) def get_cookie_domain(self, app): """Helpful helper method that returns the cookie domain that should be used for the session cookie if session cookies are used. """ if app.config['SESSION_COOKIE_DOMAIN'] is not None: return app.config['SESSION_COOKIE_DOMAIN'] if app.config['SERVER_NAME'] is not None: # chop of the port which is usually not supported by browsers rv = '.' + app.config['SERVER_NAME'].rsplit(':', 1)[0] # Google chrome does not like cookies set to .localhost, so # we just go with no domain then. Flask documents anyways that # cross domain cookies need a fully qualified domain name if rv == '.localhost': rv = None # If we infer the cookie domain from the server name we need # to check if we are in a subpath. In that case we can't # set a cross domain cookie. if rv is not None: path = self.get_cookie_path(app) if path != '/': rv = rv.lstrip('.') return rv def get_cookie_path(self, app): """Returns the path for which the cookie should be valid. The default implementation uses the value from the SESSION_COOKIE_PATH`` config var if it's set, and falls back to ``APPLICATION_ROOT`` or uses ``/`` if it's `None`. """ return app.config['SESSION_COOKIE_PATH'] or \ app.config['APPLICATION_ROOT'] or '/' def get_cookie_httponly(self, app): """Returns True if the session cookie should be httponly. This currently just returns the value of the ``SESSION_COOKIE_HTTPONLY`` config var. """ return app.config['SESSION_COOKIE_HTTPONLY'] def get_cookie_secure(self, app): """Returns True if the cookie should be secure. This currently just returns the value of the ``SESSION_COOKIE_SECURE`` setting. """ return app.config['SESSION_COOKIE_SECURE'] def get_expiration_time(self, app, session): """A helper method that returns an expiration date for the session or `None` if the session is linked to the browser session. The default implementation returns now + the permanent session lifetime configured on the application. """ if session.permanent: return datetime.utcnow() + app.permanent_session_lifetime def should_set_cookie(self, app, session): """Indicates weather a cookie should be set now or not. This is used by session backends to figure out if they should emit a set-cookie header or not. The default behavior is controlled by the ``SESSION_REFRESH_EACH_REQUEST`` config variable. If it's set to `False` then a cookie is only set if the session is modified, if set to `True` it's always set if the session is permanent. This check is usually skipped if sessions get deleted. .. versionadded:: 1.0 """ if session.modified: return True save_each = app.config['SESSION_REFRESH_EACH_REQUEST'] return save_each and session.permanent def open_session(self, app, request): """This method has to be implemented and must either return `None` in case the loading failed because of a configuration error or an instance of a session object which implements a dictionary like interface + the methods and attributes on :class:`SessionMixin`. """ raise NotImplementedError() def save_session(self, app, session, response): """This is called for actual sessions returned by :meth:`open_session` at the end of the request. This is still called during a request context so if you absolutely need access to the request you can do that. """ raise NotImplementedError() class SecureCookieSessionInterface(SessionInterface): """The default session interface that stores sessions in signed cookies through the :mod:`itsdangerous` module. """ #: the salt that should be applied on top of the secret key for the #: signing of cookie based sessions. salt = 'cookie-session' #: the hash function to use for the signature. The default is sha1 digest_method = staticmethod(hashlib.sha1) #: the name of the itsdangerous supported key derivation. The default #: is hmac. key_derivation = 'hmac' #: A python serializer for the payload. The default is a compact #: JSON derived serializer with support for some extra Python types #: such as datetime objects or tuples. serializer = session_json_serializer session_class = SecureCookieSession def get_signing_serializer(self, app): if not app.secret_key: return None signer_kwargs = dict( key_derivation=self.key_derivation, digest_method=self.digest_method ) return URLSafeTimedSerializer(app.secret_key, salt=self.salt, serializer=self.serializer, signer_kwargs=signer_kwargs) def open_session(self, app, request): s = self.get_signing_serializer(app) if s is None: return None val = request.cookies.get(app.session_cookie_name) if not val: return self.session_class() max_age = total_seconds(app.permanent_session_lifetime) try: data = s.loads(val, max_age=max_age) return self.session_class(data) except BadSignature: return self.session_class() def save_session(self, app, session, response): domain = self.get_cookie_domain(app) path = self.get_cookie_path(app) # Delete case. If there is no session we bail early. # If the session was modified to be empty we remove the # whole cookie. if not session: if session.modified: response.delete_cookie(app.session_cookie_name, domain=domain, path=path) return # Modification case. There are upsides and downsides to # emitting a set-cookie header each request. The behavior # is controlled by the :meth:`should_set_cookie` method # which performs a quick check to figure out if the cookie # should be set or not. This is controlled by the # SESSION_REFRESH_EACH_REQUEST config flag as well as # the permanent flag on the session itself. if not self.should_set_cookie(app, session): return httponly = self.get_cookie_httponly(app) secure = self.get_cookie_secure(app) expires = self.get_expiration_time(app, session) val = self.get_signing_serializer(app).dumps(dict(session)) response.set_cookie(app.session_cookie_name, val, expires=expires, httponly=httponly, domain=domain, path=path, secure=secure) from flask.debughelpers import UnexpectedUnicodeError
# Copyright (c) 202 Mitch Garnaat http://garnaat.org/ # Copyright (c) 2012 Amazon.com, Inc. or its affiliates. # All Rights Reserved # # Permission is hereby granted, free of charge, to any person obtaining a # copy of this software and associated documentation files (the # "Software"), to deal in the Software without restriction, including # without limitation the rights to use, copy, modify, merge, publish, dis- # tribute, sublicense, and/or sell copies of the Software, and to permit # persons to whom the Software is furnished to do so, subject to the fol- # lowing conditions: # # The above copyright notice and this permission notice shall be included # in all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS # OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL- # ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT # SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, # WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS # IN THE SOFTWARE. class SourceAttribute(object): """ Provide information about attributes for an index field. A maximum of 20 source attributes can be configured for each index field. :ivar default: Optional default value if the source attribute is not specified in a document. :ivar name: The name of the document source field to add to this ``IndexField``. :ivar data_function: Identifies the transformation to apply when copying data from a source attribute. :ivar data_map: The value is a dict with the following keys: * cases - A dict that translates source field values to custom values. * default - An optional default value to use if the source attribute is not specified in a document. * name - the name of the document source field to add to this ``IndexField`` :ivar data_trim_title: Trims common title words from a source document attribute when populating an ``IndexField``. This can be used to create an ``IndexField`` you can use for sorting. The value is a dict with the following fields: * default - An optional default value. * language - an IETF RFC 4646 language code. * separator - The separator that follows the text to trim. * name - The name of the document source field to add. """ ValidDataFunctions = ('Copy', 'TrimTitle', 'Map') def __init__(self): self.data_copy = {} self._data_function = self.ValidDataFunctions[0] self.data_map = {} self.data_trim_title = {} @property def data_function(self): return self._data_function @data_function.setter def data_function(self, value): if value not in self.ValidDataFunctions: valid = '|'.join(self.ValidDataFunctions) raise ValueError('data_function must be one of: %s' % valid) self._data_function = value
import os import testtools from jenkins_jobs.cli import entry from tests.base import LoggingFixture from tests.base import mock class CmdTestsBase(LoggingFixture, testtools.TestCase): fixtures_path = os.path.join(os.path.dirname(__file__), 'fixtures') def setUp(self): super(CmdTestsBase, self).setUp() # Testing the cmd module can sometimes result in the CacheStorage class # attempting to create the cache directory multiple times as the tests # are run in parallel. Stub out the CacheStorage to ensure that each # test can safely create the cache directory without risk of # interference. cache_patch = mock.patch('jenkins_jobs.builder.CacheStorage', autospec=True) self.cache_mock = cache_patch.start() self.addCleanup(cache_patch.stop) self.default_config_file = os.path.join(self.fixtures_path, 'empty_builder.ini') def execute_jenkins_jobs_with_args(self, args): jenkins_jobs = entry.JenkinsJobs(args) jenkins_jobs.execute() class TestCmd(CmdTestsBase): def test_with_empty_args(self): """ User passes no args, should fail with SystemExit """ with mock.patch('sys.stderr'): self.assertRaises(SystemExit, entry.JenkinsJobs, [])
#!/usr/bin/env python # Meran - MERAN UNLP is a ILS (Integrated Library System) wich provides Catalog, # Circulation and User's Management. It's written in Perl, and uses Apache2 # Web-Server, MySQL database and Sphinx 2 indexing. # Copyright (C) 2009-2013 Grupo de desarrollo de Meran CeSPI-UNLP # # This file is part of Meran. # # Meran is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Meran is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Meran. If not, see <http://www.gnu.org/licenses/>. # encoding: utf-8 # Meran - MERAN UNLP is a ILS (Integrated Library System) wich provides Catalog, # Circulation and User's Management. It's written in Perl, and uses Apache2 # Web-Server, MySQL database and Sphinx 2 indexing. # Copyright (C) 2009-2013 Grupo de desarrollo de Meran CeSPI-UNLP # # This file is part of Meran. # # Meran is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Meran is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Meran. If not, see <http://www.gnu.org/licenses/>. # Thomas Nagy, 2006 (ita) "Base for c programs/libraries" import os import TaskGen, Build, Utils, Task from Logs import debug import ccroot from TaskGen import feature, before, extension, after g_cc_flag_vars = [ 'CCDEPS', 'FRAMEWORK', 'FRAMEWORKPATH', 'STATICLIB', 'LIB', 'LIBPATH', 'LINKFLAGS', 'RPATH', 'CCFLAGS', 'CPPPATH', 'CPPFLAGS', 'CCDEFINES'] EXT_CC = ['.c'] g_cc_type_vars = ['CCFLAGS', 'LINKFLAGS'] # TODO remove in waf 1.6 class cc_taskgen(ccroot.ccroot_abstract): pass @feature('cc') @before('apply_type_vars') @after('default_cc') def init_cc(self): self.p_flag_vars = set(self.p_flag_vars).union(g_cc_flag_vars) self.p_type_vars = set(self.p_type_vars).union(g_cc_type_vars) if not self.env['CC_NAME']: raise Utils.WafError("At least one compiler (gcc, ..) must be selected") @feature('cc') @after('apply_incpaths') def apply_obj_vars_cc(self): """after apply_incpaths for INC_PATHS""" env = self.env app = env.append_unique cpppath_st = env['CPPPATH_ST'] # local flags come first # set the user-defined includes paths for i in env['INC_PATHS']: app('_CCINCFLAGS', cpppath_st % i.bldpath(env)) app('_CCINCFLAGS', cpppath_st % i.srcpath(env)) # set the library include paths for i in env['CPPPATH']: app('_CCINCFLAGS', cpppath_st % i) @feature('cc') @after('apply_lib_vars') def apply_defines_cc(self): """after uselib is set for CCDEFINES""" self.defines = getattr(self, 'defines', []) lst = self.to_list(self.defines) + self.to_list(self.env['CCDEFINES']) milst = [] # now process the local defines for defi in lst: if not defi in milst: milst.append(defi) # CCDEFINES_ libs = self.to_list(self.uselib) for l in libs: val = self.env['CCDEFINES_'+l] if val: milst += val self.env['DEFLINES'] = ["%s %s" % (x[0], Utils.trimquotes('='.join(x[1:]))) for x in [y.split('=') for y in milst]] y = self.env['CCDEFINES_ST'] self.env['_CCDEFFLAGS'] = [y%x for x in milst] @extension(EXT_CC) def c_hook(self, node): # create the compilation task: cpp or cc if getattr(self, 'obj_ext', None): obj_ext = self.obj_ext else: obj_ext = '_%d.o' % self.idx task = self.create_task('cc', node, node.change_ext(obj_ext)) try: self.compiled_tasks.append(task) except AttributeError: raise Utils.WafError('Have you forgotten to set the feature "cc" on %s?' % str(self)) return task cc_str = '${CC} ${CCFLAGS} ${CPPFLAGS} ${_CCINCFLAGS} ${_CCDEFFLAGS} ${CC_SRC_F}${SRC} ${CC_TGT_F}${TGT}' cls = Task.simple_task_type('cc', cc_str, 'GREEN', ext_out='.o', ext_in='.c', shell=False) cls.scan = ccroot.scan cls.vars.append('CCDEPS') link_str = '${LINK_CC} ${CCLNK_SRC_F}${SRC} ${CCLNK_TGT_F}${TGT[0].abspath(env)} ${LINKFLAGS}' cls = Task.simple_task_type('cc_link', link_str, color='YELLOW', ext_in='.o', ext_out='.bin', shell=False) cls.maxjobs = 1 cls.install = Utils.nada
Enum DEFINITIONS IMPLICIT TAGS ::= BEGIN -- EXPORTS P1, P2; -- F.2.3.1 -- Use an enumerated type to model the values of a variable -- with three or more states. -- Assign values starting with zero if their only -- constraint is distinctness. -- EXAMPLE DayOfTheWeek ::= ENUMERATED {sunday(0), monday(1), tuesday(2), wednesday(3), thursday(4), friday(5), saturday(6)} firstDay DayOfTheWeek ::= sunday -- F.2.3.2 -- Use an enumerated type to model the values of a variable that -- has just two states now, -- but that may have additional states in a future version of the protocol. -- EXAMPLE MaritalStatus ::= ENUMERATED {single(0), married(1)} -- in anticipation of MaritalStatus2 ::= ENUMERATED {single(0), married(1), widowed(2)} E1 ::= ENUMERATED {blue,green,yellow} E2 ::= ENUMERATED {monday(0),thuesday(1),wednesday(2),thursday(3),friday(4)} E3 ::= ENUMERATED {monday,thuesday(0)} S ::= SEQUENCE { e1 ENUMERATED {hej,hopp}, e2 [2] EXPLICIT ENUMERATED {san,sa} } enumVal E3 ::= monday --enumWrongVal E3 ::= sunday END
# -*- coding: utf-8 -*- import time import pika import json import logging import hashlib class ExamplePublisher(object): """This is an example publisher that will handle unexpected interactions with RabbitMQ such as channel and connection closures. If RabbitMQ closes the connection, it will reopen it. You should look at the output, as there are limited reasons why the connection may be closed, which usually are tied to permission related issues or socket timeouts. It uses delivery confirmations and illustrates one way to keep track of messages that have been sent and if they've been confirmed by RabbitMQ. """ EXCHANGE = 'transactions' EXCHANGE_TYPE = 'topic' PUBLISH_INTERVAL = 60 QUEUE = 'bancandes' ROUTING_KEY = 'llamabank.requests' def __init__(self, logger, amqp_url='amqp://llamabank:123llama123@margffoy-tuay.com:5672'): """Setup the example publisher object, passing in the URL we will use to connect to RabbitMQ. :param str amqp_url: The URL for connecting to RabbitMQ """ self._connection = None self._channel = None self._deliveries = [] self._acked = 0 self._nacked = 0 self._message_number = 0 self._stopping = False self._url = amqp_url self._closing = False self.logger = logger def connect(self): """This method connects to RabbitMQ, returning the connection handle. When the connection is established, the on_connection_open method will be invoked by pika. If you want the reconnection to work, make sure you set stop_ioloop_on_close to False, which is not the default behavior of this adapter. :rtype: pika.SelectConnection """ self.logger.info('Connecting to %s', self._url) cred = pika.PlainCredentials('llamabank', '123llama123') param = pika.ConnectionParameters( host='margffoy-tuay.com', port=5672, virtual_host='bancandesh', credentials=cred ) self._connection = pika.TornadoConnection(param, self.on_connection_open, stop_ioloop_on_close=False) def on_connection_open(self, unused_connection): """This method is called by pika once the connection to RabbitMQ has been established. It passes the handle to the connection object in case we need it, but in this case, we'll just mark it unused. :type unused_connection: pika.SelectConnection """ self.logger.info('Connection opened') self.add_on_connection_close_callback() self.open_channel() def add_on_connection_close_callback(self): """This method adds an on close callback that will be invoked by pika when RabbitMQ closes the connection to the publisher unexpectedly. """ self.logger.info('Adding connection close callback') self._connection.add_on_close_callback(self.on_connection_closed) def on_connection_closed(self, connection, reply_code, reply_text): """This method is invoked by pika when the connection to RabbitMQ is closed unexpectedly. Since it is unexpected, we will reconnect to RabbitMQ if it disconnects. :param pika.connection.Connection connection: The closed connection obj :param int reply_code: The server provided reply_code if given :param str reply_text: The server provided reply_text if given """ self._channel = None if self._closing: self._connection.ioloop.stop() else: self.logger.warning('Connection closed, reopening in 5 seconds: (%s) %s', reply_code, reply_text) self._connection.add_timeout(5, self.reconnect) def reconnect(self): """Will be invoked by the IOLoop timer if the connection is closed. See the on_connection_closed method. """ self._deliveries = [] self._acked = 0 self._nacked = 0 self._message_number = 0 # This is the old connection IOLoop instance, stop its ioloop # self._connection.ioloop.stop() # Create a new connection self.connect() # There is now a new connection, needs a new ioloop to run # self._connection.ioloop.start() def open_channel(self): """This method will open a new channel with RabbitMQ by issuing the Channel.Open RPC command. When RabbitMQ confirms the channel is open by sending the Channel.OpenOK RPC reply, the on_channel_open method will be invoked. """ self.logger.info('Creating a new channel') self._connection.channel(on_open_callback=self.on_channel_open) def on_channel_open(self, channel): """This method is invoked by pika when the channel has been opened. The channel object is passed in so we can make use of it. Since the channel is now open, we'll declare the exchange to use. :param pika.channel.Channel channel: The channel object """ self.logger.info('Channel opened') self._channel = channel self.add_on_channel_close_callback() self.setup_exchange(self.EXCHANGE) def add_on_channel_close_callback(self): """This method tells pika to call the on_channel_closed method if RabbitMQ unexpectedly closes the channel. """ self.logger.info('Adding channel close callback') self._channel.add_on_close_callback(self.on_channel_closed) def on_channel_closed(self, channel, reply_code, reply_text): """Invoked by pika when RabbitMQ unexpectedly closes the channel. Channels are usually closed if you attempt to do something that violates the protocol, such as re-declare an exchange or queue with different parameters. In this case, we'll close the connection to shutdown the object. :param pika.channel.Channel: The closed channel :param int reply_code: The numeric reason the channel was closed :param str reply_text: The text reason the channel was closed """ self.logger.warning('Channel was closed: (%s) %s', reply_code, reply_text) if not self._closing: self._connection.close() def setup_exchange(self, exchange_name): """Setup the exchange on RabbitMQ by invoking the Exchange.Declare RPC command. When it is complete, the on_exchange_declareok method will be invoked by pika. :param str|unicode exchange_name: The name of the exchange to declare """ self.logger.info('Declaring exchange %s', exchange_name) self._channel.exchange_declare(self.on_exchange_declareok, exchange_name, self.EXCHANGE_TYPE) def on_exchange_declareok(self, unused_frame): """Invoked by pika when RabbitMQ has finished the Exchange.Declare RPC command. :param pika.Frame.Method unused_frame: Exchange.DeclareOk response frame """ self.logger.info('Exchange declared') self.setup_queue(self.QUEUE) def setup_queue(self, queue_name): """Setup the queue on RabbitMQ by invoking the Queue.Declare RPC command. When it is complete, the on_queue_declareok method will be invoked by pika. :param str|unicode queue_name: The name of the queue to declare. """ self.logger.info('Declaring queue %s', queue_name) self._channel.queue_declare(self.on_queue_declareok, queue_name) def on_queue_declareok(self, method_frame): """Method invoked by pika when the Queue.Declare RPC call made in setup_queue has completed. In this method we will bind the queue and exchange together with the routing key by issuing the Queue.Bind RPC command. When this command is complete, the on_bindok method will be invoked by pika. :param pika.frame.Method method_frame: The Queue.DeclareOk frame """ self.logger.info('Binding %s to %s with %s', self.EXCHANGE, self.QUEUE, self.ROUTING_KEY) self._channel.queue_bind(self.on_bindok, self.QUEUE, self.EXCHANGE, self.ROUTING_KEY) def on_bindok(self, unused_frame): """This method is invoked by pika when it receives the Queue.BindOk response from RabbitMQ. Since we know we're now setup and bound, it's time to start publishing.""" self.logger.info('Queue bound') self.start_publishing() def start_publishing(self): """This method will enable delivery confirmations and schedule the first message to be sent to RabbitMQ """ self.logger.info('Issuing consumer related RPC commands') self.enable_delivery_confirmations() # self.schedule_next_message() def enable_delivery_confirmations(self): """Send the Confirm.Select RPC method to RabbitMQ to enable delivery confirmations on the channel. The only way to turn this off is to close the channel and create a new one. When the message is confirmed from RabbitMQ, the on_delivery_confirmation method will be invoked passing in a Basic.Ack or Basic.Nack method from RabbitMQ that will indicate which messages it is confirming or rejecting. """ self.logger.info('Issuing Confirm.Select RPC command') self._channel.confirm_delivery(self.on_delivery_confirmation) def on_delivery_confirmation(self, method_frame): """Invoked by pika when RabbitMQ responds to a Basic.Publish RPC command, passing in either a Basic.Ack or Basic.Nack frame with the delivery tag of the message that was published. The delivery tag is an integer counter indicating the message number that was sent on the channel via Basic.Publish. Here we're just doing house keeping to keep track of stats and remove message numbers that we expect a delivery confirmation of from the list used to keep track of messages that are pending confirmation. :param pika.frame.Method method_frame: Basic.Ack or Basic.Nack frame """ confirmation_type = method_frame.method.NAME.split('.')[1].lower() self.logger.info('Received %s for delivery tag: %i', confirmation_type, method_frame.method.delivery_tag) if confirmation_type == 'ack': self._acked += 1 elif confirmation_type == 'nack': self._nacked += 1 self._deliveries.remove(method_frame.method.delivery_tag) self.logger.info('Published %i messages, %i have yet to be confirmed, ' '%i were acked and %i were nacked', self._message_number, len(self._deliveries), self._acked, self._nacked) def schedule_next_message(self): """If we are not closing our connection to RabbitMQ, schedule another message to be delivered in PUBLISH_INTERVAL seconds. """ if self._stopping: return self.logger.info('Scheduling next message for %0.1f seconds', self.PUBLISH_INTERVAL) self._connection.add_timeout(self.PUBLISH_INTERVAL, self.publish_message) def publish_message(self, message): """If the class is not stopping, publish a message to RabbitMQ, appending a list of deliveries with the message number that was sent. This list will be used to check for delivery confirmations in the on_delivery_confirmations method. Once the message has been sent, schedule another message to be sent. The main reason I put scheduling in was just so you can get a good idea of how the process is flowing by slowing down and speeding up the delivery intervals by changing the PUBLISH_INTERVAL constant in the class. """ if self._stopping: return print message # message = {u'message': msg} properties = pika.BasicProperties(app_id='llamabank', content_type='application/json', headers=message) self._channel.basic_publish(self.EXCHANGE, self.ROUTING_KEY, json.dumps(message, ensure_ascii=False), properties) self._message_number += 1 self._deliveries.append(self._message_number) self.logger.info('Published message # %i', self._message_number) # self.schedule_next_message() def close_channel(self): """Invoke this command to close the channel with RabbitMQ by sending the Channel.Close RPC command. """ self.logger.info('Closing the channel') if self._channel: self._channel.close()
# -*- coding: utf-8 -*- # Copyright 2011-2012 Antoine Bertin <diaoulael@gmail.com> # # This file is part of subliminal. # # subliminal is free software; you can redistribute it and/or modify it under # the terms of the GNU Lesser General Public License as published by # the Free Software Foundation; either version 3 of the License, or # (at your option) any later version. # # subliminal is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Lesser General Public License for more details. # # You should have received a copy of the GNU Lesser General Public License # along with subliminal. If not, see <http://www.gnu.org/licenses/>. from .core import (SERVICES, LANGUAGE_INDEX, SERVICE_INDEX, SERVICE_CONFIDENCE, MATCHING_CONFIDENCE, create_list_tasks, consume_task, create_download_tasks, group_by_video, key_subtitles) from .language import language_set, language_list, LANGUAGES import logging __all__ = ['list_subtitles', 'download_subtitles'] logger = logging.getLogger(__name__) def list_subtitles(paths, languages=None, services=None, force=True, multi=False, cache_dir=None, max_depth=3, scan_filter=None): """List subtitles in given paths according to the criteria :param paths: path(s) to video file or folder :type paths: string or list :param languages: languages to search for, in preferred order :type languages: list of :class:`~subliminal.language.Language` or string :param list services: services to use for the search, in preferred order :param bool force: force searching for subtitles even if some are detected :param bool multi: search multiple languages for the same video :param string cache_dir: path to the cache directory to use :param int max_depth: maximum depth for scanning entries :param function scan_filter: filter function that takes a path as argument and returns a boolean indicating whether it has to be filtered out (``True``) or not (``False``) :return: found subtitles :rtype: dict of :class:`~subliminal.videos.Video` => [:class:`~subliminal.subtitles.ResultSubtitle`] """ services = services or SERVICES languages = language_set(languages) if languages is not None else language_set(LANGUAGES) if isinstance(paths, basestring): paths = [paths] if any([not isinstance(p, unicode) for p in paths]): logger.warning(u'Not all entries are unicode') results = [] service_instances = {} tasks = create_list_tasks(paths, languages, services, force, multi, cache_dir, max_depth, scan_filter) for task in tasks: try: result = consume_task(task, service_instances) results.append((task.video, result)) except: logger.error(u'Error consuming task %r' % task, exc_info=True) for service_instance in service_instances.itervalues(): service_instance.terminate() return group_by_video(results) def download_subtitles(paths, languages=None, services=None, force=True, multi=False, cache_dir=None, max_depth=3, scan_filter=None, order=None): """Download subtitles in given paths according to the criteria :param paths: path(s) to video file or folder :type paths: string or list :param languages: languages to search for, in preferred order :type languages: list of :class:`~subliminal.language.Language` or string :param list services: services to use for the search, in preferred order :param bool force: force searching for subtitles even if some are detected :param bool multi: search multiple languages for the same video :param string cache_dir: path to the cache directory to use :param int max_depth: maximum depth for scanning entries :param function scan_filter: filter function that takes a path as argument and returns a boolean indicating whether it has to be filtered out (``True``) or not (``False``) :param order: preferred order for subtitles sorting :type list: list of :data:`~subliminal.core.LANGUAGE_INDEX`, :data:`~subliminal.core.SERVICE_INDEX`, :data:`~subliminal.core.SERVICE_CONFIDENCE`, :data:`~subliminal.core.MATCHING_CONFIDENCE` :return: downloaded subtitles :rtype: dict of :class:`~subliminal.videos.Video` => [:class:`~subliminal.subtitles.ResultSubtitle`] .. note:: If you use ``multi=True``, :data:`~subliminal.core.LANGUAGE_INDEX` has to be the first item of the ``order`` list or you might get unexpected results. """ services = services or SERVICES languages = language_list(languages) if languages is not None else language_list(LANGUAGES) if isinstance(paths, basestring): paths = [paths] order = order or [LANGUAGE_INDEX, SERVICE_INDEX, SERVICE_CONFIDENCE, MATCHING_CONFIDENCE] subtitles_by_video = list_subtitles(paths, languages, services, force, multi, cache_dir, max_depth, scan_filter) for video, subtitles in subtitles_by_video.iteritems(): subtitles.sort(key=lambda s: key_subtitles(s, video, languages, services, order), reverse=True) results = [] service_instances = {} tasks = create_download_tasks(subtitles_by_video, languages, multi) for task in tasks: try: result = consume_task(task, service_instances) results.append((task.video, result)) except: logger.error(u'Error consuming task %r' % task, exc_info=True) for service_instance in service_instances.itervalues(): service_instance.terminate() return group_by_video(results)
#!/usr/bin/env python # -*- coding: utf-8 -*- import numpy as np import cv2 from time import clock from numpy import pi, sin, cos import common class VideoSynthBase(object): def __init__(self, size=None, noise=0.0, bg = None, **params): self.bg = None self.frame_size = (640, 480) if bg is not None: self.bg = cv2.imread(bg, 1) h, w = self.bg.shape[:2] self.frame_size = (w, h) if size is not None: w, h = map(int, size.split('x')) self.frame_size = (w, h) self.bg = cv2.resize(self.bg, self.frame_size) self.noise = float(noise) def render(self, dst): pass def read(self, dst=None): w, h = self.frame_size if self.bg is None: buf = np.zeros((h, w, 3), np.uint8) else: buf = self.bg.copy() self.render(buf) if self.noise > 0.0: noise = np.zeros((h, w, 3), np.int8) cv2.randn(noise, np.zeros(3), np.ones(3)*255*self.noise) buf = cv2.add(buf, noise, dtype=cv2.CV_8UC3) return True, buf class Chess(VideoSynthBase): def __init__(self, **kw): super(Chess, self).__init__(**kw) w, h = self.frame_size self.grid_size = sx, sy = 10, 7 white_quads = [] black_quads = [] for i, j in np.ndindex(sy, sx): q = [[j, i, 0], [j+1, i, 0], [j+1, i+1, 0], [j, i+1, 0]] [white_quads, black_quads][(i + j) % 2].append(q) self.white_quads = np.float32(white_quads) self.black_quads = np.float32(black_quads) fx = 0.9 self.K = np.float64([[fx*w, 0, 0.5*(w-1)], [0, fx*w, 0.5*(h-1)], [0.0,0.0, 1.0]]) self.dist_coef = np.float64([-0.2, 0.1, 0, 0]) self.t = 0 def draw_quads(self, img, quads, color = (0, 255, 0)): img_quads = cv2.projectPoints(quads.reshape(-1, 3), self.rvec, self.tvec, self.K, self.dist_coef) [0] img_quads.shape = quads.shape[:2] + (2,) for q in img_quads: cv2.fillConvexPoly(img, np.int32(q*4), color, cv2.CV_AA, shift=2) def render(self, dst): t = self.t self.t += 1.0/30.0 sx, sy = self.grid_size center = np.array([0.5*sx, 0.5*sy, 0.0]) phi = pi/3 + sin(t*3)*pi/8 c, s = cos(phi), sin(phi) ofs = np.array([sin(1.2*t), cos(1.8*t), 0]) * sx * 0.2 eye_pos = center + np.array([cos(t)*c, sin(t)*c, s]) * 15.0 + ofs target_pos = center + ofs R, self.tvec = common.lookat(eye_pos, target_pos) self.rvec = common.mtx2rvec(R) self.draw_quads(dst, self.white_quads, (245, 245, 245)) self.draw_quads(dst, self.black_quads, (10, 10, 10)) classes = dict(chess=Chess) def create_capture(source): ''' source: <int> or '<int>' or '<filename>' or 'synth:<params>' ''' try: source = int(source) except ValueError: pass else: return cv2.VideoCapture(source) source = str(source).strip() if source.startswith('synth'): ss = filter(None, source.split(':')) params = dict( s.split('=') for s in ss[1:] ) try: Class = classes[params['class']] except: Class = VideoSynthBase return Class(**params) return cv2.VideoCapture(source) presets = dict( empty = 'synth:', lena = 'synth:bg=../cpp/lena.jpg:noise=0.1', chess = 'synth:class=chess:bg=../cpp/lena.jpg:noise=0.1:size=640x480' ) if __name__ == '__main__': import sys import getopt print 'USAGE: video.py [--shotdir <dir>] [source0] [source1] ...' print "source: '<int>' or '<filename>' or 'synth:<params>'" print args, sources = getopt.getopt(sys.argv[1:], '', 'shotdir=') args = dict(args) shotdir = args.get('--shotdir', '.') if len(sources) == 0: sources = [ presets['chess'] ] print 'Press SPACE to save current frame' caps = map(create_capture, sources) shot_idx = 0 while True: imgs = [] for i, cap in enumerate(caps): ret, img = cap.read() imgs.append(img) cv2.imshow('capture %d' % i, img) ch = cv2.waitKey(1) if ch == 27: break if ch == ord(' '): for i, img in enumerate(imgs): fn = '%s/shot_%d_%03d.bmp' % (shotdir, i, shot_idx) cv2.imwrite(fn, img) print fn, 'saved' shot_idx += 1
#!/usr/bin/python from PyQt4.QtCore import * from PyQt4.QtSql import * from PyQt4.QtGui import * from blur.Stone import * from blur.Classes import * from blur.Classesui import * import blur.email, blur.jabber import sys, time, re, os from math import ceil import traceback try: import popen2 except: pass app = QApplication(sys.argv) initConfig( "/etc/db.ini", "/var/log/path_lister.log" ) blur.RedirectOutputToLog() blurqt_loader() VERBOSE_DEBUG = False if VERBOSE_DEBUG: Database.instance().setEchoMode( Database.EchoUpdate | Database.EchoDelete )# | Database.EchoSelect ) FreezerCore.instance().reconnect() class ProjectChooserDialog( QDialog ): def __init__(self): QDialog.__init__(self) # Project Chooser Widget self.projectCombo = ProjectCombo(self) self.projectCombo.setShowSpecialItem(False) # Ok | Cancel buttons dbb = QDialogButtonBox( QDialogButtonBox.Ok | QDialogButtonBox.Cancel, Qt.Horizontal, self ) self.connect( dbb, SIGNAL( 'accepted()' ), self.accept ) self.connect( dbb, SIGNAL( 'rejected()' ), self.reject ) # Layout l = QVBoxLayout(self) l.addWidget( self.projectCombo ) l.addWidget( dbb ) def project(self): return self.projectCombo.project() project = None regenPaths = len(sys.argv) > 2 and sys.argv[2] if len(sys.argv) > 1: project = Project.recordByName( sys.argv[1] ) if not project or not project.isRecord(): d = ProjectChooserDialog() if d.exec_() == QDialog.Accepted: project = d.project() if project.isRecord(): storageLocations = project.projectStorages() def print_paths( asset, tabs = 0 ): for c in asset.children(): print (' ' * tabs), c.name(), c.assetTemplate().name(), c.pathTemplate().name() for storage in storageLocations: path = c.path(storage) pt = c.pathTracker(storage) if not path.isEmpty() or pt.isRecord(): print (' ' * tabs) + ' ', path gen_path = pt.generatePathFromTemplate(storage) if path != gen_path: if regenPaths: print "Changing path to match template: ", gen_path pt.setPath(gen_path) pt.commit() else: print "Path doesnt match template: ", gen_path print_paths( c, tabs + 1 ) print_paths( project )
from datasketch import MinHashLSHForest, MinHash data1 = ['minhash', 'is', 'a', 'probabilistic', 'data', 'structure', 'for', 'estimating', 'the', 'similarity', 'between', 'datasets'] data2 = ['minhash', 'is', 'a', 'probability', 'data', 'structure', 'for', 'estimating', 'the', 'similarity', 'between', 'documents'] data3 = ['minhash', 'is', 'probability', 'data', 'structure', 'for', 'estimating', 'the', 'similarity', 'between', 'documents'] # Create MinHash objects m1 = MinHash(num_perm=128) m2 = MinHash(num_perm=128) m3 = MinHash(num_perm=128) for d in data1: m1.update(d.encode('utf8')) for d in data2: m2.update(d.encode('utf8')) for d in data3: m3.update(d.encode('utf8')) forest = MinHashLSHForest(num_perm=128) # Add m2 and m3 into the index forest.add("m2", m2) forest.add("m3", m3) # IMPORTANT: must call index() otherwise the keys won't be searchable forest.index() # Check for membership using the key print("m2" in forest) print("m3" in forest) # Using m1 as the query, retrieve top 2 keys that have the higest Jaccard result = forest.query(m1, 2) print("Top 2 candidates", result)
"""Base translators for translating Grow content.""" import copy import json import logging import os import threading import progressbar import texttable import yaml from protorpc import message_types from protorpc import messages from protorpc import protojson from grow.common import progressbar_non from grow.common import utils from grow.translators import errors as translator_errors class TranslatorStat(messages.Message): lang = messages.StringField(1) num_words = messages.IntegerField(2) num_words_translated = messages.IntegerField(3) source_lang = messages.StringField(4) ident = messages.StringField(5) url = messages.StringField(6) uploaded = message_types.DateTimeField(7) service = messages.StringField(8) downloaded = message_types.DateTimeField(9) def translator_stat_representer(dumper, stat): content = json.loads(protojson.encode_message(stat)) content.pop('lang') # Exclude from serialization. return dumper.represent_mapping('tag:yaml.org,2002:map', content) yaml.SafeDumper.add_representer(TranslatorStat, translator_stat_representer) class TranslatorServiceError(Exception): def __init__(self, message, ident=None, locale=None): if locale: new_message = 'Error for locale "{}" -> {}'.format(locale, message) elif ident: new_message = 'Error for resource "{}" -> {}'.format( ident, message) else: new_message = message self.message = new_message super(TranslatorServiceError, self).__init__(new_message) class Translator(object): TRANSLATOR_STATS_PATH = '/translators.yaml' KIND = None has_immutable_translation_resources = False has_multiple_langs_in_one_resource = False def __init__(self, pod, config=None, project_title=None, instructions=None): self.pod = pod self.config = config or {} self.project_title = project_title or 'Untitled Grow Website' self.instructions = instructions def _cleanup_locales(self, locales): """Certain locales should be ignored.""" clean_locales = [] default_locale = self.pod.podspec.default_locale skipped = { 'symlink': set(), 'po': set(), } for locale in locales: locale_path = os.path.join('translations', str(locale)) # Silently ignore the default locale. if default_locale and str(locale) == str(default_locale): continue # Ignore the symlinks. if os.path.islink(locale_path): skipped['symlink'].add(str(locale)) continue # Ignore the locales without a `.PO` file. po_path = os.path.join(locale_path, 'LC_MESSAGES', 'messages.po') if not self.pod.file_exists(po_path): skipped['po'].add(str(locale)) continue clean_locales.append(locale) # Summary of skipped files. if skipped['symlink']: self.pod.logger.info('Skipping: {} (symlinked)'.format( ', '.join(sorted(skipped['symlink'])))) if skipped['po']: self.pod.logger.info('Skipping: {} (no `.po` file)'.format( ', '.join(sorted(skipped['po'])))) return clean_locales def _download_content(self, stat): raise NotImplementedError def _log_catalog_changes(self, unchanged_locales, changed_locales): if unchanged_locales: self.pod.logger.info('No translations updated: {}'.format( ', '.join(sorted(unchanged_locales)))) if changed_locales: for locale, value in changed_locales.items(): self.pod.logger.info('Updated {} of {} translations: {}'.format( value['imported'], value['total'], locale)) def _upload_catalog(self, catalog, source_lang, prune): raise NotImplementedError def _upload_catalogs(self, catalogs, source_lang, prune=False): raise NotImplementedError def _update_acl(self, stat, locale): raise NotImplementedError def _update_acls(self, stat, locales): raise NotImplementedError def _update_meta(self, stat, locale, catalog): raise NotImplementedError def needs_meta_update(self): """Allow to be flagged for additional meta update after uploading.""" return False def _get_stats_to_download(self, locales): # 'stats' maps the service name to a mapping of languages to stats. if not self.pod.file_exists(Translator.TRANSLATOR_STATS_PATH): return {} stats = self.pod.read_yaml(Translator.TRANSLATOR_STATS_PATH) if self.KIND not in stats: self.pod.logger.info( 'Nothing found to download from {}'.format(self.KIND)) return {} stats_to_download = stats[self.KIND] if locales: stats_to_download = dict([(lang, stat) for (lang, stat) in stats_to_download.items() if lang in locales]) for lang, stat in stats_to_download.items(): if isinstance(stat, TranslatorStat): stat = json.loads(protojson.encode_message(stat)) stat['lang'] = lang stat_message = protojson.decode_message(TranslatorStat, json.dumps(stat)) stats_to_download[lang] = stat_message return stats_to_download def download(self, locales, save_stats=True, include_obsolete=False): # TODO: Rename to `download_and_import`. if not self.pod.file_exists(Translator.TRANSLATOR_STATS_PATH): text = 'File {} not found. Nothing to download.' self.pod.logger.info(text.format(Translator.TRANSLATOR_STATS_PATH)) return stats_to_download = self._get_stats_to_download(locales) if not stats_to_download: return num_files = len(stats_to_download) text = 'Downloading translations: %(value)d/{} (in %(time_elapsed).9s)' widgets = [progressbar.FormatLabel(text.format(num_files))] bar = progressbar_non.create_progressbar( "Downloading translations...", widgets=widgets, max_value=num_files) bar.start() threads = [] langs_to_translations = {} new_stats = [] def _do_download(lang, stat): try: new_stat, content = self._download_content(stat) except translator_errors.NotFoundError: text = 'No translations to download for: {}' self.pod.logger.info(text.format(lang)) return new_stat.uploaded = stat.uploaded # Preserve uploaded field. langs_to_translations[lang] = content new_stats.append(new_stat) for i, (lang, stat) in enumerate(stats_to_download.items()): thread = utils.ProgressBarThread( bar, True, target=_do_download, args=(lang, stat)) threads.append(thread) thread.start() # Perform the first operation synchronously to avoid oauth2 refresh # locking issues. if i == 0: thread.join() for i, thread in enumerate(threads): if i > 0: thread.join() bar.finish() has_changed_content = False unchanged_locales = [] changed_locales = {} for lang, translations in langs_to_translations.items(): has_changed_content, imported_translations, total_translations = self.pod.catalogs.import_translations( locale=lang, content=translations, include_obsolete=include_obsolete) if imported_translations == 0: unchanged_locales.append(lang) else: changed_locales[lang] = { 'imported': imported_translations, 'total': total_translations, } if has_changed_content: has_changed_content = True if save_stats and has_changed_content: self.save_stats(new_stats) self._log_catalog_changes(unchanged_locales, changed_locales) return new_stats def update_acl(self, locales=None): locales = locales or self.pod.catalogs.list_locales() locales = self._cleanup_locales(locales) if not locales: self.pod.logger.info('No locales to found to update.') return stats_to_download = self._get_stats_to_download(locales) if not stats_to_download: self.pod.logger.info('No documents found to update.') return if self.has_multiple_langs_in_one_resource: self._update_acls(stats_to_download, locales) stat = list(stats_to_download.values())[0] self.pod.logger.info('ACL updated -> {}'.format(stat.ident)) return threads = [] for i, (locale, stat) in enumerate(stats_to_download.items()): thread = threading.Thread( target=self._update_acl, args=(stat, locale)) threads.append(thread) thread.start() if i == 0: thread.join() self.pod.logger.info( 'ACL updated ({}): {}'.format(stat.lang, stat.url)) for i, thread in enumerate(threads): if i > 0: thread.join() def update_meta(self, locales=None): locales = locales or self.pod.catalogs.list_locales() locales = self._cleanup_locales(locales) if not locales: self.pod.logger.info('No locales to found to update.') return stats_to_download = self._get_stats_to_download(locales) if not stats_to_download: self.pod.logger.info('No documents found to update.') return threads = [] for i, (locale, stat) in enumerate(stats_to_download.items()): catalog_for_meta = self.pod.catalogs.get(locale) thread = threading.Thread( target=self._update_meta, args=(stat, locale, catalog_for_meta)) threads.append(thread) thread.start() if i == 0: thread.join() self.pod.logger.info('Meta information updated ({}): {}'.format( stat.lang, stat.url)) for i, thread in enumerate(threads): if i > 0: thread.join() def upload(self, locales=None, force=True, verbose=False, save_stats=True, prune=False): source_lang = self.pod.podspec.default_locale locales = locales or self.pod.catalogs.list_locales() locales = self._cleanup_locales(locales) stats = [] num_files = len(locales) if not locales: self.pod.logger.info('No locales to upload.') return if not force: if (self.has_immutable_translation_resources and self.pod.file_exists(Translator.TRANSLATOR_STATS_PATH)): text = 'Found existing translator data in: {}' self.pod.logger.info(text.format( Translator.TRANSLATOR_STATS_PATH)) text = 'This will be updated with new data after the upload is complete.' self.pod.logger.info(text) text = 'Proceed to upload {} translation catalogs?' text = text.format(num_files) if not utils.interactive_confirm(text): self.pod.logger.info('Aborted.') return if self.has_multiple_langs_in_one_resource: catalogs_to_upload = [] for locale in locales: catalog_to_upload = self.pod.catalogs.get(locale) if catalog_to_upload: catalogs_to_upload.append(catalog_to_upload) stats = self._upload_catalogs(catalogs_to_upload, source_lang, prune=prune) else: text = 'Uploading translations: %(value)d/{} (in %(time_elapsed).9s)' widgets = [progressbar.FormatLabel(text.format(num_files))] bar = progressbar_non.create_progressbar( "Uploading translations...", widgets=widgets, max_value=num_files) bar.start() threads = [] def _do_upload(locale): catalog = self.pod.catalogs.get(locale) stat = self._upload_catalog(catalog, source_lang, prune=prune) stats.append(stat) for i, locale in enumerate(locales): thread = utils.ProgressBarThread( bar, True, target=_do_upload, args=(locale,)) threads.append(thread) thread.start() # Perform the first operation synchronously to avoid oauth2 refresh # locking issues. if i == 0: thread.join() for i, thread in enumerate(threads): if i > 0: thread.join() bar.finish() stats = sorted(stats, key=lambda stat: stat.lang) if verbose: self.pretty_print_stats(stats) if save_stats: self.save_stats(stats) return stats def save_stats(self, stats): """Merges a list of stats into the translator stats file.""" if self.pod.file_exists(Translator.TRANSLATOR_STATS_PATH): content = self.pod.read_yaml(Translator.TRANSLATOR_STATS_PATH) create = False else: content = {} create = True if self.KIND not in content: content[self.KIND] = {} for stat in copy.deepcopy(stats): content[self.KIND][stat.lang] = stat yaml_content = yaml.safe_dump(content, default_flow_style=False) self.pod.write_file(Translator.TRANSLATOR_STATS_PATH, yaml_content) if create: self.pod.logger.info('Saved: {}'.format( Translator.TRANSLATOR_STATS_PATH)) else: self.pod.logger.info('Updated: {}'.format( Translator.TRANSLATOR_STATS_PATH)) @classmethod def pretty_print_stats(cls, stats): table = texttable.Texttable(max_width=0) table.set_deco(texttable.Texttable.HEADER) rows = [] rows.append(['Language', 'URL', 'Wordcount']) for stat in stats: rows.append([stat.lang, stat.url, stat.num_words or '--']) table.add_rows(rows) logging.info('\n' + table.draw() + '\n') def get_edit_url(self, doc): if not doc.locale: return stats = self._get_stats_to_download([doc.locale]) if doc.locale not in stats: return stat = stats[doc.locale] return stat.url
from unittest import TestCase from zoom.www.entities.application_state import ApplicationState class TestApplicationState(TestCase): def setUp(self): self.state = ApplicationState(application_name="1", configuration_path="2", application_status="3", application_host=None, last_update=1388556000, start_stop_time="6", error_state="7", delete="8", local_mode="9", login_user="10", last_command="12", pd_disabled=False, grayed=True, read_only=True, load_times=1, restart_count=0, platform=0) def test_to_dictionary(self): self.assertEquals( { 'application_name': "1", 'configuration_path': "2", 'application_status': "unknown", 'application_host': "", 'last_update': '2014-01-01 00:00:00', 'start_stop_time': "6", 'error_state': "7", 'delete': "8", 'local_mode': "9", 'login_user': "10", 'last_command': "12", 'pd_disabled': False, 'grayed': True, 'read_only': True, 'load_times': 1, 'restart_count': 0, 'platform': 0 }, self.state.to_dictionary() )
#!/usr/bin/env python # # Copyright 2009 Google Inc. All Rights Reserved. # # Redistribution and use in source and binary forms, with or without # modification, are permitted provided that the following conditions are # met: # # * Redistributions of source code must retain the above copyright # notice, this list of conditions and the following disclaimer. # * Redistributions in binary form must reproduce the above # copyright notice, this list of conditions and the following disclaimer # in the documentation and/or other materials provided with the # distribution. # * Neither the name of Google Inc. nor the names of its # contributors may be used to endorse or promote products derived from # this software without specific prior written permission. # # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS # "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT # LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR # A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT # OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, # SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT # LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, # DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY # THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT # (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE # OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. """Verifies that test shuffling works.""" __author__ = 'wan@google.com (Zhanyong Wan)' import os import gtest_test_utils # Command to run the gtest_shuffle_test_ program. COMMAND = gtest_test_utils.GetTestExecutablePath('gtest_shuffle_test_') # The environment variables for test sharding. TOTAL_SHARDS_ENV_VAR = 'GTEST_TOTAL_SHARDS' SHARD_INDEX_ENV_VAR = 'GTEST_SHARD_INDEX' TEST_FILTER = 'A*.A:A*.B:C*' ALL_TESTS = [] ACTIVE_TESTS = [] FILTERED_TESTS = [] SHARDED_TESTS = [] SHUFFLED_ALL_TESTS = [] SHUFFLED_ACTIVE_TESTS = [] SHUFFLED_FILTERED_TESTS = [] SHUFFLED_SHARDED_TESTS = [] def AlsoRunDisabledTestsFlag(): return '--gtest_also_run_disabled_tests' def FilterFlag(test_filter): return '--gtest_filter=%s' % (test_filter,) def RepeatFlag(n): return '--gtest_repeat=%s' % (n,) def ShuffleFlag(): return '--gtest_shuffle' def RandomSeedFlag(n): return '--gtest_random_seed=%s' % (n,) def RunAndReturnOutput(extra_env, args): """Runs the test program and returns its output.""" environ_copy = os.environ.copy() environ_copy.update(extra_env) return gtest_test_utils.Subprocess([COMMAND] + args, env=environ_copy).output def GetTestsForAllIterations(extra_env, args): """Runs the test program and returns a list of test lists. Args: extra_env: a map from environment variables to their values args: command line flags to pass to gtest_shuffle_test_ Returns: A list where the i-th element is the list of tests run in the i-th test iteration. """ test_iterations = [] for line in RunAndReturnOutput(extra_env, args).split('\n'): if line.startswith('----'): tests = [] test_iterations.append(tests) elif line.strip(): tests.append(line.strip()) # 'TestCaseName.TestName' return test_iterations def GetTestCases(tests): """Returns a list of test cases in the given full test names. Args: tests: a list of full test names Returns: A list of test cases from 'tests', in their original order. Consecutive duplicates are removed. """ test_cases = [] for test in tests: test_case = test.split('.')[0] if not test_case in test_cases: test_cases.append(test_case) return test_cases def CalculateTestLists(): """Calculates the list of tests run under different flags.""" if not ALL_TESTS: ALL_TESTS.extend( GetTestsForAllIterations({}, [AlsoRunDisabledTestsFlag()])[0]) if not ACTIVE_TESTS: ACTIVE_TESTS.extend(GetTestsForAllIterations({}, [])[0]) if not FILTERED_TESTS: FILTERED_TESTS.extend( GetTestsForAllIterations({}, [FilterFlag(TEST_FILTER)])[0]) if not SHARDED_TESTS: SHARDED_TESTS.extend( GetTestsForAllIterations({TOTAL_SHARDS_ENV_VAR: '3', SHARD_INDEX_ENV_VAR: '1'}, [])[0]) if not SHUFFLED_ALL_TESTS: SHUFFLED_ALL_TESTS.extend(GetTestsForAllIterations( {}, [AlsoRunDisabledTestsFlag(), ShuffleFlag(), RandomSeedFlag(1)])[0]) if not SHUFFLED_ACTIVE_TESTS: SHUFFLED_ACTIVE_TESTS.extend(GetTestsForAllIterations( {}, [ShuffleFlag(), RandomSeedFlag(1)])[0]) if not SHUFFLED_FILTERED_TESTS: SHUFFLED_FILTERED_TESTS.extend(GetTestsForAllIterations( {}, [ShuffleFlag(), RandomSeedFlag(1), FilterFlag(TEST_FILTER)])[0]) if not SHUFFLED_SHARDED_TESTS: SHUFFLED_SHARDED_TESTS.extend( GetTestsForAllIterations({TOTAL_SHARDS_ENV_VAR: '3', SHARD_INDEX_ENV_VAR: '1'}, [ShuffleFlag(), RandomSeedFlag(1)])[0]) class GTestShuffleUnitTest(gtest_test_utils.TestCase): """Tests test shuffling.""" def setUp(self): CalculateTestLists() def testShufflePreservesNumberOfTests(self): self.assertEqual(len(ALL_TESTS), len(SHUFFLED_ALL_TESTS)) self.assertEqual(len(ACTIVE_TESTS), len(SHUFFLED_ACTIVE_TESTS)) self.assertEqual(len(FILTERED_TESTS), len(SHUFFLED_FILTERED_TESTS)) self.assertEqual(len(SHARDED_TESTS), len(SHUFFLED_SHARDED_TESTS)) def testShuffleChangesTestOrder(self): self.assert_(SHUFFLED_ALL_TESTS != ALL_TESTS, SHUFFLED_ALL_TESTS) self.assert_(SHUFFLED_ACTIVE_TESTS != ACTIVE_TESTS, SHUFFLED_ACTIVE_TESTS) self.assert_(SHUFFLED_FILTERED_TESTS != FILTERED_TESTS, SHUFFLED_FILTERED_TESTS) self.assert_(SHUFFLED_SHARDED_TESTS != SHARDED_TESTS, SHUFFLED_SHARDED_TESTS) def testShuffleChangesTestCaseOrder(self): self.assert_(GetTestCases(SHUFFLED_ALL_TESTS) != GetTestCases(ALL_TESTS), GetTestCases(SHUFFLED_ALL_TESTS)) self.assert_( GetTestCases(SHUFFLED_ACTIVE_TESTS) != GetTestCases(ACTIVE_TESTS), GetTestCases(SHUFFLED_ACTIVE_TESTS)) self.assert_( GetTestCases(SHUFFLED_FILTERED_TESTS) != GetTestCases(FILTERED_TESTS), GetTestCases(SHUFFLED_FILTERED_TESTS)) self.assert_( GetTestCases(SHUFFLED_SHARDED_TESTS) != GetTestCases(SHARDED_TESTS), GetTestCases(SHUFFLED_SHARDED_TESTS)) def testShuffleDoesNotRepeatTest(self): for test in SHUFFLED_ALL_TESTS: self.assertEqual(1, SHUFFLED_ALL_TESTS.count(test), '%s appears more than once' % (test,)) for test in SHUFFLED_ACTIVE_TESTS: self.assertEqual(1, SHUFFLED_ACTIVE_TESTS.count(test), '%s appears more than once' % (test,)) for test in SHUFFLED_FILTERED_TESTS: self.assertEqual(1, SHUFFLED_FILTERED_TESTS.count(test), '%s appears more than once' % (test,)) for test in SHUFFLED_SHARDED_TESTS: self.assertEqual(1, SHUFFLED_SHARDED_TESTS.count(test), '%s appears more than once' % (test,)) def testShuffleDoesNotCreateNewTest(self): for test in SHUFFLED_ALL_TESTS: self.assert_(test in ALL_TESTS, '%s is an invalid test' % (test,)) for test in SHUFFLED_ACTIVE_TESTS: self.assert_(test in ACTIVE_TESTS, '%s is an invalid test' % (test,)) for test in SHUFFLED_FILTERED_TESTS: self.assert_(test in FILTERED_TESTS, '%s is an invalid test' % (test,)) for test in SHUFFLED_SHARDED_TESTS: self.assert_(test in SHARDED_TESTS, '%s is an invalid test' % (test,)) def testShuffleIncludesAllTests(self): for test in ALL_TESTS: self.assert_(test in SHUFFLED_ALL_TESTS, '%s is missing' % (test,)) for test in ACTIVE_TESTS: self.assert_(test in SHUFFLED_ACTIVE_TESTS, '%s is missing' % (test,)) for test in FILTERED_TESTS: self.assert_(test in SHUFFLED_FILTERED_TESTS, '%s is missing' % (test,)) for test in SHARDED_TESTS: self.assert_(test in SHUFFLED_SHARDED_TESTS, '%s is missing' % (test,)) def testShuffleLeavesDeathTestsAtFront(self): non_death_test_found = False for test in SHUFFLED_ACTIVE_TESTS: if 'DeathTest.' in test: self.assert_(not non_death_test_found, '%s appears after a non-death test' % (test,)) else: non_death_test_found = True def _VerifyTestCasesDoNotInterleave(self, tests): test_cases = [] for test in tests: [test_case, _] = test.split('.') if test_cases and test_cases[-1] != test_case: test_cases.append(test_case) self.assertEqual(1, test_cases.count(test_case), 'Test case %s is not grouped together in %s' % (test_case, tests)) def testShuffleDoesNotInterleaveTestCases(self): self._VerifyTestCasesDoNotInterleave(SHUFFLED_ALL_TESTS) self._VerifyTestCasesDoNotInterleave(SHUFFLED_ACTIVE_TESTS) self._VerifyTestCasesDoNotInterleave(SHUFFLED_FILTERED_TESTS) self._VerifyTestCasesDoNotInterleave(SHUFFLED_SHARDED_TESTS) def testShuffleRestoresOrderAfterEachIteration(self): # Get the test lists in all 3 iterations, using random seed 1, 2, # and 3 respectively. Google Test picks a different seed in each # iteration, and this test depends on the current implementation # picking successive numbers. This dependency is not ideal, but # makes the test much easier to write. [tests_in_iteration1, tests_in_iteration2, tests_in_iteration3] = ( GetTestsForAllIterations( {}, [ShuffleFlag(), RandomSeedFlag(1), RepeatFlag(3)])) # Make sure running the tests with random seed 1 gets the same # order as in iteration 1 above. [tests_with_seed1] = GetTestsForAllIterations( {}, [ShuffleFlag(), RandomSeedFlag(1)]) self.assertEqual(tests_in_iteration1, tests_with_seed1) # Make sure running the tests with random seed 2 gets the same # order as in iteration 2 above. Success means that Google Test # correctly restores the test order before re-shuffling at the # beginning of iteration 2. [tests_with_seed2] = GetTestsForAllIterations( {}, [ShuffleFlag(), RandomSeedFlag(2)]) self.assertEqual(tests_in_iteration2, tests_with_seed2) # Make sure running the tests with random seed 3 gets the same # order as in iteration 3 above. Success means that Google Test # correctly restores the test order before re-shuffling at the # beginning of iteration 3. [tests_with_seed3] = GetTestsForAllIterations( {}, [ShuffleFlag(), RandomSeedFlag(3)]) self.assertEqual(tests_in_iteration3, tests_with_seed3) def testShuffleGeneratesNewOrderInEachIteration(self): [tests_in_iteration1, tests_in_iteration2, tests_in_iteration3] = ( GetTestsForAllIterations( {}, [ShuffleFlag(), RandomSeedFlag(1), RepeatFlag(3)])) self.assert_(tests_in_iteration1 != tests_in_iteration2, tests_in_iteration1) self.assert_(tests_in_iteration1 != tests_in_iteration3, tests_in_iteration1) self.assert_(tests_in_iteration2 != tests_in_iteration3, tests_in_iteration2) def testShuffleShardedTestsPreservesPartition(self): # If we run M tests on N shards, the same M tests should be run in # total, regardless of the random seeds used by the shards. [tests1] = GetTestsForAllIterations({TOTAL_SHARDS_ENV_VAR: '3', SHARD_INDEX_ENV_VAR: '0'}, [ShuffleFlag(), RandomSeedFlag(1)]) [tests2] = GetTestsForAllIterations({TOTAL_SHARDS_ENV_VAR: '3', SHARD_INDEX_ENV_VAR: '1'}, [ShuffleFlag(), RandomSeedFlag(20)]) [tests3] = GetTestsForAllIterations({TOTAL_SHARDS_ENV_VAR: '3', SHARD_INDEX_ENV_VAR: '2'}, [ShuffleFlag(), RandomSeedFlag(25)]) sorted_sharded_tests = tests1 + tests2 + tests3 sorted_sharded_tests.sort() sorted_active_tests = [] sorted_active_tests.extend(ACTIVE_TESTS) sorted_active_tests.sort() self.assertEqual(sorted_active_tests, sorted_sharded_tests) if __name__ == '__main__': gtest_test_utils.Main()
##################################################################### # File: ReqManagerHandler.py ######################################################################## """ :mod: ReqManagerHandler .. module: ReqManagerHandler :synopsis: Implementation of the RequestDB service in the DISET framework """ __RCSID__ = "$Id$" # # imports import json import datetime import math from types import DictType, IntType, LongType, ListType, StringTypes, BooleanType # # from DIRAC from DIRAC import gLogger, S_OK, S_ERROR from DIRAC.Core.DISET.RequestHandler import RequestHandler, getServiceOption # # from RMS from DIRAC.RequestManagementSystem.Client.Request import Request from DIRAC.RequestManagementSystem.private.RequestValidator import RequestValidator from DIRAC.RequestManagementSystem.DB.RequestDB import RequestDB class ReqManagerHandler( RequestHandler ): """ .. class:: ReqManagerHandler RequestDB interface in the DISET framework. """ # # request validator __validator = None # # request DB instance __requestDB = None @classmethod def initializeHandler( cls, serviceInfoDict ): """ initialize handler """ try: cls.__requestDB = RequestDB() except RuntimeError, error: gLogger.exception( error ) return S_ERROR( error ) # If there is a constant delay to be applied to each request cls.constantRequestDelay = getServiceOption( serviceInfoDict, 'ConstantRequestDelay', 0 ) # # create tables for empty db return cls.__requestDB.createTables() # # helper functions @classmethod def validate( cls, request ): """ request validation """ if not cls.__validator: cls.__validator = RequestValidator() return cls.__validator.validate( request ) types_getRequestIDForName = [ StringTypes ] @classmethod def export_getRequestIDForName( cls, requestName ): """ get requestID for given :requestName: """ if type( requestName ) in StringTypes: result = cls.__requestDB.getRequestIDForName( requestName ) if not result["OK"]: return result requestID = result["Value"] return S_OK( requestID ) types_cancelRequest = [ ( IntType, LongType ) ] @classmethod def export_cancelRequest( cls , requestID ): """ Cancel a request """ return cls.__requestDB.cancelRequest( requestID ) types_putRequest = [ StringTypes ] @classmethod def export_putRequest( cls, requestJSON ): """ put a new request into RequestDB :param cls: class ref :param str requestJSON: request serialized to JSON format """ requestDict = json.loads( requestJSON ) requestName = requestDict.get( "RequestID", requestDict.get( 'RequestName', "***UNKNOWN***" ) ) request = Request( requestDict ) optimized = request.optimize() if optimized.get( "Value", False ): gLogger.debug( "putRequest: request was optimized" ) else: gLogger.debug( "putRequest: request unchanged", optimized.get( "Message", "Nothing could be optimized" ) ) valid = cls.validate( request ) if not valid["OK"]: gLogger.error( "putRequest: request %s not valid: %s" % ( requestName, valid["Message"] ) ) return valid # If NotBefore is not set or user defined, we calculate its value now = datetime.datetime.utcnow().replace( microsecond = 0 ) extraDelay = datetime.timedelta( 0 ) if request.Status not in Request.FINAL_STATES and ( not request.NotBefore or request.NotBefore < now ) : # We don't delay if it is the first insertion if getattr( request, 'RequestID', 0 ): # If it is a constant delay, just set it if cls.constantRequestDelay: extraDelay = datetime.timedelta( minutes = cls.constantRequestDelay ) else: # If there is a waiting Operation with Files op = request.getWaiting().get( 'Value' ) if op and len( op ): attemptList = [ opFile.Attempt for opFile in op if opFile.Status == "Waiting" ] if attemptList: maxWaitingAttempt = max( [ opFile.Attempt for opFile in op if opFile.Status == "Waiting" ] ) # In case it is the first attempt, extraDelay is 0 # maxWaitingAttempt can be None if the operation has no File, like the ForwardDiset extraDelay = datetime.timedelta( minutes = 2 * math.log( maxWaitingAttempt ) if maxWaitingAttempt else 0 ) request.NotBefore = now + extraDelay gLogger.info( "putRequest: request %s not before %s (extra delay %s)" % ( request.RequestName, request.NotBefore, extraDelay ) ) requestName = request.RequestName gLogger.info( "putRequest: Attempting to set request '%s'" % requestName ) return cls.__requestDB.putRequest( request ) types_getScheduledRequest = [ ( IntType, LongType ) ] @classmethod def export_getScheduledRequest( cls , operationID ): """ read scheduled request given operationID """ scheduled = cls.__requestDB.getScheduledRequest( operationID ) if not scheduled["OK"]: gLogger.error( "getScheduledRequest: %s" % scheduled["Message"] ) return scheduled if not scheduled["Value"]: return S_OK() requestJSON = scheduled["Value"].toJSON() if not requestJSON["OK"]: gLogger.error( "getScheduledRequest: %s" % requestJSON["Message"] ) return requestJSON types_getDBSummary = [] @classmethod def export_getDBSummary( cls ): """ Get the summary of requests in the Request DB """ return cls.__requestDB.getDBSummary() types_getRequest = [ ( LongType, IntType ) ] @classmethod def export_getRequest( cls, requestID = 0 ): """ Get a request of given type from the database """ getRequest = cls.__requestDB.getRequest( requestID ) if not getRequest["OK"]: gLogger.error( "getRequest: %s" % getRequest["Message"] ) return getRequest if getRequest["Value"]: getRequest = getRequest["Value"] toJSON = getRequest.toJSON() if not toJSON["OK"]: gLogger.error( toJSON["Message"] ) return toJSON return S_OK() types_getBulkRequests = [ IntType, BooleanType ] @classmethod def export_getBulkRequests( cls, numberOfRequest, assigned ): """ Get a request of given type from the database :param numberOfRequest : size of the bulk (default 10) :return S_OK( {Failed : message, Successful : list of Request.toJSON()} ) """ getRequests = cls.__requestDB.getBulkRequests( numberOfRequest = numberOfRequest, assigned = assigned ) if not getRequests["OK"]: gLogger.error( "getRequests: %s" % getRequests["Message"] ) return getRequests if getRequests["Value"]: getRequests = getRequests["Value"] toJSONDict = {"Successful" : {}, "Failed" : {}} for rId in getRequests: toJSON = getRequests[rId].toJSON() if not toJSON["OK"]: gLogger.error( toJSON["Message"] ) toJSONDict["Failed"][rId] = toJSON["Message"] else: toJSONDict["Successful"][rId] = toJSON["Value"] return S_OK( toJSONDict ) return S_OK() types_peekRequest = [ ( LongType, IntType ) ] @classmethod def export_peekRequest( cls, requestID = 0 ): """ peek request given its id """ peekRequest = cls.__requestDB.peekRequest( requestID ) if not peekRequest["OK"]: gLogger.error( "peekRequest: %s" % peekRequest["Message"] ) return peekRequest if peekRequest["Value"]: peekRequest = peekRequest["Value"].toJSON() if not peekRequest["OK"]: gLogger.error( peekRequest["Message"] ) return peekRequest types_getRequestSummaryWeb = [ DictType, ListType, IntType, IntType ] @classmethod def export_getRequestSummaryWeb( cls, selectDict, sortList, startItem, maxItems ): """ Returns a list of Request for the web portal :param dict selectDict: parameter on which to restrain the query {key : Value} key can be any of the Request columns, 'Type' (interpreted as Operation.Type) and 'FromData' and 'ToData' are matched against the LastUpdate field :param sortList: [sorting column, ASC/DESC] :type sortList: python:list :param int startItem: start item (for pagination) :param int maxItems: max items (for pagination) """ return cls.__requestDB.getRequestSummaryWeb( selectDict, sortList, startItem, maxItems ) types_getDistinctValuesWeb = [ StringTypes ] @classmethod def export_getDistinctValuesWeb( cls, attribute ): """ Get distinct values for a given request attribute. 'Type' is interpreted as the operation type """ tableName = 'Request' if attribute == 'Type': tableName = 'Operation' return cls.__requestDB.getDistinctValues( tableName, attribute ) types_getRequestCountersWeb = [ StringTypes, DictType ] @classmethod def export_getRequestCountersWeb( cls, groupingAttribute, selectDict ): """ For the web portal. Returns a dictionary {value : counts} for a given key. The key can be any field from the RequestTable. or "Type", which will be interpreted as 'Operation.Type' :param groupingAttribute : attribute used for grouping :param selectDict : selection criteria """ return cls.__requestDB.getRequestCountersWeb( groupingAttribute, selectDict ) types_deleteRequest = [ ( IntType, LongType ) ] @classmethod def export_deleteRequest( cls, requestID ): """ Delete the request with the supplied ID""" return cls.__requestDB.deleteRequest( requestID ) types_getRequestIDsList = [ ListType, IntType, StringTypes ] @classmethod def export_getRequestIDsList( cls, statusList = None, limit = None, since = None, until = None, getJobID = False ): """ get requests' IDs with status in :statusList: """ statusList = statusList if statusList else list( Request.FINAL_STATES ) limit = limit if limit else 100 since = since if since else "" until = until if until else "" reqIDsList = cls.__requestDB.getRequestIDsList( statusList, limit, since = since, until = until, getJobID = getJobID ) if not reqIDsList["OK"]: gLogger.error( "getRequestIDsList: %s" % reqIDsList["Message"] ) return reqIDsList types_getRequestIDsForJobs = [ ListType ] @classmethod def export_getRequestIDsForJobs( cls, jobIDs ): """ Select the request IDs for supplied jobIDs """ return cls.__requestDB.getRequestIDsForJobs( jobIDs ) types_readRequestsForJobs = [ ListType ] @classmethod def export_readRequestsForJobs( cls, jobIDs ): """ read requests for jobs given list of jobIDs """ requests = cls.__requestDB.readRequestsForJobs( jobIDs ) if not requests["OK"]: gLogger.error( "readRequestsForJobs: %s" % requests["Message"] ) return requests for jobID, request in requests["Value"]["Successful"].items(): requests["Value"]["Successful"][jobID] = request.toJSON()["Value"] return requests types_getDigest = [ ( IntType, LongType ) ] @classmethod def export_getDigest( cls, requestID ): """ get digest for a request given its id :param str requestID: request's id :return: S_OK( json_str ) """ return cls.__requestDB.getDigest( requestID ) types_getRequestStatus = [ ( IntType, LongType ) ] @classmethod def export_getRequestStatus( cls, requestID ): """ get request status given its id """ status = cls.__requestDB.getRequestStatus( requestID ) if not status["OK"]: gLogger.error( "getRequestStatus: %s" % status["Message"] ) return status types_getRequestFileStatus = [ [ IntType, LongType ], list( StringTypes ) + [ListType] ] @classmethod def export_getRequestFileStatus( cls, requestID, lfnList ): """ get request file status for a given LFNs list and requestID """ if type( lfnList ) == str: lfnList = [lfnList] res = cls.__requestDB.getRequestFileStatus( requestID, lfnList ) if not res["OK"]: gLogger.error( "getRequestFileStatus: %s" % res["Message"] ) return res # types_getRequestName = [ ( IntType, LongType ) ] # @classmethod # def export_getRequestName( cls, requestID ): # """ get request name for a given requestID """ # requestName = cls.__requestDB.getRequestName( requestID ) # if not requestName["OK"]: # gLogger.error( "getRequestName: %s" % requestName["Message"] ) # return requestName types_getRequestInfo = [ [ IntType, LongType ] ] @classmethod def export_getRequestInfo( cls, requestID ): """ get request info for a given requestID """ requestInfo = cls.__requestDB.getRequestInfo( requestID ) if not requestInfo["OK"]: gLogger.error( "getRequestInfo: %s" % requestInfo["Message"] ) return requestInfo
# Copyright 2013 Cloudbase Solutions Srl # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import mock from oslo_utils import units from nova import test from nova.virt.hyperv import constants from nova.virt.hyperv import vhdutils from nova.virt.hyperv import vmutils class VHDUtilsBaseTestCase(test.NoDBTestCase): "Base Class unit test classes of Hyper-V VHD Utils classes." _FAKE_VHD_PATH = "C:\\fake_path.vhdx" _FAKE_PARENT_PATH = "C:\\fake_parent_path.vhdx" _FAKE_FORMAT = 3 _FAKE_TYPE = 3 _FAKE_MAX_INTERNAL_SIZE = units.Gi _FAKE_DYNAMIC_BLK_SIZE = 2097152 _FAKE_BAD_TYPE = 5 _FAKE_JOB_PATH = 'fake_job_path' _FAKE_RET_VAL = 0 _FAKE_VHD_INFO_XML = ( """<INSTANCE CLASSNAME="Msvm_VirtualHardDiskSettingData"> <PROPERTY NAME="BlockSize" TYPE="uint32"> <VALUE>33554432</VALUE> </PROPERTY> <PROPERTY NAME="Caption" TYPE="string"> <VALUE>Virtual Hard Disk Setting Data</VALUE> </PROPERTY> <PROPERTY NAME="Description" TYPE="string"> <VALUE>Setting Data for a Virtual Hard Disk.</VALUE> </PROPERTY> <PROPERTY NAME="ElementName" TYPE="string"> <VALUE>fake_path.vhdx</VALUE> </PROPERTY> <PROPERTY NAME="Format" TYPE="uint16"> <VALUE>%(format)s</VALUE> </PROPERTY> <PROPERTY NAME="InstanceID" TYPE="string"> <VALUE>52794B89-AC06-4349-AC57-486CAAD52F69</VALUE> </PROPERTY> <PROPERTY NAME="LogicalSectorSize" TYPE="uint32"> <VALUE>4096</VALUE> </PROPERTY> <PROPERTY NAME="MaxInternalSize" TYPE="uint64"> <VALUE>%(max_internal_size)s</VALUE> </PROPERTY> <PROPERTY NAME="ParentPath" TYPE="string"> <VALUE>%(parent_path)s</VALUE> </PROPERTY> <PROPERTY NAME="Path" TYPE="string"> <VALUE>%(path)s</VALUE> </PROPERTY> <PROPERTY NAME="PhysicalSectorSize" TYPE="uint32"> <VALUE>4096</VALUE> </PROPERTY> <PROPERTY NAME="Type" TYPE="uint16"> <VALUE>%(type)s</VALUE> </PROPERTY> </INSTANCE>""" % {'path': _FAKE_VHD_PATH, 'parent_path': _FAKE_PARENT_PATH, 'format': _FAKE_FORMAT, 'max_internal_size': _FAKE_MAX_INTERNAL_SIZE, 'type': _FAKE_TYPE}) class VHDUtilsTestCase(VHDUtilsBaseTestCase): """Unit tests for the Hyper-V VHDUtils class.""" def setUp(self): super(VHDUtilsTestCase, self).setUp() self._vhdutils = vhdutils.VHDUtils() self._vhdutils._conn = mock.MagicMock() self._vhdutils._vmutils = mock.MagicMock() self._fake_vhd_info = { 'ParentPath': self._FAKE_PARENT_PATH, 'MaxInternalSize': self._FAKE_MAX_INTERNAL_SIZE, 'Type': self._FAKE_TYPE} def test_validate_vhd(self): mock_img_svc = self._vhdutils._conn.Msvm_ImageManagementService()[0] mock_img_svc.ValidateVirtualHardDisk.return_value = ( self._FAKE_JOB_PATH, self._FAKE_RET_VAL) self._vhdutils.validate_vhd(self._FAKE_VHD_PATH) mock_img_svc.ValidateVirtualHardDisk.assert_called_once_with( Path=self._FAKE_VHD_PATH) def test_get_vhd_info(self): self._mock_get_vhd_info() vhd_info = self._vhdutils.get_vhd_info(self._FAKE_VHD_PATH) self.assertEqual(self._fake_vhd_info, vhd_info) def _mock_get_vhd_info(self): mock_img_svc = self._vhdutils._conn.Msvm_ImageManagementService()[0] mock_img_svc.GetVirtualHardDiskInfo.return_value = ( self._FAKE_VHD_INFO_XML, self._FAKE_JOB_PATH, self._FAKE_RET_VAL) def test_create_dynamic_vhd(self): self._vhdutils.get_vhd_info = mock.MagicMock( return_value={'Format': self._FAKE_FORMAT}) mock_img_svc = self._vhdutils._conn.Msvm_ImageManagementService()[0] mock_img_svc.CreateDynamicVirtualHardDisk.return_value = ( self._FAKE_JOB_PATH, self._FAKE_RET_VAL) self._vhdutils.create_dynamic_vhd(self._FAKE_VHD_PATH, self._FAKE_MAX_INTERNAL_SIZE, constants.DISK_FORMAT_VHD) mock_img_svc.CreateDynamicVirtualHardDisk.assert_called_once_with( Path=self._FAKE_VHD_PATH, MaxInternalSize=self._FAKE_MAX_INTERNAL_SIZE) self._vhdutils._vmutils.check_ret_val.assert_called_once_with( self._FAKE_RET_VAL, self._FAKE_JOB_PATH) def test_reconnect_parent_vhd(self): mock_img_svc = self._vhdutils._conn.Msvm_ImageManagementService()[0] mock_img_svc.ReconnectParentVirtualHardDisk.return_value = ( self._FAKE_JOB_PATH, self._FAKE_RET_VAL) self._vhdutils.reconnect_parent_vhd(self._FAKE_VHD_PATH, self._FAKE_PARENT_PATH) mock_img_svc.ReconnectParentVirtualHardDisk.assert_called_once_with( ChildPath=self._FAKE_VHD_PATH, ParentPath=self._FAKE_PARENT_PATH, Force=True) self._vhdutils._vmutils.check_ret_val.assert_called_once_with( self._FAKE_RET_VAL, self._FAKE_JOB_PATH) def test_merge_vhd(self): mock_img_svc = self._vhdutils._conn.Msvm_ImageManagementService()[0] mock_img_svc.MergeVirtualHardDisk.return_value = ( self._FAKE_JOB_PATH, self._FAKE_RET_VAL) self._vhdutils.merge_vhd(self._FAKE_VHD_PATH, self._FAKE_VHD_PATH) mock_img_svc.MergeVirtualHardDisk.assert_called_once_with( SourcePath=self._FAKE_VHD_PATH, DestinationPath=self._FAKE_VHD_PATH) self._vhdutils._vmutils.check_ret_val.assert_called_once_with( self._FAKE_RET_VAL, self._FAKE_JOB_PATH) def test_resize_vhd(self): mock_img_svc = self._vhdutils._conn.Msvm_ImageManagementService()[0] mock_img_svc.ExpandVirtualHardDisk.return_value = ( self._FAKE_JOB_PATH, self._FAKE_RET_VAL) self._vhdutils.get_internal_vhd_size_by_file_size = mock.MagicMock( return_value=self._FAKE_MAX_INTERNAL_SIZE) self._vhdutils.resize_vhd(self._FAKE_VHD_PATH, self._FAKE_MAX_INTERNAL_SIZE) mock_img_svc.ExpandVirtualHardDisk.assert_called_once_with( Path=self._FAKE_VHD_PATH, MaxInternalSize=self._FAKE_MAX_INTERNAL_SIZE) self._vhdutils._vmutils.check_ret_val.assert_called_once_with( self._FAKE_RET_VAL, self._FAKE_JOB_PATH) def _mocked_get_internal_vhd_size(self, root_vhd_size, vhd_type): mock_get_vhd_info = mock.MagicMock(return_value={'Type': vhd_type}) mock_get_blk_size = mock.MagicMock( return_value=self._FAKE_DYNAMIC_BLK_SIZE) with mock.patch.multiple(self._vhdutils, get_vhd_info=mock_get_vhd_info, _get_vhd_dynamic_blk_size=mock_get_blk_size): return self._vhdutils.get_internal_vhd_size_by_file_size( None, root_vhd_size) def test_create_differencing_vhd(self): mock_img_svc = self._vhdutils._conn.Msvm_ImageManagementService()[0] mock_img_svc.CreateDifferencingVirtualHardDisk.return_value = ( self._FAKE_JOB_PATH, self._FAKE_RET_VAL) self._vhdutils.create_differencing_vhd(self._FAKE_VHD_PATH, self._FAKE_PARENT_PATH) mock_img_svc.CreateDifferencingVirtualHardDisk.assert_called_once_with( Path=self._FAKE_VHD_PATH, ParentPath=self._FAKE_PARENT_PATH) def test_get_internal_vhd_size_by_file_size_fixed(self): root_vhd_size = 1 * 1024 ** 3 real_size = self._mocked_get_internal_vhd_size( root_vhd_size, constants.VHD_TYPE_FIXED) expected_vhd_size = 1 * 1024 ** 3 - 512 self.assertEqual(expected_vhd_size, real_size) def test_get_internal_vhd_size_by_file_size_dynamic(self): root_vhd_size = 20 * 1024 ** 3 real_size = self._mocked_get_internal_vhd_size( root_vhd_size, constants.VHD_TYPE_DYNAMIC) expected_vhd_size = 20 * 1024 ** 3 - 43008 self.assertEqual(expected_vhd_size, real_size) def test_get_internal_vhd_size_by_file_size_differencing(self): # For differencing images, the internal size of the parent vhd # is returned vhdutil = vhdutils.VHDUtils() root_vhd_size = 20 * 1024 ** 3 vhdutil.get_vhd_info = mock.MagicMock() vhdutil.get_vhd_parent_path = mock.MagicMock() vhdutil.get_vhd_parent_path.return_value = self._FAKE_VHD_PATH vhdutil.get_vhd_info.side_effect = [ {'Type': 4}, {'Type': constants.VHD_TYPE_DYNAMIC}] vhdutil._get_vhd_dynamic_blk_size = mock.MagicMock() vhdutil._get_vhd_dynamic_blk_size.return_value = 2097152 real_size = vhdutil.get_internal_vhd_size_by_file_size(None, root_vhd_size) expected_vhd_size = 20 * 1024 ** 3 - 43008 self.assertEqual(expected_vhd_size, real_size) def test_get_vhd_format_vhdx(self): with mock.patch('nova.virt.hyperv.vhdutils.open', mock.mock_open(read_data=vhdutils.VHDX_SIGNATURE), create=True): format = self._vhdutils.get_vhd_format(self._FAKE_VHD_PATH) self.assertEqual(constants.DISK_FORMAT_VHDX, format) def test_get_vhd_format_vhd(self): with mock.patch('nova.virt.hyperv.vhdutils.open', mock.mock_open(), create=True) as mock_open: f = mock_open.return_value f.tell.return_value = 1024 readdata = ['notthesig', vhdutils.VHD_SIGNATURE] def read(*args): for content in readdata: yield content f.read.side_effect = read() format = self._vhdutils.get_vhd_format(self._FAKE_VHD_PATH) self.assertEqual(constants.DISK_FORMAT_VHD, format) def test_get_vhd_format_invalid_format(self): with mock.patch('nova.virt.hyperv.vhdutils.open', mock.mock_open(read_data='invalid'), create=True) as mock_open: f = mock_open.return_value f.tell.return_value = 1024 self.assertRaises(vmutils.HyperVException, self._vhdutils.get_vhd_format, self._FAKE_VHD_PATH) def test_get_vhd_format_zero_length_file(self): with mock.patch('nova.virt.hyperv.vhdutils.open', mock.mock_open(read_data=''), create=True) as mock_open: f = mock_open.return_value f.tell.return_value = 0 self.assertRaises(vmutils.HyperVException, self._vhdutils.get_vhd_format, self._FAKE_VHD_PATH) f.seek.assert_called_once_with(0, 2) def test_get_supported_vhd_format(self): fmt = self._vhdutils.get_best_supported_vhd_format() self.assertEqual(constants.DISK_FORMAT_VHD, fmt)
# Copyright (c) 2015 OpenStack Foundation. # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import functools from oslo_config import cfg from oslo_log import log as logging from sqlalchemy.orm import exc from neutron.api.v2 import attributes from neutron.common import constants from neutron.common import exceptions as n_exc from neutron.common import utils from neutron.db import common_db_mixin from neutron.db import models_v2 LOG = logging.getLogger(__name__) class DbBasePluginCommon(common_db_mixin.CommonDbMixin): """Stores getters and helper methods for db_base_plugin_v2 All private getters and simple helpers like _make_*_dict were moved from db_base_plugin_v2. More complicated logic and public methods left in db_base_plugin_v2. Main purpose of this class is to make getters accessible for Ipam backends. """ @staticmethod def _generate_mac(): return utils.get_random_mac(cfg.CONF.base_mac.split(':')) @staticmethod def _delete_ip_allocation(context, network_id, subnet_id, ip_address): # Delete the IP address from the IPAllocate table LOG.debug("Delete allocated IP %(ip_address)s " "(%(network_id)s/%(subnet_id)s)", {'ip_address': ip_address, 'network_id': network_id, 'subnet_id': subnet_id}) context.session.query(models_v2.IPAllocation).filter_by( network_id=network_id, ip_address=ip_address, subnet_id=subnet_id).delete() @staticmethod def _store_ip_allocation(context, ip_address, network_id, subnet_id, port_id): LOG.debug("Allocated IP %(ip_address)s " "(%(network_id)s/%(subnet_id)s/%(port_id)s)", {'ip_address': ip_address, 'network_id': network_id, 'subnet_id': subnet_id, 'port_id': port_id}) allocated = models_v2.IPAllocation( network_id=network_id, port_id=port_id, ip_address=ip_address, subnet_id=subnet_id ) context.session.add(allocated) def _make_subnet_dict(self, subnet, fields=None, context=None): res = {'id': subnet['id'], 'name': subnet['name'], 'tenant_id': subnet['tenant_id'], 'network_id': subnet['network_id'], 'ip_version': subnet['ip_version'], 'cidr': subnet['cidr'], 'subnetpool_id': subnet.get('subnetpool_id'), 'allocation_pools': [{'start': pool['first_ip'], 'end': pool['last_ip']} for pool in subnet['allocation_pools']], 'gateway_ip': subnet['gateway_ip'], 'enable_dhcp': subnet['enable_dhcp'], 'ipv6_ra_mode': subnet['ipv6_ra_mode'], 'ipv6_address_mode': subnet['ipv6_address_mode'], 'dns_nameservers': [dns['address'] for dns in subnet['dns_nameservers']], 'host_routes': [{'destination': route['destination'], 'nexthop': route['nexthop']} for route in subnet['routes']], } # The shared attribute for a subnet is the same as its parent network res['shared'] = self._make_network_dict(subnet.networks, context=context)['shared'] # Call auxiliary extend functions, if any self._apply_dict_extend_functions(attributes.SUBNETS, res, subnet) return self._fields(res, fields) def _make_subnetpool_dict(self, subnetpool, fields=None): default_prefixlen = str(subnetpool['default_prefixlen']) min_prefixlen = str(subnetpool['min_prefixlen']) max_prefixlen = str(subnetpool['max_prefixlen']) res = {'id': subnetpool['id'], 'name': subnetpool['name'], 'tenant_id': subnetpool['tenant_id'], 'default_prefixlen': default_prefixlen, 'min_prefixlen': min_prefixlen, 'max_prefixlen': max_prefixlen, 'shared': subnetpool['shared'], 'prefixes': [prefix['cidr'] for prefix in subnetpool['prefixes']], 'ip_version': subnetpool['ip_version'], 'default_quota': subnetpool['default_quota']} return self._fields(res, fields) def _make_port_dict(self, port, fields=None, process_extensions=True): res = {"id": port["id"], 'name': port['name'], "network_id": port["network_id"], 'tenant_id': port['tenant_id'], "mac_address": port["mac_address"], "admin_state_up": port["admin_state_up"], "status": port["status"], "fixed_ips": [{'subnet_id': ip["subnet_id"], 'ip_address': ip["ip_address"]} for ip in port["fixed_ips"]], "device_id": port["device_id"], "device_owner": port["device_owner"]} # Call auxiliary extend functions, if any if process_extensions: self._apply_dict_extend_functions( attributes.PORTS, res, port) return self._fields(res, fields) def _get_network(self, context, id): try: network = self._get_by_id(context, models_v2.Network, id) except exc.NoResultFound: raise n_exc.NetworkNotFound(net_id=id) return network def _get_subnet(self, context, id): try: subnet = self._get_by_id(context, models_v2.Subnet, id) except exc.NoResultFound: raise n_exc.SubnetNotFound(subnet_id=id) return subnet def _get_subnetpool(self, context, id): try: return self._get_by_id(context, models_v2.SubnetPool, id) except exc.NoResultFound: raise n_exc.SubnetPoolNotFound(subnetpool_id=id) def _get_all_subnetpools(self, context): # NOTE(tidwellr): see note in _get_all_subnets() return context.session.query(models_v2.SubnetPool).all() def _get_port(self, context, id): try: port = self._get_by_id(context, models_v2.Port, id) except exc.NoResultFound: raise n_exc.PortNotFound(port_id=id) return port def _get_dns_by_subnet(self, context, subnet_id): dns_qry = context.session.query(models_v2.DNSNameServer) return dns_qry.filter_by(subnet_id=subnet_id).all() def _get_route_by_subnet(self, context, subnet_id): route_qry = context.session.query(models_v2.SubnetRoute) return route_qry.filter_by(subnet_id=subnet_id).all() def _get_router_gw_ports_by_network(self, context, network_id): port_qry = context.session.query(models_v2.Port) return port_qry.filter_by(network_id=network_id, device_owner=constants.DEVICE_OWNER_ROUTER_GW).all() def _get_subnets_by_network(self, context, network_id): subnet_qry = context.session.query(models_v2.Subnet) return subnet_qry.filter_by(network_id=network_id).all() def _get_subnets_by_subnetpool(self, context, subnetpool_id): subnet_qry = context.session.query(models_v2.Subnet) return subnet_qry.filter_by(subnetpool_id=subnetpool_id).all() def _get_all_subnets(self, context): # NOTE(salvatore-orlando): This query might end up putting # a lot of stress on the db. Consider adding a cache layer return context.session.query(models_v2.Subnet).all() def _get_subnets(self, context, filters=None, fields=None, sorts=None, limit=None, marker=None, page_reverse=False): marker_obj = self._get_marker_obj(context, 'subnet', limit, marker) make_subnet_dict = functools.partial(self._make_subnet_dict, context=context) return self._get_collection(context, models_v2.Subnet, make_subnet_dict, filters=filters, fields=fields, sorts=sorts, limit=limit, marker_obj=marker_obj, page_reverse=page_reverse) def _make_network_dict(self, network, fields=None, process_extensions=True, context=None): res = {'id': network['id'], 'name': network['name'], 'tenant_id': network['tenant_id'], 'admin_state_up': network['admin_state_up'], 'mtu': network.get('mtu', constants.DEFAULT_NETWORK_MTU), 'status': network['status'], 'subnets': [subnet['id'] for subnet in network['subnets']]} # The shared attribute for a network now reflects if the network # is shared to the calling tenant via an RBAC entry. shared = False matches = ('*',) + ((context.tenant_id,) if context else ()) for entry in network.rbac_entries: if (entry.action == 'access_as_shared' and entry.target_tenant in matches): shared = True break res['shared'] = shared # TODO(pritesh): Move vlan_transparent to the extension module. # vlan_transparent here is only added if the vlantransparent # extension is enabled. if ('vlan_transparent' in network and network['vlan_transparent'] != attributes.ATTR_NOT_SPECIFIED): res['vlan_transparent'] = network['vlan_transparent'] # Call auxiliary extend functions, if any if process_extensions: self._apply_dict_extend_functions( attributes.NETWORKS, res, network) return self._fields(res, fields) def _make_subnet_args(self, detail, subnet, subnetpool_id): gateway_ip = str(detail.gateway_ip) if detail.gateway_ip else None args = {'tenant_id': detail.tenant_id, 'id': detail.subnet_id, 'name': subnet['name'], 'network_id': subnet['network_id'], 'ip_version': subnet['ip_version'], 'cidr': str(detail.subnet_cidr), 'subnetpool_id': subnetpool_id, 'enable_dhcp': subnet['enable_dhcp'], 'gateway_ip': gateway_ip} if subnet['ip_version'] == 6 and subnet['enable_dhcp']: if attributes.is_attr_set(subnet['ipv6_ra_mode']): args['ipv6_ra_mode'] = subnet['ipv6_ra_mode'] if attributes.is_attr_set(subnet['ipv6_address_mode']): args['ipv6_address_mode'] = subnet['ipv6_address_mode'] return args def _make_fixed_ip_dict(self, ips): # Excludes from dict all keys except subnet_id and ip_address return [{'subnet_id': ip["subnet_id"], 'ip_address': ip["ip_address"]} for ip in ips]
""" Quick conversion command module. """ from optparse import make_option import sys from django.core.management.base import BaseCommand from django.core.management.color import no_style from django.conf import settings from django.db import models from django.core import management from django.core.exceptions import ImproperlyConfigured from south.migration import Migrations from south.hacks import hacks from south.exceptions import NoMigrations class Command(BaseCommand): option_list = BaseCommand.option_list if '--verbosity' not in [opt.get_opt_string() for opt in BaseCommand.option_list]: option_list += ( make_option('--verbosity', action='store', dest='verbosity', default='1', type='choice', choices=['0', '1', '2'], help='Verbosity level; 0=minimal output, 1=normal output, 2=all output'), ) option_list += ( make_option('--delete-ghost-migrations', action='store_true', dest='delete_ghosts', default=False, help="Tells South to delete any 'ghost' migrations (ones in the database but not on disk)."), make_option('--ignore-ghost-migrations', action='store_true', dest='ignore_ghosts', default=False, help="Tells South to ignore any 'ghost' migrations (ones in the database but not on disk) and continue to apply new migrations."), ) help = "Quickly converts the named application to use South if it is currently using syncdb." def handle(self, app=None, *args, **options): # Make sure we have an app if not app: print "Please specify an app to convert." return # See if the app exists app = app.split(".")[-1] try: app_module = models.get_app(app) except ImproperlyConfigured: print "There is no enabled application matching '%s'." % app return # Try to get its list of models model_list = models.get_models(app_module) if not model_list: print "This application has no models; this command is for applications that already have models syncdb'd." print "Make some models, and then use ./manage.py schemamigration %s --initial instead." % app return # Ask South if it thinks it's already got migrations try: Migrations(app) except NoMigrations: pass else: print "This application is already managed by South." return # Finally! It seems we've got a candidate, so do the two-command trick verbosity = int(options.get('verbosity', 0)) management.call_command("schemamigration", app, initial=True, verbosity=verbosity) # Now, we need to re-clean and sanitise appcache hacks.clear_app_cache() hacks.repopulate_app_cache() # And also clear our cached Migration classes Migrations._clear_cache() # Now, migrate management.call_command( "migrate", app, "0001", fake=True, verbosity=verbosity, ignore_ghosts=options.get("ignore_ghosts", False), delete_ghosts=options.get("delete_ghosts", False), ) print print "App '%s' converted. Note that South assumed the application's models matched the database" % app print "(i.e. you haven't changed it since last syncdb); if you have, you should delete the %s/migrations" % app print "directory, revert models.py so it matches the database, and try again."
# # Copyright (C) 2015 Red Hat, Inc. # # This program is free software; you can redistribute it and/or modify # it under the terms of the GNU Lesser General Public License as published # by the Free Software Foundation; either version 2.1 of the License, or # (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Lesser General Public License for more details. # # You should have received a copy of the GNU Lesser General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. from gladecheck import GladeTest class CheckMnemonics(GladeTest): def checkGlade(self, glade_tree): """Check for widgets with keyboard accelerators but no mnemonic""" # Look for labels with use-underline=True and no mnemonic-widget for label in glade_tree.xpath(".//object[@class='GtkLabel' and ./property[@name='use_underline' and ./text() = 'True'] and not(./property[@name='mnemonic_widget'])]"): # And now filter out the cases where the label actually does have a mnemonic. # This list is not comprehensive, probably. parent = label.getparent() # Is the label the child of a GtkButton? The button might be pretty far up there. # Assume widget names that end in "Button" are subclasses of GtkButton if parent.tag == 'child' and \ label.xpath("ancestor::object[substring(@class, string-length(@class) - string-length('Button') + 1) = 'Button']"): continue # Is the label a GtkNotebook tab? if parent.tag == 'child' and parent.get('type') == 'tab' and \ parent.getparent().get('class') == 'GtkNotebook': continue raise AssertionError("Label with accelerator and no mnemonic at %s:%d" % (label.base, label.sourceline))
# Copyright (c) 2012 Spotify AB # # Licensed under the Apache License, Version 2.0 (the "License"); you may not # use this file except in compliance with the License. You may obtain a copy of # the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations under # the License. import os import sys try: from setuptools import setup except: from distutils.core import setup def get_static_files(path): return [os.path.join(dirpath.replace("luigi/", ""), ext) for (dirpath, dirnames, filenames) in os.walk(path) for ext in ["*.html", "*.js", "*.css", "*.png"]] luigi_package_data = sum(map(get_static_files, ["luigi/static", "luigi/templates"]), []) readme_note = """\ .. note:: For the latest source, discussion, etc, please visit the `GitHub repository <https://github.com/spotify/luigi>`_\n\n """ with open('README.rst') as fobj: long_description = readme_note + fobj.read() install_requires = [ 'cached_property', 'pyparsing', 'tornado', 'python-daemon', ] if os.environ.get('READTHEDOCS', None) == 'True': install_requires.append('sqlalchemy') # So that we can build documentation for luigi.db_task_history and luigi.contrib.sqla setup( name='luigi', version='1.3.0', description='Workflow mgmgt + task scheduling + dependency resolution', long_description=long_description, author='Erik Bernhardsson', author_email='erikbern@spotify.com', url='https://github.com/spotify/luigi', license='Apache License 2.0', packages=[ 'luigi', 'luigi.contrib', 'luigi.contrib.hdfs', 'luigi.tools' ], package_data={ 'luigi': luigi_package_data }, entry_points={ 'console_scripts': [ 'luigi = luigi.cmdline:luigi_run', 'luigid = luigi.cmdline:luigid', 'luigi-grep = luigi.tools.luigi_grep:main', 'luigi-deps = luigi.tools.deps:main', ] }, install_requires=install_requires, classifiers=[ 'Development Status :: 5 - Production/Stable', 'Environment :: Console', 'Environment :: Web Environment', 'Intended Audience :: Developers', 'Intended Audience :: System Administrators', 'License :: OSI Approved :: Apache Software License', 'Programming Language :: Python :: 2.7', 'Programming Language :: Python :: 3.3', 'Programming Language :: Python :: 3.4', 'Topic :: System :: Monitoring', ], )
# -*- coding: UTF-8 -*- # Copyright 2014-2015 Rumma & Ko Ltd # # License: GNU Affero General Public License v3 (see file COPYING for details) """ Choicelists for `lino_xl.lib.humanlinks`. """ from __future__ import unicode_literals from __future__ import print_function from django.utils.translation import gettext_lazy as _ from django.utils.translation import pgettext_lazy as pgettext from django.utils.text import format_lazy from lino_xl.lib.contacts.roles import ContactsStaff from lino.api import dd class LinkType(dd.Choice): symmetric = False def __init__(self, value, name, mptext, fptext, mctext, fctext, **kw): self.mptext = mptext # male parent self.fptext = fptext self.mctext = mctext self.fctext = fctext # text = string_concat( # mptext, ' (', fptext, ') / ', mctext, ' (', fctext, ')') # text = string_concat(mctext, ' (', fctext, ')') text = format_lazy(u"{}({})",mptext, fptext) # text = "%s (%s) / %s (%s)" % (mptext, fptext, mctext, fctext) super(LinkType, self).__init__(value, text, name, **kw) def as_parent(self, human): if human is None: return self.text return human.mf(self.mptext, self.fptext) def as_child(self, human): if human is None: return self.text return human.mf(self.mctext, self.fctext) class LinkTypes(dd.ChoiceList): """The global list of human link types. This is used as choicelist for the :attr:`type <lino_xl.lib.humanlinks.models.Link.type>` field of a human link. The default list contains the following data: .. django2rst:: rt.show(humanlinks.LinkTypes) .. attribute:: adoptive_parent A person who adopts a child of other parents as his or her own child. .. attribute:: stepparent Someone that your mother or father marries after the marriage to or relationship with your other parent has ended .. attribute:: foster_parent A man (woman) who looks after or brings up a child or children as a father (mother), in place of the natural or adoptive father (mother). [`thefreedictionary <http://www.thefreedictionary.com/foster+father>`_] """ required_roles = dd.login_required(ContactsStaff) verbose_name = _("Parency type") verbose_name_plural = _("Parency types") item_class = LinkType add = LinkTypes.add_item add('01', 'parent', _("Father"), _("Mother"), _("Son"), _("Daughter")) add('02', 'adoptive_parent', _("Adoptive father"), _("Adoptive mother"), _("Adopted son"), _("Adopted daughter")) add('03', 'grandparent', _("Grandfather"), _("Grandmother"), _("Grandson"), _("Granddaughter")) add('05', 'spouse', _("Husband"), _("Wife"), _("Husband"), _("Wife"), symmetric=True) add('06', 'friend', pgettext("male", "Friend"), pgettext("female", "Friend"), pgettext("male", "Friend"), pgettext("female", "Friend"), symmetric=True) add('07', 'partner', pgettext("male", "Partner"), pgettext("female", "Partner"), pgettext("male", "Partner"), pgettext("female", "Partner"), symmetric=True) add('08', 'stepparent', _("Stepfather"), _("Stepmother"), _("Stepson"), _("Stepdaughter")) add('09', 'foster_parent', _("Foster father"), _("Foster mother"), _("Foster son"), _("Foster daughter")) add('10', 'sibling', pgettext("male", "Brother"), pgettext("female", "Sister"), pgettext("male", "Brother"), pgettext("female", "Sister"), symmetric=True) add('11', 'cousin', pgettext("male", "Cousin"), pgettext("female", "Cousin"), pgettext("male", "Cousin"), pgettext("female", "Cousin"), symmetric=True) add('12', 'uncle', _("Uncle"), _("Aunt"), _("Nephew"), _("Niece")) add('80', 'relative', pgettext("male", "Relative"), pgettext("female", "Relative"), pgettext("male", "Relative"), pgettext("female", "Relative"), symmetric=True) add('90', 'other', pgettext("male", "Other"), pgettext("female", "Other"), pgettext("male", "Other"), pgettext("female", "Other"), symmetric=True)
from __future__ import absolute_import import pytest from itertools import islice from kafka import ( KafkaClient, SimpleProducer, ) from tests.pgshovel.fixtures import ( cluster, create_temporary_database, ) from tests.pgshovel.streams.fixtures import ( DEFAULT_PUBLISHER, begin, transaction, transactions, ) from pgshovel.interfaces.common_pb2 import Snapshot from pgshovel.interfaces.configurations_pb2 import ReplicationSetConfiguration from pgshovel.interfaces.replication_pb2 import ( ConsumerState, State, BootstrapState, TransactionState, ) from pgshovel.interfaces.streams_pb2 import ( Header, Message, ) from pgshovel.replication.streams.kafka import KafkaStream from pgshovel.replication.validation.consumers import SequencingError from pgshovel.replication.validation.transactions import InvalidEventError from pgshovel.relay.streams.kafka import KafkaWriter from pgshovel.streams.utilities import UnableToPrimeError @pytest.yield_fixture def configuration(): yield {'hosts': 'kafka:9092'} @pytest.yield_fixture def stream(configuration, cluster, client): stream = KafkaStream.configure(configuration, cluster, 'default') client.ensure_topic_exists(stream.topic) yield stream @pytest.yield_fixture def client(configuration): yield KafkaClient(configuration['hosts']) @pytest.yield_fixture def writer(client, stream): producer = SimpleProducer(client) yield KafkaWriter(producer, stream.topic) @pytest.yield_fixture def state(): bootstrap_state = BootstrapState( node='1234', snapshot=Snapshot(min=1, max=2), ) yield State(bootstrap_state=bootstrap_state) @pytest.yield_fixture def sliced_transaction(): two_transactions = list(islice(transactions(), 6)) head, remainder = two_transactions[0], two_transactions[1:] assert head.batch_operation.begin_operation == begin yield remainder def test_starts_at_beginning_of_stream_for_bootstrapped_state(writer, stream, state): writer.push(transaction) consumed = list(islice(stream.consume(state), 3)) assert [message for _, _, message in consumed] == transaction def test_yields_new_update_state_after_each_message(writer, stream, state): expected_states = { 0: 'in_transaction', 1: 'in_transaction', 2: 'committed' } writer.push(transaction) for state, offset, message in islice(stream.consume(state), 3): assert state.stream_state.consumer_state.offset == offset assert state.stream_state.consumer_state.header == message.header assert state.stream_state.transaction_state.WhichOneof('state') == expected_states[offset] def test_uses_existing_stream_state_if_it_exists(writer, stream, state): writer.push(islice(transactions(), 6)) iterator = stream.consume(state) next(iterator) next(iterator) (new_state, offset, message) = next(iterator) new_iterator = stream.consume(new_state) (_, new_offset, _) = next(new_iterator) assert new_offset == 3 def test_crashes_on_no_state(stream): with pytest.raises(AttributeError): next(stream.consume(None)) def test_validates_stream_and_crashes_when_invalid(writer, stream, state): messages = list(islice(transactions(), 3)) messages[1] = messages[0] writer.push(messages) with pytest.raises(SequencingError): list(stream.consume(state)) def test_discards_messages_until_start_of_transaction(writer, stream, state, sliced_transaction): writer.push(sliced_transaction) consumed = list(islice(stream.consume(state), 3)) assert [message for _, _, message in consumed] == sliced_transaction[-3:] def test_discarded_messages_is_configurable(configuration, cluster, client, state, writer, sliced_transaction): writer.push(sliced_transaction) configuration['prime_threshold'] = 1 antsy_stream = KafkaStream.configure(configuration, cluster, 'default') less_antsy_config = configuration.copy() less_antsy_config['prime_threshold'] = 3 less_antsy_stream = KafkaStream.configure(less_antsy_config, cluster, 'default') client.ensure_topic_exists(antsy_stream.topic) with pytest.raises(UnableToPrimeError): list(islice(antsy_stream.consume(state), 3)) consumed = list(islice(less_antsy_stream.consume(state), 3)) assert [message for _, _, message in consumed] == sliced_transaction[-3:]
from django import forms from django.conf import settings from django.contrib.flatpages.models import FlatPage from django.utils.translation import ugettext, ugettext_lazy as _ class FlatpageForm(forms.ModelForm): url = forms.RegexField( label=_("URL"), max_length=100, regex=r'^[-\w/\.~]+$', help_text=_("Example: '/about/contact/'. Make sure to have leading and trailing slashes."), error_messages={ "invalid": _( "This value must contain only letters, numbers, dots, " "underscores, dashes, slashes or tildes." ), }, ) class Meta: model = FlatPage fields = '__all__' def clean_url(self): url = self.cleaned_data['url'] if not url.startswith('/'): raise forms.ValidationError( ugettext("URL is missing a leading slash."), code='missing_leading_slash', ) if (settings.APPEND_SLASH and ( (settings.MIDDLEWARE and 'django.middleware.common.CommonMiddleware' in settings.MIDDLEWARE) or 'django.middleware.common.CommonMiddleware' in settings.MIDDLEWARE_CLASSES) and not url.endswith('/')): raise forms.ValidationError( ugettext("URL is missing a trailing slash."), code='missing_trailing_slash', ) return url def clean(self): url = self.cleaned_data.get('url') sites = self.cleaned_data.get('sites') same_url = FlatPage.objects.filter(url=url) if self.instance.pk: same_url = same_url.exclude(pk=self.instance.pk) if sites and same_url.filter(sites__in=sites).exists(): for site in sites: if same_url.filter(sites=site).exists(): raise forms.ValidationError( _('Flatpage with url %(url)s already exists for site %(site)s'), code='duplicate_url', params={'url': url, 'site': site}, ) return super(FlatpageForm, self).clean()
#!/usr/bin/python #coding: utf-8 -*- # (c) 2017, Wayne Witzel III <wayne@riotousliving.com> # # This module is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # This software is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this software. If not, see <http://www.gnu.org/licenses/>. ANSIBLE_METADATA = {'metadata_version': '1.0', 'status': ['preview'], 'supported_by': 'community'} DOCUMENTATION = ''' --- module: tower_job_cancel author: "Wayne Witzel III (@wwitzel3)" version_added: "2.3" short_description: Cancel an Ansible Tower Job. description: - Cancel Ansible Tower jobs. See U(https://www.ansible.com/tower) for an overview. options: job_id: description: - ID of the job to cancel required: True fail_if_not_running: description: - Fail loudly if the job_id does not reference a running job. default: False extends_documentation_fragment: tower ''' EXAMPLES = ''' - name: Cancel job tower_job_cancel: job_id: job.id ''' RETURN = ''' id: description: job id requesting to cancel returned: success type: int sample: 94 status: description: status of the cancel request returned: success type: string sample: canceled ''' from ansible.module_utils.basic import AnsibleModule try: import tower_cli import tower_cli.utils.exceptions as exc from tower_cli.conf import settings from ansible.module_utils.ansible_tower import ( tower_auth_config, tower_check_mode, tower_argument_spec, ) HAS_TOWER_CLI = True except ImportError: HAS_TOWER_CLI = False def main(): argument_spec = tower_argument_spec() argument_spec.update(dict( job_id = dict(type='int', required=True), fail_if_not_running = dict(type='bool', default=False), )) module = AnsibleModule( argument_spec = argument_spec, supports_check_mode=True, ) if not HAS_TOWER_CLI: module.fail_json(msg='ansible-tower-cli required for this module') job_id = module.params.get('job_id') json_output = {} tower_auth = tower_auth_config(module) with settings.runtime_values(**tower_auth): tower_check_mode(module) job = tower_cli.get_resource('job') params = module.params.copy() try: result = job.cancel(job_id, **params) json_output['id'] = job_id except (exc.ConnectionError, exc.BadRequest, exc.TowerCLIError) as excinfo: module.fail_json(msg='Unable to cancel job_id/{0}: {1}'.format(job_id, excinfo), changed=False) json_output['changed'] = result['changed'] json_output['status'] = result['status'] module.exit_json(**json_output) if __name__ == '__main__': main()
# -*- coding: utf-8 -*- # # This file is part of Invenio. # Copyright (C) 2004, 2005, 2006, 2007, 2008, 2010, 2011 CERN. # # Invenio is free software; you can redistribute it and/or # modify it under the terms of the GNU General Public License as # published by the Free Software Foundation; either version 2 of the # License, or (at your option) any later version. # # Invenio is distributed in the hope that it will be useful, but # WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU # General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Invenio; if not, write to the Free Software Foundation, Inc., # 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA. """Unit tests for the indexing engine.""" __revision__ = "$Id$" from invenio.testutils import InvenioTestCase from invenio import bibindex_engine_stemmer from invenio.testutils import make_test_suite, run_test_suite class TestStemmer(InvenioTestCase): """Test stemmer.""" def test_stemmer_none(self): """bibindex engine - no stemmer""" self.assertEqual("information", bibindex_engine_stemmer.stem("information", None)) def test_stemmer_english(self): """bibindex engine - English stemmer""" english_test_cases = [['information', 'inform'], ['experiment', 'experi'], ['experiments', 'experi'], ['experimented', 'experi'], ['experimenting', 'experi'], ['experimental', 'experiment'], ['experimentally', 'experiment'], ['experimentation', 'experiment'], ['experimentalism', 'experiment'], ['experimenter', 'experiment'], ['experimentalise', 'experimentalis'], ['experimentalist', 'experimentalist'], ['experimentalists', 'experimentalist'], ['GeV', 'GeV'], ['$\Omega$', '$\Omega$'], ['e^-', 'e^-'], ['C#', 'C#'], ['C++', 'C++']] for test_word, expected_result in english_test_cases: self.assertEqual(expected_result, bibindex_engine_stemmer.stem(test_word, "en")) def test_stemmer_greek(self): """bibindex engine - Greek stemmer""" greek_test_cases = [['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['E=mc^2', 'E=MC^2'], ['+=', '+=']] for test_word, expected_result in greek_test_cases: self.assertEqual(expected_result, bibindex_engine_stemmer.stem(test_word, "el")) TEST_SUITE = make_test_suite(TestStemmer,) if __name__ == "__main__": run_test_suite(TEST_SUITE)
"""The match_hostname() function from Python 3.3.3, essential when using SSL.""" import re __version__ = '3.4.0.2' class CertificateError(ValueError): pass def _dnsname_match(dn, hostname, max_wildcards=1): """Matching according to RFC 6125, section 6.4.3 http://tools.ietf.org/html/rfc6125#section-6.4.3 """ pats = [] if not dn: return False # Ported from python3-syntax: # leftmost, *remainder = dn.split(r'.') parts = dn.split(r'.') leftmost = parts[0] remainder = parts[1:] wildcards = leftmost.count('*') if wildcards > max_wildcards: # Issue #17980: avoid denials of service by refusing more # than one wildcard per fragment. A survey of established # policy among SSL implementations showed it to be a # reasonable choice. raise CertificateError( "too many wildcards in certificate DNS name: " + repr(dn)) # speed up common case w/o wildcards if not wildcards: return dn.lower() == hostname.lower() # RFC 6125, section 6.4.3, subitem 1. # The client SHOULD NOT attempt to match a presented identifier in which # the wildcard character comprises a label other than the left-most label. if leftmost == '*': # When '*' is a fragment by itself, it matches a non-empty dotless # fragment. pats.append('[^.]+') elif leftmost.startswith('xn--') or hostname.startswith('xn--'): # RFC 6125, section 6.4.3, subitem 3. # The client SHOULD NOT attempt to match a presented identifier # where the wildcard character is embedded within an A-label or # U-label of an internationalized domain name. pats.append(re.escape(leftmost)) else: # Otherwise, '*' matches any dotless string, e.g. www* pats.append(re.escape(leftmost).replace(r'\*', '[^.]*')) # add the remaining fragments, ignore any wildcards for frag in remainder: pats.append(re.escape(frag)) pat = re.compile(r'\A' + r'\.'.join(pats) + r'\Z', re.IGNORECASE) return pat.match(hostname) def match_hostname(cert, hostname): """Verify that *cert* (in decoded format as returned by SSLSocket.getpeercert()) matches the *hostname*. RFC 2818 and RFC 6125 rules are followed, but IP addresses are not accepted for *hostname*. CertificateError is raised on failure. On success, the function returns nothing. """ if not cert: raise ValueError("empty or no certificate") dnsnames = [] san = cert.get('subjectAltName', ()) for key, value in san: if key == 'DNS': if _dnsname_match(value, hostname): return dnsnames.append(value) if not dnsnames: # The subject is only checked when there is no dNSName entry # in subjectAltName for sub in cert.get('subject', ()): for key, value in sub: # XXX according to RFC 2818, the most specific Common Name # must be used. if key == 'commonName': if _dnsname_match(value, hostname): return dnsnames.append(value) if len(dnsnames) > 1: raise CertificateError("hostname %r " "doesn't match either of %s" % (hostname, ', '.join(map(repr, dnsnames)))) elif len(dnsnames) == 1: raise CertificateError("hostname %r " "doesn't match %r" % (hostname, dnsnames[0])) else: raise CertificateError("no appropriate commonName or " "subjectAltName fields were found")
#!/usr/bin/env python import unittest from test import support from test.test_urllib2 import sanepathname2url import os import socket import sys import urllib.error import urllib.request def _retry_thrice(func, exc, *args, **kwargs): for i in range(3): try: return func(*args, **kwargs) except exc as e: last_exc = e continue except: raise raise last_exc def _wrap_with_retry_thrice(func, exc): def wrapped(*args, **kwargs): return _retry_thrice(func, exc, *args, **kwargs) return wrapped # Connecting to remote hosts is flaky. Make it more robust by retrying # the connection several times. _urlopen_with_retry = _wrap_with_retry_thrice(urllib.request.urlopen, urllib.error.URLError) class AuthTests(unittest.TestCase): """Tests urllib2 authentication features.""" ## Disabled at the moment since there is no page under python.org which ## could be used to HTTP authentication. # # def test_basic_auth(self): # import http.client # # test_url = "http://www.python.org/test/test_urllib2/basic_auth" # test_hostport = "www.python.org" # test_realm = 'Test Realm' # test_user = 'test.test_urllib2net' # test_password = 'blah' # # # failure # try: # _urlopen_with_retry(test_url) # except urllib2.HTTPError, exc: # self.assertEqual(exc.code, 401) # else: # self.fail("urlopen() should have failed with 401") # # # success # auth_handler = urllib2.HTTPBasicAuthHandler() # auth_handler.add_password(test_realm, test_hostport, # test_user, test_password) # opener = urllib2.build_opener(auth_handler) # f = opener.open('http://localhost/') # response = _urlopen_with_retry("http://www.python.org/") # # # The 'userinfo' URL component is deprecated by RFC 3986 for security # # reasons, let's not implement it! (it's already implemented for proxy # # specification strings (that is, URLs or authorities specifying a # # proxy), so we must keep that) # self.assertRaises(http.client.InvalidURL, # urllib2.urlopen, "http://evil:thing@example.com") class CloseSocketTest(unittest.TestCase): def test_close(self): import socket, http.client, gc # calling .close() on urllib2's response objects should close the # underlying socket response = _urlopen_with_retry("http://www.python.org/") sock = response.fp self.assert_(not sock.closed) response.close() self.assert_(sock.closed) class OtherNetworkTests(unittest.TestCase): def setUp(self): if 0: # for debugging import logging logger = logging.getLogger("test_urllib2net") logger.addHandler(logging.StreamHandler()) # XXX The rest of these tests aren't very good -- they don't check much. # They do sometimes catch some major disasters, though. def test_ftp(self): urls = [ 'ftp://ftp.kernel.org/pub/linux/kernel/README', 'ftp://ftp.kernel.org/pub/linux/kernel/non-existant-file', #'ftp://ftp.kernel.org/pub/leenox/kernel/test', 'ftp://gatekeeper.research.compaq.com/pub/DEC/SRC' '/research-reports/00README-Legal-Rules-Regs', ] self._test_urls(urls, self._extra_handlers()) def test_file(self): TESTFN = support.TESTFN f = open(TESTFN, 'w') try: f.write('hi there\n') f.close() urls = [ 'file:' + sanepathname2url(os.path.abspath(TESTFN)), ('file:///nonsensename/etc/passwd', None, urllib.error.URLError), ] self._test_urls(urls, self._extra_handlers(), retry=True) finally: os.remove(TESTFN) # XXX Following test depends on machine configurations that are internal # to CNRI. Need to set up a public server with the right authentication # configuration for test purposes. ## def test_cnri(self): ## if socket.gethostname() == 'bitdiddle': ## localhost = 'bitdiddle.cnri.reston.va.us' ## elif socket.gethostname() == 'bitdiddle.concentric.net': ## localhost = 'localhost' ## else: ## localhost = None ## if localhost is not None: ## urls = [ ## 'file://%s/etc/passwd' % localhost, ## 'http://%s/simple/' % localhost, ## 'http://%s/digest/' % localhost, ## 'http://%s/not/found.h' % localhost, ## ] ## bauth = HTTPBasicAuthHandler() ## bauth.add_password('basic_test_realm', localhost, 'jhylton', ## 'password') ## dauth = HTTPDigestAuthHandler() ## dauth.add_password('digest_test_realm', localhost, 'jhylton', ## 'password') ## self._test_urls(urls, self._extra_handlers()+[bauth, dauth]) def _test_urls(self, urls, handlers, retry=True): import socket import time import logging debug = logging.getLogger("test_urllib2").debug urlopen = urllib.request.build_opener(*handlers).open if retry: urlopen = _wrap_with_retry_thrice(urlopen, urllib.error.URLError) for url in urls: if isinstance(url, tuple): url, req, expected_err = url else: req = expected_err = None debug(url) try: f = urlopen(url, req) except EnvironmentError as err: debug(err) if expected_err: msg = ("Didn't get expected error(s) %s for %s %s, got %s: %s" % (expected_err, url, req, type(err), err)) self.assert_(isinstance(err, expected_err), msg) else: with support.transient_internet(): buf = f.read() f.close() debug("read %d bytes" % len(buf)) debug("******** next url coming up...") time.sleep(0.1) def _extra_handlers(self): handlers = [] cfh = urllib.request.CacheFTPHandler() cfh.setTimeout(1) handlers.append(cfh) return handlers class TimeoutTest(unittest.TestCase): def test_http_basic(self): self.assertTrue(socket.getdefaulttimeout() is None) u = _urlopen_with_retry("http://www.python.org") self.assertTrue(u.fp._sock.gettimeout() is None) def test_http_default_timeout(self): self.assertTrue(socket.getdefaulttimeout() is None) socket.setdefaulttimeout(60) try: u = _urlopen_with_retry("http://www.python.org") finally: socket.setdefaulttimeout(None) self.assertEqual(u.fp._sock.gettimeout(), 60) def test_http_no_timeout(self): self.assertTrue(socket.getdefaulttimeout() is None) socket.setdefaulttimeout(60) try: u = _urlopen_with_retry("http://www.python.org", timeout=None) finally: socket.setdefaulttimeout(None) self.assertTrue(u.fp._sock.gettimeout() is None) def test_http_timeout(self): u = _urlopen_with_retry("http://www.python.org", timeout=120) self.assertEqual(u.fp._sock.gettimeout(), 120) FTP_HOST = "ftp://ftp.mirror.nl/pub/mirror/gnu/" def test_ftp_basic(self): self.assertTrue(socket.getdefaulttimeout() is None) u = _urlopen_with_retry(self.FTP_HOST) self.assertTrue(u.fp.fp.raw._sock.gettimeout() is None) def test_ftp_default_timeout(self): self.assertTrue(socket.getdefaulttimeout() is None) socket.setdefaulttimeout(60) try: u = _urlopen_with_retry(self.FTP_HOST) finally: socket.setdefaulttimeout(None) self.assertEqual(u.fp.fp.raw._sock.gettimeout(), 60) def test_ftp_no_timeout(self): self.assertTrue(socket.getdefaulttimeout() is None) socket.setdefaulttimeout(60) try: u = _urlopen_with_retry(self.FTP_HOST, timeout=None) finally: socket.setdefaulttimeout(None) self.assertTrue(u.fp.fp.raw._sock.gettimeout() is None) def test_ftp_timeout(self): u = _urlopen_with_retry(self.FTP_HOST, timeout=60) self.assertEqual(u.fp.fp.raw._sock.gettimeout(), 60) def test_main(): support.requires("network") support.run_unittest(AuthTests, OtherNetworkTests, CloseSocketTest, TimeoutTest, ) if __name__ == "__main__": test_main()
# Copyright 2015 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== """Standard functions for creating slots. A slot is a `Variable` created with the same shape as a primary variable or `Tensor`. A slot is always scoped in the namespace of the primary object and typically has the same device and type. Slots are typically used as accumulators to track values associated with the primary object: ```python # Optimizers can create a slot for each variable to track accumulators accumulators = {var : create_zeros_slot(var, "momentum") for var in vs} for var in vs: apply_momentum(var, accumulators[var], lr, grad, momentum_tensor) # Slots can also be used for moving averages mavg = create_slot(var, var.initialized_value(), "exponential_moving_avg") update_mavg = mavg.assign_sub((mavg - var) * (1 - decay)) ``` """ # pylint: disable=g-bad-name from __future__ import absolute_import from __future__ import division from __future__ import print_function from tensorflow.python.framework import ops from tensorflow.python.ops import array_ops from tensorflow.python.ops import variables from tensorflow.python.ops import variable_scope def _create_slot_var(primary, val, scope): """Helper function for creating a slot variable.""" # TODO(lukaszkaiser): Consider allowing partitioners to be set in the current # scope. current_partitioner = variable_scope.get_variable_scope().partitioner variable_scope.get_variable_scope().set_partitioner(None) slot = variable_scope.get_variable(scope, initializer=val, trainable=False) variable_scope.get_variable_scope().set_partitioner(current_partitioner) # pylint: disable=protected-access if isinstance(primary, variables.Variable) and primary._save_slice_info: # Primary is a partitioned variable, so we need to also indicate that # the slot is a partitioned variable. Slots have the same partitioning # as their primaries. # For examples when using AdamOptimizer in linear model, slot.name # here can be "linear//weights/Adam:0", while primary.op.name is # "linear//weight". We want to get 'Adam' as real_slot_name, so we # remove "'linear//weight' + '/'" and ':0'. real_slot_name = slot.name[len(primary.op.name + "/"):-2] slice_info = primary._save_slice_info slot._set_save_slice_info(variables.Variable.SaveSliceInfo( slice_info.full_name + "/" + real_slot_name, slice_info.full_shape[:], slice_info.var_offset[:], slice_info.var_shape[:])) # pylint: enable=protected-access return slot def create_slot(primary, val, name, colocate_with_primary=True): """Create a slot initialized to the given value. The type of the slot is determined by the given value. Args: primary: The primary `Variable` or `Tensor`. val: A `Tensor` specifying the initial value of the slot. name: Name to use for the slot variable. colocate_with_primary: Boolean. If True the slot is located on the same device as `primary`. Returns: A `Variable` object. """ # Scope the slot name in the namespace of the primary variable. # Set "primary.op.name + '/' + name" as default name, so the scope name of # optimizer can be shared when reuse is True. Meanwhile when reuse is False # and the same name has been previously used, the scope name will add '_N' # as suffix for unique identifications. with variable_scope.variable_scope(None, primary.op.name + '/' + name): if colocate_with_primary: with ops.colocate_with(primary): return _create_slot_var(primary, val, '') else: return _create_slot_var(primary, val, '') def create_zeros_slot(primary, name, dtype=None, colocate_with_primary=True): """Create a slot initialized to 0 with same shape as the primary object. Args: primary: The primary `Variable` or `Tensor`. name: Name to use for the slot variable. dtype: Type of the slot variable. Defaults to the type of `primary`. colocate_with_primary: Boolean. If True the slot is located on the same device as `primary`. Returns: A `Variable` object. """ if dtype is None: dtype = primary.dtype val = array_ops.zeros(primary.get_shape().as_list(), dtype=dtype) return create_slot(primary, val, name, colocate_with_primary=colocate_with_primary)
# -*- coding: utf-8 -*- """ *************************************************************************** RasterCalculator.py --------------------- Date : May 2014 Copyright : (C) 2014 by Victor Olaya Email : volayaf at gmail dot com *************************************************************************** * * * This program is free software; you can redistribute it and/or modify * * it under the terms of the GNU General Public License as published by * * the Free Software Foundation; either version 2 of the License, or * * (at your option) any later version. * * * *************************************************************************** """ from processing.core.parameters import ParameterMultipleInput from processing.algs.saga.SagaAlgorithm import SagaAlgorithm from processing.core.GeoAlgorithm import GeoAlgorithm from processing.core.parameters import ParameterString from processing.algs.saga.SagaGroupNameDecorator import SagaGroupNameDecorator __author__ = 'Victor Olaya' __date__ = 'May 2014' __copyright__ = '(C) 2014, Victor Olaya' # This will get replaced with a git SHA1 when you do a git archive __revision__ = '$Format:%H$' from PyQt4 import QtGui from processing.core.parameters import ParameterRaster from processing.core.outputs import OutputRaster from processing.tools.system import * class RasterCalculator(SagaAlgorithm): FORMULA = "FORMULA" GRIDS = 'GRIDS' XGRIDS = 'XGRIDS' RESULT = "RESULT" def __init__(self): self.allowUnmatchingGridExtents = True self.hardcodedStrings = [] GeoAlgorithm.__init__(self) def getCopy(self): newone = RasterCalculator() newone.provider = self.provider return newone def defineCharacteristics(self): self.name = 'Raster calculator' self.cmdname = 'Grid Calculator' self.undecoratedGroup = "grid_calculus" self.group = SagaGroupNameDecorator.getDecoratedName(self.undecoratedGroup) self.addParameter(ParameterRaster(self.GRIDS, 'Main input layers')) self.addParameter(ParameterMultipleInput(self.XGRIDS, 'Additional layers', ParameterMultipleInput.TYPE_RASTER, True)) self.addParameter(ParameterString(self.FORMULA, "Formula")) self.addOutput(OutputRaster(self.RESULT, "Result")) #=========================================================================== # def processAlgorithm(self, progress): # xgrids = self.getParameterValue(self.XGRIDS) # layers = xgrids.split(';') # grid = layers[0] # self.setParameterValue(self.GRIDS, grid) # xgrids = ";".join(layers[1:]) # if xgrids == "": xgrids = None # self.setParameterValue(self.XGRIDS, xgrids) # SagaAlgorithm.processAlgorithm(self, progress) #===========================================================================
# -*- coding: utf-8 -*- """ jinja2._compat ~~~~~~~~~~~~~~ Some py2/py3 compatibility support based on a stripped down version of six so we don't have to depend on a specific version of it. :copyright: Copyright 2013 by the Jinja team, see AUTHORS. :license: BSD, see LICENSE for details. """ import sys PY2 = sys.version_info[0] == 2 PYPY = hasattr(sys, 'pypy_translation_info') _identity = lambda x: x if not PY2: unichr = chr range_type = range text_type = str string_types = (str,) iterkeys = lambda d: iter(d.keys()) itervalues = lambda d: iter(d.values()) iteritems = lambda d: iter(d.items()) import pickle from io import BytesIO, StringIO NativeStringIO = StringIO def reraise(tp, value, tb=None): if value.__traceback__ is not tb: raise value.with_traceback(tb) raise value ifilter = filter imap = map izip = zip intern = sys.intern implements_iterator = _identity implements_to_string = _identity encode_filename = _identity get_next = lambda x: x.__next__ else: unichr = unichr text_type = unicode range_type = xrange string_types = (str, unicode) iterkeys = lambda d: d.iterkeys() itervalues = lambda d: d.itervalues() iteritems = lambda d: d.iteritems() import cPickle as pickle from cStringIO import StringIO as BytesIO, StringIO NativeStringIO = BytesIO exec('def reraise(tp, value, tb=None):\n raise tp, value, tb') from itertools import imap, izip, ifilter intern = intern def implements_iterator(cls): cls.next = cls.__next__ del cls.__next__ return cls def implements_to_string(cls): cls.__unicode__ = cls.__str__ cls.__str__ = lambda x: x.__unicode__().encode('utf-8') return cls get_next = lambda x: x.next def encode_filename(filename): if isinstance(filename, unicode): return filename.encode('utf-8') return filename try: next = next except NameError: def next(it): return it.next() def with_metaclass(meta, *bases): # This requires a bit of explanation: the basic idea is to make a # dummy metaclass for one level of class instanciation that replaces # itself with the actual metaclass. Because of internal type checks # we also need to make sure that we downgrade the custom metaclass # for one level to something closer to type (that's why __call__ and # __init__ comes back from type etc.). # # This has the advantage over six.with_metaclass in that it does not # introduce dummy classes into the final MRO. class metaclass(meta): __call__ = type.__call__ __init__ = type.__init__ def __new__(cls, name, this_bases, d): if this_bases is None: return type.__new__(cls, name, (), d) return meta(name, bases, d) return metaclass('temporary_class', None, {}) try: from collections import Mapping as mapping_types except ImportError: import UserDict mapping_types = (UserDict.UserDict, UserDict.DictMixin, dict) # common types. These do exist in the special types module too which however # does not exist in IronPython out of the box. Also that way we don't have # to deal with implementation specific stuff here class _C(object): def method(self): pass def _func(): yield None function_type = type(_func) generator_type = type(_func()) method_type = type(_C().method) code_type = type(_C.method.__code__) try: raise TypeError() except TypeError: _tb = sys.exc_info()[2] traceback_type = type(_tb) frame_type = type(_tb.tb_frame) try: from urllib.parse import quote_from_bytes as url_quote except ImportError: from urllib import quote as url_quote try: from thread import allocate_lock except ImportError: try: from threading import Lock as allocate_lock except ImportError: from dummy_thread import allocate_lock
# Copyright (c) 2015, Intel Corporation # # Redistribution and use in source and binary forms, with or without modification, # are permitted provided that the following conditions are met: # # * Redistributions of source code must retain the above copyright notice, # this list of conditions and the following disclaimer. # * Redistributions in binary form must reproduce the above copyright notice, # this list of conditions and the following disclaimer in the documentation # and/or other materials provided with the distribution. # * Neither the name of Intel Corporation nor the names of its contributors # may be used to endorse or promote products derived from this software # without specific prior written permission. # # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND # ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED # WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE # DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR # ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES # (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; # LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON # ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT # (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS # SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. from connect import Connect from device import Device from account import Account from user import User from data import Data from rule import Rule from component import Component from utils import prettyprint, update_properties import json import os import sys # load Configuration file and store values as class attributes def load_config(infile): ''' Load global settings for Cloud connection - server name - username - password - proxy server(s) - API root location on REST server ''' if os.path.isfile(infile): obj = sys.modules[__name__] js = open(infile) data = json.load(js) update_properties(obj, data) js.close() return data else: raise ValueError("Config file not found: %s" % infile)
from hachoir_metadata.timezone import UTC from datetime import date, datetime # Year in 1850..2030 MIN_YEAR = 1850 MAX_YEAR = 2030 class Filter: def __init__(self, valid_types, min=None, max=None): self.types = valid_types self.min = min self.max = max def __call__(self, value): if not isinstance(value, self.types): return True if self.min is not None and value < self.min: return False if self.max is not None and self.max < value: return False return True class NumberFilter(Filter): def __init__(self, min=None, max=None): Filter.__init__(self, (int, long, float), min, max) class DatetimeFilter(Filter): def __init__(self, min=None, max=None): Filter.__init__(self, (date, datetime), datetime(MIN_YEAR, 1, 1), datetime(MAX_YEAR, 12, 31)) self.min_date = date(MIN_YEAR, 1, 1) self.max_date = date(MAX_YEAR, 12, 31) self.min_tz = datetime(MIN_YEAR, 1, 1, tzinfo=UTC) self.max_tz = datetime(MAX_YEAR, 12, 31, tzinfo=UTC) def __call__(self, value): """ Use different min/max values depending on value type (datetime with timezone, datetime or date). """ if not isinstance(value, self.types): return True if hasattr(value, "tzinfo") and value.tzinfo: return (self.min_tz <= value <= self.max_tz) elif isinstance(value, datetime): return (self.min <= value <= self.max) else: return (self.min_date <= value <= self.max_date) DATETIME_FILTER = DatetimeFilter()
# -*- coding: utf-8 -*- # # Doctrine 2 ORM documentation build configuration file, created by # sphinx-quickstart on Fri Dec 3 18:10:24 2010. # # This file is execfile()d with the current directory set to its containing dir. # # Note that not all possible configuration values are present in this # autogenerated file. # # All configuration values have a default; values that are commented out # serve to show the default. import sys, os # If extensions (or modules to document with autodoc) are in another directory, # add these directories to sys.path here. If the directory is relative to the # documentation root, use os.path.abspath to make it absolute, like shown here. sys.path.append(os.path.abspath('_exts')) # -- General configuration ----------------------------------------------------- # Add any Sphinx extension module names here, as strings. They can be extensions # coming with Sphinx (named 'sphinx.ext.*') or your custom ones. extensions = ['configurationblock'] # Add any paths that contain templates here, relative to this directory. templates_path = ['_templates'] # The suffix of source filenames. source_suffix = '.rst' # The encoding of source files. #source_encoding = 'utf-8' # The master toctree document. master_doc = 'index' # General information about the project. project = u'Doctrine 2 ORM' copyright = u'2010-12, Doctrine Project Team' # The version info for the project you're documenting, acts as replacement for # |version| and |release|, also used in various other places throughout the # built documents. # # The short X.Y version. version = '2' # The full version, including alpha/beta/rc tags. release = '2' # The language for content autogenerated by Sphinx. Refer to documentation # for a list of supported languages. language = 'en' # There are two options for replacing |today|: either, you set today to some # non-false value, then it is used: #today = '' # Else, today_fmt is used as the format for a strftime call. #today_fmt = '%B %d, %Y' # List of documents that shouldn't be included in the build. #unused_docs = [] # List of directories, relative to source directory, that shouldn't be searched # for source files. exclude_trees = ['_build'] # The reST default role (used for this markup: `text`) to use for all documents. #default_role = None # If true, '()' will be appended to :func: etc. cross-reference text. #add_function_parentheses = True # If true, the current module name will be prepended to all description # unit titles (such as .. function::). #add_module_names = True # If true, sectionauthor and moduleauthor directives will be shown in the # output. They are ignored by default. show_authors = True # The name of the Pygments (syntax highlighting) style to use. pygments_style = 'sphinx' # A list of ignored prefixes for module index sorting. #modindex_common_prefix = [] # -- Options for HTML output --------------------------------------------------- # The theme to use for HTML and HTML Help pages. Major themes that come with # Sphinx are currently 'default' and 'sphinxdoc'. html_theme = 'doctrine' # Theme options are theme-specific and customize the look and feel of a theme # further. For a list of options available for each theme, see the # documentation. #html_theme_options = {} # Add any paths that contain custom themes here, relative to this directory. html_theme_path = ['_theme'] # The name for this set of Sphinx documents. If None, it defaults to # "<project> v<release> documentation". #html_title = None # A shorter title for the navigation bar. Default is the same as html_title. #html_short_title = None # The name of an image file (relative to this directory) to place at the top # of the sidebar. #html_logo = None # The name of an image file (within the static path) to use as favicon of the # docs. This file should be a Windows icon file (.ico) being 16x16 or 32x32 # pixels large. #html_favicon = None # Add any paths that contain custom static files (such as style sheets) here, # relative to this directory. They are copied after the builtin static files, # so a file named "default.css" will overwrite the builtin "default.css". html_static_path = ['_static'] # If not '', a 'Last updated on:' timestamp is inserted at every page bottom, # using the given strftime format. #html_last_updated_fmt = '%b %d, %Y' # If true, SmartyPants will be used to convert quotes and dashes to # typographically correct entities. #html_use_smartypants = True # Custom sidebar templates, maps document names to template names. #html_sidebars = {} # Additional templates that should be rendered to pages, maps page names to # template names. #html_additional_pages = {} # If false, no module index is generated. #html_use_modindex = True # If false, no index is generated. #html_use_index = True # If true, the index is split into individual pages for each letter. #html_split_index = False # If true, links to the reST sources are added to the pages. #html_show_sourcelink = True # If true, an OpenSearch description file will be output, and all pages will # contain a <link> tag referring to it. The value of this option must be the # base URL from which the finished HTML is served. #html_use_opensearch = '' # If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml"). #html_file_suffix = '' # Output file base name for HTML help builder. htmlhelp_basename = 'Doctrine2ORMdoc' # -- Options for LaTeX output -------------------------------------------------- # The paper size ('letter' or 'a4'). #latex_paper_size = 'letter' # The font size ('10pt', '11pt' or '12pt'). #latex_font_size = '10pt' # Grouping the document tree into LaTeX files. List of tuples # (source start file, target name, title, author, documentclass [howto/manual]). latex_documents = [ ('index', 'Doctrine2ORM.tex', u'Doctrine 2 ORM Documentation', u'Doctrine Project Team', 'manual'), ] # The name of an image file (relative to this directory) to place at the top of # the title page. #latex_logo = None # For "manual" documents, if this is true, then toplevel headings are parts, # not chapters. #latex_use_parts = False # Additional stuff for the LaTeX preamble. #latex_preamble = '' # Documents to append as an appendix to all manuals. #latex_appendices = [] # If false, no module index is generated. #latex_use_modindex = True primary_domain = "dcorm" def linkcode_resolve(domain, info): if domain == 'dcorm': return 'http://' return None
# copyright 2003-2013 LOGILAB S.A. (Paris, FRANCE), all rights reserved. # contact http://www.logilab.fr/ -- mailto:contact@logilab.fr # # This file is part of astroid. # # astroid is free software: you can redistribute it and/or modify it # under the terms of the GNU Lesser General Public License as published by the # Free Software Foundation, either version 2.1 of the License, or (at your # option) any later version. # # astroid is distributed in the hope that it will be useful, but # WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or # FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License # for more details. # # You should have received a copy of the GNU Lesser General Public License along # with astroid. If not, see <http://www.gnu.org/licenses/>. """this module contains exceptions used in the astroid library """ __doctype__ = "restructuredtext en" class AstroidError(Exception): """base exception class for all astroid related exceptions""" class AstroidBuildingException(AstroidError): """exception class when we are unable to build an astroid representation""" class ResolveError(AstroidError): """base class of astroid resolution/inference error""" class NotFoundError(ResolveError): """raised when we are unable to resolve a name""" class InferenceError(ResolveError): """raised when we are unable to infer a node""" class UseInferenceDefault(Exception): """exception to be raised in custom inference function to indicate that it should go back to the default behaviour """ class UnresolvableName(InferenceError): """raised when we are unable to resolve a name""" class NoDefault(AstroidError): """raised by function's `default_value` method when an argument has no default value """
""" InaSAFE Disaster risk assessment tool developed by AusAid and World Bank - **Ftp Client for Retrieving ftp data.** Contact : ole.moller.nielsen@gmail.com .. note:: This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version. """ __author__ = 'imajimatika@gmail.com' __version__ = '0.5.0' __date__ = '10/01/2013' __copyright__ = ('Copyright 2012, Australia Indonesia Facility for ' 'Disaster Reduction') import sys import paramiko import ntpath from stat import S_ISDIR from errno import ENOENT import os import logging from utils import mkDir # The logger is intialised in utils.py by init LOGGER = logging.getLogger('InaSAFE') my_host = '118.97.83.243' my_username = 'geospasial' try: my_password = os.environ['QUAKE_SERVER_PASSWORD'] except KeyError: LOGGER.exception('QUAKE_SERVER_PASSWORD not set!') sys.exit() my_remote_path = 'shakemaps' class SFtpClient: """A utility class that contains methods to fetch a listings and files from an SSH protocol""" def __init__(self, the_host=my_host, the_username=my_username, the_password=my_password, the_working_dir=my_remote_path): self.host = the_host self.username = the_username self.password = the_password self.working_dir = the_working_dir # create transport object self.transport = paramiko.Transport(self.host) self.transport.connect(username=self.username, password=self.password) # create sftp object self.sftp = paramiko.SFTPClient.from_transport(self.transport) # go to remote_path folder, this is the default folder if not self.working_dir is None: self.sftp.chdir(self.working_dir) self.workdir_path = self.sftp.getcwd() def download_path(self, remote_path, local_path): """ Download remote_dir to local_dir. for example : remote_path = '20130111133900' will be download to local_dir/remote_path Must be in the parent directory of remote dir. """ # Check if remote_dir is exist if not self.is_path_exist(remote_path): print 'remote path is not exist %s' % remote_path return False if self.is_dir(remote_path): # get directory name dir_name = get_path_tail(remote_path) # create directory in local machine local_dir_path = os.path.join(local_path, dir_name) mkDir(local_dir_path) # list all directory in remote path list_dir = self.sftp.listdir(remote_path) # iterate recursive for my_dir in list_dir: new_remote_path = os.path.join(remote_path, my_dir) self.download_path(new_remote_path, local_dir_path) else: # download file to local_path file_name = get_path_tail(remote_path) local_file_path = os.path.join(local_path, file_name) LOGGER.info('file %s will be downloaded to %s' % (remote_path, local_file_path)) self.sftp.get(remote_path, local_file_path) def is_dir(self, path): """Check if a path is a directory or not in sftp Reference: http://stackoverflow.com/a/8307575/1198772 """ try: return S_ISDIR(self.sftp.stat(path).st_mode) except IOError: #Path does not exist, so by definition not a directory return False def is_path_exist(self, path): """os.path.exists for paramiko's SCP object Reference: http://stackoverflow.com/q/850749/1198772 """ try: self.sftp.stat(path) except IOError, e: if e.errno == ENOENT: return False raise else: return True def getListing(self, remote_dir=None, my_func=None): """Return list of files and directories name under a remote_dir and return true when it is input to my_func """ if remote_dir is None: remote_dir = self.workdir_path if self.is_path_exist(remote_dir): temp_list = self.sftp.listdir(remote_dir) else: LOGGER.debug('Directory %s is not exist, return None' % remote_dir) return None retval = [] for my_temp in temp_list: if my_func(my_temp): retval.append(my_temp) return retval def get_path_tail(path): '''Return tail of a path Reference : http://stackoverflow.com/a/8384788/1198772 ''' head, tail = ntpath.split(path) return tail or ntpath.basename(head)
from __future__ import unicode_literals from django.contrib.contenttypes.models import ContentType from django.test import TestCase from guardian.compat import get_user_model from guardian.shortcuts import assign_perm, remove_perm class CustomPKModelTest(TestCase): """ Tests agains custom model with primary key other than *standard* ``id`` integer field. """ def setUp(self): self.user = get_user_model().objects.create(username='joe') self.ctype = ContentType.objects.create(model='bar', app_label='fake-for-guardian-tests') def test_assign_perm(self): assign_perm('contenttypes.change_contenttype', self.user, self.ctype) self.assertTrue(self.user.has_perm('contenttypes.change_contenttype', self.ctype)) def test_remove_perm(self): assign_perm('contenttypes.change_contenttype', self.user, self.ctype) self.assertTrue(self.user.has_perm('contenttypes.change_contenttype', self.ctype)) remove_perm('contenttypes.change_contenttype', self.user, self.ctype) self.assertFalse(self.user.has_perm('contenttypes.change_contenttype', self.ctype))
""" Copyright (c) 2003-2005 Gustavo Niemeyer <gustavo@niemeyer.net> This module offers extensions to the standard python 2.3+ datetime module. """ from dateutil.tz import tzfile from tarfile import TarFile import os __author__ = "Gustavo Niemeyer <gustavo@niemeyer.net>" __license__ = "PSF License" __all__ = ["setcachesize", "gettz", "rebuild"] CACHE = [] CACHESIZE = 10 class tzfile(tzfile): def __reduce__(self): return (gettz, (self._filename,)) def getzoneinfofile(): filenames = os.listdir(os.path.join(os.path.dirname(__file__))) filenames.sort() filenames.reverse() for entry in filenames: if entry.startswith("zoneinfo") and ".tar." in entry: return os.path.join(os.path.dirname(__file__), entry) return None ZONEINFOFILE = getzoneinfofile() del getzoneinfofile def setcachesize(size): global CACHESIZE, CACHE CACHESIZE = size del CACHE[size:] def gettz(name): tzinfo = None if ZONEINFOFILE: for cachedname, tzinfo in CACHE: if cachedname == name: break else: tf = TarFile.open(ZONEINFOFILE) try: zonefile = tf.extractfile(name) except KeyError: tzinfo = None else: tzinfo = tzfile(zonefile) tf.close() CACHE.insert(0, (name, tzinfo)) del CACHE[CACHESIZE:] return tzinfo def rebuild(filename, tag=None, format="gz"): import tempfile, shutil tmpdir = tempfile.mkdtemp() zonedir = os.path.join(tmpdir, "zoneinfo") moduledir = os.path.dirname(__file__) if tag: tag = "-"+tag targetname = "zoneinfo%s.tar.%s" % (tag, format) try: tf = TarFile.open(filename) for name in tf.getnames(): if not (name.endswith(".sh") or name.endswith(".tab") or name == "leapseconds"): tf.extract(name, tmpdir) filepath = os.path.join(tmpdir, name) os.system("zic -d %s %s" % (zonedir, filepath)) tf.close() target = os.path.join(moduledir, targetname) for entry in os.listdir(moduledir): if entry.startswith("zoneinfo") and ".tar." in entry: os.unlink(os.path.join(moduledir, entry)) tf = TarFile.open(target, "w:%s" % format) for entry in os.listdir(zonedir): entrypath = os.path.join(zonedir, entry) tf.add(entrypath, entry) tf.close() finally: shutil.rmtree(tmpdir)
""" Python wrapper for wkhtmltopdf library, for rendering images from html files. [ doc: https://wkhtmltopdf.org/docs.html ] * Currently supporting only Linux distributions! This script designed to be run on devices without display. Uses xvfb (X virtual framebuffer) for that functionality. [ doc: https://www.x.org/archive/X11R7.6/doc/man/man1/Xvfb.1.xhtml ]. Dependencies : [xvfb, wkhtmltopdf] Dependencies install: apt-get install wkhtmltopdf apt-get install xvfb """ from subprocess import call import logging import argparse import shutil from config import Config __version__ = '1.0.0' __description__ = "Python wrapper for wkhtmltopdf library - renders html to image (.jpeg) file" XVFB_RUN = "xvfb-run" WKHTML_TO_IMAGE = "wkhtmltoimage" XVFB_CMD = XVFB_RUN + " " + WKHTML_TO_IMAGE + " --window-status ready_to_print --crop-h 396 {0} {1}" def dependencies_installed(): """ Checks whether xvfb-run and wkhtmltoimage packages installed on local machine.abs """ result = False if Config.linux_host(): xvfb = shutil.which(XVFB_RUN) is not None wkhtml = shutil.which(WKHTML_TO_IMAGE) is not None result = xvfb and wkhtml return result def convert_html_to_image(html_path, image_out_path): """Converts html file to image and stores to disk Returns: whether the operations succeeded """ result = False if dependencies_installed(): try: cmd = XVFB_CMD.format(html_path, image_out_path) call(cmd, shell=True) result = True except IOError as ex: logging.getLogger("PYNETWORK").exception(ex) return result def main(): """Main entry point""" arg_parser = argparse.ArgumentParser( description=__description__, usage='%(prog)s [OPTION]...') arg_parser.add_argument("html", help="Html file path") arg_parser.add_argument("image", help="Image Output file path") args = arg_parser.parse_args() if args: convert_html_to_image(args.html, args.image) if __name__ == "__main__": main()
""" Widgets for various HTML5 input types. """ from .core import Input __all__ = ( 'ColorInput', 'DateInput', 'DateTimeInput', 'DateTimeLocalInput', 'EmailInput', 'MonthInput', 'NumberInput', 'RangeInput', 'SearchInput', 'TelInput', 'TimeInput', 'URLInput', 'WeekInput', ) class SearchInput(Input): """ Renders an input with type "search". """ input_type = 'search' class TelInput(Input): """ Renders an input with type "tel". """ input_type = 'tel' class URLInput(Input): """ Renders an input with type "url". """ input_type = 'url' class EmailInput(Input): """ Renders an input with type "email". """ input_type = 'email' class DateTimeInput(Input): """ Renders an input with type "datetime". """ input_type = 'datetime' class DateInput(Input): """ Renders an input with type "date". """ input_type = 'date' class MonthInput(Input): """ Renders an input with type "month". """ input_type = 'month' class WeekInput(Input): """ Renders an input with type "week". """ input_type = 'week' class TimeInput(Input): """ Renders an input with type "time". """ input_type = 'time' class DateTimeLocalInput(Input): """ Renders an input with type "datetime-local". """ input_type = 'datetime-local' class NumberInput(Input): """ Renders an input with type "number". """ input_type = 'number' def __init__(self, step=None): self.step = step def __call__(self, field, **kwargs): if self.step is not None: kwargs.setdefault('step', self.step) return super(NumberInput, self).__call__(field, **kwargs) class RangeInput(Input): """ Renders an input with type "range". """ input_type = 'range' def __init__(self, step=None): self.step = step def __call__(self, field, **kwargs): if self.step is not None: kwargs.setdefault('step', self.step) return super(RangeInput, self).__call__(field, **kwargs) class ColorInput(Input): """ Renders an input with type "color". """ input_type = 'color'
# -*- coding: utf-8 -*- # Form implementation generated from reading ui file 'finalize.ui' # # Created: Fri Apr 10 01:36:55 2015 # by: PyQt4 UI code generator 4.11.2 # # WARNING! All changes made in this file will be lost! from PyQt4 import QtCore, QtGui try: _fromUtf8 = QtCore.QString.fromUtf8 except AttributeError: def _fromUtf8(s): return s try: _encoding = QtGui.QApplication.UnicodeUTF8 def _translate(context, text, disambig): return QtGui.QApplication.translate(context, text, disambig, _encoding) except AttributeError: def _translate(context, text, disambig): return QtGui.QApplication.translate(context, text, disambig) class Ui_FinalImage(object): def setupUi(self, FinalImage): FinalImage.setObjectName(_fromUtf8("FinalImage")) FinalImage.resize(623, 154) self.horizontalLayout = QtGui.QHBoxLayout(FinalImage) self.horizontalLayout.setObjectName(_fromUtf8("horizontalLayout")) self.gridLayout = QtGui.QGridLayout() self.gridLayout.setObjectName(_fromUtf8("gridLayout")) self.statusText = QtGui.QLabel(FinalImage) self.statusText.setObjectName(_fromUtf8("statusText")) self.gridLayout.addWidget(self.statusText, 4, 1, 1, 1) self.label_4 = QtGui.QLabel(FinalImage) self.label_4.setObjectName(_fromUtf8("label_4")) self.gridLayout.addWidget(self.label_4, 4, 0, 1, 1) self.progressBar_3 = QtGui.QProgressBar(FinalImage) self.progressBar_3.setProperty("value", 0) self.progressBar_3.setObjectName(_fromUtf8("progressBar_3")) self.gridLayout.addWidget(self.progressBar_3, 2, 1, 1, 1) self.label = QtGui.QLabel(FinalImage) self.label.setObjectName(_fromUtf8("label")) self.gridLayout.addWidget(self.label, 2, 0, 1, 1) self.label_2 = QtGui.QLabel(FinalImage) self.label_2.setObjectName(_fromUtf8("label_2")) self.gridLayout.addWidget(self.label_2, 0, 0, 1, 1) self.progressBar_2 = QtGui.QProgressBar(FinalImage) self.progressBar_2.setProperty("value", 0) self.progressBar_2.setObjectName(_fromUtf8("progressBar_2")) self.gridLayout.addWidget(self.progressBar_2, 1, 1, 1, 1) self.progressBar_1 = QtGui.QProgressBar(FinalImage) self.progressBar_1.setProperty("value", 0) self.progressBar_1.setObjectName(_fromUtf8("progressBar_1")) self.gridLayout.addWidget(self.progressBar_1, 0, 1, 1, 1) self.label_3 = QtGui.QLabel(FinalImage) self.label_3.setObjectName(_fromUtf8("label_3")) self.gridLayout.addWidget(self.label_3, 1, 0, 1, 1) self.line = QtGui.QFrame(FinalImage) self.line.setFrameShape(QtGui.QFrame.HLine) self.line.setFrameShadow(QtGui.QFrame.Sunken) self.line.setObjectName(_fromUtf8("line")) self.gridLayout.addWidget(self.line, 3, 1, 1, 1) self.horizontalLayout.addLayout(self.gridLayout) self.verticalLayout = QtGui.QVBoxLayout() self.verticalLayout.setObjectName(_fromUtf8("verticalLayout")) self.browseButton = QtGui.QPushButton(FinalImage) self.browseButton.setObjectName(_fromUtf8("browseButton")) self.verticalLayout.addWidget(self.browseButton) self.imageFormat = QtGui.QComboBox(FinalImage) self.imageFormat.setObjectName(_fromUtf8("imageFormat")) self.imageFormat.addItem(_fromUtf8("")) self.imageFormat.addItem(_fromUtf8("")) self.verticalLayout.addWidget(self.imageFormat) self.startButton = QtGui.QPushButton(FinalImage) self.startButton.setObjectName(_fromUtf8("startButton")) self.verticalLayout.addWidget(self.startButton) self.abortButton = QtGui.QPushButton(FinalImage) self.abortButton.setEnabled(False) self.abortButton.setText(_fromUtf8("Abort")) self.abortButton.setObjectName(_fromUtf8("abortButton")) self.verticalLayout.addWidget(self.abortButton) self.closeButton = QtGui.QPushButton(FinalImage) self.closeButton.setEnabled(True) self.closeButton.setObjectName(_fromUtf8("closeButton")) self.verticalLayout.addWidget(self.closeButton) self.horizontalLayout.addLayout(self.verticalLayout) self.retranslateUi(FinalImage) QtCore.QObject.connect(self.closeButton, QtCore.SIGNAL(_fromUtf8("pressed()")), FinalImage.reject) QtCore.QMetaObject.connectSlotsByName(FinalImage) def retranslateUi(self, FinalImage): FinalImage.setWindowTitle(_translate("FinalImage", "Export to images", None)) self.statusText.setText(_translate("FinalImage", "...", None)) self.label_4.setText(_translate("FinalImage", "status:", None)) self.label.setText(_translate("FinalImage", "Total", None)) self.label_2.setText(_translate("FinalImage", "progress of \n" " the one plane", None)) self.label_3.setText(_translate("FinalImage", "progress of\n" " the sample", None)) self.browseButton.setText(_translate("FinalImage", "Save to folder...", None)) self.imageFormat.setItemText(0, _translate("FinalImage", "*.tif", None)) self.imageFormat.setItemText(1, _translate("FinalImage", "*.png", None)) self.startButton.setText(_translate("FinalImage", "Start", None)) self.closeButton.setText(_translate("FinalImage", "Close", None))
# -*- coding: utf-8 -*- """ werkzeug.contrib.jsrouting ~~~~~~~~~~~~~~~~~~~~~~~~~~ Addon module that allows to create a JavaScript function from a map that generates rules. :copyright: (c) 2014 by the Werkzeug Team, see AUTHORS for more details. :license: BSD, see LICENSE for more details. """ try: from simplejson import dumps except ImportError: try: from json import dumps except ImportError: def dumps(*args): raise RuntimeError('simplejson required for jsrouting') from inspect import getmro from werkzeug.routing import NumberConverter from werkzeug._compat import iteritems def render_template(name_parts, rules, converters): result = u'' if name_parts: for idx in range(0, len(name_parts) - 1): name = u'.'.join(name_parts[:idx + 1]) result += u"if (typeof %s === 'undefined') %s = {}\n" % (name, name) result += '%s = ' % '.'.join(name_parts) result += """(function (server_name, script_name, subdomain, url_scheme) { var converters = [%(converters)s]; var rules = %(rules)s; function in_array(array, value) { if (array.indexOf != undefined) { return array.indexOf(value) != -1; } for (var i = 0; i < array.length; i++) { if (array[i] == value) { return true; } } return false; } function array_diff(array1, array2) { array1 = array1.slice(); for (var i = array1.length-1; i >= 0; i--) { if (in_array(array2, array1[i])) { array1.splice(i, 1); } } return array1; } function split_obj(obj) { var names = []; var values = []; for (var name in obj) { if (typeof(obj[name]) != 'function') { names.push(name); values.push(obj[name]); } } return {names: names, values: values, original: obj}; } function suitable(rule, args) { var default_args = split_obj(rule.defaults || {}); var diff_arg_names = array_diff(rule.arguments, default_args.names); for (var i = 0; i < diff_arg_names.length; i++) { if (!in_array(args.names, diff_arg_names[i])) { return false; } } if (array_diff(rule.arguments, args.names).length == 0) { if (rule.defaults == null) { return true; } for (var i = 0; i < default_args.names.length; i++) { var key = default_args.names[i]; var value = default_args.values[i]; if (value != args.original[key]) { return false; } } } return true; } function build(rule, args) { var tmp = []; var processed = rule.arguments.slice(); for (var i = 0; i < rule.trace.length; i++) { var part = rule.trace[i]; if (part.is_dynamic) { var converter = converters[rule.converters[part.data]]; var data = converter(args.original[part.data]); if (data == null) { return null; } tmp.push(data); processed.push(part.name); } else { tmp.push(part.data); } } tmp = tmp.join(''); var pipe = tmp.indexOf('|'); var subdomain = tmp.substring(0, pipe); var url = tmp.substring(pipe+1); var unprocessed = array_diff(args.names, processed); var first_query_var = true; for (var i = 0; i < unprocessed.length; i++) { if (first_query_var) { url += '?'; } else { url += '&'; } first_query_var = false; url += encodeURIComponent(unprocessed[i]); url += '='; url += encodeURIComponent(args.original[unprocessed[i]]); } return {subdomain: subdomain, path: url}; } function lstrip(s, c) { while (s && s.substring(0, 1) == c) { s = s.substring(1); } return s; } function rstrip(s, c) { while (s && s.substring(s.length-1, s.length) == c) { s = s.substring(0, s.length-1); } return s; } return function(endpoint, args, force_external) { args = split_obj(args); var rv = null; for (var i = 0; i < rules.length; i++) { var rule = rules[i]; if (rule.endpoint != endpoint) continue; if (suitable(rule, args)) { rv = build(rule, args); if (rv != null) { break; } } } if (rv == null) { return null; } if (!force_external && rv.subdomain == subdomain) { return rstrip(script_name, '/') + '/' + lstrip(rv.path, '/'); } else { return url_scheme + '://' + (rv.subdomain ? rv.subdomain + '.' : '') + server_name + rstrip(script_name, '/') + '/' + lstrip(rv.path, '/'); } }; })""" % {'converters': u', '.join(converters), 'rules': rules} return result def generate_map(map, name='url_map'): """ Generates a JavaScript function containing the rules defined in this map, to be used with a MapAdapter's generate_javascript method. If you don't pass a name the returned JavaScript code is an expression that returns a function. Otherwise it's a standalone script that assigns the function with that name. Dotted names are resolved (so you an use a name like 'obj.url_for') In order to use JavaScript generation, simplejson must be installed. Note that using this feature will expose the rules defined in your map to users. If your rules contain sensitive information, don't use JavaScript generation! """ from warnings import warn warn(DeprecationWarning('This module is deprecated')) map.update() rules = [] converters = [] for rule in map.iter_rules(): trace = [{ 'is_dynamic': is_dynamic, 'data': data } for is_dynamic, data in rule._trace] rule_converters = {} for key, converter in iteritems(rule._converters): js_func = js_to_url_function(converter) try: index = converters.index(js_func) except ValueError: converters.append(js_func) index = len(converters) - 1 rule_converters[key] = index rules.append({ u'endpoint': rule.endpoint, u'arguments': list(rule.arguments), u'converters': rule_converters, u'trace': trace, u'defaults': rule.defaults }) return render_template(name_parts=name and name.split('.') or [], rules=dumps(rules), converters=converters) def generate_adapter(adapter, name='url_for', map_name='url_map'): """Generates the url building function for a map.""" values = { u'server_name': dumps(adapter.server_name), u'script_name': dumps(adapter.script_name), u'subdomain': dumps(adapter.subdomain), u'url_scheme': dumps(adapter.url_scheme), u'name': name, u'map_name': map_name } return u'''\ var %(name)s = %(map_name)s( %(server_name)s, %(script_name)s, %(subdomain)s, %(url_scheme)s );''' % values def js_to_url_function(converter): """Get the JavaScript converter function from a rule.""" if hasattr(converter, 'js_to_url_function'): data = converter.js_to_url_function() else: for cls in getmro(type(converter)): if cls in js_to_url_functions: data = js_to_url_functions[cls](converter) break else: return 'encodeURIComponent' return '(function(value) { %s })' % data def NumberConverter_js_to_url(conv): if conv.fixed_digits: return u'''\ var result = value.toString(); while (result.length < %s) result = '0' + result; return result;''' % conv.fixed_digits return u'return value.toString();' js_to_url_functions = { NumberConverter: NumberConverter_js_to_url }
#!/usr/bin/env python2.7 # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # # Runs a CI script when new diffs are posted on Review Board. # TODO(wfarner): Also add support for pinging stale reviews. from __future__ import print_function import argparse import base64 import json import subprocess import sys import urllib import urllib2 class ReviewBoard(object): def __init__(self, host, user, password): self._host = host self.user = user self._password = password def api_url(self, api_path): return 'https://%s/api/%s/' % (self._host, api_path) def get_resource_data(self, href, args=None, accept='application/json', data=None): href = '%s?%s' % (href, urllib.urlencode(args)) if args else href print('Request: %s' % href) request = urllib2.Request(href) base64string = base64.encodestring('%s:%s' % (self.user, self._password)).replace('\n', '') request.add_header('Authorization', 'Basic %s' % base64string) request.add_header('Accept', accept) result = urllib2.urlopen(request, data=data) if result.getcode() / 100 != 2: print('Non-ok response: %s\n%s' % (result.getcode(), result)) sys.exit(1) return result.read() def get_resource(self, href, args=None, data=None): return json.loads(self.get_resource_data(href, args=args, data=data)) # Thrown when the patch from a review diff could not be applied. class PatchApplyError(Exception): pass def _apply_patch(patch_data, clean_excludes): subprocess.check_call(['git', 'clean', '-fdx'] + ['--exclude=%s' % e for e in clean_excludes]) subprocess.check_call(['git', 'reset', '--hard', 'origin/master']) patch_file = 'diff.patch' with open(patch_file, 'w') as f: f.write(patch_data) try: subprocess.check_call(['git', 'apply', patch_file]) except subprocess.CalledProcessError: raise PatchApplyError() def _get_latest_diff_time(server, request): diffs = server.get_resource(request['links']['diffs']['href'])['diffs'] return diffs[-1]['timestamp'] REPLY_REQUEST = '@ReviewBot retry' def _get_latest_user_request(reviews): reply_requests = [r for r in reviews if REPLY_REQUEST.lower() in r['body_top'].lower()] if reply_requests: return reply_requests[-1]['timestamp'] def _needs_reply(server, request): print('Inspecting review %d: %s' % (request['id'], request['summary'])) reviews_response = server.get_resource(request['links']['reviews']['href']) reviews = reviews_response['reviews'] # The default response limit is 25. When the responses are limited, a 'next' link will be # included. When that happens, continue to walk until there are no reviews left. while 'next' in reviews_response['links']: print('Fetching next page of reviews.') reviews_response = server.get_resource(reviews_response['links']['next']['href']) reviews.extend(reviews_response['reviews']) feedback_reviews = [r for r in reviews if r['links']['user']['title'] == server.user] if feedback_reviews: # Determine whether another round of feedback is necessary. latest_feedback_time = feedback_reviews[-1]['timestamp'] latest_request = _get_latest_user_request(reviews) latest_diff = _get_latest_diff_time(server, request) print('Latest feedback was given at %s' % latest_feedback_time) print('Latest build request from a user at %s' % latest_request) print('Latest diff was posted at %s' % latest_diff) return ((latest_request and (latest_request > latest_feedback_time)) or (latest_diff and (latest_diff > latest_feedback_time))) return True def _missing_tests(server, diff): # Get files that were modified by the change, flag if test coverage appears lacking. diff_files = server.get_resource(diff['links']['files']['href'])['files'] paths = [f['source_file'] for f in diff_files] return (filter(lambda f: f.startswith('src/main/'), paths) and not filter(lambda f: f.startswith('src/test/'), paths)) def main(): parser = argparse.ArgumentParser() parser.add_argument('--server', dest='server', help='Review Board server.', required=True) parser.add_argument( '--reviewboard-credentials-file', type=argparse.FileType(), help='Review Board credentials file, formatted as <user>\\n<password>', required=True) parser.add_argument( '--repository', help='Inspect reviews posted for this repository.', required=True) parser.add_argument( '--command', help='Build verification command.', required=True) parser.add_argument( '--tail-lines', type=int, default=20, help='Number of lines of command output to include in red build reviews.', required=True) parser.add_argument( '--git-clean-exclude', help='Patterns to pass to git-clean --exclude.', nargs='*') args = parser.parse_args() credentials = args.reviewboard_credentials_file.readlines() server = ReviewBoard( host=args.server, user=credentials[0].strip(), password=credentials[1].strip()) # Find the numeric ID for the repository, required by other API calls. repositories = server.get_resource( server.api_url('repositories'), args={'name': args.repository})['repositories'] if not repositories: print('Failed to find repository %s' % args.repository) sys.exit(1) repository_id = repositories[0]['id'] # Fetch all in-flight reviews. (Note: this does not do pagination, required when > 200 results.) requests = server.get_resource(server.api_url('review-requests'), args={ 'repository': repository_id, 'status': 'pending' })['review_requests'] print('Found %d review requests to inspect' % len(requests)) for request in requests: if not _needs_reply(server, request): continue diffs = server.get_resource(request['links']['diffs']['href'])['diffs'] if not diffs: continue latest_diff = diffs[-1] print('Applying diff %d' % latest_diff['id']) patch_data = server.get_resource_data( latest_diff['links']['self']['href'], accept='text/x-patch') sha = subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD']).strip() ship = False try: _apply_patch(patch_data, args.git_clean_exclude) print('Running build command.') build_output = 'build_output' command = args.command # Pipe to a file in case output is large, also tee the output to simplify # debugging. Since we pipe the output, we must set pipefail to ensure # a failing build command fails the bash pipeline. result = subprocess.call([ 'bash', '-c', 'set -o pipefail; %s 2>&1 | tee %s' % (command, build_output)]) if result == 0: review_text = 'Master (%s) is green with this patch.\n %s' % (sha, command) if _missing_tests(server, latest_diff): review_text = '%s\n\nHowever, it appears that it might lack test coverage.' % review_text else: ship = True else: build_tail = subprocess.check_output(['tail', '-n', str(args.tail_lines), build_output]) review_text = ( 'Master (%s) is red with this patch.\n %s\n\n%s' % (sha, command, build_tail)) except PatchApplyError: review_text = ( 'This patch does not apply cleanly on master (%s), do you need to rebase?' % sha) review_text = ('%s\n\nI will refresh this build result if you post a review containing "%s"' % (review_text, REPLY_REQUEST)) print('Replying to review %d:\n%s' % (request['id'], review_text)) print(server.get_resource( request['links']['reviews']['href'], data=urllib.urlencode({ 'body_top': review_text, 'public': 'true', 'ship_it': 'true' if ship else 'false' }))) if __name__=="__main__": main()
# Copyright 2015, Google Inc. # All rights reserved. # # Redistribution and use in source and binary forms, with or without # modification, are permitted provided that the following conditions are # met: # # * Redistributions of source code must retain the above copyright # notice, this list of conditions and the following disclaimer. # * Redistributions in binary form must reproduce the above # copyright notice, this list of conditions and the following disclaimer # in the documentation and/or other materials provided with the # distribution. # * Neither the name of Google Inc. nor the names of its # contributors may be used to endorse or promote products derived from # this software without specific prior written permission. # # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS # "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT # LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR # A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT # OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, # SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT # LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, # DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY # THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT # (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE # OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. """Test of the GRPC-backed ForeLink and RearLink.""" import threading import unittest from grpc._adapter import _proto_scenarios from grpc._adapter import _test_links from grpc._adapter import fore from grpc._adapter import rear from grpc.framework.base import interfaces from grpc.framework.foundation import logging_pool _IDENTITY = lambda x: x _TIMEOUT = 2 class RoundTripTest(unittest.TestCase): def setUp(self): self.fore_link_pool = logging_pool.pool(80) self.rear_link_pool = logging_pool.pool(80) def tearDown(self): self.rear_link_pool.shutdown(wait=True) self.fore_link_pool.shutdown(wait=True) def testZeroMessageRoundTrip(self): test_operation_id = object() test_method = 'test method' test_fore_link = _test_links.ForeLink(None, None) def rear_action(front_to_back_ticket, fore_link): if front_to_back_ticket.kind in ( interfaces.FrontToBackTicket.Kind.COMPLETION, interfaces.FrontToBackTicket.Kind.ENTIRE): back_to_front_ticket = interfaces.BackToFrontTicket( front_to_back_ticket.operation_id, 0, interfaces.BackToFrontTicket.Kind.COMPLETION, None) fore_link.accept_back_to_front_ticket(back_to_front_ticket) test_rear_link = _test_links.RearLink(rear_action, None) fore_link = fore.ForeLink( self.fore_link_pool, {test_method: None}, {test_method: None}, None, ()) fore_link.join_rear_link(test_rear_link) test_rear_link.join_fore_link(fore_link) fore_link.start() port = fore_link.port() rear_link = rear.RearLink( 'localhost', port, self.rear_link_pool, {test_method: None}, {test_method: None}, False, None, None, None) rear_link.join_fore_link(test_fore_link) test_fore_link.join_rear_link(rear_link) rear_link.start() front_to_back_ticket = interfaces.FrontToBackTicket( test_operation_id, 0, interfaces.FrontToBackTicket.Kind.ENTIRE, test_method, interfaces.ServicedSubscription.Kind.FULL, None, None, _TIMEOUT) rear_link.accept_front_to_back_ticket(front_to_back_ticket) with test_fore_link.condition: while (not test_fore_link.tickets or test_fore_link.tickets[-1].kind is interfaces.BackToFrontTicket.Kind.CONTINUATION): test_fore_link.condition.wait() rear_link.stop() fore_link.stop() with test_fore_link.condition: self.assertIs( test_fore_link.tickets[-1].kind, interfaces.BackToFrontTicket.Kind.COMPLETION) def testEntireRoundTrip(self): test_operation_id = object() test_method = 'test method' test_front_to_back_datum = b'\x07' test_back_to_front_datum = b'\x08' test_fore_link = _test_links.ForeLink(None, None) rear_sequence_number = [0] def rear_action(front_to_back_ticket, fore_link): if front_to_back_ticket.payload is None: payload = None else: payload = test_back_to_front_datum terminal = front_to_back_ticket.kind in ( interfaces.FrontToBackTicket.Kind.COMPLETION, interfaces.FrontToBackTicket.Kind.ENTIRE) if payload is not None or terminal: if terminal: kind = interfaces.BackToFrontTicket.Kind.COMPLETION else: kind = interfaces.BackToFrontTicket.Kind.CONTINUATION back_to_front_ticket = interfaces.BackToFrontTicket( front_to_back_ticket.operation_id, rear_sequence_number[0], kind, payload) rear_sequence_number[0] += 1 fore_link.accept_back_to_front_ticket(back_to_front_ticket) test_rear_link = _test_links.RearLink(rear_action, None) fore_link = fore.ForeLink( self.fore_link_pool, {test_method: _IDENTITY}, {test_method: _IDENTITY}, None, ()) fore_link.join_rear_link(test_rear_link) test_rear_link.join_fore_link(fore_link) fore_link.start() port = fore_link.port() rear_link = rear.RearLink( 'localhost', port, self.rear_link_pool, {test_method: _IDENTITY}, {test_method: _IDENTITY}, False, None, None, None) rear_link.join_fore_link(test_fore_link) test_fore_link.join_rear_link(rear_link) rear_link.start() front_to_back_ticket = interfaces.FrontToBackTicket( test_operation_id, 0, interfaces.FrontToBackTicket.Kind.ENTIRE, test_method, interfaces.ServicedSubscription.Kind.FULL, None, test_front_to_back_datum, _TIMEOUT) rear_link.accept_front_to_back_ticket(front_to_back_ticket) with test_fore_link.condition: while (not test_fore_link.tickets or test_fore_link.tickets[-1].kind is not interfaces.BackToFrontTicket.Kind.COMPLETION): test_fore_link.condition.wait() rear_link.stop() fore_link.stop() with test_rear_link.condition: front_to_back_payloads = tuple( ticket.payload for ticket in test_rear_link.tickets if ticket.payload is not None) with test_fore_link.condition: back_to_front_payloads = tuple( ticket.payload for ticket in test_fore_link.tickets if ticket.payload is not None) self.assertTupleEqual((test_front_to_back_datum,), front_to_back_payloads) self.assertTupleEqual((test_back_to_front_datum,), back_to_front_payloads) def _perform_scenario_test(self, scenario): test_operation_id = object() test_method = scenario.method() test_fore_link = _test_links.ForeLink(None, None) rear_lock = threading.Lock() rear_sequence_number = [0] def rear_action(front_to_back_ticket, fore_link): with rear_lock: if front_to_back_ticket.payload is not None: response = scenario.response_for_request(front_to_back_ticket.payload) else: response = None terminal = front_to_back_ticket.kind in ( interfaces.FrontToBackTicket.Kind.COMPLETION, interfaces.FrontToBackTicket.Kind.ENTIRE) if response is not None or terminal: if terminal: kind = interfaces.BackToFrontTicket.Kind.COMPLETION else: kind = interfaces.BackToFrontTicket.Kind.CONTINUATION back_to_front_ticket = interfaces.BackToFrontTicket( front_to_back_ticket.operation_id, rear_sequence_number[0], kind, response) rear_sequence_number[0] += 1 fore_link.accept_back_to_front_ticket(back_to_front_ticket) test_rear_link = _test_links.RearLink(rear_action, None) fore_link = fore.ForeLink( self.fore_link_pool, {test_method: scenario.deserialize_request}, {test_method: scenario.serialize_response}, None, ()) fore_link.join_rear_link(test_rear_link) test_rear_link.join_fore_link(fore_link) fore_link.start() port = fore_link.port() rear_link = rear.RearLink( 'localhost', port, self.rear_link_pool, {test_method: scenario.serialize_request}, {test_method: scenario.deserialize_response}, False, None, None, None) rear_link.join_fore_link(test_fore_link) test_fore_link.join_rear_link(rear_link) rear_link.start() commencement_ticket = interfaces.FrontToBackTicket( test_operation_id, 0, interfaces.FrontToBackTicket.Kind.COMMENCEMENT, test_method, interfaces.ServicedSubscription.Kind.FULL, None, None, _TIMEOUT) fore_sequence_number = 1 rear_link.accept_front_to_back_ticket(commencement_ticket) for request in scenario.requests(): continuation_ticket = interfaces.FrontToBackTicket( test_operation_id, fore_sequence_number, interfaces.FrontToBackTicket.Kind.CONTINUATION, None, None, None, request, None) fore_sequence_number += 1 rear_link.accept_front_to_back_ticket(continuation_ticket) completion_ticket = interfaces.FrontToBackTicket( test_operation_id, fore_sequence_number, interfaces.FrontToBackTicket.Kind.COMPLETION, None, None, None, None, None) fore_sequence_number += 1 rear_link.accept_front_to_back_ticket(completion_ticket) with test_fore_link.condition: while (not test_fore_link.tickets or test_fore_link.tickets[-1].kind is not interfaces.BackToFrontTicket.Kind.COMPLETION): test_fore_link.condition.wait() rear_link.stop() fore_link.stop() with test_rear_link.condition: requests = tuple( ticket.payload for ticket in test_rear_link.tickets if ticket.payload is not None) with test_fore_link.condition: responses = tuple( ticket.payload for ticket in test_fore_link.tickets if ticket.payload is not None) self.assertTrue(scenario.verify_requests(requests)) self.assertTrue(scenario.verify_responses(responses)) def testEmptyScenario(self): self._perform_scenario_test(_proto_scenarios.EmptyScenario()) def testBidirectionallyUnaryScenario(self): self._perform_scenario_test(_proto_scenarios.BidirectionallyUnaryScenario()) def testBidirectionallyStreamingScenario(self): self._perform_scenario_test( _proto_scenarios.BidirectionallyStreamingScenario()) if __name__ == '__main__': unittest.main()
from django.core.management.base import BaseCommand, CommandError from django.contrib.staticfiles.handlers import StaticFilesHandler from django.core.servers.basehttp import get_internal_wsgi_application #from django.utils import autoreload from trunserv import autoreload from twisted.application import internet, service, app from twisted.web import server, resource, wsgi, static from twisted.python import threadpool, log from twisted.internet import reactor from optparse import make_option import sys import os import re naiveip_re = re.compile(r"""^(?: (?P<addr> (?P<ipv4>\d{1,3}(?:\.\d{1,3}){3}) | # IPv4 address (?P<ipv6>\[[a-fA-F0-9:]+\]) | # IPv6 address (?P<fqdn>[a-zA-Z0-9-]+(?:\.[a-zA-Z0-9-]+)*) # FQDN ):)?(?P<port>\d+)$""", re.X) DEFAULT_PORT = "8000" class Root(resource.Resource): def __init__(self, wsgi_resource): resource.Resource.__init__(self) self.wsgi_resource = wsgi_resource def getChild(self, path, request): path0 = request.prepath.pop(0) request.postpath.insert(0, path0) return self.wsgi_resource def wsgi_resource(): pool = threadpool.ThreadPool() pool.start() # Allow Ctrl-C to get you out cleanly: reactor.addSystemEventTrigger('after', 'shutdown', pool.stop) handler = StaticFilesHandler(get_internal_wsgi_application()) wsgi_resource = wsgi.WSGIResource(reactor, pool, handler) return wsgi_resource class Command(BaseCommand): option_list = BaseCommand.option_list + ( make_option('--noreload', action='store_false', dest='use_reloader', default=True, help='Do NOT use the auto-reloader.'), ) help = "Starts a Twisted Web server for development." args = '[optional port number, or ipaddr:port]' # Validation is called explicitly each time the server is reloaded. requires_model_validation = False def handle(self, addrport='', *args, **options): if not addrport: self.addr = '' self.port = DEFAULT_PORT else: m = re.match(naiveip_re, addrport) if m is None: raise CommandError('"%s" is not a valid port number ' 'or address:port pair.' % addrport) self.addr, _ipv4, _ipv6, _fqdn, self.port = m.groups() if not self.port.isdigit(): raise CommandError("%r is not a valid port." % self.port) if not self.addr: self.addr = '127.0.0.1' self.run(*args, **options) def run(self, *args, **options): use_reloader = options.get('use_reloader', True) def _inner_run(): # Initialize logging log.startLogging(sys.stdout) # Setup Twisted application application = service.Application('django') wsgi_root = wsgi_resource() root = Root(wsgi_root) main_site = server.Site(root) internet.TCPServer(int(self.port), main_site ).setServiceParent(application) service.IService(application).startService() app.startApplication(application, False) reactor.addSystemEventTrigger('before', 'shutdown', service.IService(application).stopService) reactor.run() if use_reloader: try: autoreload.main(_inner_run) except TypeError: # autoreload was in the middle of something pass else: _inner_run()
""" Regression tests for proper working of ForeignKey(null=True). Tests these bugs: * #7512: including a nullable foreign key reference in Meta ordering has un xpected results """ from __future__ import unicode_literals from django.db import models from django.utils.encoding import python_2_unicode_compatible # The first two models represent a very simple null FK ordering case. class Author(models.Model): name = models.CharField(max_length=150) @python_2_unicode_compatible class Article(models.Model): title = models.CharField(max_length=150) author = models.ForeignKey(Author, models.SET_NULL, null=True) def __str__(self): return 'Article titled: %s' % (self.title, ) class Meta: ordering = ['author__name', ] # These following 4 models represent a far more complex ordering case. class SystemInfo(models.Model): system_name = models.CharField(max_length=32) class Forum(models.Model): system_info = models.ForeignKey(SystemInfo, models.CASCADE) forum_name = models.CharField(max_length=32) @python_2_unicode_compatible class Post(models.Model): forum = models.ForeignKey(Forum, models.SET_NULL, null=True) title = models.CharField(max_length=32) def __str__(self): return self.title @python_2_unicode_compatible class Comment(models.Model): post = models.ForeignKey(Post, models.SET_NULL, null=True) comment_text = models.CharField(max_length=250) class Meta: ordering = ['post__forum__system_info__system_name', 'comment_text'] def __str__(self): return self.comment_text
from decimal import Decimal as D from django.test import TestCase import mock from oscar.apps.shipping import methods from oscar.apps.basket.models import Basket class TestFreeShipppingForEmptyBasket(TestCase): def setUp(self): self.method = methods.Free() self.basket = Basket() self.charge = self.method.calculate(self.basket) def test_is_free(self): self.assertEqual(D('0.00'), self.charge.incl_tax) self.assertEqual(D('0.00'), self.charge.excl_tax) def test_has_tax_known(self): self.assertTrue(self.charge.is_tax_known) def test_has_same_currency_as_basket(self): self.assertEqual(self.basket.currency, self.charge.currency) class TestFreeShipppingForNonEmptyBasket(TestCase): def setUp(self): self.method = methods.Free() self.basket = mock.Mock() self.basket.num_items = 1 self.charge = self.method.calculate(self.basket) def test_is_free(self): self.assertEqual(D('0.00'), self.charge.incl_tax) self.assertEqual(D('0.00'), self.charge.excl_tax) class TestNoShippingRequired(TestCase): def setUp(self): self.method = methods.NoShippingRequired() basket = Basket() self.charge = self.method.calculate(basket) def test_is_free_for_empty_basket(self): self.assertEqual(D('0.00'), self.charge.incl_tax) self.assertEqual(D('0.00'), self.charge.excl_tax) def test_has_a_different_code_to_free(self): self.assertTrue(methods.NoShippingRequired.code != methods.Free.code) class TestFixedPriceShippingWithoutTax(TestCase): def setUp(self): self.method = methods.FixedPrice(D('10.00')) basket = Basket() self.charge = self.method.calculate(basket) def test_has_correct_charge(self): self.assertEqual(D('10.00'), self.charge.excl_tax) def test_does_not_include_tax(self): self.assertFalse(self.charge.is_tax_known) class TestFixedPriceShippingWithTax(TestCase): def setUp(self): self.method = methods.FixedPrice( charge_excl_tax=D('10.00'), charge_incl_tax=D('12.00')) basket = Basket() self.charge = self.method.calculate(basket) def test_has_correct_charge(self): self.assertEqual(D('10.00'), self.charge.excl_tax) self.assertEqual(D('12.00'), self.charge.incl_tax) def test_does_include_tax(self): self.assertTrue(self.charge.is_tax_known)
# Copyright 2021, Kay Hayen, mailto:kay.hayen@gmail.com # # Python test originally created or extracted from other peoples work. The # parts from me are licensed as below. It is at least Free Software where # it's copied from other people. In these cases, that will normally be # indicated. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # module_value1 = 5000 module_value2 = 3000 def calledRepeatedly(): # Force frame and eliminate forward propagation (currently). module_value1 local_value = module_value1 s = module_value1 t = module_value2 # construct_begin t = s + t # construct_end return s, t, local_value import itertools for x in itertools.repeat(None, 50000): calledRepeatedly() print("OK.")
#!/usr/bin/env python # # Copyright 2014 Matthew Wall # See the file LICENSE.txt for your rights. """Driver for ADS WS1 weather stations. Thanks to Steve (sesykes71) for the testing that made this driver possible. Thanks to Jay Nugent (WB8TKL) and KRK6 for weather-2.kr6k-V2.1 http://server1.nuge.com/~weather/ """ from __future__ import with_statement import serial import syslog import time import weewx.drivers DRIVER_NAME = 'WS1' DRIVER_VERSION = '0.19' def loader(config_dict, _): return WS1Driver(**config_dict[DRIVER_NAME]) def confeditor_loader(): return WS1ConfEditor() INHG_PER_MBAR = 0.0295333727 METER_PER_FOOT = 0.3048 MILE_PER_KM = 0.621371 DEFAULT_PORT = '/dev/ttyS0' DEBUG_READ = 0 def logmsg(level, msg): syslog.syslog(level, 'ws1: %s' % msg) def logdbg(msg): logmsg(syslog.LOG_DEBUG, msg) def loginf(msg): logmsg(syslog.LOG_INFO, msg) def logerr(msg): logmsg(syslog.LOG_ERR, msg) class WS1Driver(weewx.drivers.AbstractDevice): """weewx driver that communicates with an ADS-WS1 station port - serial port [Required. Default is /dev/ttyS0] max_tries - how often to retry serial communication before giving up [Optional. Default is 5] retry_wait - how long to wait, in seconds, before retrying after a failure [Optional. Default is 10] """ def __init__(self, **stn_dict): self.port = stn_dict.get('port', DEFAULT_PORT) self.max_tries = int(stn_dict.get('max_tries', 5)) self.retry_wait = int(stn_dict.get('retry_wait', 10)) self.last_rain = None loginf('driver version is %s' % DRIVER_VERSION) loginf('using serial port %s' % self.port) global DEBUG_READ DEBUG_READ = int(stn_dict.get('debug_read', DEBUG_READ)) self.station = Station(self.port) self.station.open() def closePort(self): if self.station is not None: self.station.close() self.station = None @property def hardware_name(self): return "WS1" def genLoopPackets(self): while True: packet = {'dateTime': int(time.time() + 0.5), 'usUnits': weewx.US} readings = self.station.get_readings_with_retry(self.max_tries, self.retry_wait) data = Station.parse_readings(readings) packet.update(data) self._augment_packet(packet) yield packet def _augment_packet(self, packet): # calculate the rain delta from rain total if self.last_rain is not None: packet['rain'] = packet['long_term_rain'] - self.last_rain else: packet['rain'] = None self.last_rain = packet['long_term_rain'] # no wind direction when wind speed is zero if 'windSpeed' in packet and not packet['windSpeed']: packet['windDir'] = None class Station(object): def __init__(self, port): self.port = port self.baudrate = 2400 self.timeout = 3 self.serial_port = None def __enter__(self): self.open() return self def __exit__(self, _, value, traceback): self.close() def open(self): logdbg("open serial port %s" % self.port) self.serial_port = serial.Serial(self.port, self.baudrate, timeout=self.timeout) def close(self): if self.serial_port is not None: logdbg("close serial port %s" % self.port) self.serial_port.close() self.serial_port = None # FIXME: use either CR or LF as line terminator. apparently some ws1 # hardware occasionally ends a line with only CR instead of the standard # CR-LF, resulting in a line that is too long. def get_readings(self): buf = self.serial_port.readline() if DEBUG_READ: logdbg("bytes: '%s'" % ' '.join(["%0.2X" % ord(c) for c in buf])) buf = buf.strip() return buf def get_readings_with_retry(self, max_tries=5, retry_wait=10): for ntries in range(0, max_tries): try: buf = self.get_readings() Station.validate_string(buf) return buf except (serial.serialutil.SerialException, weewx.WeeWxIOError), e: loginf("Failed attempt %d of %d to get readings: %s" % (ntries + 1, max_tries, e)) time.sleep(retry_wait) else: msg = "Max retries (%d) exceeded for readings" % max_tries logerr(msg) raise weewx.RetriesExceeded(msg) @staticmethod def validate_string(buf): if len(buf) != 50: raise weewx.WeeWxIOError("Unexpected buffer length %d" % len(buf)) if buf[0:2] != '!!': raise weewx.WeeWxIOError("Unexpected header bytes '%s'" % buf[0:2]) return buf @staticmethod def parse_readings(raw): """WS1 station emits data in PeetBros format: http://www.peetbros.com/shop/custom.aspx?recid=29 Each line has 50 characters - 2 header bytes and 48 data bytes: !!000000BE02EB000027700000023A023A0025005800000000 SSSSXXDDTTTTLLLLPPPPttttHHHHhhhhddddmmmmRRRRWWWW SSSS - wind speed (0.1 kph) XX - wind direction calibration DD - wind direction (0-255) TTTT - outdoor temperature (0.1 F) LLLL - long term rain (0.01 in) PPPP - pressure (0.1 mbar) tttt - indoor temperature (0.1 F) HHHH - outdoor humidity (0.1 %) hhhh - indoor humidity (0.1 %) dddd - date (day of year) mmmm - time (minute of day) RRRR - daily rain (0.01 in) WWWW - one minute wind average (0.1 kph) """ # FIXME: peetbros could be 40 bytes or 44 bytes, what about ws1? # FIXME: peetbros uses two's complement for temp, what about ws1? # FIXME: for ws1 is the pressure reading 'pressure' or 'barometer'? buf = raw[2:] data = dict() data['windSpeed'] = Station._decode(buf[0:4], 0.1 * MILE_PER_KM) # mph data['windDir'] = Station._decode(buf[6:8], 1.411764) # compass deg data['outTemp'] = Station._decode(buf[8:12], 0.1) # degree_F data['long_term_rain'] = Station._decode(buf[12:16], 0.01) # inch data['pressure'] = Station._decode(buf[16:20], 0.1 * INHG_PER_MBAR) # inHg data['inTemp'] = Station._decode(buf[20:24], 0.1) # degree_F data['outHumidity'] = Station._decode(buf[24:28], 0.1) # percent data['inHumidity'] = Station._decode(buf[28:32], 0.1) # percent data['day_of_year'] = Station._decode(buf[32:36]) data['minute_of_day'] = Station._decode(buf[36:40]) data['daily_rain'] = Station._decode(buf[40:44], 0.01) # inch data['wind_average'] = Station._decode(buf[44:48], 0.1 * MILE_PER_KM) # mph return data @staticmethod def _decode(s, multiplier=None, neg=False): v = None try: v = int(s, 16) if neg: bits = 4 * len(s) if v & (1 << (bits - 1)) != 0: v -= (1 << bits) if multiplier is not None: v *= multiplier except ValueError, e: if s != '----': logdbg("decode failed for '%s': %s" % (s, e)) return v class WS1ConfEditor(weewx.drivers.AbstractConfEditor): @property def default_stanza(self): return """ [WS1] # This section is for the ADS WS1 series of weather stations. # Serial port such as /dev/ttyS0, /dev/ttyUSB0, or /dev/cuaU0 port = /dev/ttyUSB0 # The driver to use: driver = weewx.drivers.ws1 """ def prompt_for_settings(self): print "Specify the serial port on which the station is connected, for" print "example /dev/ttyUSB0 or /dev/ttyS0." port = self._prompt('port', '/dev/ttyUSB0') return {'port': port} # define a main entry point for basic testing of the station without weewx # engine and service overhead. invoke this as follows from the weewx root dir: # # PYTHONPATH=bin python bin/weewx/drivers/ws1.py if __name__ == '__main__': import optparse usage = """%prog [options] [--help]""" syslog.openlog('ws1', syslog.LOG_PID | syslog.LOG_CONS) syslog.setlogmask(syslog.LOG_UPTO(syslog.LOG_DEBUG)) parser = optparse.OptionParser(usage=usage) parser.add_option('--version', dest='version', action='store_true', help='display driver version') parser.add_option('--port', dest='port', metavar='PORT', help='serial port to which the station is connected', default=DEFAULT_PORT) (options, args) = parser.parse_args() if options.version: print "ADS WS1 driver version %s" % DRIVER_VERSION exit(0) with Station(options.port) as s: while True: print time.time(), s.get_readings()
# -*- coding: utf-8 -*- """ jinja2.testsuite ~~~~~~~~~~~~~~~~ All the unittests of Jinja2. These tests can be executed by either running run-tests.py using multiple Python versions at the same time. :copyright: (c) 2010 by the Jinja Team. :license: BSD, see LICENSE for more details. """ import os import re import sys import unittest from traceback import format_exception from jinja2 import loaders from jinja2._compat import PY2 here = os.path.dirname(os.path.abspath(__file__)) dict_loader = loaders.DictLoader({ 'justdict.html': 'FOO' }) package_loader = loaders.PackageLoader('jinja2.testsuite.res', 'templates') filesystem_loader = loaders.FileSystemLoader(here + '/res/templates') function_loader = loaders.FunctionLoader({'justfunction.html': 'FOO'}.get) choice_loader = loaders.ChoiceLoader([dict_loader, package_loader]) prefix_loader = loaders.PrefixLoader({ 'a': filesystem_loader, 'b': dict_loader }) class JinjaTestCase(unittest.TestCase): ### use only these methods for testing. If you need standard ### unittest method, wrap them! def setup(self): pass def teardown(self): pass def setUp(self): self.setup() def tearDown(self): self.teardown() def assert_equal(self, a, b): return self.assertEqual(a, b) def assert_raises(self, *args, **kwargs): return self.assertRaises(*args, **kwargs) def assert_traceback_matches(self, callback, expected_tb): try: callback() except Exception as e: tb = format_exception(*sys.exc_info()) if re.search(expected_tb.strip(), ''.join(tb)) is None: raise self.fail('Traceback did not match:\n\n%s\nexpected:\n%s' % (''.join(tb), expected_tb)) else: self.fail('Expected exception') def find_all_tests(suite): """Yields all the tests and their names from a given suite.""" suites = [suite] while suites: s = suites.pop() try: suites.extend(s) except TypeError: yield s, '%s.%s.%s' % ( s.__class__.__module__, s.__class__.__name__, s._testMethodName ) class BetterLoader(unittest.TestLoader): """A nicer loader that solves two problems. First of all we are setting up tests from different sources and we're doing this programmatically which breaks the default loading logic so this is required anyways. Secondly this loader has a nicer interpolation for test names than the default one so you can just do ``run-tests.py ViewTestCase`` and it will work. """ def getRootSuite(self): return suite() def loadTestsFromName(self, name, module=None): root = self.getRootSuite() if name == 'suite': return root all_tests = [] for testcase, testname in find_all_tests(root): if testname == name or \ testname.endswith('.' + name) or \ ('.' + name + '.') in testname or \ testname.startswith(name + '.'): all_tests.append(testcase) if not all_tests: raise LookupError('could not find test case for "%s"' % name) if len(all_tests) == 1: return all_tests[0] rv = unittest.TestSuite() for test in all_tests: rv.addTest(test) return rv def suite(): from jinja2.testsuite import ext, filters, tests, core_tags, \ loader, inheritance, imports, lexnparse, security, api, \ regression, debug, utils, bytecode_cache, doctests suite = unittest.TestSuite() suite.addTest(ext.suite()) suite.addTest(filters.suite()) suite.addTest(tests.suite()) suite.addTest(core_tags.suite()) suite.addTest(loader.suite()) suite.addTest(inheritance.suite()) suite.addTest(imports.suite()) suite.addTest(lexnparse.suite()) suite.addTest(security.suite()) suite.addTest(api.suite()) suite.addTest(regression.suite()) suite.addTest(debug.suite()) suite.addTest(utils.suite()) suite.addTest(bytecode_cache.suite()) # doctests will not run on python 3 currently. Too many issues # with that, do not test that on that platform. if PY2: suite.addTest(doctests.suite()) return suite def main(): """Runs the testsuite as command line application.""" try: unittest.main(testLoader=BetterLoader(), defaultTest='suite') except Exception as e: print('Error: %s' % e)
# MIT License # Copyright (c) 2017 MassChallenge, Inc. from django.apps import apps from django.conf import settings from django.conf.urls import ( include, url, ) from django.conf.urls.static import static from django.contrib import admin from django.views.decorators.csrf import csrf_exempt from django.views.generic import TemplateView from drf_auto_endpoint.router import router as schema_router from impact.graphql.utils.custom_error_view import SafeGraphQLView from rest_framework import routers from rest_framework_jwt.views import ( obtain_jwt_token, refresh_jwt_token, verify_jwt_token, ) from impact.graphql.middleware import IsAuthenticatedMiddleware from impact.graphql.schema import ( auth_schema, schema, ) from impact.model_utils import model_name_to_snake from impact.schema import schema_view from impact.v0.urls import v0_urlpatterns from impact.v1.urls import v1_urlpatterns from impact.views import ( CalendarReminderView, AlgoliaApiKeyView, GeneralViewSet, IndexView, JWTCookieNameView, ) from .views.general_view_set import MODELS_TO_EXCLUDE_FROM_URL_BINDING accelerator_router = routers.DefaultRouter() simpleuser_router = routers.DefaultRouter() simpleuser_router.register('User', GeneralViewSet, base_name='User') for model in apps.get_models('accelerator'): if (model._meta.app_label == 'accelerator' and not model._meta.auto_created and model.__name__ not in MODELS_TO_EXCLUDE_FROM_URL_BINDING): schema_router.register( model, url=model_name_to_snake(model.__name__)) sso_urlpatterns = [ url(r'^api-token-auth/', obtain_jwt_token), url(r'^api-token-refresh/', refresh_jwt_token), url(r'^api-token-verify/', verify_jwt_token), ] account_urlpatterns = [ url(r'^', include('registration.backends.simple.urls')), ] urls = [ url(r'^api/sso/token_name/', JWTCookieNameView.as_view(), name=JWTCookieNameView.view_name), url(r'^api/algolia/api_key/$', AlgoliaApiKeyView.as_view(), name=AlgoliaApiKeyView.view_name), url(r'^api/calendar/reminder/$', CalendarReminderView.as_view(), name=CalendarReminderView.view_name), url(r'^api/v0/', include(v0_urlpatterns)), url(r'^api/v1/', include(v1_urlpatterns)), url(r'^api/(?P<app>\w+)/(?P<model>[a-z_]+)/' r'(?P<related_model>[a-z_]+)/$', GeneralViewSet.as_view({'get': 'list', 'post': 'create'}), name='related-object-list'), url(r'^api/(?P<app>\w+)/(?P<model>[a-z_]+)/' r'(?P<related_model>[a-z_]+)/' r'(?P<pk>[0-9]+)/$', GeneralViewSet.as_view({ 'get': 'retrieve', 'put': 'update', 'patch': 'partial_update', 'delete': 'destroy' }), name='related-object-detail'), url(r'^api/(?P<app>\w+)/(?P<model>[a-z_]+)/$', GeneralViewSet.as_view({'get': 'list', 'post': 'create'}), name='object-list'), url(r'^api/(?P<app>\w+)/(?P<model>[a-z_]+)/(?P<pk>[0-9]+)/$', GeneralViewSet.as_view({ 'get': 'retrieve', 'put': 'update', 'patch': 'partial_update', 'delete': 'destroy' }), name='object-detail'), url(r'^api/simpleuser/', include(simpleuser_router.urls)), url(r'^api/accelerator/', include(schema_router.urls), name='api-root'), url(r'^sso/', include(sso_urlpatterns)), url(r'^admin/', admin.site.urls), url(r'^accounts/', include(account_urlpatterns)), url(r'^graphql/$', csrf_exempt(SafeGraphQLView.as_view( graphiql=settings.DEBUG, schema=schema, middleware=[ IsAuthenticatedMiddleware])), name="graphql"), url(r'^graphql/auth/$', csrf_exempt(SafeGraphQLView.as_view( graphiql=settings.DEBUG, schema=auth_schema)), name="graphql-auth"), url(r'^oauth/', include('oauth2_provider.urls', namespace='oauth2_provider')), url(r'^schema/$', schema_view, name='schema'), url(r'^directory/(?:.*)$', TemplateView.as_view( template_name='front-end.html'), name="directory"), url(r'^allocator/(?:.*)$', TemplateView.as_view( template_name='front-end.html'), name="allocator"), url(r'^people/$', TemplateView.as_view( template_name='front-end.html'), name="entreprenuer_directory"), url(r'^people/(.*)/$', TemplateView.as_view( template_name='front-end.html'), name="entreprenuer_profile"), url(r'^startups/$', TemplateView.as_view( template_name='front-end.html'), name="startup_directory"), url(r'^openid/', include('oidc_provider.urls', namespace='oidc_provider')), url(r'^$', IndexView.as_view()), ] # use staticfiles with waitress (not recommneded!) # TODO: switch to a real static file handler urls += ( static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)) if settings.DEBUG: # add debug toolbar import debug_toolbar # pragma: no cover urls += [ # pragma: no cover url(r"^__debug__/", include(debug_toolbar.urls)), # pragma: no cover ] + static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT) urlpatterns = urls
"""Capa's specialized use of codejail.safe_exec.""" from codejail.safe_exec import safe_exec as codejail_safe_exec from codejail.safe_exec import not_safe_exec as codejail_not_safe_exec from codejail.safe_exec import json_safe, SafeExecException from . import lazymod from dogapi import dog_stats_api import hashlib # Establish the Python environment for Capa. # Capa assumes float-friendly division always. # The name "random" is a properly-seeded stand-in for the random module. CODE_PROLOG = """\ from __future__ import division import random as random_module import sys random = random_module.Random(%r) random.Random = random_module.Random sys.modules['random'] = random """ ASSUMED_IMPORTS = [ ("numpy", "numpy"), ("math", "math"), ("scipy", "scipy"), ("calc", "calc"), ("eia", "eia"), ("chemcalc", "chem.chemcalc"), ("chemtools", "chem.chemtools"), ("miller", "chem.miller"), ("draganddrop", "verifiers.draganddrop"), ] # We'll need the code from lazymod.py for use in safe_exec, so read it now. lazymod_py_file = lazymod.__file__ if lazymod_py_file.endswith("c"): lazymod_py_file = lazymod_py_file[:-1] lazymod_py = open(lazymod_py_file).read() LAZY_IMPORTS = [lazymod_py] for name, modname in ASSUMED_IMPORTS: LAZY_IMPORTS.append("{} = LazyModule('{}')\n".format(name, modname)) LAZY_IMPORTS = "".join(LAZY_IMPORTS) def update_hash(hasher, obj): """ Update a `hashlib` hasher with a nested object. To properly cache nested structures, we need to compute a hash from the entire structure, canonicalizing at every level. `hasher`'s `.update()` method is called a number of times, touching all of `obj` in the process. Only primitive JSON-safe types are supported. """ hasher.update(str(type(obj))) if isinstance(obj, (tuple, list)): for e in obj: update_hash(hasher, e) elif isinstance(obj, dict): for k in sorted(obj): update_hash(hasher, k) update_hash(hasher, obj[k]) else: hasher.update(repr(obj)) @dog_stats_api.timed('capa.safe_exec.time') def safe_exec( code, globals_dict, random_seed=None, python_path=None, extra_files=None, cache=None, slug=None, unsafely=False, ): """ Execute python code safely. `code` is the Python code to execute. It has access to the globals in `globals_dict`, and any changes it makes to those globals are visible in `globals_dict` when this function returns. `random_seed` will be used to see the `random` module available to the code. `python_path` is a list of filenames or directories to add to the Python path before execution. If the name is not in `extra_files`, then it will also be copied into the sandbox. `extra_files` is a list of (filename, contents) pairs. These files are created in the sandbox. `cache` is an object with .get(key) and .set(key, value) methods. It will be used to cache the execution, taking into account the code, the values of the globals, and the random seed. `slug` is an arbitrary string, a description that's meaningful to the caller, that will be used in log messages. If `unsafely` is true, then the code will actually be executed without sandboxing. """ # Check the cache for a previous result. if cache: safe_globals = json_safe(globals_dict) md5er = hashlib.md5() md5er.update(repr(code)) update_hash(md5er, safe_globals) key = "safe_exec.%r.%s" % (random_seed, md5er.hexdigest()) cached = cache.get(key) if cached is not None: # We have a cached result. The result is a pair: the exception # message, if any, else None; and the resulting globals dictionary. emsg, cleaned_results = cached globals_dict.update(cleaned_results) if emsg: raise SafeExecException(emsg) return # Create the complete code we'll run. code_prolog = CODE_PROLOG % random_seed # Decide which code executor to use. if unsafely: exec_fn = codejail_not_safe_exec else: exec_fn = codejail_safe_exec # Run the code! Results are side effects in globals_dict. try: exec_fn( code_prolog + LAZY_IMPORTS + code, globals_dict, python_path=python_path, extra_files=extra_files, slug=slug, ) except SafeExecException as e: emsg = e.message else: emsg = None # Put the result back in the cache. This is complicated by the fact that # the globals dict might not be entirely serializable. if cache: cleaned_results = json_safe(globals_dict) cache.set(key, (emsg, cleaned_results)) # If an exception happened, raise it now. if emsg: raise e
# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com> # # This file is part of Ansible # # Ansible is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Ansible is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Ansible. If not, see <http://www.gnu.org/licenses/>. # Make coding more python3-ish from __future__ import (absolute_import, division, print_function) __metaclass__ = type class Attribute: def __init__(self, isa=None, private=False, default=None, required=False, listof=None): self.isa = isa self.private = private self.default = default self.required = required self.listof = listof class FieldAttribute(Attribute): pass
# -*- encoding: utf-8 -*- ############################################################################## # # Module - Parent Dependencies module for Odoo # Copyright (C) 2014 GRAP (http://www.grap.coop) # @author Sylvain LE GAL (https://twitter.com/legalsylvain) # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## from . import model
""" These classes are light wrappers around Django's database API that provide convenience functionality and permalink functions for the databrowse app. """ from django.db import models from django.utils import formats from django.utils.text import capfirst from django.utils.encoding import smart_unicode, smart_str, iri_to_uri from django.utils.safestring import mark_safe from django.db.models.query import QuerySet EMPTY_VALUE = '(None)' DISPLAY_SIZE = 100 class EasyModel(object): def __init__(self, site, model): self.site = site self.model = model self.model_list = site.registry.keys() self.verbose_name = model._meta.verbose_name self.verbose_name_plural = model._meta.verbose_name_plural def __repr__(self): return '<EasyModel for %s>' % smart_str(self.model._meta.object_name) def model_databrowse(self): "Returns the ModelDatabrowse class for this model." return self.site.registry[self.model] def url(self): return mark_safe('%s%s/%s/' % (self.site.root_url, self.model._meta.app_label, self.model._meta.module_name)) def objects(self, **kwargs): return self.get_query_set().filter(**kwargs) def get_query_set(self): easy_qs = self.model._default_manager.get_query_set()._clone(klass=EasyQuerySet) easy_qs._easymodel = self return easy_qs def object_by_pk(self, pk): return EasyInstance(self, self.model._default_manager.get(pk=pk)) def sample_objects(self): for obj in self.model._default_manager.all()[:3]: yield EasyInstance(self, obj) def field(self, name): try: f = self.model._meta.get_field(name) except models.FieldDoesNotExist: return None return EasyField(self, f) def fields(self): return [EasyField(self, f) for f in (self.model._meta.fields + self.model._meta.many_to_many)] class EasyField(object): def __init__(self, easy_model, field): self.model, self.field = easy_model, field def __repr__(self): return smart_str(u'<EasyField for %s.%s>' % (self.model.model._meta.object_name, self.field.name)) def choices(self): for value, label in self.field.choices: yield EasyChoice(self.model, self, value, label) def url(self): if self.field.choices: return mark_safe('%s%s/%s/%s/' % (self.model.site.root_url, self.model.model._meta.app_label, self.model.model._meta.module_name, self.field.name)) elif self.field.rel: return mark_safe('%s%s/%s/' % (self.model.site.root_url, self.model.model._meta.app_label, self.model.model._meta.module_name)) class EasyChoice(object): def __init__(self, easy_model, field, value, label): self.model, self.field = easy_model, field self.value, self.label = value, label def __repr__(self): return smart_str(u'<EasyChoice for %s.%s>' % (self.model.model._meta.object_name, self.field.name)) def url(self): return mark_safe('%s%s/%s/%s/%s/' % (self.model.site.root_url, self.model.model._meta.app_label, self.model.model._meta.module_name, self.field.field.name, iri_to_uri(self.value))) class EasyInstance(object): def __init__(self, easy_model, instance): self.model, self.instance = easy_model, instance def __repr__(self): return smart_str(u'<EasyInstance for %s (%s)>' % (self.model.model._meta.object_name, self.instance._get_pk_val())) def __unicode__(self): val = smart_unicode(self.instance) if len(val) > DISPLAY_SIZE: return val[:DISPLAY_SIZE] + u'...' return val def __str__(self): return self.__unicode__().encode('utf-8') def pk(self): return self.instance._get_pk_val() def url(self): return mark_safe('%s%s/%s/objects/%s/' % (self.model.site.root_url, self.model.model._meta.app_label, self.model.model._meta.module_name, iri_to_uri(self.pk()))) def fields(self): """ Generator that yields EasyInstanceFields for each field in this EasyInstance's model. """ for f in self.model.model._meta.fields + self.model.model._meta.many_to_many: yield EasyInstanceField(self.model, self, f) def related_objects(self): """ Generator that yields dictionaries of all models that have this EasyInstance's model as a ForeignKey or ManyToManyField, along with lists of related objects. """ for rel_object in self.model.model._meta.get_all_related_objects() + self.model.model._meta.get_all_related_many_to_many_objects(): if rel_object.model not in self.model.model_list: continue # Skip models that aren't in the model_list em = EasyModel(self.model.site, rel_object.model) yield { 'model': em, 'related_field': rel_object.field.verbose_name, 'object_list': [EasyInstance(em, i) for i in getattr(self.instance, rel_object.get_accessor_name()).all()], } class EasyInstanceField(object): def __init__(self, easy_model, instance, field): self.model, self.field, self.instance = easy_model, field, instance self.raw_value = getattr(instance.instance, field.name) def __repr__(self): return smart_str(u'<EasyInstanceField for %s.%s>' % (self.model.model._meta.object_name, self.field.name)) def values(self): """ Returns a list of values for this field for this instance. It's a list so we can accomodate many-to-many fields. """ # This import is deliberately inside the function because it causes # some settings to be imported, and we don't want to do that at the # module level. if self.field.rel: if isinstance(self.field.rel, models.ManyToOneRel): objs = getattr(self.instance.instance, self.field.name) elif isinstance(self.field.rel, models.ManyToManyRel): # ManyToManyRel return list(getattr(self.instance.instance, self.field.name).all()) elif self.field.choices: objs = dict(self.field.choices).get(self.raw_value, EMPTY_VALUE) elif isinstance(self.field, models.DateField) or isinstance(self.field, models.TimeField): if self.raw_value: if isinstance(self.field, models.DateTimeField): objs = capfirst(formats.date_format(self.raw_value, 'DATETIME_FORMAT')) elif isinstance(self.field, models.TimeField): objs = capfirst(formats.time_format(self.raw_value, 'TIME_FORMAT')) else: objs = capfirst(formats.date_format(self.raw_value, 'DATE_FORMAT')) else: objs = EMPTY_VALUE elif isinstance(self.field, models.BooleanField) or isinstance(self.field, models.NullBooleanField): objs = {True: 'Yes', False: 'No', None: 'Unknown'}[self.raw_value] else: objs = self.raw_value return [objs] def urls(self): "Returns a list of (value, URL) tuples." # First, check the urls() method for each plugin. plugin_urls = [] for plugin_name, plugin in self.model.model_databrowse().plugins.items(): urls = plugin.urls(plugin_name, self) if urls is not None: #plugin_urls.append(urls) values = self.values() return zip(self.values(), urls) if self.field.rel: m = EasyModel(self.model.site, self.field.rel.to) if self.field.rel.to in self.model.model_list: lst = [] for value in self.values(): if value is None: continue url = mark_safe('%s%s/%s/objects/%s/' % (self.model.site.root_url, m.model._meta.app_label, m.model._meta.module_name, iri_to_uri(value._get_pk_val()))) lst.append((smart_unicode(value), url)) else: lst = [(value, None) for value in self.values()] elif self.field.choices: lst = [] for value in self.values(): url = mark_safe('%s%s/%s/fields/%s/%s/' % (self.model.site.root_url, self.model.model._meta.app_label, self.model.model._meta.module_name, self.field.name, iri_to_uri(self.raw_value))) lst.append((value, url)) elif isinstance(self.field, models.URLField): val = self.values()[0] lst = [(val, iri_to_uri(val))] else: lst = [(self.values()[0], None)] return lst class EasyQuerySet(QuerySet): """ When creating (or cloning to) an `EasyQuerySet`, make sure to set the `_easymodel` variable to the related `EasyModel`. """ def iterator(self, *args, **kwargs): for obj in super(EasyQuerySet, self).iterator(*args, **kwargs): yield EasyInstance(self._easymodel, obj) def _clone(self, *args, **kwargs): c = super(EasyQuerySet, self)._clone(*args, **kwargs) c._easymodel = self._easymodel return c
# Copyright 2014 Google Inc. All rights reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. from . import encode from . import number_types as N class Table(object): """Table wraps a byte slice and provides read access to its data. The variable `Pos` indicates the root of the FlatBuffers object therein.""" __slots__ = ("Bytes", "Pos") def __init__(self, buf, pos): N.enforce_number(pos, N.UOffsetTFlags) self.Bytes = buf self.Pos = pos def Offset(self, vtableOffset): """Offset provides access into the Table's vtable. Deprecated fields are ignored by checking the vtable's length.""" vtable = self.Pos - self.Get(N.SOffsetTFlags, self.Pos) vtableEnd = self.Get(N.VOffsetTFlags, vtable) if vtableOffset < vtableEnd: return self.Get(N.VOffsetTFlags, vtable + vtableOffset) return 0 def Indirect(self, off): """Indirect retrieves the relative offset stored at `offset`.""" N.enforce_number(off, N.UOffsetTFlags) return off + encode.Get(N.UOffsetTFlags.packer_type, self.Bytes, off) def String(self, off): """String gets a string from data stored inside the flatbuffer.""" N.enforce_number(off, N.UOffsetTFlags) off += encode.Get(N.UOffsetTFlags.packer_type, self.Bytes, off) start = off + N.UOffsetTFlags.bytewidth length = encode.Get(N.UOffsetTFlags.packer_type, self.Bytes, off) return bytes(self.Bytes[start:start+length]) def VectorLen(self, off): """VectorLen retrieves the length of the vector whose offset is stored at "off" in this object.""" N.enforce_number(off, N.UOffsetTFlags) off += self.Pos off += encode.Get(N.UOffsetTFlags.packer_type, self.Bytes, off) ret = encode.Get(N.UOffsetTFlags.packer_type, self.Bytes, off) return ret def Vector(self, off): """Vector retrieves the start of data of the vector whose offset is stored at "off" in this object.""" N.enforce_number(off, N.UOffsetTFlags) off += self.Pos x = off + self.Get(N.UOffsetTFlags, off) # data starts after metadata containing the vector length x += N.UOffsetTFlags.bytewidth return x def Union(self, t2, off): """Union initializes any Table-derived type to point to the union at the given offset.""" assert type(t2) is Table N.enforce_number(off, N.UOffsetTFlags) off += self.Pos t2.Pos = off + self.Get(N.UOffsetTFlags, off) t2.Bytes = self.Bytes def Get(self, flags, off): """ Get retrieves a value of the type specified by `flags` at the given offset. """ N.enforce_number(off, N.UOffsetTFlags) return flags.py_type(encode.Get(flags.packer_type, self.Bytes, off)) def GetSlot(self, slot, d, validator_flags): N.enforce_number(slot, N.VOffsetTFlags) if validator_flags is not None: N.enforce_number(d, validator_flags) off = self.Offset(slot) if off == 0: return d return self.Get(validator_flags, self.Pos + off) def GetVOffsetTSlot(self, slot, d): """ GetVOffsetTSlot retrieves the VOffsetT that the given vtable location points to. If the vtable value is zero, the default value `d` will be returned. """ N.enforce_number(slot, N.VOffsetTFlags) N.enforce_number(d, N.VOffsetTFlags) off = self.Offset(slot) if off == 0: return d return off
# (c) 2016 Red Hat Inc. # # This file is part of Ansible # # Ansible is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Ansible is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Ansible. If not, see <http://www.gnu.org/licenses/>. # Make coding more python3-ish from __future__ import (absolute_import, division, print_function) __metaclass__ = type import json from ansible.compat.tests.mock import patch from ansible.modules.network.eos import eos_config from .eos_module import TestEosModule, load_fixture, set_module_args class TestEosConfigModule(TestEosModule): module = eos_config def setUp(self): self.mock_get_config = patch('ansible.modules.network.eos.eos_config.get_config') self.get_config = self.mock_get_config.start() self.mock_load_config = patch('ansible.modules.network.eos.eos_config.load_config') self.load_config = self.mock_load_config.start() def tearDown(self): self.mock_get_config.stop() self.mock_load_config.stop() def load_fixtures(self, commands=None, transport='cli'): self.get_config.return_value = load_fixture('eos_config_config.cfg') self.load_config.return_value = dict(diff=None, session='session') def test_eos_config_no_change(self): args = dict(lines=['hostname localhost']) set_module_args(args) result = self.execute_module() def test_eos_config_src(self): args = dict(src=load_fixture('eos_config_candidate.cfg')) set_module_args(args) result = self.execute_module(changed=True) config = ['hostname switch01', 'interface Ethernet1', 'description test interface', 'no shutdown', 'ip routing'] self.assertEqual(sorted(config), sorted(result['commands']), result['commands']) def test_eos_config_lines(self): args = dict(lines=['hostname switch01', 'ip domain-name eng.ansible.com']) set_module_args(args) result = self.execute_module(changed=True) config = ['hostname switch01'] self.assertEqual(sorted(config), sorted(result['commands']), result['commands']) def test_eos_config_before(self): args = dict(lines=['hostname switch01', 'ip domain-name eng.ansible.com'], before=['before command']) set_module_args(args) result = self.execute_module(changed=True) config = ['before command', 'hostname switch01'] self.assertEqual(sorted(config), sorted(result['commands']), result['commands']) self.assertEqual('before command', result['commands'][0]) def test_eos_config_after(self): args = dict(lines=['hostname switch01', 'ip domain-name eng.ansible.com'], after=['after command']) set_module_args(args) result = self.execute_module(changed=True) config = ['after command', 'hostname switch01'] self.assertEqual(sorted(config), sorted(result['commands']), result['commands']) self.assertEqual('after command', result['commands'][-1]) def test_eos_config_parents(self): args = dict(lines=['ip address 1.2.3.4/5', 'no shutdown'], parents=['interface Ethernet10']) set_module_args(args) result = self.execute_module(changed=True) config = ['interface Ethernet10', 'ip address 1.2.3.4/5', 'no shutdown'] self.assertEqual(config, result['commands'], result['commands']) def test_eos_config_src_and_lines_fails(self): args = dict(src='foo', lines='foo') set_module_args(args) result = self.execute_module(failed=True) def test_eos_config_match_exact_requires_lines(self): args = dict(match='exact') set_module_args(args) result = self.execute_module(failed=True) def test_eos_config_match_strict_requires_lines(self): args = dict(match='strict') set_module_args(args) result = self.execute_module(failed=True) def test_eos_config_replace_block_requires_lines(self): args = dict(replace='block') set_module_args(args) result = self.execute_module(failed=True) def test_eos_config_replace_config_requires_src(self): args = dict(replace='config') set_module_args(args) result = self.execute_module(failed=True) def test_eos_config_backup_returns__backup__(self): args = dict(backup=True) set_module_args(args) result = self.execute_module() self.assertIn('__backup__', result)
from django.core.management.base import copy_helper, CommandError, LabelCommand from django.utils.importlib import import_module import os import re from random import choice class Command(LabelCommand): help = "Creates a Django project directory structure for the given project name in the current directory." args = "[projectname]" label = 'project name' requires_model_validation = False # Can't import settings during this command, because they haven't # necessarily been created. can_import_settings = False def handle_label(self, project_name, **options): # Determine the project_name a bit naively -- by looking at the name of # the parent directory. directory = os.getcwd() # Check that the project_name cannot be imported. try: import_module(project_name) except ImportError: pass else: raise CommandError("%r conflicts with the name of an existing Python module and cannot be used as a project name. Please try another name." % project_name) copy_helper(self.style, 'project', project_name, directory) # Create a random SECRET_KEY hash, and put it in the main settings. main_settings_file = os.path.join(directory, project_name, 'settings.py') settings_contents = open(main_settings_file, 'r').read() fp = open(main_settings_file, 'w') secret_key = ''.join([choice('abcdefghijklmnopqrstuvwxyz0123456789!@#$%^&*(-_=+)') for i in range(50)]) settings_contents = re.sub(r"(?<=SECRET_KEY = ')'", secret_key + "'", settings_contents) fp.write(settings_contents) fp.close()
# -*- coding: utf-8 -*- # # This file is part of Invenio. # Copyright (C) 2013, 2015, 2016 CERN. # # Invenio is free software; you can redistribute it and/or # modify it under the terms of the GNU General Public License as # published by the Free Software Foundation; either version 2 of the # License, or (at your option) any later version. # # Invenio is distributed in the hope that it will be useful, but # WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU # General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Invenio; if not, write to the Free Software Foundation, Inc., # 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA. # # In applying this license, CERN does not # waive the privileges and immunities granted to it by virtue of its status # as an Intergovernmental Organization or submit itself to any jurisdiction. r"""Invenio module for OAI-PMH metadata harvesting between repositories. Harvesting is simple ==================== .. code-block:: shell youroverlay oaiharvester harvest -u http://export.arxiv.org/oai2 \ -i oai:arXiv.org:1507.07286 > my_record.xml This will harvest the repository for a specific record and print the records to stdout - which in this case will save it to a file called ``my_record.xml``. If you want to have your harvested records saved in a directory automatically, its easy: .. code-block:: shell youroverlay oaiharvester harvest -u http://export.arxiv.org/oai2 \ -i oai:arXiv.org:1507.07286 -d /tmp Note the directory ``-d`` parameter that specifies a directory to save harvested XML files. Integration with your application ================================= If you want to integrate ``invenio-oaiharvester`` into your application, you could hook into the signals sent by the harvester after a completed harvest. See ``invenio_oaiharvester.signals:oaiharvest_finished``. Check also the defined Celery tasks under ``invenio_oaiharvester.tasks``. Managing OAI-PMH sources ======================== If you want to store configuration for an OAI repository, you can use the SQLAlchemy model ``invenio_oaiharvester.models:OAIHarvestConfig``. This is useful if you regularly need to query a server. Here you can add information about the server URL, metadataPrefix to use etc. This information is also available when scheduling and running tasks: .. code-block:: shell youroverlay oaiharvester get -n somerepo -i oai:example.org:1234 Here we are using the `-n, --name` parameter to specify which configured OAI-PMH source to query, using the ``name`` property. """ from __future__ import absolute_import, print_function from .api import get_records, list_records from .ext import InvenioOAIHarvester from .version import __version__ __all__ = ('__version__', 'InvenioOAIHarvester', 'get_records', 'list_records')
#!/usr/bin/env python """ Install wagtailsettings using setuptools """ from wagtailsettings import __version__ with open('README.rst', 'r') as f: readme = f.read() try: from setuptools import setup, find_packages except ImportError: from ez_setup import use_setuptools use_setuptools() from setuptools import setup, find_packages setup( name='wagtailsettings', version=__version__, description='Admin-editable settings for Wagtail projects', long_description=readme, author='Tim Heap', author_email='tim@takeflight.com.au', url='https://bitbucket.org/takeflight/wagtailsettings', install_requires=[], extras_require={ 'full': ['wagtail>=0.6'], }, zip_safe=False, license='BSD License', packages=find_packages(), include_package_data=True, package_data={ }, classifiers=[ 'Environment :: Web Environment', 'Intended Audience :: Developers', 'Operating System :: OS Independent', 'Programming Language :: Python', 'Framework :: Django', 'License :: OSI Approved :: BSD License', ], )
from setuptools import setup, find_packages setup( name='stevedore-examples', version='1.0', description='Demonstration package for stevedore', author='Doug Hellmann', author_email='doug@doughellmann.com', url='http://git.openstack.org/cgit/openstack/stevedore', classifiers=['Development Status :: 3 - Alpha', 'License :: OSI Approved :: Apache Software License', 'Programming Language :: Python', 'Programming Language :: Python :: 2', 'Programming Language :: Python :: 2.7', 'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.4', 'Intended Audience :: Developers', 'Environment :: Console', ], platforms=['Any'], scripts=[], provides=['stevedore.examples', ], packages=find_packages(), include_package_data=True, entry_points={ 'stevedore.example.formatter': [ 'simple = stevedore.example.simple:Simple', 'plain = stevedore.example.simple:Simple', ], }, zip_safe=False, )
# -*- coding: utf-8 -*- # Copyright (c) 2012 Thomas Parslow http://almostobsolete.net/ # Copyright (c) 2012 Robie Basak <robie@justgohome.co.uk> # Tree hash implementation from Aaron Brady bradya@gmail.com # # Permission is hereby granted, free of charge, to any person obtaining a # copy of this software and associated documentation files (the # "Software"), to deal in the Software without restriction, including # without limitation the rights to use, copy, modify, merge, publish, dis- # tribute, sublicense, and/or sell copies of the Software, and to permit # persons to whom the Software is furnished to do so, subject to the fol- # lowing conditions: # # The above copyright notice and this permission notice shall be included # in all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS # OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL- # ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT # SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, # WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS # IN THE SOFTWARE. # import hashlib from boto.glacier.utils import chunk_hashes, tree_hash, bytes_to_hex # This import is provided for backwards compatibility. This function is # now in boto.glacier.utils, but any existing code can still import # this directly from this module. from boto.glacier.utils import compute_hashes_from_fileobj _ONE_MEGABYTE = 1024 * 1024 class _Partitioner(object): """Convert variable-size writes into part-sized writes Call write(data) with variable sized data as needed to write all data. Call flush() after all data is written. This instance will call send_fn(part_data) as needed in part_size pieces, except for the final part which may be shorter than part_size. Make sure to call flush() to ensure that a short final part results in a final send_fn call. """ def __init__(self, part_size, send_fn): self.part_size = part_size self.send_fn = send_fn self._buffer = [] self._buffer_size = 0 def write(self, data): if data == b'': return self._buffer.append(data) self._buffer_size += len(data) while self._buffer_size > self.part_size: self._send_part() def _send_part(self): data = b''.join(self._buffer) # Put back any data remaining over the part size into the # buffer if len(data) > self.part_size: self._buffer = [data[self.part_size:]] self._buffer_size = len(self._buffer[0]) else: self._buffer = [] self._buffer_size = 0 # The part we will send part = data[:self.part_size] self.send_fn(part) def flush(self): if self._buffer_size > 0: self._send_part() class _Uploader(object): """Upload to a Glacier upload_id. Call upload_part for each part (in any order) and then close to complete the upload. """ def __init__(self, vault, upload_id, part_size, chunk_size=_ONE_MEGABYTE): self.vault = vault self.upload_id = upload_id self.part_size = part_size self.chunk_size = chunk_size self.archive_id = None self._uploaded_size = 0 self._tree_hashes = [] self.closed = False def _insert_tree_hash(self, index, raw_tree_hash): list_length = len(self._tree_hashes) if index >= list_length: self._tree_hashes.extend([None] * (list_length - index + 1)) self._tree_hashes[index] = raw_tree_hash def upload_part(self, part_index, part_data): """Upload a part to Glacier. :param part_index: part number where 0 is the first part :param part_data: data to upload corresponding to this part """ if self.closed: raise ValueError("I/O operation on closed file") # Create a request and sign it part_tree_hash = tree_hash(chunk_hashes(part_data, self.chunk_size)) self._insert_tree_hash(part_index, part_tree_hash) hex_tree_hash = bytes_to_hex(part_tree_hash) linear_hash = hashlib.sha256(part_data).hexdigest() start = self.part_size * part_index content_range = (start, (start + len(part_data)) - 1) response = self.vault.layer1.upload_part(self.vault.name, self.upload_id, linear_hash, hex_tree_hash, content_range, part_data) response.read() self._uploaded_size += len(part_data) def skip_part(self, part_index, part_tree_hash, part_length): """Skip uploading of a part. The final close call needs to calculate the tree hash and total size of all uploaded data, so this is the mechanism for resume functionality to provide it without actually uploading the data again. :param part_index: part number where 0 is the first part :param part_tree_hash: binary tree_hash of part being skipped :param part_length: length of part being skipped """ if self.closed: raise ValueError("I/O operation on closed file") self._insert_tree_hash(part_index, part_tree_hash) self._uploaded_size += part_length def close(self): if self.closed: return if None in self._tree_hashes: raise RuntimeError("Some parts were not uploaded.") # Complete the multiplart glacier upload hex_tree_hash = bytes_to_hex(tree_hash(self._tree_hashes)) response = self.vault.layer1.complete_multipart_upload( self.vault.name, self.upload_id, hex_tree_hash, self._uploaded_size) self.archive_id = response['ArchiveId'] self.closed = True def generate_parts_from_fobj(fobj, part_size): data = fobj.read(part_size) while data: yield data.encode('utf-8') data = fobj.read(part_size) def resume_file_upload(vault, upload_id, part_size, fobj, part_hash_map, chunk_size=_ONE_MEGABYTE): """Resume upload of a file already part-uploaded to Glacier. The resumption of an upload where the part-uploaded section is empty is a valid degenerate case that this function can handle. In this case, part_hash_map should be an empty dict. :param vault: boto.glacier.vault.Vault object. :param upload_id: existing Glacier upload id of upload being resumed. :param part_size: part size of existing upload. :param fobj: file object containing local data to resume. This must read from the start of the entire upload, not just from the point being resumed. Use fobj.seek(0) to achieve this if necessary. :param part_hash_map: {part_index: part_tree_hash, ...} of data already uploaded. Each supplied part_tree_hash will be verified and the part re-uploaded if there is a mismatch. :param chunk_size: chunk size of tree hash calculation. This must be 1 MiB for Amazon. """ uploader = _Uploader(vault, upload_id, part_size, chunk_size) for part_index, part_data in enumerate( generate_parts_from_fobj(fobj, part_size)): part_tree_hash = tree_hash(chunk_hashes(part_data, chunk_size)) if (part_index not in part_hash_map or part_hash_map[part_index] != part_tree_hash): uploader.upload_part(part_index, part_data) else: uploader.skip_part(part_index, part_tree_hash, len(part_data)) uploader.close() return uploader.archive_id class Writer(object): """ Presents a file-like object for writing to a Amazon Glacier Archive. The data is written using the multi-part upload API. """ def __init__(self, vault, upload_id, part_size, chunk_size=_ONE_MEGABYTE): self.uploader = _Uploader(vault, upload_id, part_size, chunk_size) self.partitioner = _Partitioner(part_size, self._upload_part) self.closed = False self.next_part_index = 0 def write(self, data): if self.closed: raise ValueError("I/O operation on closed file") self.partitioner.write(data) def _upload_part(self, part_data): self.uploader.upload_part(self.next_part_index, part_data) self.next_part_index += 1 def close(self): if self.closed: return self.partitioner.flush() self.uploader.close() self.closed = True def get_archive_id(self): self.close() return self.uploader.archive_id @property def current_tree_hash(self): """ Returns the current tree hash for the data that's been written **so far**. Only once the writing is complete is the final tree hash returned. """ return tree_hash(self.uploader._tree_hashes) @property def current_uploaded_size(self): """ Returns the current uploaded size for the data that's been written **so far**. Only once the writing is complete is the final uploaded size returned. """ return self.uploader._uploaded_size @property def upload_id(self): return self.uploader.upload_id @property def vault(self): return self.uploader.vault
# coding=utf-8 """ Collect stats from puppet agent's last_run_summary.yaml #### Dependencies * yaml """ try: import yaml except ImportError: yaml = None import diamond.collector class PuppetAgentCollector(diamond.collector.Collector): def get_default_config_help(self): config_help = super(PuppetAgentCollector, self).get_default_config_help() config_help.update({ 'yaml_path': "Path to last_run_summary.yaml", }) return config_help def get_default_config(self): """ Returns the default collector settings """ config = super(PuppetAgentCollector, self).get_default_config() config.update({ 'yaml_path': '/var/lib/puppet/state/last_run_summary.yaml', 'path': 'puppetagent', }) return config def _get_summary(self): summary_fp = open(self.config['yaml_path'], 'r') try: summary = yaml.load(summary_fp) finally: summary_fp.close() return summary def collect(self): if yaml is None: self.log.error('Unable to import yaml') return summary = self._get_summary() for sect, data in summary.iteritems(): for stat, value in data.iteritems(): if value is None or isinstance(value, basestring): continue metric = '.'.join([sect, stat]) self.publish(metric, value)
""" mbed SDK Copyright (c) 2011-2013 ARM Limited Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. """ from os.path import join from jinja2 import Template from tools.paths import TOOLS_DATA, MBED_RPC RPC_TEMPLATES_PATH = join(TOOLS_DATA, "rpc") RPC_TEMPLATE = "RPCClasses.h" CLASS_TEMPLATE = "class.cpp" RPC_CLASSES_PATH = join(MBED_RPC, RPC_TEMPLATE) def get_template(name): return Template(open(join(RPC_TEMPLATES_PATH, name)).read()) def write_rpc_classes(classes): template = get_template(RPC_TEMPLATE) open(RPC_CLASSES_PATH, "w").write(template.render({"classes":classes})) RPC_CLASSES = ( { "name": "DigitalOut", "cons_args": ["PinName"], "methods": [ (None , "write", ["int"]), ("int", "read" , []), ] }, { "name": "DigitalIn", "cons_args": ["PinName"], "methods": [ ("int", "read" , []), ] }, { "name": "DigitalInOut", "cons_args": ["PinName"], "methods": [ ("int", "read" , []), (None , "write" , ["int"]), (None , "input" , []), (None , "output", []), ] }, { "name": "AnalogIn", "required": "ANALOGIN", "cons_args": ["PinName"], "methods": [ ("float" , "read" , []), ("unsigned short", "read_u16", []), ] }, { "name": "AnalogOut", "required": "ANALOGOUT", "cons_args": ["PinName"], "methods": [ ("float", "read" , []), (None , "write" , ["float"]), (None , "write_u16", ["unsigned short"]), ] }, { "name": "PwmOut", "required": "PWMOUT", "cons_args": ["PinName"], "methods": [ ("float", "read" , []), (None , "write" , ["float"]), (None , "period" , ["float"]), (None , "period_ms" , ["int"]), (None , "pulsewidth" , ["float"]), (None , "pulsewidth_ms", ["int"]), ] }, { "name": "SPI", "required": "SPI", "cons_args": ["PinName", "PinName", "PinName"], "methods": [ (None , "format" , ["int", "int"]), (None , "frequency", ["int"]), ("int", "write" , ["int"]), ] }, { "name": "Serial", "required": "SERIAL", "cons_args": ["PinName", "PinName"], "methods": [ (None , "baud" , ["int"]), ("int", "readable" , []), ("int", "writeable", []), ("int", "putc" , ["int"]), ("int", "getc" , []), ("int", "puts" , ["const char *"]), ] }, { "name": "Timer", "cons_args": [], "methods": [ (None , "start" , []), (None , "stop" , []), (None , "reset" , []), ("float", "read" , []), ("int" , "read_ms", []), ("int" , "read_us", []), ] } ) def get_args_proto(args_types, extra=None): args = ["%s a%d" % (s, n) for n, s in enumerate(args_types)] if extra: args.extend(extra) return ', '.join(args) def get_args_call(args): return ', '.join(["a%d" % (n) for n in range(len(args))]) classes = [] class_template = get_template(CLASS_TEMPLATE) for c in RPC_CLASSES: c_args = c['cons_args'] data = { 'name': c['name'], 'cons_type': ', '.join(c_args + ['const char*']), "cons_proto": get_args_proto(c_args, ["const char *name=NULL"]), "cons_call": get_args_call(c_args) } c_name = "Rpc" + c['name'] methods = [] rpc_methods = [] for r, m, a in c['methods']: ret_proto = r if r else "void" args_proto = "void" ret_defin = "return " if r else "" args_defin = "" if a: args_proto = get_args_proto(a) args_defin = get_args_call(a) proto = "%s %s(%s)" % (ret_proto, m, args_proto) defin = "{%so.%s(%s);}" % (ret_defin, m, args_defin) methods.append("%s %s" % (proto, defin)) rpc_method_type = [r] if r else [] rpc_method_type.append(c_name) rpc_method_type.extend(a) rpc_methods.append('{"%s", rpc_method_caller<%s, &%s::%s>}' % (m, ', '.join(rpc_method_type), c_name, m)) data['methods'] = "\n ".join(methods) data['rpc_methods'] = ",\n ".join(rpc_methods) class_decl = class_template.render(data) if 'required' in c: class_decl = "#if DEVICE_%s\n%s\n#endif" % (c['required'], class_decl) classes.append(class_decl) write_rpc_classes('\n\n'.join(classes))
# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com> # # This file is part of Ansible # # Ansible is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Ansible is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Ansible. If not, see <http://www.gnu.org/licenses/>. # Make coding more python3-ish from __future__ import (absolute_import, division, print_function) __metaclass__ = type from ansible.compat.tests import unittest from ansible.errors import AnsibleParserError from ansible.parsing.yaml.objects import AnsibleMapping from ansible.playbook.task_include import TaskInclude from test.mock.loader import DictDataLoader class TestTaskInclude(unittest.TestCase): def setUp(self): self._fake_loader = DictDataLoader({ "foo.yml": """ - shell: echo "hello world" """ }) pass def tearDown(self): pass def test_empty_task_include(self): ti = TaskInclude() def test_basic_task_include(self): ti = TaskInclude.load(AnsibleMapping(include='foo.yml'), loader=self._fake_loader) tasks = ti.compile() def test_task_include_with_loop(self): ti = TaskInclude.load(AnsibleMapping(include='foo.yml', with_items=['a', 'b', 'c']), loader=self._fake_loader) def test_task_include_with_conditional(self): ti = TaskInclude.load(AnsibleMapping(include='foo.yml', when="1 == 1"), loader=self._fake_loader) def test_task_include_with_tags(self): ti = TaskInclude.load(AnsibleMapping(include='foo.yml', tags="foo"), loader=self._fake_loader) ti = TaskInclude.load(AnsibleMapping(include='foo.yml', tags=["foo", "bar"]), loader=self._fake_loader) def test_task_include_errors(self): self.assertRaises(AnsibleParserError, TaskInclude.load, AnsibleMapping(include=''), loader=self._fake_loader) self.assertRaises(AnsibleParserError, TaskInclude.load, AnsibleMapping(include='foo.yml', vars="1"), loader=self._fake_loader) self.assertRaises(AnsibleParserError, TaskInclude.load, AnsibleMapping(include='foo.yml a=1', vars=dict(b=2)), loader=self._fake_loader)
from django import template from django.template.loader import render_to_string from django.conf import settings from django.contrib.contenttypes.models import ContentType from django.contrib import comments from django.utils import six from django.utils.deprecation import RenameMethodsBase, RemovedInDjango18Warning from django.utils.encoding import smart_text register = template.Library() class RenameBaseCommentNodeMethods(RenameMethodsBase): renamed_methods = ( ('get_query_set', 'get_queryset', RemovedInDjango18Warning), ) class BaseCommentNode(six.with_metaclass(RenameBaseCommentNodeMethods, template.Node)): """ Base helper class (abstract) for handling the get_comment_* template tags. Looks a bit strange, but the subclasses below should make this a bit more obvious. """ @classmethod def handle_token(cls, parser, token): """Class method to parse get_comment_list/count/form and return a Node.""" tokens = token.split_contents() if tokens[1] != 'for': raise template.TemplateSyntaxError("Second argument in %r tag must be 'for'" % tokens[0]) # {% get_whatever for obj as varname %} if len(tokens) == 5: if tokens[3] != 'as': raise template.TemplateSyntaxError("Third argument in %r must be 'as'" % tokens[0]) return cls( object_expr=parser.compile_filter(tokens[2]), as_varname=tokens[4], ) # {% get_whatever for app.model pk as varname %} elif len(tokens) == 6: if tokens[4] != 'as': raise template.TemplateSyntaxError("Fourth argument in %r must be 'as'" % tokens[0]) return cls( ctype=BaseCommentNode.lookup_content_type(tokens[2], tokens[0]), object_pk_expr=parser.compile_filter(tokens[3]), as_varname=tokens[5] ) else: raise template.TemplateSyntaxError("%r tag requires 4 or 5 arguments" % tokens[0]) @staticmethod def lookup_content_type(token, tagname): try: app, model = token.split('.') return ContentType.objects.get_by_natural_key(app, model) except ValueError: raise template.TemplateSyntaxError("Third argument in %r must be in the format 'app.model'" % tagname) except ContentType.DoesNotExist: raise template.TemplateSyntaxError("%r tag has non-existant content-type: '%s.%s'" % (tagname, app, model)) def __init__(self, ctype=None, object_pk_expr=None, object_expr=None, as_varname=None, comment=None): if ctype is None and object_expr is None: raise template.TemplateSyntaxError("Comment nodes must be given either a literal object or a ctype and object pk.") self.comment_model = comments.get_model() self.as_varname = as_varname self.ctype = ctype self.object_pk_expr = object_pk_expr self.object_expr = object_expr self.comment = comment def render(self, context): qs = self.get_queryset(context) context[self.as_varname] = self.get_context_value_from_queryset(context, qs) return '' def get_queryset(self, context): ctype, object_pk = self.get_target_ctype_pk(context) if not object_pk: return self.comment_model.objects.none() qs = self.comment_model.objects.filter( content_type=ctype, object_pk=smart_text(object_pk), site__pk=settings.SITE_ID, ) # The is_public and is_removed fields are implementation details of the # built-in comment model's spam filtering system, so they might not # be present on a custom comment model subclass. If they exist, we # should filter on them. field_names = [f.name for f in self.comment_model._meta.fields] if 'is_public' in field_names: qs = qs.filter(is_public=True) if getattr(settings, 'COMMENTS_HIDE_REMOVED', True) and 'is_removed' in field_names: qs = qs.filter(is_removed=False) return qs def get_target_ctype_pk(self, context): if self.object_expr: try: obj = self.object_expr.resolve(context) except template.VariableDoesNotExist: return None, None return ContentType.objects.get_for_model(obj), obj.pk else: return self.ctype, self.object_pk_expr.resolve(context, ignore_failures=True) def get_context_value_from_queryset(self, context, qs): """Subclasses should override this.""" raise NotImplementedError('subclasses of BaseCommentNode must provide a get_context_value_from_queryset() method') class CommentListNode(BaseCommentNode): """Insert a list of comments into the context.""" def get_context_value_from_queryset(self, context, qs): return list(qs) class CommentCountNode(BaseCommentNode): """Insert a count of comments into the context.""" def get_context_value_from_queryset(self, context, qs): return qs.count() class CommentFormNode(BaseCommentNode): """Insert a form for the comment model into the context.""" def get_form(self, context): obj = self.get_object(context) if obj: return comments.get_form()(obj) else: return None def get_object(self, context): if self.object_expr: try: return self.object_expr.resolve(context) except template.VariableDoesNotExist: return None else: object_pk = self.object_pk_expr.resolve(context, ignore_failures=True) return self.ctype.get_object_for_this_type(pk=object_pk) def render(self, context): context[self.as_varname] = self.get_form(context) return '' class RenderCommentFormNode(CommentFormNode): """Render the comment form directly""" @classmethod def handle_token(cls, parser, token): """Class method to parse render_comment_form and return a Node.""" tokens = token.split_contents() if tokens[1] != 'for': raise template.TemplateSyntaxError("Second argument in %r tag must be 'for'" % tokens[0]) # {% render_comment_form for obj %} if len(tokens) == 3: return cls(object_expr=parser.compile_filter(tokens[2])) # {% render_comment_form for app.models pk %} elif len(tokens) == 4: return cls( ctype=BaseCommentNode.lookup_content_type(tokens[2], tokens[0]), object_pk_expr=parser.compile_filter(tokens[3]) ) def render(self, context): ctype, object_pk = self.get_target_ctype_pk(context) if object_pk: template_search_list = [ "comments/%s/%s/form.html" % (ctype.app_label, ctype.model), "comments/%s/form.html" % ctype.app_label, "comments/form.html" ] context.push() formstr = render_to_string(template_search_list, {"form" : self.get_form(context)}, context) context.pop() return formstr else: return '' class RenderCommentListNode(CommentListNode): """Render the comment list directly""" @classmethod def handle_token(cls, parser, token): """Class method to parse render_comment_list and return a Node.""" tokens = token.split_contents() if tokens[1] != 'for': raise template.TemplateSyntaxError("Second argument in %r tag must be 'for'" % tokens[0]) # {% render_comment_list for obj %} if len(tokens) == 3: return cls(object_expr=parser.compile_filter(tokens[2])) # {% render_comment_list for app.models pk %} elif len(tokens) == 4: return cls( ctype=BaseCommentNode.lookup_content_type(tokens[2], tokens[0]), object_pk_expr=parser.compile_filter(tokens[3]) ) def render(self, context): ctype, object_pk = self.get_target_ctype_pk(context) if object_pk: template_search_list = [ "comments/%s/%s/list.html" % (ctype.app_label, ctype.model), "comments/%s/list.html" % ctype.app_label, "comments/list.html" ] qs = self.get_queryset(context) context.push() liststr = render_to_string(template_search_list, { "comment_list" : self.get_context_value_from_queryset(context, qs) }, context) context.pop() return liststr else: return '' # We could just register each classmethod directly, but then we'd lose out on # the automagic docstrings-into-admin-docs tricks. So each node gets a cute # wrapper function that just exists to hold the docstring. @register.tag def get_comment_count(parser, token): """ Gets the comment count for the given params and populates the template context with a variable containing that value, whose name is defined by the 'as' clause. Syntax:: {% get_comment_count for [object] as [varname] %} {% get_comment_count for [app].[model] [object_id] as [varname] %} Example usage:: {% get_comment_count for event as comment_count %} {% get_comment_count for calendar.event event.id as comment_count %} {% get_comment_count for calendar.event 17 as comment_count %} """ return CommentCountNode.handle_token(parser, token) @register.tag def get_comment_list(parser, token): """ Gets the list of comments for the given params and populates the template context with a variable containing that value, whose name is defined by the 'as' clause. Syntax:: {% get_comment_list for [object] as [varname] %} {% get_comment_list for [app].[model] [object_id] as [varname] %} Example usage:: {% get_comment_list for event as comment_list %} {% for comment in comment_list %} ... {% endfor %} """ return CommentListNode.handle_token(parser, token) @register.tag def render_comment_list(parser, token): """ Render the comment list (as returned by ``{% get_comment_list %}``) through the ``comments/list.html`` template Syntax:: {% render_comment_list for [object] %} {% render_comment_list for [app].[model] [object_id] %} Example usage:: {% render_comment_list for event %} """ return RenderCommentListNode.handle_token(parser, token) @register.tag def get_comment_form(parser, token): """ Get a (new) form object to post a new comment. Syntax:: {% get_comment_form for [object] as [varname] %} {% get_comment_form for [app].[model] [object_id] as [varname] %} """ return CommentFormNode.handle_token(parser, token) @register.tag def render_comment_form(parser, token): """ Render the comment form (as returned by ``{% render_comment_form %}``) through the ``comments/form.html`` template. Syntax:: {% render_comment_form for [object] %} {% render_comment_form for [app].[model] [object_id] %} """ return RenderCommentFormNode.handle_token(parser, token) @register.simple_tag def comment_form_target(): """ Get the target URL for the comment form. Example:: <form action="{% comment_form_target %}" method="post"> """ return comments.get_form_target() @register.simple_tag def get_comment_permalink(comment, anchor_pattern=None): """ Get the permalink for a comment, optionally specifying the format of the named anchor to be appended to the end of the URL. Example:: {% get_comment_permalink comment "#c%(id)s-by-%(user_name)s" %} """ if anchor_pattern: return comment.get_absolute_url(anchor_pattern) return comment.get_absolute_url()
# (c) 2016 Red Hat Inc. # # This file is part of Ansible # # Ansible is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Ansible is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Ansible. If not, see <http://www.gnu.org/licenses/>. # Make coding more python3-ish from __future__ import (absolute_import, division, print_function) __metaclass__ = type import os import json from units.modules.utils import AnsibleExitJson, AnsibleFailJson, ModuleTestCase from units.modules.utils import set_module_args as _set_module_args def set_module_args(args): if 'provider' not in args: args['provider'] = {'transport': args.get('transport') or 'cli'} return _set_module_args(args) fixture_path = os.path.join(os.path.dirname(__file__), 'fixtures') fixture_data = {} def load_fixture(module_name, name, device=''): path = os.path.join(fixture_path, module_name, device, name) if not os.path.exists(path): path = os.path.join(fixture_path, module_name, name) if path in fixture_data: return fixture_data[path] with open(path) as f: data = f.read() try: data = json.loads(data) except Exception: pass fixture_data[path] = data return data class TestNxosModule(ModuleTestCase): def execute_module_devices(self, failed=False, changed=False, commands=None, sort=True, defaults=False): module_name = self.module.__name__.rsplit('.', 1)[1] local_fixture_path = os.path.join(fixture_path, module_name) models = [] for path in os.listdir(local_fixture_path): path = os.path.join(local_fixture_path, path) if os.path.isdir(path): models.append(os.path.basename(path)) if not models: models = [''] retvals = {} for model in models: retvals[model] = self.execute_module(failed, changed, commands, sort, device=model) return retvals def execute_module(self, failed=False, changed=False, commands=None, sort=True, device=''): self.load_fixtures(commands, device=device) if failed: result = self.failed() self.assertTrue(result['failed'], result) else: result = self.changed(changed) self.assertEqual(result['changed'], changed, result) if commands is not None: if sort: self.assertEqual(sorted(commands), sorted(result['commands']), result['commands']) else: self.assertEqual(commands, result['commands'], result['commands']) return result def failed(self): with self.assertRaises(AnsibleFailJson) as exc: self.module.main() result = exc.exception.args[0] self.assertTrue(result['failed'], result) return result def changed(self, changed=False): with self.assertRaises(AnsibleExitJson) as exc: self.module.main() result = exc.exception.args[0] self.assertEqual(result['changed'], changed, result) return result def load_fixtures(self, commands=None, device=''): pass
import string import gtk import gettext t = gettext.translation('rhn-client-tools', fallback=True) _ = t.ugettext # wrap a long line... def wrap_line(line, max_line_size = 100): if len(line) < max_line_size: return line ret = [] l = "" for w in string.split(line): if not len(l): l = w continue if len(l) > max_line_size: ret.append(l) l = w else: l = "%s %s" % (l, w) if len(l): ret.append(l) return string.join(ret, '\n') # wrap an entire piece of text def wrap_text(txt): return string.join(map(wrap_line, string.split(txt, '\n')), '\n') def addFrame(dialog): contents = dialog.get_children()[0] dialog.remove(contents) frame = gtk.Frame() frame.set_shadow_type(gtk.SHADOW_OUT) frame.add(contents) dialog.add(frame) class MessageWindow: def getrc (self): return self.rc def hide(self): self.dialog.hide() self.dialog.destroy() gtk.main_iteration() def __init__ (self, title, text, type="ok", default=None, parent=None): self.rc = None if type == 'ok': buttons = gtk.BUTTONS_OK style = gtk.MESSAGE_INFO elif type == 'warning': buttons = gtk.BUTTONS_OK style = gtk.MESSAGE_WARNING elif type == 'okcancel': buttons = gtk.BUTTONS_OK_CANCEL style = gtk.MESSAGE_WARNING elif type == 'yesno': buttons = gtk.BUTTONS_YES_NO style = gtk.MESSAGE_QUESTION elif type == "error": buttons = gtk.BUTTONS_OK style = gtk.MESSAGE_ERROR elif type == "question": buttons = gtk.BUTTONS_YES_NO style = gtk.MESSAGE_QUESTION self.dialog = gtk.MessageDialog(parent, 0, style, buttons) # Work around for bug #602609 try: self.dialog.vbox.get_children()[0].get_children()[1].\ get_children()[0].set_line_wrap(False) except: self.dialog.label.set_line_wrap(False) self.dialog.set_markup(text) if default == "no": self.dialog.set_default_response(0) elif default == "yes" or default == "ok": self.dialog.set_default_response(1) else: self.dialog.set_default_response(0) addFrame(self.dialog) self.dialog.set_position (gtk.WIN_POS_CENTER) self.dialog.show_all () rc = self.dialog.run() if rc == gtk.RESPONSE_OK or rc == gtk.RESPONSE_YES: self.rc = 1 elif (rc == gtk.RESPONSE_CANCEL or rc == gtk.RESPONSE_NO or rc == gtk.RESPONSE_CLOSE): self.rc = 0 self.dialog.destroy() class ErrorDialog(MessageWindow): def __init__ (self, text, parent=None): MessageWindow.__init__(self,_("Error:"), text, type="error", parent=parent) class YesNoDialog(MessageWindow): def __init__ (self, text, parent=None): MessageWindow.__init__(self,_("Yes/No dialog:"), text, type="yesno", parent=parent) class BulletedOkDialog: """A dialog box that can have one more sections of text. Each section can be standard blob of text or a bulleted item. """ def __init__ (self, title=None, parent=None): self.rc = None self.dialog = gtk.Dialog(title, parent, 0, ("Close", 1)) self.dialog.set_has_separator(False) # Vbox to contain just the stuff that will be add to the dialog with # addtext self.vbox = gtk.VBox(spacing=15) self.vbox.set_border_width(15) # Put our vbox into the top part of the dialog self.dialog.get_children()[0].pack_start(self.vbox, expand=False) def add_text(self, text): label = gtk.Label(text) label.set_alignment(0, 0) label.set_line_wrap(True) self.vbox.pack_start(label, expand=False) def add_bullet(self, text): label = gtk.Label(text) label.set_alignment(0, 0) label.set_line_wrap(True) hbox = gtk.HBox(spacing=5) bullet = gtk.Label(u'\u2022') bullet.set_alignment(0, 0) hbox.pack_start(bullet, expand=False) hbox.pack_start(label, expand=False) self.vbox.pack_start(hbox, expand=False) def run(self): # addFrame(self.dialog) # Need to do this differently if we want it self.dialog.set_position(gtk.WIN_POS_CENTER) self.dialog.show_all() rc = self.dialog.run() if (rc == gtk.RESPONSE_CANCEL or rc == gtk.RESPONSE_NO or rc == gtk.RESPONSE_CLOSE): self.rc = 0 self.dialog.destroy() gtk.main_iteration() def getrc (self): return self.rc
# stdlib import os from inspect import getsourcefile from os.path import abspath # project from checks import AgentCheck from hashlib import md5 from aerospike.constants import ERROR_CODES from aerospike.constants import HASH_KEY from aerospike import interface from aerospike import log from aerospike import citrusleaf as cl from aerospike import aerospike_dashboards # global variables bcrypt_flag = True try: import bcrypt except ImportError: bcrypt_flag = False class Aerospike(AgentCheck): # function to create pre-defined Aerospike Dashboards. def create_timeboard( self, api_key, api_application_key, instance_name, node_address, ns_list): response = aerospike_dashboards.draw_node_dashboard( api_key, api_application_key, instance_name, node_address) if response is None: self.log.error( 'Unable to Create Node Dashboard due to error' + ' while importing Dogapi and/or Datadog') if ns_list in ERROR_CODES: self.log.error( 'Namespace List is Empty, cannot create namespace Dashboards.') return for ns in ns_list: response = aerospike_dashboards.draw_namespace_dashboard( api_key, api_application_key, instance_name, node_address, ns) if response is None: self.log.error( 'Unable to Create Namespace: ' + str(ns) + ' Dashboard due to error while' + ' importing Dogapi and/or Datadog') def check(self, instance): global bcrypt_flag # get instance variables ip = str(instance['ip']) port = str(instance['port']) user = instance['user'] password = str(instance['password']) cls_mode = instance['cluster_mode'] debug_mode = instance['debug_mode'] instance_name = str(instance['cluster_name']) api_key = str(instance['api_key']) api_application_key = str(instance['api_application_key']) if cls_mode: log.print_log( self, 'Using Aerospike Datadog Coneector in clustered mode...') else: log.print_log( self, 'Using Aerospike Datadog Coneector in non-clustered mode...') # bcrypt check for secured Aerospike if user != 'n/s': if bcrypt_flag: valid_pwd = interface.is_valid_password(password, HASH_KEY) if valid_pwd: password = bcrypt.hashpw(password, HASH_KEY) else: log.print_log(self, 'Problem with bcrypt', error_flag=True) else: log.print_log(self, 'bcrypt not installed', error_flag=True) # Non-clustered mode check if cls_mode is False: cl.set_logger(self) ns_list = interface.get_metrics( self, ip, port, user, password, instance_name) self.create_timeboard( api_key, api_application_key, instance_name, str(ip) + ':' + str(port), ns_list) if __name__ == '__main__': check, instances = Aerospike.from_yaml('/path/to/conf.d/aerospike.yaml') for instance in instances: check.check(instance)
# This file is dual licensed under the terms of the Apache License, Version # 2.0, and the BSD License. See the LICENSE file in the root of this repository # for complete details. from __future__ import absolute_import, division, print_function import string import re from pkg_resources.extern.pyparsing import stringStart, stringEnd, originalTextFor, ParseException from pkg_resources.extern.pyparsing import ZeroOrMore, Word, Optional, Regex, Combine from pkg_resources.extern.pyparsing import Literal as L # noqa from pkg_resources.extern.six.moves.urllib import parse as urlparse from .markers import MARKER_EXPR, Marker from .specifiers import LegacySpecifier, Specifier, SpecifierSet class InvalidRequirement(ValueError): """ An invalid requirement was found, users should refer to PEP 508. """ ALPHANUM = Word(string.ascii_letters + string.digits) LBRACKET = L("[").suppress() RBRACKET = L("]").suppress() LPAREN = L("(").suppress() RPAREN = L(")").suppress() COMMA = L(",").suppress() SEMICOLON = L(";").suppress() AT = L("@").suppress() PUNCTUATION = Word("-_.") IDENTIFIER_END = ALPHANUM | (ZeroOrMore(PUNCTUATION) + ALPHANUM) IDENTIFIER = Combine(ALPHANUM + ZeroOrMore(IDENTIFIER_END)) NAME = IDENTIFIER("name") EXTRA = IDENTIFIER URI = Regex(r'[^ ]+')("url") URL = (AT + URI) EXTRAS_LIST = EXTRA + ZeroOrMore(COMMA + EXTRA) EXTRAS = (LBRACKET + Optional(EXTRAS_LIST) + RBRACKET)("extras") VERSION_PEP440 = Regex(Specifier._regex_str, re.VERBOSE | re.IGNORECASE) VERSION_LEGACY = Regex(LegacySpecifier._regex_str, re.VERBOSE | re.IGNORECASE) VERSION_ONE = VERSION_PEP440 ^ VERSION_LEGACY VERSION_MANY = Combine(VERSION_ONE + ZeroOrMore(COMMA + VERSION_ONE), joinString=",", adjacent=False)("_raw_spec") _VERSION_SPEC = Optional(((LPAREN + VERSION_MANY + RPAREN) | VERSION_MANY)) _VERSION_SPEC.setParseAction(lambda s, l, t: t._raw_spec or '') VERSION_SPEC = originalTextFor(_VERSION_SPEC)("specifier") VERSION_SPEC.setParseAction(lambda s, l, t: t[1]) MARKER_EXPR = originalTextFor(MARKER_EXPR())("marker") MARKER_EXPR.setParseAction( lambda s, l, t: Marker(s[t._original_start:t._original_end]) ) MARKER_SEPERATOR = SEMICOLON MARKER = MARKER_SEPERATOR + MARKER_EXPR VERSION_AND_MARKER = VERSION_SPEC + Optional(MARKER) URL_AND_MARKER = URL + Optional(MARKER) NAMED_REQUIREMENT = \ NAME + Optional(EXTRAS) + (URL_AND_MARKER | VERSION_AND_MARKER) REQUIREMENT = stringStart + NAMED_REQUIREMENT + stringEnd class Requirement(object): """Parse a requirement. Parse a given requirement string into its parts, such as name, specifier, URL, and extras. Raises InvalidRequirement on a badly-formed requirement string. """ # TODO: Can we test whether something is contained within a requirement? # If so how do we do that? Do we need to test against the _name_ of # the thing as well as the version? What about the markers? # TODO: Can we normalize the name and extra name? def __init__(self, requirement_string): try: req = REQUIREMENT.parseString(requirement_string) except ParseException as e: raise InvalidRequirement( "Invalid requirement, parse error at \"{0!r}\"".format( requirement_string[e.loc:e.loc + 8])) self.name = req.name if req.url: parsed_url = urlparse.urlparse(req.url) if not (parsed_url.scheme and parsed_url.netloc) or ( not parsed_url.scheme and not parsed_url.netloc): raise InvalidRequirement("Invalid URL given") self.url = req.url else: self.url = None self.extras = set(req.extras.asList() if req.extras else []) self.specifier = SpecifierSet(req.specifier) self.marker = req.marker if req.marker else None def __str__(self): parts = [self.name] if self.extras: parts.append("[{0}]".format(",".join(sorted(self.extras)))) if self.specifier: parts.append(str(self.specifier)) if self.url: parts.append("@ {0}".format(self.url)) if self.marker: parts.append("; {0}".format(self.marker)) return "".join(parts) def __repr__(self): return "<Requirement({0!r})>".format(str(self))
from collections import defaultdict from sympy import Symbol from sympy.core.compatibility import range from sympy.ntheory import n_order, is_primitive_root, is_quad_residue, \ legendre_symbol, jacobi_symbol, totient, primerange, sqrt_mod, \ primitive_root, quadratic_residues, is_nthpow_residue, nthroot_mod, \ sqrt_mod_iter, mobius from sympy.ntheory.residue_ntheory import _primitive_root_prime_iter from sympy.polys.domains import ZZ from sympy.utilities.pytest import raises def test_residue(): assert n_order(2, 13) == 12 assert [n_order(a, 7) for a in range(1, 7)] == \ [1, 3, 6, 3, 6, 2] assert n_order(5, 17) == 16 assert n_order(17, 11) == n_order(6, 11) assert n_order(101, 119) == 6 assert n_order(11, (10**50 + 151)**2) == 10000000000000000000000000000000000000000000000030100000000000000000000000000000000000000000000022650 raises(ValueError, lambda: n_order(6, 9)) assert is_primitive_root(2, 7) is False assert is_primitive_root(3, 8) is False assert is_primitive_root(11, 14) is False assert is_primitive_root(12, 17) == is_primitive_root(29, 17) raises(ValueError, lambda: is_primitive_root(3, 6)) assert [primitive_root(i) for i in range(2, 31)] == [1, 2, 3, 2, 5, 3, \ None, 2, 3, 2, None, 2, 3, None, None, 3, 5, 2, None, None, 7, 5, \ None, 2, 7, 2, None, 2, None] for p in primerange(3, 100): it = _primitive_root_prime_iter(p) assert len(list(it)) == totient(totient(p)) assert primitive_root(97) == 5 assert primitive_root(97**2) == 5 assert primitive_root(40487) == 5 # note that primitive_root(40487) + 40487 = 40492 is a primitive root # of 40487**2, but it is not the smallest assert primitive_root(40487**2) == 10 assert primitive_root(82) == 7 p = 10**50 + 151 assert primitive_root(p) == 11 assert primitive_root(2*p) == 11 assert primitive_root(p**2) == 11 raises(ValueError, lambda: primitive_root(-3)) assert is_quad_residue(3, 7) is False assert is_quad_residue(10, 13) is True assert is_quad_residue(12364, 139) == is_quad_residue(12364 % 139, 139) assert is_quad_residue(207, 251) is True assert is_quad_residue(0, 1) is True assert is_quad_residue(1, 1) is True assert is_quad_residue(0, 2) == is_quad_residue(1, 2) is True assert is_quad_residue(1, 4) is True assert is_quad_residue(2, 27) is False assert is_quad_residue(13122380800, 13604889600) is True assert [j for j in range(14) if is_quad_residue(j, 14)] == \ [0, 1, 2, 4, 7, 8, 9, 11] raises(ValueError, lambda: is_quad_residue(1.1, 2)) raises(ValueError, lambda: is_quad_residue(2, 0)) assert quadratic_residues(12) == [0, 1, 4, 9] assert quadratic_residues(13) == [0, 1, 3, 4, 9, 10, 12] assert [len(quadratic_residues(i)) for i in range(1, 20)] == \ [1, 2, 2, 2, 3, 4, 4, 3, 4, 6, 6, 4, 7, 8, 6, 4, 9, 8, 10] assert list(sqrt_mod_iter(6, 2)) == [0] assert sqrt_mod(3, 13) == 4 assert sqrt_mod(3, -13) == 4 assert sqrt_mod(6, 23) == 11 assert sqrt_mod(345, 690) == 345 for p in range(3, 100): d = defaultdict(list) for i in range(p): d[pow(i, 2, p)].append(i) for i in range(1, p): it = sqrt_mod_iter(i, p) v = sqrt_mod(i, p, True) if v: v = sorted(v) assert d[i] == v else: assert not d[i] assert sqrt_mod(9, 27, True) == [3, 6, 12, 15, 21, 24] assert sqrt_mod(9, 81, True) == [3, 24, 30, 51, 57, 78] assert sqrt_mod(9, 3**5, True) == [3, 78, 84, 159, 165, 240] assert sqrt_mod(81, 3**4, True) == [0, 9, 18, 27, 36, 45, 54, 63, 72] assert sqrt_mod(81, 3**5, True) == [9, 18, 36, 45, 63, 72, 90, 99, 117,\ 126, 144, 153, 171, 180, 198, 207, 225, 234] assert sqrt_mod(81, 3**6, True) == [9, 72, 90, 153, 171, 234, 252, 315,\ 333, 396, 414, 477, 495, 558, 576, 639, 657, 720] assert sqrt_mod(81, 3**7, True) == [9, 234, 252, 477, 495, 720, 738, 963,\ 981, 1206, 1224, 1449, 1467, 1692, 1710, 1935, 1953, 2178] for a, p in [(26214400, 32768000000), (26214400, 16384000000), (262144, 1048576), (87169610025, 163443018796875), (22315420166400, 167365651248000000)]: assert pow(sqrt_mod(a, p), 2, p) == a n = 70 a, p = 5**2*3**n*2**n, 5**6*3**(n+1)*2**(n+2) it = sqrt_mod_iter(a, p) for i in range(10): assert pow(next(it), 2, p) == a a, p = 5**2*3**n*2**n, 5**6*3**(n+1)*2**(n+3) it = sqrt_mod_iter(a, p) for i in range(2): assert pow(next(it), 2, p) == a n = 100 a, p = 5**2*3**n*2**n, 5**6*3**(n+1)*2**(n+1) it = sqrt_mod_iter(a, p) for i in range(2): assert pow(next(it), 2, p) == a assert type(next(sqrt_mod_iter(9, 27))) is int assert type(next(sqrt_mod_iter(9, 27, ZZ))) is type(ZZ(1)) assert type(next(sqrt_mod_iter(1, 7, ZZ))) is type(ZZ(1)) assert is_nthpow_residue(2, 1, 5) assert not is_nthpow_residue(2, 2, 5) assert is_nthpow_residue(8547, 12, 10007) assert nthroot_mod(1801, 11, 2663) == 44 for a, q, p in [(51922, 2, 203017), (43, 3, 109), (1801, 11, 2663), (26118163, 1303, 33333347), (1499, 7, 2663), (595, 6, 2663), (1714, 12, 2663), (28477, 9, 33343)]: r = nthroot_mod(a, q, p) assert pow(r, q, p) == a assert nthroot_mod(11, 3, 109) is None for p in primerange(5, 100): qv = range(3, p, 4) for q in qv: d = defaultdict(list) for i in range(p): d[pow(i, q, p)].append(i) for a in range(1, p - 1): res = nthroot_mod(a, q, p, True) if d[a]: assert d[a] == res else: assert res is None assert legendre_symbol(5, 11) == 1 assert legendre_symbol(25, 41) == 1 assert legendre_symbol(67, 101) == -1 assert legendre_symbol(0, 13) == 0 assert legendre_symbol(9, 3) == 0 raises(ValueError, lambda: legendre_symbol(2, 4)) assert jacobi_symbol(25, 41) == 1 assert jacobi_symbol(-23, 83) == -1 assert jacobi_symbol(3, 9) == 0 assert jacobi_symbol(42, 97) == -1 assert jacobi_symbol(3, 5) == -1 assert jacobi_symbol(7, 9) == 1 assert jacobi_symbol(0, 3) == 0 assert jacobi_symbol(0, 1) == 1 assert jacobi_symbol(2, 1) == 1 assert jacobi_symbol(1, 3) == 1 raises(ValueError, lambda: jacobi_symbol(3, 8)) assert mobius(13*7) == 1 assert mobius(1) == 1 assert mobius(13*7*5) == -1 assert mobius(13**2) == 0 raises(ValueError, lambda: mobius(-3)) p = Symbol('p', integer=True, positive=True, prime=True) x = Symbol('x', positive=True) i = Symbol('i', integer=True) assert mobius(p) == -1 raises(TypeError, lambda: mobius(x)) raises(ValueError, lambda: mobius(i))
# # Licensed to the Apache Software Foundation (ASF) under one # or more contributor license agreements. See the NOTICE file # distributed with this work for additional information # regarding copyright ownership. The ASF licenses this file # to you under the Apache License, Version 2.0 (the # "License"); you may not use this file except in compliance # with the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, # software distributed under the License is distributed on an # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY # KIND, either express or implied. See the License for the # specific language governing permissions and limitations # under the License. """This module is deprecated. Please use `airflow.sensors.weekday_sensor`.""" import warnings # pylint: disable=unused-import from airflow.sensors.weekday_sensor import DayOfWeekSensor # noqa warnings.warn( "This module is deprecated. Please use `airflow.sensors.weekday_sensor`.", DeprecationWarning, stacklevel=2, )
from __future__ import absolute_import import contextlib import logging import logging.handlers import os try: import threading except ImportError: import dummy_threading as threading from pip.compat import WINDOWS from pip.utils import ensure_dir try: from pip._vendor import colorama # Lots of different errors can come from this, including SystemError and # ImportError. except Exception: colorama = None _log_state = threading.local() _log_state.indentation = 0 @contextlib.contextmanager def indent_log(num=2): """ A context manager which will cause the log output to be indented for any log messages emitted inside it. """ _log_state.indentation += num try: yield finally: _log_state.indentation -= num def get_indentation(): return getattr(_log_state, 'indentation', 0) class IndentingFormatter(logging.Formatter): def format(self, record): """ Calls the standard formatter, but will indent all of the log messages by our current indentation level. """ formatted = logging.Formatter.format(self, record) formatted = "".join([ (" " * get_indentation()) + line for line in formatted.splitlines(True) ]) return formatted def _color_wrap(*colors): def wrapped(inp): return "".join(list(colors) + [inp, colorama.Style.RESET_ALL]) return wrapped class ColorizedStreamHandler(logging.StreamHandler): # Don't build up a list of colors if we don't have colorama if colorama: COLORS = [ # This needs to be in order from highest logging level to lowest. (logging.ERROR, _color_wrap(colorama.Fore.RED)), (logging.WARNING, _color_wrap(colorama.Fore.YELLOW)), ] else: COLORS = [] def __init__(self, stream=None): logging.StreamHandler.__init__(self, stream) if WINDOWS and colorama: self.stream = colorama.AnsiToWin32(self.stream) def should_color(self): # Don't colorize things if we do not have colorama if not colorama: return False real_stream = ( self.stream if not isinstance(self.stream, colorama.AnsiToWin32) else self.stream.wrapped ) # If the stream is a tty we should color it if hasattr(real_stream, "isatty") and real_stream.isatty(): return True # If we have an ASNI term we should color it if os.environ.get("TERM") == "ANSI": return True # If anything else we should not color it return False def format(self, record): msg = logging.StreamHandler.format(self, record) if self.should_color(): for level, color in self.COLORS: if record.levelno >= level: msg = color(msg) break return msg class BetterRotatingFileHandler(logging.handlers.RotatingFileHandler): def _open(self): ensure_dir(os.path.dirname(self.baseFilename)) return logging.handlers.RotatingFileHandler._open(self) class MaxLevelFilter(logging.Filter): def __init__(self, level): self.level = level def filter(self, record): return record.levelno < self.level
# Copyright (c) 2012 OpenStack Foundation. # Administrator of the National Aeronautics and Space Administration. # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from __future__ import print_function import errno import gc import os import pprint import socket import sys import traceback import eventlet import eventlet.backdoor import greenlet from oslo.config import cfg from nova.openstack.common._i18n import _LI from nova.openstack.common import log as logging help_for_backdoor_port = ( "Acceptable values are 0, <port>, and <start>:<end>, where 0 results " "in listening on a random tcp port number; <port> results in listening " "on the specified port number (and not enabling backdoor if that port " "is in use); and <start>:<end> results in listening on the smallest " "unused port number within the specified range of port numbers. The " "chosen port is displayed in the service's log file.") eventlet_backdoor_opts = [ cfg.StrOpt('backdoor_port', help="Enable eventlet backdoor. %s" % help_for_backdoor_port) ] CONF = cfg.CONF CONF.register_opts(eventlet_backdoor_opts) LOG = logging.getLogger(__name__) class EventletBackdoorConfigValueError(Exception): def __init__(self, port_range, help_msg, ex): msg = ('Invalid backdoor_port configuration %(range)s: %(ex)s. ' '%(help)s' % {'range': port_range, 'ex': ex, 'help': help_msg}) super(EventletBackdoorConfigValueError, self).__init__(msg) self.port_range = port_range def _dont_use_this(): print("Don't use this, just disconnect instead") def _find_objects(t): return [o for o in gc.get_objects() if isinstance(o, t)] def _print_greenthreads(): for i, gt in enumerate(_find_objects(greenlet.greenlet)): print(i, gt) traceback.print_stack(gt.gr_frame) print() def _print_nativethreads(): for threadId, stack in sys._current_frames().items(): print(threadId) traceback.print_stack(stack) print() def _parse_port_range(port_range): if ':' not in port_range: start, end = port_range, port_range else: start, end = port_range.split(':', 1) try: start, end = int(start), int(end) if end < start: raise ValueError return start, end except ValueError as ex: raise EventletBackdoorConfigValueError(port_range, ex, help_for_backdoor_port) def _listen(host, start_port, end_port, listen_func): try_port = start_port while True: try: return listen_func((host, try_port)) except socket.error as exc: if (exc.errno != errno.EADDRINUSE or try_port >= end_port): raise try_port += 1 def initialize_if_enabled(): backdoor_locals = { 'exit': _dont_use_this, # So we don't exit the entire process 'quit': _dont_use_this, # So we don't exit the entire process 'fo': _find_objects, 'pgt': _print_greenthreads, 'pnt': _print_nativethreads, } if CONF.backdoor_port is None: return None start_port, end_port = _parse_port_range(str(CONF.backdoor_port)) # NOTE(johannes): The standard sys.displayhook will print the value of # the last expression and set it to __builtin__._, which overwrites # the __builtin__._ that gettext sets. Let's switch to using pprint # since it won't interact poorly with gettext, and it's easier to # read the output too. def displayhook(val): if val is not None: pprint.pprint(val) sys.displayhook = displayhook sock = _listen('localhost', start_port, end_port, eventlet.listen) # In the case of backdoor port being zero, a port number is assigned by # listen(). In any case, pull the port number out here. port = sock.getsockname()[1] LOG.info( _LI('Eventlet backdoor listening on %(port)s for process %(pid)d') % {'port': port, 'pid': os.getpid()} ) eventlet.spawn_n(eventlet.backdoor.backdoor_server, sock, locals=backdoor_locals) return port
#!/usr/bin/env python # Copyright (c) 2012 The Chromium Authors. All rights reserved. # Use of this source code is governed by a BSD-style license that can be # found in the LICENSE file. """ A simple wrapper for protoc. - Adds includes in generated headers. - Handles building with system protobuf as an option. """ import optparse import os.path import shutil import subprocess import sys import tempfile PROTOC_INCLUDE_POINT = '// @@protoc_insertion_point(includes)\n' def ModifyHeader(header_file, extra_header): """Adds |extra_header| to |header_file|. Returns 0 on success. |extra_header| is the name of the header file to include. |header_file| is a generated protobuf cpp header. """ include_point_found = False header_contents = [] with open(header_file) as f: for line in f: header_contents.append(line) if line == PROTOC_INCLUDE_POINT: extra_header_msg = '#include "%s"\n' % extra_header header_contents.append(extra_header_msg) include_point_found = True; if not include_point_found: return 1 with open(header_file, 'wb') as f: f.write(''.join(header_contents)) return 0 def RewriteProtoFilesForSystemProtobuf(path): wrapper_dir = tempfile.mkdtemp() try: for filename in os.listdir(path): if not filename.endswith('.proto'): continue with open(os.path.join(path, filename), 'r') as src_file: with open(os.path.join(wrapper_dir, filename), 'w') as dst_file: for line in src_file: # Remove lines that break build with system protobuf. # We cannot optimize for lite runtime, because system lite runtime # does not have a Chromium-specific hack to retain unknown fields. # Similarly, it does not understand corresponding option to control # the usage of that hack. if 'LITE_RUNTIME' in line or 'retain_unknown_fields' in line: continue dst_file.write(line) return wrapper_dir except: shutil.rmtree(wrapper_dir) raise def main(argv): parser = optparse.OptionParser() parser.add_option('--include', dest='extra_header', help='The extra header to include. This must be specified ' 'along with --protobuf.') parser.add_option('--protobuf', dest='generated_header', help='The c++ protobuf header to add the extra header to. ' 'This must be specified along with --include.') parser.add_option('--proto-in-dir', help='The directory containing .proto files.') parser.add_option('--proto-in-file', help='Input file to compile.') parser.add_option('--use-system-protobuf', type=int, default=0, help='Option to use system-installed protobuf ' 'instead of bundled one.') (options, args) = parser.parse_args(sys.argv) if len(args) < 2: return 1 proto_path = options.proto_in_dir if options.use_system_protobuf == 1: proto_path = RewriteProtoFilesForSystemProtobuf(proto_path) try: # Run what is hopefully protoc. protoc_args = args[1:] protoc_args += ['--proto_path=%s' % proto_path, os.path.join(proto_path, options.proto_in_file)] ret = subprocess.call(protoc_args) if ret != 0: return ret finally: if options.use_system_protobuf == 1: # Remove temporary directory holding re-written files. shutil.rmtree(proto_path) # protoc succeeded, check to see if the generated cpp header needs editing. if not options.extra_header or not options.generated_header: return 0 return ModifyHeader(options.generated_header, options.extra_header) if __name__ == '__main__': sys.exit(main(sys.argv))
# orm/scoping.py # Copyright (C) 2005-2014 the SQLAlchemy authors and contributors <see AUTHORS file> # # This module is part of SQLAlchemy and is released under # the MIT License: http://www.opensource.org/licenses/mit-license.php from .. import exc as sa_exc from ..util import ScopedRegistry, ThreadLocalRegistry, warn from . import class_mapper, exc as orm_exc from .session import Session __all__ = ['scoped_session'] class scoped_session(object): """Provides scoped management of :class:`.Session` objects. See :ref:`unitofwork_contextual` for a tutorial. """ def __init__(self, session_factory, scopefunc=None): """Construct a new :class:`.scoped_session`. :param session_factory: a factory to create new :class:`.Session` instances. This is usually, but not necessarily, an instance of :class:`.sessionmaker`. :param scopefunc: optional function which defines the current scope. If not passed, the :class:`.scoped_session` object assumes "thread-local" scope, and will use a Python ``threading.local()`` in order to maintain the current :class:`.Session`. If passed, the function should return a hashable token; this token will be used as the key in a dictionary in order to store and retrieve the current :class:`.Session`. """ self.session_factory = session_factory if scopefunc: self.registry = ScopedRegistry(session_factory, scopefunc) else: self.registry = ThreadLocalRegistry(session_factory) def __call__(self, **kw): """Return the current :class:`.Session`, creating it using the session factory if not present. :param \**kw: Keyword arguments will be passed to the session factory callable, if an existing :class:`.Session` is not present. If the :class:`.Session` is present and keyword arguments have been passed, :exc:`~sqlalchemy.exc.InvalidRequestError` is raised. """ if kw: scope = kw.pop('scope', False) if scope is not None: if self.registry.has(): raise sa_exc.InvalidRequestError( "Scoped session is already present; " "no new arguments may be specified.") else: sess = self.session_factory(**kw) self.registry.set(sess) return sess else: return self.session_factory(**kw) else: return self.registry() def remove(self): """Dispose of the current :class:`.Session`, if present. This will first call :meth:`.Session.close` method on the current :class:`.Session`, which releases any existing transactional/connection resources still being held; transactions specifically are rolled back. The :class:`.Session` is then discarded. Upon next usage within the same scope, the :class:`.scoped_session` will produce a new :class:`.Session` object. """ if self.registry.has(): self.registry().close() self.registry.clear() def configure(self, **kwargs): """reconfigure the :class:`.sessionmaker` used by this :class:`.scoped_session`. See :meth:`.sessionmaker.configure`. """ if self.registry.has(): warn('At least one scoped session is already present. ' ' configure() can not affect sessions that have ' 'already been created.') self.session_factory.configure(**kwargs) def query_property(self, query_cls=None): """return a class property which produces a :class:`.Query` object against the class and the current :class:`.Session` when called. e.g.:: Session = scoped_session(sessionmaker()) class MyClass(object): query = Session.query_property() # after mappers are defined result = MyClass.query.filter(MyClass.name=='foo').all() Produces instances of the session's configured query class by default. To override and use a custom implementation, provide a ``query_cls`` callable. The callable will be invoked with the class's mapper as a positional argument and a session keyword argument. There is no limit to the number of query properties placed on a class. """ class query(object): def __get__(s, instance, owner): try: mapper = class_mapper(owner) if mapper: if query_cls: # custom query class return query_cls(mapper, session=self.registry()) else: # session's configured query class return self.registry().query(mapper) except orm_exc.UnmappedClassError: return None return query() ScopedSession = scoped_session """Old name for backwards compatibility.""" def instrument(name): def do(self, *args, **kwargs): return getattr(self.registry(), name)(*args, **kwargs) return do for meth in Session.public_methods: setattr(scoped_session, meth, instrument(meth)) def makeprop(name): def set(self, attr): setattr(self.registry(), name, attr) def get(self): return getattr(self.registry(), name) return property(get, set) for prop in ('bind', 'dirty', 'deleted', 'new', 'identity_map', 'is_active', 'autoflush', 'no_autoflush', 'info'): setattr(scoped_session, prop, makeprop(prop)) def clslevel(name): def do(cls, *args, **kwargs): return getattr(Session, name)(*args, **kwargs) return classmethod(do) for prop in ('close_all', 'object_session', 'identity_key'): setattr(scoped_session, prop, clslevel(prop))
#!/usr/bin/env python # Author: Noel Eck <noel.eck@intel.com> # Copyright (c) 2016 Intel Corporation. # # Permission is hereby granted, free of charge, to any person obtaining # a copy of this software and associated documentation files (the # "Software"), to deal in the Software without restriction, including # without limitation the rights to use, copy, modify, merge, publish, # distribute, sublicense, and/or sell copies of the Software, and to # permit persons to whom the Software is furnished to do so, subject to # the following conditions: # # The above copyright notice and this permission notice shall be # included in all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, # EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF # MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND # NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE # LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION # OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION # WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. from __future__ import print_function import time, sys, signal, atexit from upm import pyupm_ims def main(): # Create an instance of the I2C Moisture Sensor # I2C bus 0, default address = 0x20 ims = pyupm_ims.IMS(0) print ('I2C moisture sensor example...') while (1): try: print ('Version: %d light: 0x%04x moisture: 0x%04x temp: %3.2f C' \ % (ims.get_version(), ims.get_light(), ims.get_moisture(), ims.get_temperature())) time.sleep(1) except KeyboardInterrupt: break if __name__ == '__main__': main()
# Copyright 2012 United States Government as represented by the # Administrator of the National Aeronautics and Space Administration. # All Rights Reserved. # # Copyright 2012 Nebula, Inc. # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from __future__ import absolute_import from keystoneclient.v2_0 import client as keystone_client from openstack_dashboard import api from openstack_dashboard.test import helpers as test class FakeConnection(object): pass class ClientConnectionTests(test.TestCase): def setUp(self): super(ClientConnectionTests, self).setUp() self.mox.StubOutWithMock(keystone_client, "Client") self.internal_url = api.base.url_for(self.request, 'identity', endpoint_type='internalURL') self.admin_url = api.base.url_for(self.request, 'identity', endpoint_type='adminURL') self.conn = FakeConnection() class RoleAPITests(test.APITestCase): def setUp(self): super(RoleAPITests, self).setUp() self.role = self.roles.member self.roles = self.roles.list() def test_remove_tenant_user(self): """Tests api.keystone.remove_tenant_user Verifies that remove_tenant_user is called with the right arguments after iterating the user's roles. There are no assertions in this test because the checking is handled by mox in the VerifyAll() call in tearDown(). """ keystoneclient = self.stub_keystoneclient() tenant = self.tenants.first() keystoneclient.roles = self.mox.CreateMockAnything() keystoneclient.roles.roles_for_user(self.user.id, tenant.id).AndReturn(self.roles) for role in self.roles: keystoneclient.roles.revoke(role.id, domain=None, group=None, project=tenant.id, user=self.user.id) self.mox.ReplayAll() api.keystone.remove_tenant_user(self.request, tenant.id, self.user.id) def test_get_default_role(self): keystoneclient = self.stub_keystoneclient() keystoneclient.roles = self.mox.CreateMockAnything() keystoneclient.roles.list().AndReturn(self.roles) self.mox.ReplayAll() role = api.keystone.get_default_role(self.request) self.assertEqual(self.role, role) # Verify that a second call doesn't hit the API again, # (it would show up in mox as an unexpected method call) role = api.keystone.get_default_role(self.request) class ServiceAPITests(test.APITestCase): def test_service_wrapper(self): catalog = self.service_catalog identity_data = api.base.get_service_from_catalog(catalog, "identity") identity_data['id'] = 1 region = identity_data["endpoints"][0]["region"] service = api.keystone.Service(identity_data, region) self.assertEqual(u"identity (native backend)", unicode(service)) self.assertEqual(identity_data["endpoints"][0]["region"], service.region) self.assertEqual("http://int.keystone.example.com:5000/v2.0", service.url) self.assertEqual("http://public.keystone.example.com:5000/v2.0", service.public_url) self.assertEqual("int.keystone.example.com", service.host) def test_service_wrapper_service_in_region(self): catalog = self.service_catalog compute_data = api.base.get_service_from_catalog(catalog, "compute") compute_data['id'] = 1 region = compute_data["endpoints"][1]["region"] service = api.keystone.Service(compute_data, region) self.assertEqual(u"compute", unicode(service)) self.assertEqual(compute_data["endpoints"][1]["region"], service.region) self.assertEqual("http://int.nova2.example.com:8774/v2", service.url) self.assertEqual("http://public.nova2.example.com:8774/v2", service.public_url) self.assertEqual("int.nova2.example.com", service.host)
# -*- coding: utf-8 - # # This file is part of gunicorn released under the MIT license. # See the NOTICE for more information. """The debug module contains utilities and functions for better debugging Gunicorn.""" import sys import linecache import re import inspect __all__ = ['spew', 'unspew'] _token_spliter = re.compile('\W+') class Spew(object): """ """ def __init__(self, trace_names=None, show_values=True): self.trace_names = trace_names self.show_values = show_values def __call__(self, frame, event, arg): if event == 'line': lineno = frame.f_lineno if '__file__' in frame.f_globals: filename = frame.f_globals['__file__'] if (filename.endswith('.pyc') or filename.endswith('.pyo')): filename = filename[:-1] name = frame.f_globals['__name__'] line = linecache.getline(filename, lineno) else: name = '[unknown]' try: src = inspect.getsourcelines(frame) line = src[lineno] except IOError: line = 'Unknown code named [%s]. VM instruction #%d' % ( frame.f_code.co_name, frame.f_lasti) if self.trace_names is None or name in self.trace_names: print('%s:%s: %s' % (name, lineno, line.rstrip())) if not self.show_values: return self details = [] tokens = _token_spliter.split(line) for tok in tokens: if tok in frame.f_globals: details.append('%s=%r' % (tok, frame.f_globals[tok])) if tok in frame.f_locals: details.append('%s=%r' % (tok, frame.f_locals[tok])) if details: print("\t%s" % ' '.join(details)) return self def spew(trace_names=None, show_values=False): """Install a trace hook which writes incredibly detailed logs about what code is being executed to stdout. """ sys.settrace(Spew(trace_names, show_values)) def unspew(): """Remove the trace hook installed by spew. """ sys.settrace(None)
# -*- coding: utf-8 -*- import itertools import unittest2 from lxml import etree as ET, html from lxml.html import builder as h from openerp.tests import common def attrs(**kwargs): return dict(('data-oe-%s' % key, str(value)) for key, value in kwargs.iteritems()) class TestViewSaving(common.TransactionCase): def eq(self, a, b): self.assertEqual(a.tag, b.tag) self.assertEqual(a.attrib, b.attrib) self.assertEqual((a.text or '').strip(), (b.text or '').strip()) self.assertEqual((a.tail or '').strip(), (b.tail or '').strip()) for ca, cb in itertools.izip_longest(a, b): self.eq(ca, cb) def setUp(self): super(TestViewSaving, self).setUp() self.arch = h.DIV( h.DIV( h.H3("Column 1"), h.UL( h.LI("Item 1"), h.LI("Item 2"), h.LI("Item 3"))), h.DIV( h.H3("Column 2"), h.UL( h.LI("Item 1"), h.LI(h.SPAN("My Company", attrs(model='res.company', id=1, field='name', type='char'))), h.LI(h.SPAN("+00 00 000 00 0 000", attrs(model='res.company', id=1, field='phone', type='char'))) )) ) self.view_id = self.registry('ir.ui.view').create(self.cr, self.uid, { 'name': "Test View", 'type': 'qweb', 'arch': ET.tostring(self.arch, encoding='utf-8').decode('utf-8') }) def test_embedded_extraction(self): fields = self.registry('ir.ui.view').extract_embedded_fields( self.cr, self.uid, self.arch, context=None) expect = [ h.SPAN("My Company", attrs(model='res.company', id=1, field='name', type='char')), h.SPAN("+00 00 000 00 0 000", attrs(model='res.company', id=1, field='phone', type='char')), ] for actual, expected in itertools.izip_longest(fields, expect): self.eq(actual, expected) def test_embedded_save(self): embedded = h.SPAN("+00 00 000 00 0 000", attrs( model='res.company', id=1, field='phone', type='char')) self.registry('ir.ui.view').save_embedded_field(self.cr, self.uid, embedded) company = self.registry('res.company').browse(self.cr, self.uid, 1) self.assertEqual(company.phone, "+00 00 000 00 0 000") @unittest2.skip("save conflict for embedded (saved by third party or previous version in page) not implemented") def test_embedded_conflict(self): e1 = h.SPAN("My Company", attrs(model='res.company', id=1, field='name')) e2 = h.SPAN("Leeroy Jenkins", attrs(model='res.company', id=1, field='name')) View = self.registry('ir.ui.view') View.save_embedded_field(self.cr, self.uid, e1) # FIXME: more precise exception with self.assertRaises(Exception): View.save_embedded_field(self.cr, self.uid, e2) def test_embedded_to_field_ref(self): View = self.registry('ir.ui.view') embedded = h.SPAN("My Company", attrs(expression="bob")) self.eq( View.to_field_ref(self.cr, self.uid, embedded, context=None), h.SPAN({'t-field': 'bob'}) ) def test_to_field_ref_keep_attributes(self): View = self.registry('ir.ui.view') att = attrs(expression="bob", model="res.company", id=1, field="name") att['id'] = "whop" att['class'] = "foo bar" embedded = h.SPAN("My Company", att) self.eq(View.to_field_ref(self.cr, self.uid, embedded, context=None), h.SPAN({'t-field': 'bob', 'class': 'foo bar', 'id': 'whop'})) def test_replace_arch(self): replacement = h.P("Wheee") result = self.registry('ir.ui.view').replace_arch_section( self.cr, self.uid, self.view_id, None, replacement) self.eq(result, h.DIV("Wheee")) def test_replace_arch_2(self): replacement = h.DIV(h.P("Wheee")) result = self.registry('ir.ui.view').replace_arch_section( self.cr, self.uid, self.view_id, None, replacement) self.eq(result, replacement) def test_fixup_arch(self): replacement = h.H1("I am the greatest title alive!") result = self.registry('ir.ui.view').replace_arch_section( self.cr, self.uid, self.view_id, '/div/div[1]/h3', replacement) self.eq(result, h.DIV( h.DIV( h.H3("I am the greatest title alive!"), h.UL( h.LI("Item 1"), h.LI("Item 2"), h.LI("Item 3"))), h.DIV( h.H3("Column 2"), h.UL( h.LI("Item 1"), h.LI(h.SPAN("My Company", attrs(model='res.company', id=1, field='name', type='char'))), h.LI(h.SPAN("+00 00 000 00 0 000", attrs(model='res.company', id=1, field='phone', type='char'))) )) )) def test_multiple_xpath_matches(self): with self.assertRaises(ValueError): self.registry('ir.ui.view').replace_arch_section( self.cr, self.uid, self.view_id, '/div/div/h3', h.H6("Lol nope")) def test_save(self): Company = self.registry('res.company') View = self.registry('ir.ui.view') replacement = ET.tostring(h.DIV( h.H3("Column 2"), h.UL( h.LI("wob wob wob"), h.LI(h.SPAN("Acme Corporation", attrs(model='res.company', id=1, field='name', expression="bob", type='char'))), h.LI(h.SPAN("+12 3456789", attrs(model='res.company', id=1, field='phone', expression="edmund", type='char'))), ) ), encoding='utf-8') View.save(self.cr, self.uid, res_id=self.view_id, value=replacement, xpath='/div/div[2]') company = Company.browse(self.cr, self.uid, 1) self.assertEqual(company.name, "Acme Corporation") self.assertEqual(company.phone, "+12 3456789") self.eq( ET.fromstring(View.browse(self.cr, self.uid, self.view_id).arch.encode('utf-8')), h.DIV( h.DIV( h.H3("Column 1"), h.UL( h.LI("Item 1"), h.LI("Item 2"), h.LI("Item 3"))), h.DIV( h.H3("Column 2"), h.UL( h.LI("wob wob wob"), h.LI(h.SPAN({'t-field': "bob"})), h.LI(h.SPAN({'t-field': "edmund"})) )) ) ) def test_save_escaped_text(self): view_id = self.registry('ir.ui.view').create(self.cr, self.uid, { 'arch':'<t>hello world</t>', 'type':'qweb' }) view = self.registry('ir.ui.view').browse(self.cr, self.uid, view_id) replacement = 'hello world &amp; &lt;angle brackets&gt;!' view.save(replacement, xpath='/t') self.assertEqual(view.render(), replacement, 'html special characters wrongly escaped') def test_save_only_embedded(self): Company = self.registry('res.company') company_id = 1 Company.write(self.cr, self.uid, company_id, {'name': "Foo Corporation"}) node = html.tostring(h.SPAN( "Acme Corporation", attrs(model='res.company', id=company_id, field="name", expression='bob', type='char'))) self.registry('ir.ui.view').save(self.cr, self.uid, res_id=company_id,value=node) company = Company.browse(self.cr, self.uid, company_id) self.assertEqual(company.name, "Acme Corporation") def test_field_tail(self): View = self.registry('ir.ui.view') replacement = ET.tostring( h.LI(h.SPAN("+12 3456789", attrs( model='res.company', id=1, type='char', field='phone', expression="edmund")), "whop whop" ), encoding="utf-8") View.save(self.cr, self.uid, res_id = self.view_id, value=replacement, xpath='/div/div[2]/ul/li[3]') self.eq( ET.fromstring(View.browse(self.cr, self.uid, self.view_id).arch.encode('utf-8')), h.DIV( h.DIV( h.H3("Column 1"), h.UL( h.LI("Item 1"), h.LI("Item 2"), h.LI("Item 3"))), h.DIV( h.H3("Column 2"), h.UL( h.LI("Item 1"), h.LI(h.SPAN("My Company", attrs(model='res.company', id=1, field='name', type='char'))), h.LI(h.SPAN({'t-field': "edmund"}), "whop whop"), )) ) )
# -*- coding: utf-8 -*- """ This file is part of Radar. Radar is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. Radar is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the Lesser GNU General Public License for more details. You should have received a copy of the Lesser GNU General Public License along with Radar. If not, see <http://www.gnu.org/licenses/>. Copyright 2015 Lucas Liendo. """ from io import open from os.path import join as join_path from ast import parse as ast_parse from ast import walk as ast_walk from ast import ClassDef from pkgutil import iter_modules from sys import path as module_search_path from ..logger import RadarLogger class ClassLoaderError(Exception): pass class ClassLoader(object): """ This class offers a simple mechanism to get all user-defined classes from an external module. This is useful if you want to load unknown classes dynamically at run-time. """ ENCODING_DECLARATION = '# -*- coding: utf-8 -*-' def __init__(self, module_path): self._module_path = module_path module_search_path.append(module_path) def _get_class_names(self, filename): class_names = [] try: with open(filename) as fd: parsed_source = ast_parse(fd.read().strip(self.ENCODING_DECLARATION)) class_names = [n.name for n in ast_walk(parsed_source) if isinstance(n, ClassDef)] except IOError as e: raise ClassLoaderError('Error - Couldn\'t open : \'{:}\'. Reason : {:}.'.format(filename, e.strerror)) except SyntaxError as e: raise ClassLoaderError('Error - Couldn\'t parse \'{:}\'. Reason: {:}.'.format(filename, e)) return class_names def get_classes(self, subclass=object): classes = [] for _, module_name, _ in iter_modules(path=[self._module_path]): module_path = join_path(self._module_path, module_name) try: class_names = self._get_class_names(module_path + '/__init__.py') imported_module = __import__(module_name) classes += [getattr(imported_module, class_name) for class_name in class_names] except ClassLoaderError as e: RadarLogger.log(e) return [C for C in classes if issubclass(C, subclass)]
#!/usr/bin/env python # # Copyright 2008 The Closure Linter Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS-IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. """Classes to represent tokens and positions within them.""" __author__ = ('robbyw@google.com (Robert Walker)', 'ajp@google.com (Andy Perelson)') class TokenType(object): """Token types common to all languages.""" NORMAL = 'normal' WHITESPACE = 'whitespace' BLANK_LINE = 'blank line' class Token(object): """Token class for intelligent text splitting. The token class represents a string of characters and an identifying type. Attributes: type: The type of token. string: The characters the token comprises. length: The length of the token. line: The text of the line the token is found in. line_number: The number of the line the token is found in. values: Dictionary of values returned from the tokens regex match. previous: The token before this one. next: The token after this one. start_index: The character index in the line where this token starts. attached_object: Object containing more information about this token. metadata: Object containing metadata about this token. Must be added by a separate metadata pass. """ def __init__(self, string, token_type, line, line_number, values=None, orig_line_number=None): """Creates a new Token object. Args: string: The string of input the token contains. token_type: The type of token. line: The text of the line this token is in. line_number: The line number of the token. values: A dict of named values within the token. For instance, a function declaration may have a value called 'name' which captures the name of the function. orig_line_number: The line number of the original file this token comes from. This should be only set during the tokenization process. For newly created error fix tokens after that, it should be None. """ self.type = token_type self.string = string self.length = len(string) self.line = line self.line_number = line_number self.orig_line_number = orig_line_number self.values = values self.is_deleted = False # These parts can only be computed when the file is fully tokenized self.previous = None self.next = None self.start_index = None # This part is set in statetracker.py # TODO(robbyw): Wrap this in to metadata self.attached_object = None # This part is set in *metadatapass.py self.metadata = None def IsFirstInLine(self): """Tests if this token is the first token in its line. Returns: Whether the token is the first token in its line. """ return not self.previous or self.previous.line_number != self.line_number def IsLastInLine(self): """Tests if this token is the last token in its line. Returns: Whether the token is the last token in its line. """ return not self.next or self.next.line_number != self.line_number def IsType(self, token_type): """Tests if this token is of the given type. Args: token_type: The type to test for. Returns: True if the type of this token matches the type passed in. """ return self.type == token_type def IsAnyType(self, *token_types): """Tests if this token is any of the given types. Args: token_types: The types to check. Also accepts a single array. Returns: True if the type of this token is any of the types passed in. """ if not isinstance(token_types[0], basestring): return self.type in token_types[0] else: return self.type in token_types def __repr__(self): return '<Token: %s, "%s", %r, %d, %r>' % (self.type, self.string, self.values, self.line_number, self.metadata) def __iter__(self): """Returns a token iterator.""" node = self while node: yield node node = node.next def __reversed__(self): """Returns a reverse-direction token iterator.""" node = self while node: yield node node = node.previous
#!/usr/bin/env python # # __COPYRIGHT__ # # Permission is hereby granted, free of charge, to any person obtaining # a copy of this software and associated documentation files (the # "Software"), to deal in the Software without restriction, including # without limitation the rights to use, copy, modify, merge, publish, # distribute, sublicense, and/or sell copies of the Software, and to # permit persons to whom the Software is furnished to do so, subject to # the following conditions: # # The above copyright notice and this permission notice shall be included # in all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY # KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE # WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND # NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE # LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION # OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION # WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. # __revision__ = "__FILE__ __REVISION__ __DATE__ __DEVELOPER__" """ Check that all auxilary files created by LaTeX are properly cleaned by scons -c. """ import os import TestSCons test = TestSCons.TestSCons() latex = test.where_is('latex') if not latex: test.skip_test("Could not find tex or latex; skipping test(s).\n") comment = os.system('kpsewhich comment.sty') if not comment==0: test.skip_test("comment.sty not installed; skipping test(s).\n") # package hyperref generates foo.out # package comment generates comment.cut # todo: add makeindex etc. input_file = r""" \documentclass{article} \usepackage{hyperref} \usepackage{comment} \specialcomment{foocom}{}{} \begin{document} \begin{foocom} Hi \end{foocom} As stated in \cite{X}, this is a bug-a-boo. \bibliography{fooref} \bibliographystyle{plain} \end{document} """ bibfile = r""" @Article{X, author = "Mr. X", title = "A determination of bug-a-boo-ness", journal = "Journal of B.a.B.", year = 1920, volume = 62, pages = 291 } """ test.write('SConstruct', """\ import os env = Environment(tools = ['tex', 'latex']) env.DVI( "foo.ltx" ) """) test.write('foo.ltx', input_file) test.write('fooref.bib', bibfile) test.run() test.must_exist('foo.log') test.must_exist('foo.aux') test.must_exist('foo.bbl') test.must_exist('foo.blg') test.must_exist('comment.cut') test.must_exist('foo.out') test.run(arguments = '-c') test.must_not_exist('foo.log') test.must_not_exist('foo.aux') test.must_not_exist('foo.bbl') test.must_not_exist('foo.blg') test.must_not_exist('comment.cut') test.must_not_exist('foo.out') test.pass_test() # Local Variables: # tab-width:4 # indent-tabs-mode:nil # End: # vim: set expandtab tabstop=4 shiftwidth=4:
#!/usr/bin/python # # This file is part of Ansible # # Ansible is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Ansible is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Ansible. If not, see <http://www.gnu.org/licenses/>. # ANSIBLE_METADATA = {'metadata_version': '1.0', 'status': ['deprecated'], 'supported_by': 'community'} DOCUMENTATION = """ --- module: iosxr_template version_added: "2.1" author: "Ricardo Carrillo Cruz (@rcarrillocruz)" short_description: Manage Cisco IOS XR device configurations over SSH description: - Manages network device configurations over SSH. This module allows implementers to work with the device running-config. It provides a way to push a set of commands onto a network device by evaluating the current running-config and only pushing configuration commands that are not already configured. The config source can be a set of commands or a template. deprecated: Deprecated in 2.2. Use M(iosxr_config) instead. extends_documentation_fragment: iosxr options: src: description: - The path to the config source. The source can be either a file with config or a template that will be merged during runtime. By default the task will first search for the source file in role or playbook root folder in templates unless a full path to the file is given. required: false default: null force: description: - The force argument instructs the module not to consider the current device running-config. When set to true, this will cause the module to push the contents of I(src) into the device without first checking if already configured. required: false default: false choices: [ "true", "false" ] backup: description: - When this argument is configured true, the module will backup the running-config from the node prior to making any changes. The backup file will be written to backup_{{ hostname }} in the root of the playbook directory. required: false default: false choices: [ "true", "false" ] config: description: - The module, by default, will connect to the remote device and retrieve the current running-config to use as a base for comparing against the contents of source. There are times when it is not desirable to have the task get the current running-config for every task. The I(config) argument allows the implementer to pass in the configuration to use as the base config for comparison. required: false default: null """ EXAMPLES = """ - name: push a configuration onto the device iosxr_template: src: config.j2 - name: forceable push a configuration onto the device iosxr_template: src: config.j2 force: yes - name: provide the base configuration for comparison iosxr_template: src: candidate_config.txt config: current_config.txt """ RETURN = """ updates: description: The set of commands that will be pushed to the remote device returned: always type: list sample: ['...', '...'] """ from ansible.module_utils.basic import AnsibleModule from ansible.module_utils.netcfg import NetworkConfig, dumps from ansible.module_utils.iosxr import get_config, load_config from ansible.module_utils.iosxr import iosxr_argument_spec, check_args def main(): """ main entry point for module execution """ argument_spec = dict( src=dict(), force=dict(default=False, type='bool'), backup=dict(default=False, type='bool'), config=dict(), ) argument_spec.update(iosxr_argument_spec) mutually_exclusive = [('config', 'backup'), ('config', 'force')] module = AnsibleModule(argument_spec=argument_spec, mutually_exclusive=mutually_exclusive, supports_check_mode=True) warnings = list() check_args(module, warnings) result = dict(changed=False, warnings=warnings) candidate = NetworkConfig(contents=module.params['src'], indent=1) if module.params['backup']: result['__backup__'] = get_config(module) if not module.params['force']: contents = get_config(module) configobj = NetworkConfig(contents=contents, indent=1) commands = candidate.difference(configobj) commands = dumps(commands, 'commands').split('\n') commands = [str(c).strip() for c in commands if c] else: commands = [c.strip() for c in str(candidate).split('\n')] if commands: load_config(module, commands, result['warnings'], not module.check_mode) result['changed'] = not module.check_mode result['updates'] = commands result['commands'] = commands module.exit_json(**result) if __name__ == '__main__': main()
# Wrapper for loading templates from eggs via pkg_resources.resource_string. try: from pkg_resources import resource_string except ImportError: resource_string = None from django.template import TemplateDoesNotExist from django.template.loader import BaseLoader from django.conf import settings class Loader(BaseLoader): is_usable = resource_string is not None def load_template_source(self, template_name, template_dirs=None): """ Loads templates from Python eggs via pkg_resource.resource_string. For every installed app, it tries to get the resource (app, template_name). """ if resource_string is not None: pkg_name = 'templates/' + template_name for app in settings.INSTALLED_APPS: try: return (resource_string(app, pkg_name).decode(settings.FILE_CHARSET), 'egg:%s:%s' % (app, pkg_name)) except: pass raise TemplateDoesNotExist(template_name) _loader = Loader() def load_template_source(template_name, template_dirs=None): import warnings warnings.warn( "'django.template.loaders.eggs.load_template_source' is deprecated; use 'django.template.loaders.eggs.Loader' instead.", PendingDeprecationWarning ) return _loader.load_template_source(template_name, template_dirs) load_template_source.is_usable = resource_string is not None
# Ansible module to manage CheckPoint Firewall (c) 2019 # # Ansible is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Ansible is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Ansible. If not, see <http://www.gnu.org/licenses/>. # from __future__ import absolute_import, division, print_function __metaclass__ = type import pytest from units.modules.utils import set_module_args, exit_json, fail_json, AnsibleExitJson from ansible.module_utils import basic from ansible.modules.network.check_point import cp_mgmt_tag OBJECT = { "name": "My New Tag1", "tags": [ "tag1", "tag2" ] } CREATE_PAYLOAD = { "name": "My New Tag1", "tags": [ "tag1", "tag2" ] } UPDATE_PAYLOAD = { "name": "My New Tag1" } OBJECT_AFTER_UPDATE = UPDATE_PAYLOAD DELETE_PAYLOAD = { "name": "My New Tag1", "state": "absent" } function_path = 'ansible.modules.network.check_point.cp_mgmt_tag.api_call' api_call_object = 'tag' class TestCheckpointTag(object): module = cp_mgmt_tag @pytest.fixture(autouse=True) def module_mock(self, mocker): return mocker.patch.multiple(basic.AnsibleModule, exit_json=exit_json, fail_json=fail_json) @pytest.fixture def connection_mock(self, mocker): connection_class_mock = mocker.patch('ansible.module_utils.network.checkpoint.checkpoint.Connection') return connection_class_mock.return_value def test_create(self, mocker, connection_mock): mock_function = mocker.patch(function_path) mock_function.return_value = {'changed': True, api_call_object: OBJECT} result = self._run_module(CREATE_PAYLOAD) assert result['changed'] assert OBJECT.items() == result[api_call_object].items() def test_create_idempotent(self, mocker, connection_mock): mock_function = mocker.patch(function_path) mock_function.return_value = {'changed': False, api_call_object: OBJECT} result = self._run_module(CREATE_PAYLOAD) assert not result['changed'] def test_update(self, mocker, connection_mock): mock_function = mocker.patch(function_path) mock_function.return_value = {'changed': True, api_call_object: OBJECT_AFTER_UPDATE} result = self._run_module(UPDATE_PAYLOAD) assert result['changed'] assert OBJECT_AFTER_UPDATE.items() == result[api_call_object].items() def test_update_idempotent(self, mocker, connection_mock): mock_function = mocker.patch(function_path) mock_function.return_value = {'changed': False, api_call_object: OBJECT_AFTER_UPDATE} result = self._run_module(UPDATE_PAYLOAD) assert not result['changed'] def test_delete(self, mocker, connection_mock): mock_function = mocker.patch(function_path) mock_function.return_value = {'changed': True} result = self._run_module(DELETE_PAYLOAD) assert result['changed'] def test_delete_idempotent(self, mocker, connection_mock): mock_function = mocker.patch(function_path) mock_function.return_value = {'changed': False} result = self._run_module(DELETE_PAYLOAD) assert not result['changed'] def _run_module(self, module_args): set_module_args(module_args) with pytest.raises(AnsibleExitJson) as ex: self.module.main() return ex.value.args[0]
# Copyright (C) 2012 Google Inc. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. """A keyring based Storage. A Storage for Credentials that uses the keyring module. """ __author__ = 'jcgregorio@google.com (Joe Gregorio)' import keyring import threading from client import Storage as BaseStorage from client import Credentials class Storage(BaseStorage): """Store and retrieve a single credential to and from the keyring. To use this module you must have the keyring module installed. See <http://pypi.python.org/pypi/keyring/>. This is an optional module and is not installed with oauth2client by default because it does not work on all the platforms that oauth2client supports, such as Google App Engine. The keyring module <http://pypi.python.org/pypi/keyring/> is a cross-platform library for access the keyring capabilities of the local system. The user will be prompted for their keyring password when this module is used, and the manner in which the user is prompted will vary per platform. Usage: from oauth2client.keyring_storage import Storage s = Storage('name_of_application', 'user1') credentials = s.get() """ def __init__(self, service_name, user_name): """Constructor. Args: service_name: string, The name of the service under which the credentials are stored. user_name: string, The name of the user to store credentials for. """ self._service_name = service_name self._user_name = user_name self._lock = threading.Lock() def acquire_lock(self): """Acquires any lock necessary to access this Storage. This lock is not reentrant.""" self._lock.acquire() def release_lock(self): """Release the Storage lock. Trying to release a lock that isn't held will result in a RuntimeError. """ self._lock.release() def locked_get(self): """Retrieve Credential from file. Returns: oauth2client.client.Credentials """ credentials = None content = keyring.get_password(self._service_name, self._user_name) if content is not None: try: credentials = Credentials.new_from_json(content) credentials.set_store(self) except ValueError: pass return credentials def locked_put(self, credentials): """Write Credentials to file. Args: credentials: Credentials, the credentials to store. """ keyring.set_password(self._service_name, self._user_name, credentials.to_json()) def locked_delete(self): """Delete Credentials file. Args: credentials: Credentials, the credentials to store. """ keyring.set_password(self._service_name, self._user_name, '')
#!/usr/bin/env python # -*- coding: utf-8 -*- # # Copyright 2011 Yesudeep Mangalapilly <yesudeep@gmail.com> # Copyright 2012 Google, Inc. # Copyright 2014 Thomas Amland <thomas.amland@gmail.com> # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. """ :module: watchdog.utils.dirsnapshot :synopsis: Directory snapshots and comparison. :author: yesudeep@google.com (Yesudeep Mangalapilly) .. ADMONITION:: Where are the moved events? They "disappeared" This implementation does not take partition boundaries into consideration. It will only work when the directory tree is entirely on the same file system. More specifically, any part of the code that depends on inode numbers can break if partition boundaries are crossed. In these cases, the snapshot diff will represent file/directory movement as created and deleted events. Classes ------- .. autoclass:: DirectorySnapshot :members: :show-inheritance: .. autoclass:: DirectorySnapshotDiff :members: :show-inheritance: """ import os from stat import S_ISDIR from watchdog.utils import platform from watchdog.utils import stat as default_stat class DirectorySnapshotDiff(object): """ Compares two directory snapshots and creates an object that represents the difference between the two snapshots. :param ref: The reference directory snapshot. :type ref: :class:`DirectorySnapshot` :param snapshot: The directory snapshot which will be compared with the reference snapshot. :type snapshot: :class:`DirectorySnapshot` """ def __init__(self, ref, snapshot): created = snapshot.paths - ref.paths deleted = ref.paths - snapshot.paths # check that all unchanged paths have the same inode for path in ref.paths & snapshot.paths: if ref.inode(path) != snapshot.inode(path): created.add(path) deleted.add(path) # find moved paths moved = set() for path in set(deleted): inode = ref.inode(path) new_path = snapshot.path(inode) if new_path: # file is not deleted but moved deleted.remove(path) moved.add((path, new_path)) for path in set(created): inode = snapshot.inode(path) old_path = ref.path(inode) if old_path: created.remove(path) moved.add((old_path, path)) # find modified paths # first check paths that have not moved modified = set() for path in ref.paths & snapshot.paths: if ref.inode(path) == snapshot.inode(path): if ref.mtime(path) != snapshot.mtime(path): modified.add(path) for (old_path, new_path) in moved: if ref.mtime(old_path) != snapshot.mtime(new_path): modified.add(old_path) self._dirs_created = [path for path in created if snapshot.isdir(path)] self._dirs_deleted = [path for path in deleted if ref.isdir(path)] self._dirs_modified = [path for path in modified if ref.isdir(path)] self._dirs_moved = [(frm, to) for (frm, to) in moved if ref.isdir(frm)] self._files_created = list(created - set(self._dirs_created)) self._files_deleted = list(deleted - set(self._dirs_deleted)) self._files_modified = list(modified - set(self._dirs_modified)) self._files_moved = list(moved - set(self._dirs_moved)) @property def files_created(self): """List of files that were created.""" return self._files_created @property def files_deleted(self): """List of files that were deleted.""" return self._files_deleted @property def files_modified(self): """List of files that were modified.""" return self._files_modified @property def files_moved(self): """ List of files that were moved. Each event is a two-tuple the first item of which is the path that has been renamed to the second item in the tuple. """ return self._files_moved @property def dirs_modified(self): """ List of directories that were modified. """ return self._dirs_modified @property def dirs_moved(self): """ List of directories that were moved. Each event is a two-tuple the first item of which is the path that has been renamed to the second item in the tuple. """ return self._dirs_moved @property def dirs_deleted(self): """ List of directories that were deleted. """ return self._dirs_deleted @property def dirs_created(self): """ List of directories that were created. """ return self._dirs_created class DirectorySnapshot(object): """ A snapshot of stat information of files in a directory. :param path: The directory path for which a snapshot should be taken. :type path: ``str`` :param recursive: ``True`` if the entire directory tree should be included in the snapshot; ``False`` otherwise. :type recursive: ``bool`` :param walker_callback: .. deprecated:: 0.7.2 :param stat: Use custom stat function that returns a stat structure for path. Currently only st_dev, st_ino, st_mode and st_mtime are needed. A function with the signature ``walker_callback(path, stat_info)`` which will be called for every entry in the directory tree. :param listdir: Use custom listdir function. See ``os.listdir`` for details. """ def __init__(self, path, recursive=True, walker_callback=(lambda p, s: None), stat=default_stat, listdir=os.listdir): self._stat_info = {} self._inode_to_path = {} st = stat(path) self._stat_info[path] = st self._inode_to_path[(st.st_ino, st.st_dev)] = path def walk(root): paths = [os.path.join(root, name) for name in listdir(root)] entries = [] for p in paths: try: entries.append((p, stat(p))) except OSError: continue for _ in entries: yield _ if recursive: for path, st in entries: if S_ISDIR(st.st_mode): for _ in walk(path): yield _ for p, st in walk(path): i = (st.st_ino, st.st_dev) self._inode_to_path[i] = p self._stat_info[p] = st walker_callback(p, st) @property def paths(self): """ Set of file/directory paths in the snapshot. """ return set(self._stat_info.keys()) def path(self, id): """ Returns path for id. None if id is unknown to this snapshot. """ return self._inode_to_path.get(id) def inode(self, path): """ Returns an id for path. """ st = self._stat_info[path] return (st.st_ino, st.st_dev) def isdir(self, path): return S_ISDIR(self._stat_info[path].st_mode) def mtime(self, path): return self._stat_info[path].st_mtime def stat_info(self, path): """ Returns a stat information object for the specified path from the snapshot. Attached information is subject to change. Do not use unless you specify `stat` in constructor. Use :func:`inode`, :func:`mtime`, :func:`isdir` instead. :param path: The path for which stat information should be obtained from a snapshot. """ return self._stat_info[path] def __sub__(self, previous_dirsnap): """Allow subtracting a DirectorySnapshot object instance from another. :returns: A :class:`DirectorySnapshotDiff` object. """ return DirectorySnapshotDiff(previous_dirsnap, self) def __str__(self): return self.__repr__() def __repr__(self): return str(self._stat_info)
"""WebDriver element location functionality.""" class SearchContext(object): """Abstract class that provides the core element location functionality.""" def find_element_by_css(self, selector): """Find the first element matching a css selector.""" return self._find_element('css selector', selector) def find_elements_by_css(self, selector): """Find all elements matching a css selector.""" return self._find_elements('css selector', selector) def find_element_by_link_text(self, text): """Find the first link with the given text.""" return self._find_element('link text', text) def find_elements_by_link_text(self, text): """Find all links with the given text.""" return self._find_elements('link text', text) def find_element_by_partial_link_text(self, text): """Find the first link containing the given text.""" return self._find_element('partial link text', text) def find_elements_by_partial_link_text(self, text): """Find all links containing the given text.""" return self._find_elements('partial link text', text) def find_element_by_xpath(self, xpath): """Find the first element matching the xpath.""" return self._find_element('xpath', xpath) def find_elements_by_xpath(self, xpath): """Find all elements matching the xpath.""" return self._find_elements('xpath', xpath) def _find_element(self, strategy, value): return self.execute('POST', '/element', 'findElement', self._get_locator(strategy, value)) def _find_elements(self, strategy, value): return self.execute('POST', '/elements', 'findElements', self._get_locator(strategy, value)) def _get_locator(self, strategy, value): if self.mode == 'strict': return {'strategy': strategy, 'value': value} elif self.mode == 'compatibility': return {'using': strategy, 'value': value}
# -*- coding: utf-8 -*- # ##### BEGIN GPL LICENSE BLOCK ##### # # This program is free software; you can redistribute it and/or # modify it under the terms of the GNU General Public License # as published by the Free Software Foundation; either version 2 # of the License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program; if not, write to the Free Software Foundation, # Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA. # # ##### END GPL LICENSE BLOCK ##### """ ui_utils.py Some UI utility functions """ class GUI: @classmethod def drawIconButton(cls, enabled, layout, iconName, operator, frame=True): col = layout.column() col.enabled = enabled bt = col.operator(operator, text='', icon=iconName, emboss=frame) @classmethod def drawTextButton(cls, enabled, layout, text, operator, frame=True): col = layout.column() col.enabled = enabled bt = col.operator(operator, text=text, emboss=frame)
# Generated by the pRPC protocol buffer compiler plugin. DO NOT EDIT! # source: isolated.proto import base64 import zlib from google.protobuf import descriptor_pb2 # Includes description of the isolated.proto and all of its transitive # dependencies. Includes source code info. FILE_DESCRIPTOR_SET = descriptor_pb2.FileDescriptorSet() FILE_DESCRIPTOR_SET.ParseFromString(zlib.decompress(base64.b64decode( 'eJy9Ws1vG0l2H4qWTLc1njbHMyNrvmo1q7E4Q5H6sMdrabJYSqTk9lCklh/22oOB3WoWpd5pdn' 'PZTcmar0OQwy4SLJB7guzmuPf8AbkGOQcIcsklQP6BADkF+b1X1c2WLdvJBogg21WvXr2vevU+' 'qm385fvGFTcMPDuSvdJwFERB/nIyP16d//AwCA49Wealg3G/HLkDGUb2YKiwF36fMWbbkR2FLf' 'mrMVbym8Zl2h1Gjwl1LiMyS5fX5kuKTimmU+rEdFqGQidA/rZhjCTYjyM38OemsPfK2jullECl' 'VrLcSqHm3zUuDe1D+Th0v5FzWeybbuUI0MZ8oWm8rkUMh4EfyvxPjdmBtMPxSA6kH4UQMstCph' 'nxjrZvD8OjIGqdwV/41ylNMV7P3zEMaDP6Hyt9ibFZ5znj4njoBXYvZIWzrXia/8h4XQ8fH5zC' 'RqxYtjWrgVsEy79nXOoFJ74icIERJoD8DeONZKKJTDPOlQSsyHxqXHUCP7JdP3w8UkcZzs0wqh' 'kv6CMO8wUjgT32guDr8TCcu8i4b8TwugLn541cQi7HKMmc1vq268Gs4dwltRbPP9k1jMlRA/Pt' 'Vq3drHc7VrPxuNto79e2rR2rVjVfyxvGzJ7V6HZqZiafMy7cbXZb5lT+opGtVh6a2bW6kbP0se' 'Z/ZkzzseWvP3/UWrn5c7wg9puF1+7989tGzrxgvmbumhnj7zK5WZ7k1/6QEdvB8HTkHh5FYm1l' '9SeicyRFvbttico4OgpGYUlUPE8wQiigoRwdg4UhuqEUQV9ER24owmA8cqRwgp4UmB4Gx3Lky5' '4Y+z05AooUlaHtEGHXkRCoKO7LUQgDibXSigEEOxKO7YsDKfoBNgnX5111a7vWaNdE34U3GkYu' 'N2XOQOxrGGZzr5k5jAvGbG4GYwPjK9CNZ7kM5jnTNL7k2RTWZ80p8+b8F4INAz2i8ciH4Ji5Ye' 'Q6IRiPlDJ0E8TI9g+lgFgkhj54KDS5uRDHjImD3SwEu5yCTAEya86lIFnMPzJXjM9z0xDnKsR9' '28zMl8TEW0QUiHEoWZI2zAxblWJxWQDwnKXd4HfVnDbfMK7wjNTLQ733wW06kYggcynIFCDvmu' '8lezLmm8B4PcHIaEguBZkC5LI5m+yZMq8B43KCQVQIMpOCEM4l00j2ZM23gGEkGFnsIch0CjIF' 'SM68ZHyauwBdrsM2C7DN+0L7NlskvgzKJGyKC6zodWy9ZvxLhqdki/dBvTj/DxmhAjR5JJ2iZ2' 'OMMDg6JUPD1cKhdNy+KxN3i80srL7wg2iCUDRiWj3Zt8cejgQkaIszHo1Ak52G932nEL9jrkEA' 'mg6tq38M5u/KUJy4uFSQYeCGoesfCptlPBX2SDLrU4kbEQyGY9KYjKeVy82weu+mIBlA3jNvpC' 'BZQD4xPzVuaEjG/BB73pt/Jzbo856sEUGeUGdTENr8uvlOCpIFZB4iVDVkyvwR9rw9f1Ps2U/d' 'wXgg/PHgABcf4eFMtkm0jwJ9A1O8p3LTTOdiCpIBJGdeTUGygFwz32JPyZiLFAC0p6hQRzzP9Z' 'QMo+ew9a8zPCVPWQK/T+b/IsNi6jgQxsLCKUYSgQxEnaNR4AdecOg6tieCEaIaDq1EsRLnpSPX' 'UeDJ0Ih9yQO5TRqN5I1Q2HAczz4FGlxZxk4Rm4MO3RlJe3LWLF7uAguYhswAcpmvdQzJAHLdXE' 'xBsoAsIS5uAIKLoGL+fPGZo4jznQhVpEmFQm0xMn+RLVbgGRmsBHk+nb9OtEbRJPxrolr4KS1q' 'KXHUKS1qCY76cQqSBaRgfmLsa0jGXMWea/M/E43Eg1y/5x67vTEMr+sHnS9wOzwZxbdXB+7+yB' '7IlBwZeNVq4lVT2qNXodYbKUgWkLz5pvFXGQ2aMm9h0/z8n2dSknAZopif0LmHY8eRYdgfe96p' 'lu0F0sBRADm2vTGd/ZCyqB+pqERqYB5iJ9MvCrefACkpnCA0ICv0UkrRVbl1Riky3S0+qwkkC8' 'iceT0xbta8zdf0BcZN6iqlIdd5rzBuFnLcPiMHBffbyZVVEOJLV/ZvYuNeMDc5HP32pcbV/FWo' 'joX7/7HvBei1eUYvCjab0OudFCQLCIXBLzRk2vwp9nwwv5nSqpxcs7h4FCiqHSXPy0w7rcldTE' 'EygORwoBNIFpD3kPcHGjJjVqgOmP9KdIIIZzqJw24kB6GgmpfKsiFnVGUnXH0dshwq0Yo0sw8P' 'R/LQpuxgCNsZBSGCFBJWa387TAk5AyErZ4ScgZAVCDmXgmQBocLjcw25aFax5y1EpGeFTGyEQ0' 'egdcYUxsXP99spnhfBs3qG50XwrHK1N4FkAXkTVcGyhuTMHeb5fups/MBfXltZSbimmOTAZOcM' 'kxyY7JxhkgOTHWJyMMN90rrxX2XjVW1n/o1nWquFTeNS0l1RSxVKuEwv5FYMLZWe5q8Z077tB6' 'rVmm6pydYPxptw5Wfbta0rCcV9Au1nHn166EZH44MSsMuHyI7+4UTEYXQ6lOFE0v/MZP52Kru7' 'v/WHqQ92FeX9uBF8ID3vCx+XsUN77v1ZybhkfoAM85sMSu9/nEVb8QG3FX8/K3iPE3hia9zvI4' 'uKZaGoIR327MiGn0Vy5BxxoQ13HNgoj9K9yMpP9AZh+U5JvKANOYqiYbhRLveQqr1gCEaxOUjX' 'oRZi+UAJUUasaMke8tzIPVBFt412g6puuL1uYwhy4Po2CkWSCyHjBNZD2ud/gzHkHAQ9FIYO35' 'Ai529wHrgRRSzwREil0HWk+4d+4HnBCZV5dJgubeKkDzoy2oBI9PPJM4KF5KPpxmowDqmOpHDC' 'VO0DtFlY0hYzqG5EIi+qoEL1B1FIc/R7z4gDfo5n4+BHpRcJAWYpW8RCQMfeGIIlchgTQf5Pci' 'T1Uy9wxvRoYceHVIb9A6qnBDwF9ZfthRNT8wFh0RBp6ROlGtLlnUTYR6AlgdK+5QeTtVCFy9Dg' 'EolJoQsGU6reOFNQeJJ+L6DKkMNoMAgQp5RN4J1UHcI5RR8LRtwg96MTchPtQXFr4WCXS441It' '/xlRdxXuKsdtdqi3Zzp/Og0qoJjPdbzftWtVYVWw+xWBPbzf2HLWv3bkfcbdartVZbVBpVQBud' 'lrXV7TRbbUMsVNrYusArlcZDUfvFfqvWbotmS1h7+3UL1EC+VWl0rFq7KKzGdr1btRq7RQEKot' 'HsGGjD96wO8DrNIrN9fp9o7oi9Wmv7LqaVLatudR4ywx2r0yBmO82WISpiv9LqWNvdeqUl9rut' '/SZae9KsarW36xVrr1ZFC9UAT1G7X2t0RPtupV4/q6ghmg8atRZJn1ZTbNUgZWWrXiNWrGfVat' 'W2O6TQZLQN40HAOvo5fonBCPaoQZ1K62FRE23Xft4FFhZFtbJX2YV2S6+yCg5mu9uq7ZHUMEW7' 'u9XuWJ1upyZ2m80qG7tda923tmvtTVFvttlg3XYNglQrnQqzBg2YC+sYb3XbFhvOanRqrVZ3n9' '6PCjjlB7AMpKxgb5Ut3GyQtuQrtWbrIZElO/AJFMWDuzXAW2RUtlaFzNCG1bY7aTQwhBGh0kRP' '0ajt1q3dWmO7RstNIvPAatcKODCrTQgWM4YPgGmXtaaDglyGGqdct8jnKawdUanet0hyjQ0PaF' 'vaXdhs23e1zfVrj0A2mcMoZy5gtEnA3KIeE/QjjD5k6Id6TNAfY7TF0Mt6TFBqFosMzegxQT/G' 'qMzQeEyjG/QEwVBDjwm6hNGPGPpjPf6Pd/nF4gedAuf/7V14eZJ907WoLYYBMh6HN3pgQuUth4' 'gi1Aci4Nj+qYJ/E/gcVbwAFZBBZRCQ7FEREYeyQI9eCmyEprHap+sDjqmoIZ1J5ogXKDFQscDz' 'dNvPL32KkEspFcUWwhYFN/Szchg4R/T+1e1si4Hb8zmyUzl4z/bHlA5Wi2L1zu2VYhywEf48OU' 'TkF7uoHQMEaD+RXpwcuSAnnyLGcYOBQH0O1oHtfI0o2eMC8FQCAmNQIKTUP3D9caSb5c9WEv28' 'gBrxurSHE5WBsRAOsF/2FhB6VSL2A+EBy9BoIrIPPH6t9KUku1JFzCXJkHKsSuxj9TYjvly7uY' 'ywjVNxfZAFDaL+1dLLiw86zzJjFlSPEj8rginlBrGysrK6zL+dlZUN/n1Eqt/Bz/Lq2vL6amdt' 'fePWHfyW7sQ/j0pi69Sgg0RywnlDwEiryNRRrUg4Cz36x+0Uv1BA6WOJpp3PVyUn8WVrZ9sQ6+' 'vrdya6nJyclFwZ9UvB6LA86jv0hzBK0dOoQJUb9Wb0/EEFs/hI1J7a1IiHmOihWN1AIcdvV6m7' 'wAxx4a1fiCdkmaXCk5IufSZISRG6qVYm5XMoo8f6gJd4e6NbrxcK5+Kxvy+tYHEi09qrZDqUEV' 'EJ+j37NCUbdEVSZwboL0V0rDmeQf84Oi4KFmjzj1XpuBQd0+xlGikklCAOappVeM8ZDddfqOED' '119fE092ZdQ+DdEI0nIl3HE92Tl7EDtWvdZBHhb9SIvxoj0f96NY0i5y1Gc3IbDzdSj+RCwtLS' 'lIoR+Veid3ETiqcBraVRCffy7W1wriO8Fr9eAkXortVi4jgEJeNPwhk6TLAlVTMSwsJQgqSq1+' '9vw1SqjR9tXPbt68eXv9s5VJ2NAPcV3ffRpTQTB7lkrpjzvMJaU/TKGMUubDop8CuqCUOK/wYK' 'JD5orpLKbosAMUzjjAzRc6wD372BZP1EGW9JM1oey5HurzlANQNEWkJSiO8sUbXuLm2JdAS748' '2Rq7HiripQIp1tYW0iyUYQqKFv0QTkPpjlhMmmtMpbpWmy1QKB0QZZZlYoNbL7RB+qWectb+KS' 'pxP1b8XPGXCs+eDa7D9sQaWKcIeK+NImzPHg4RFAGwfAVRPW2Rk2PKTmicySnPpHMVUHUmNTgs' '/6+ismJFGd2mZF5UZBSUmC18S9n0++VvB2hpjvAvgtb3nW8ppX2/8S0yK/6G837/ZelbKiLIkb' '//6tGCgbxNb3FqNxGyvRP7FMI/jd/UVIbsU27suYdomyjV4xw0p6JgVihzFTPMiVuRUxCz5Gz9' 'jRwFy0O711PNVXQSxNSk7RypSiWubqgq0hetqOsKSm+HAT1rUfKMty65JVnSwNXza6ACBCP+wV' 'BRVpwWHqFqGPf7CA30PurY6mVSsh9wfba0gLJoobB5BmqoMupXYxd1RwlhTD0LKWcIuWN1oagI' 'YQmvF5uSnh6oxlqyw4QbPVUaJEaBDsCnHtFXif55VyJD2mdYDW00pgmbA8jFlQ7lfcdBhSYO0E' 'YzT9qrWupYh/A5OagYDPp93EsuYnZQJUl114piYW1l9TbFzNVbnZXVjfWVjdVbpZVVmE95N0Iv' 'zZOgO7RDVKOMyfzR2CfV5K0ifX6+XdIXCAGr7YzcIe4PGTxdwNiCkoYIDn4pnUjVPm4YO7vyR3' 'Z/GImqyh7uUxRY7WabL9lS4ZyyrTQIvkGcsfl2SX+52y73AicsP5AH5Yko5Zbs4zr4jizvesGB' '7T1usgxhmQQqp5gU+GXnKIAbWHGkKfI9VyKJJ1RH8TfDePAkVkh/TdLaSoh/nopQ6gmiRp+3pj' 'SC1KWhimyky1rZcw9GMDAXo6WjaOB9xKN4b4FfJIzEkWMm9D4hbiw+XF4cLC/2Oot3Nxb3Nhbb' 'pcX+oxsot92v5YkbqsdiMtDklODPitq9oGezs94IIStME6f6HRWsenqK7PPVknrH03Hul9jJ0t' 'Ngmatoe+jygcRQVVsrWcvP02Y9YwaLa1X8GqJAhgwO+P3M1nqi3IfUQ74gaJoOpS9Htrpq8TUL' 'k3d6HWWRbpJvzz/wt47fTb49/2mGP2D9NiNak94v9n9wILdnO+MQnXT9YZxfgIg9enCj/xfxko' 'bBOK9jeAS5HQ/Ockwt1NXk4/C0kvFiCpQhEH0Rm4CyBKJPYv8e65Yxf0378vP/lBGNwF/2+QPB' 'sTzbdtpxe0Ud1/ltZ0NvTDox/m4Tqje8CTF+aQwj+k5+ZB9Lfq5PeDJpvTH+/yPcyeKMqIOM2+' 'xn7ae7q6L+Y5xrI/pm+OuzNsoo9XPm6ylQlkCmeTX+APDfdjTQIQ=='))) _INDEX = { f.name: { 'descriptor': f, 'services': {s.name: s for s in f.service}, } for f in FILE_DESCRIPTOR_SET.file } IsolatedServiceDescription = { 'file_descriptor_set': FILE_DESCRIPTOR_SET, 'file_descriptor': _INDEX[u'isolated.proto']['descriptor'], 'service_descriptor': _INDEX[u'isolated.proto']['services'][u'Isolated'], }
# Copyright (c) Twisted Matrix Laboratories. # See LICENSE for details. """ Test cases for twisted.protocols.stateful """ from twisted.trial.unittest import TestCase from twisted.protocols.test import test_basic from twisted.protocols.stateful import StatefulProtocol from struct import pack, unpack, calcsize class MyInt32StringReceiver(StatefulProtocol): """ A stateful Int32StringReceiver. """ MAX_LENGTH = 99999 structFormat = "!I" prefixLength = calcsize(structFormat) def getInitialState(self): return self._getHeader, 4 def lengthLimitExceeded(self, length): self.transport.loseConnection() def _getHeader(self, msg): length, = unpack("!i", msg) if length > self.MAX_LENGTH: self.lengthLimitExceeded(length) return return self._getString, length def _getString(self, msg): self.stringReceived(msg) return self._getHeader, 4 def stringReceived(self, msg): """ Override this. """ raise NotImplementedError def sendString(self, data): """ Send an int32-prefixed string to the other end of the connection. """ self.transport.write(pack(self.structFormat, len(data)) + data) class TestInt32(MyInt32StringReceiver): def connectionMade(self): self.received = [] def stringReceived(self, s): self.received.append(s) MAX_LENGTH = 50 closed = 0 def connectionLost(self, reason): self.closed = 1 class Int32TestCase(TestCase, test_basic.IntNTestCaseMixin): protocol = TestInt32 strings = ["a", "b" * 16] illegalStrings = ["\x10\x00\x00\x00aaaaaa"] partialStrings = ["\x00\x00\x00", "hello there", ""] def test_bigReceive(self): r = self.getProtocol() big = "" for s in self.strings * 4: big += pack("!i", len(s)) + s r.dataReceived(big) self.assertEqual(r.received, self.strings * 4)
# -*- coding: utf-8 -*- ############################################################################## # # OpenERP, Open Source Management Solution # Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>). # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## { 'name': 'Marketing Campaigns', 'version': '1.1', 'depends': ['marketing', 'document', 'email_template', 'decimal_precision' ], 'author': 'OpenERP SA', 'category': 'Marketing', 'description': """ This module provides leads automation through marketing campaigns (campaigns can in fact be defined on any resource, not just CRM Leads). ========================================================================================================================================= The campaigns are dynamic and multi-channels. The process is as follows: ------------------------------------------------------------------------ * Design marketing campaigns like workflows, including email templates to send, reports to print and send by email, custom actions * Define input segments that will select the items that should enter the campaign (e.g leads from certain countries.) * Run your campaign in simulation mode to test it real-time or accelerated, and fine-tune it * You may also start the real campaign in manual mode, where each action requires manual validation * Finally launch your campaign live, and watch the statistics as the campaign does everything fully automatically. While the campaign runs you can of course continue to fine-tune the parameters, input segments, workflow. **Note:** If you need demo data, you can install the marketing_campaign_crm_demo module, but this will also install the CRM application as it depends on CRM Leads. """, 'website': 'https://www.odoo.com/page/lead-automation', 'data': [ 'marketing_campaign_view.xml', 'marketing_campaign_data.xml', 'marketing_campaign_workflow.xml', 'report/campaign_analysis_view.xml', 'security/marketing_campaign_security.xml', 'security/ir.model.access.csv' ], 'demo': ['marketing_campaign_demo.xml'], 'test': ['test/marketing_campaign.yml'], 'installable': True, 'auto_install': False, } # vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
# coding=utf8 import os import sys import time import unittest import vk from vk.utils import HandyList, make_handy, HandyDict sys.path.append(os.path.join(os.path.dirname(__file__), '..')) # copy to test_props.py and fill it APP_ID = '4643961' # aka API/Client id USER_LOGIN = 'tsvstar@gmail.com' # user email or phone number USER_PASSWORD = 'byyfrbtd' from test_props import APP_ID, USER_LOGIN, USER_PASSWORD class VkTestCase(unittest.TestCase): def setUp(self): self.vk_api = vk.API(APP_ID, USER_LOGIN, USER_PASSWORD) self.vk_token_api = vk.API(access_token=self.vk_api.access_token) def test_get_server_time(self): time_1 = time.time() - 1 time_2 = time_1 + 10 server_time = self.vk_api.getServerTime() self.assertTrue(time_1 <= server_time <= time_2) def test_get_server_time_via_token_api(self): time_1 = time.time() - 1 time_2 = time_1 + 10 server_time = self.vk_token_api.getServerTime() self.assertTrue(time_1 <= server_time <= time_2) def test_get_profiles_via_token(self): profiles = self.vk_api.users.get(user_id=1) profiles = make_handy(profiles) self.assertEqual(profiles.first.last_name, u'') class HandyContainersTestCase(unittest.TestCase): def test_list(self): handy_list = make_handy([1, 2, 3]) self.assertIsInstance(handy_list, HandyList) self.assertEqual(handy_list.first, 1) def test_handy_dict(self): handy_dict = make_handy({'key1': 'val1', 'key2': 'val2'}) self.assertIsInstance(handy_dict, HandyDict) self.assertEqual(handy_dict.key1, 'val1') if __name__ == '__main__': unittest.main()
# -*- coding: utf-8 -*- # This file is part of pygal # # A python svg graph plotting library # Copyright  2012-2015 Kozea # # This library is free software: you can redistribute it and/or modify it under # the terms of the GNU Lesser General Public License as published by the Free # Software Foundation, either version 3 of the License, or (at your option) any # later version. # # This library is distributed in the hope that it will be useful, but WITHOUT # ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS # FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more # details. # # You should have received a copy of the GNU Lesser General Public License # along with pygal. If not, see <http://www.gnu.org/licenses/>. """Bar chart related tests""" from pygal import Bar def test_simple_bar(): """Simple bar test""" bar = Bar() rng = [-3, -32, -39] bar.add('test1', rng) bar.add('test2', map(abs, rng)) bar.x_labels = map(str, rng) bar.title = "Bar test" q = bar.render_pyquery() assert len(q(".axis.x")) == 1 assert len(q(".axis.y")) == 1 assert len(q(".legend")) == 2 assert len(q(".plot .series rect")) == 2 * 3
# QEMU qtest library # # Copyright (C) 2015 Red Hat Inc. # # Authors: # Fam Zheng <famz@redhat.com> # # This work is licensed under the terms of the GNU GPL, version 2. See # the COPYING file in the top-level directory. # # Based on qmp.py. # import errno import socket class QEMUQtestProtocol(object): def __init__(self, address, server=False): """ Create a QEMUQtestProtocol object. @param address: QEMU address, can be either a unix socket path (string) or a tuple in the form ( address, port ) for a TCP connection @param server: server mode, listens on the socket (bool) @raise socket.error on socket connection errors @note No connection is established, this is done by the connect() or accept() methods """ self._address = address self._sock = self._get_sock() if server: self._sock.bind(self._address) self._sock.listen(1) def _get_sock(self): if isinstance(self._address, tuple): family = socket.AF_INET else: family = socket.AF_UNIX return socket.socket(family, socket.SOCK_STREAM) def connect(self): """ Connect to the qtest socket. @raise socket.error on socket connection errors """ self._sock.connect(self._address) def accept(self): """ Await connection from QEMU. @raise socket.error on socket connection errors """ self._sock, _ = self._sock.accept() def cmd(self, qtest_cmd): """ Send a qtest command on the wire. @param qtest_cmd: qtest command text to be sent """ self._sock.sendall(qtest_cmd + "\n") def close(self): self._sock.close() def settimeout(self, timeout): self._sock.settimeout(timeout)
# -*- coding: utf-8 -*- ############################################################################## # # OpenERP, Open Source Management Solution # Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>). # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## import time from openerp.osv import fields, osv class account_budget_crossvered_report(osv.osv_memory): _name = "account.budget.crossvered.report" _description = "Account Budget crossvered report" _columns = { 'date_from': fields.date('Start of period', required=True), 'date_to': fields.date('End of period', required=True), } _defaults = { 'date_from': lambda *a: time.strftime('%Y-01-01'), 'date_to': lambda *a: time.strftime('%Y-%m-%d'), } def check_report(self, cr, uid, ids, context=None): if context is None: context = {} data = self.read(cr, uid, ids, context=context)[0] datas = { 'ids': context.get('active_ids', []), 'model': 'crossovered.budget', 'form': data } datas['form']['ids'] = datas['ids'] datas['form']['report'] = 'analytic-full' return self.pool['report'].get_action(cr, uid, [], 'account_budget.report_crossoveredbudget', data=datas, context=context) # vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
from django.db import models class Author(models.Model): first_name = models.CharField(max_length=255) last_name = models.CharField(max_length=255) dob = models.DateField() def __init__(self, *args, **kwargs): super(Author, self).__init__(*args, **kwargs) # Protect against annotations being passed to __init__ -- # this'll make the test suite get angry if annotations aren't # treated differently than fields. for k in kwargs: assert k in [f.attname for f in self._meta.fields], \ "Author.__init__ got an unexpected parameter: %s" % k class Book(models.Model): title = models.CharField(max_length=255) author = models.ForeignKey(Author) paperback = models.BooleanField(default=False) opening_line = models.TextField() class Coffee(models.Model): brand = models.CharField(max_length=255, db_column="name") class Reviewer(models.Model): reviewed = models.ManyToManyField(Book) class FriendlyAuthor(Author): pass
#!/usr/bin/env python """ ===================== Lasso path using LARS ===================== Computes Lasso Path along the regularization parameter using the LARS algorithm on the diabetes dataset. Each color represents a different feature of the coefficient vector, and this is displayed as a function of the regularization parameter. """ print(__doc__) # Author: Fabian Pedregosa <fabian.pedregosa@inria.fr> # Alexandre Gramfort <alexandre.gramfort@inria.fr> # License: BSD 3 clause import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from sklearn import datasets diabetes = datasets.load_diabetes() X = diabetes.data y = diabetes.target print("Computing regularization path using the LARS ...") alphas, _, coefs = linear_model.lars_path(X, y, method='lasso', verbose=True) xx = np.sum(np.abs(coefs.T), axis=1) xx /= xx[-1] plt.plot(xx, coefs.T) ymin, ymax = plt.ylim() plt.vlines(xx, ymin, ymax, linestyle='dashed') plt.xlabel('|coef| / max|coef|') plt.ylabel('Coefficients') plt.title('LASSO Path') plt.axis('tight') plt.show()
# Copyright (c) 2012 Mitch Garnaat http://garnaat.org/ # Copyright (c) 2012 Amazon.com, Inc. or its affiliates. All Rights Reserved # # Permission is hereby granted, free of charge, to any person obtaining a # copy of this software and associated documentation files (the # "Software"), to deal in the Software without restriction, including # without limitation the rights to use, copy, modify, merge, publish, dis- # tribute, sublicense, and/or sell copies of the Software, and to permit # persons to whom the Software is furnished to do so, subject to the fol- # lowing conditions: # # The above copyright notice and this permission notice shall be included # in all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS # OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL- # ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT # SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, # WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS # IN THE SOFTWARE. # class CORSRule(object): """ CORS rule for a bucket. :ivar id: A unique identifier for the rule. The ID value can be up to 255 characters long. The IDs help you find a rule in the configuration. :ivar allowed_methods: An HTTP method that you want to allow the origin to execute. Each CORSRule must identify at least one origin and one method. Valid values are: GET|PUT|HEAD|POST|DELETE :ivar allowed_origin: An origin that you want to allow cross-domain requests from. This can contain at most one * wild character. Each CORSRule must identify at least one origin and one method. The origin value can include at most one '*' wild character. For example, "http://*.example.com". You can also specify only * as the origin value allowing all origins cross-domain access. :ivar allowed_header: Specifies which headers are allowed in a pre-flight OPTIONS request via the Access-Control-Request-Headers header. Each header name specified in the Access-Control-Request-Headers header must have a corresponding entry in the rule. Amazon S3 will send only the allowed headers in a response that were requested. This can contain at most one * wild character. :ivar max_age_seconds: The time in seconds that your browser is to cache the preflight response for the specified resource. :ivar expose_header: One or more headers in the response that you want customers to be able to access from their applications (for example, from a JavaScript XMLHttpRequest object). You add one ExposeHeader element in the rule for each header. """ def __init__(self, allowed_method=None, allowed_origin=None, id=None, allowed_header=None, max_age_seconds=None, expose_header=None): if allowed_method is None: allowed_method = [] self.allowed_method = allowed_method if allowed_origin is None: allowed_origin = [] self.allowed_origin = allowed_origin self.id = id if allowed_header is None: allowed_header = [] self.allowed_header = allowed_header self.max_age_seconds = max_age_seconds if expose_header is None: expose_header = [] self.expose_header = expose_header def __repr__(self): return '<Rule: %s>' % self.id def startElement(self, name, attrs, connection): return None def endElement(self, name, value, connection): if name == 'ID': self.id = value elif name == 'AllowedMethod': self.allowed_method.append(value) elif name == 'AllowedOrigin': self.allowed_origin.append(value) elif name == 'AllowedHeader': self.allowed_header.append(value) elif name == 'MaxAgeSeconds': self.max_age_seconds = int(value) elif name == 'ExposeHeader': self.expose_header.append(value) else: setattr(self, name, value) def to_xml(self): s = '<CORSRule>' for allowed_method in self.allowed_method: s += '<AllowedMethod>%s</AllowedMethod>' % allowed_method for allowed_origin in self.allowed_origin: s += '<AllowedOrigin>%s</AllowedOrigin>' % allowed_origin for allowed_header in self.allowed_header: s += '<AllowedHeader>%s</AllowedHeader>' % allowed_header for expose_header in self.expose_header: s += '<ExposeHeader>%s</ExposeHeader>' % expose_header if self.max_age_seconds: s += '<MaxAgeSeconds>%d</MaxAgeSeconds>' % self.max_age_seconds if self.id: s += '<ID>%s</ID>' % self.id s += '</CORSRule>' return s class CORSConfiguration(list): """ A container for the rules associated with a CORS configuration. """ def startElement(self, name, attrs, connection): if name == 'CORSRule': rule = CORSRule() self.append(rule) return rule return None def endElement(self, name, value, connection): setattr(self, name, value) def to_xml(self): """ Returns a string containing the XML version of the Lifecycle configuration as defined by S3. """ s = '<CORSConfiguration>' for rule in self: s += rule.to_xml() s += '</CORSConfiguration>' return s def add_rule(self, allowed_method, allowed_origin, id=None, allowed_header=None, max_age_seconds=None, expose_header=None): """ Add a rule to this CORS configuration. This only adds the rule to the local copy. To install the new rule(s) on the bucket, you need to pass this CORS config object to the set_cors method of the Bucket object. :type allowed_methods: list of str :param allowed_methods: An HTTP method that you want to allow the origin to execute. Each CORSRule must identify at least one origin and one method. Valid values are: GET|PUT|HEAD|POST|DELETE :type allowed_origin: list of str :param allowed_origin: An origin that you want to allow cross-domain requests from. This can contain at most one * wild character. Each CORSRule must identify at least one origin and one method. The origin value can include at most one '*' wild character. For example, "http://*.example.com". You can also specify only * as the origin value allowing all origins cross-domain access. :type id: str :param id: A unique identifier for the rule. The ID value can be up to 255 characters long. The IDs help you find a rule in the configuration. :type allowed_header: list of str :param allowed_header: Specifies which headers are allowed in a pre-flight OPTIONS request via the Access-Control-Request-Headers header. Each header name specified in the Access-Control-Request-Headers header must have a corresponding entry in the rule. Amazon S3 will send only the allowed headers in a response that were requested. This can contain at most one * wild character. :type max_age_seconds: int :param max_age_seconds: The time in seconds that your browser is to cache the preflight response for the specified resource. :type expose_header: list of str :param expose_header: One or more headers in the response that you want customers to be able to access from their applications (for example, from a JavaScript XMLHttpRequest object). You add one ExposeHeader element in the rule for each header. """ if not isinstance(allowed_method, (list, tuple)): allowed_method = [allowed_method] if not isinstance(allowed_origin, (list, tuple)): allowed_origin = [allowed_origin] if not isinstance(allowed_origin, (list, tuple)): if allowed_origin is None: allowed_origin = [] else: allowed_origin = [allowed_origin] if not isinstance(expose_header, (list, tuple)): if expose_header is None: expose_header = [] else: expose_header = [expose_header] rule = CORSRule(allowed_method, allowed_origin, id, allowed_header, max_age_seconds, expose_header) self.append(rule)
# -*- coding: utf-8 -*- """ pygments.styles.perldoc ~~~~~~~~~~~~~~~~~~~~~~~ Style similar to the style used in the `perldoc`_ code blocks. .. _perldoc: http://perldoc.perl.org/ :copyright: Copyright 2006-2013 by the Pygments team, see AUTHORS. :license: BSD, see LICENSE for details. """ from pygments.style import Style from pygments.token import Keyword, Name, Comment, String, Error, \ Number, Operator, Generic, Whitespace class PerldocStyle(Style): """ Style similar to the style used in the perldoc code blocks. """ background_color = '#eeeedd' default_style = '' styles = { Whitespace: '#bbbbbb', Comment: '#228B22', Comment.Preproc: '#1e889b', Comment.Special: '#8B008B bold', String: '#CD5555', String.Heredoc: '#1c7e71 italic', String.Regex: '#B452CD', String.Other: '#cb6c20', String.Regex: '#1c7e71', Number: '#B452CD', Operator.Word: '#8B008B', Keyword: '#8B008B bold', Keyword.Type: '#a7a7a7', Name.Class: '#008b45 bold', Name.Exception: '#008b45 bold', Name.Function: '#008b45', Name.Namespace: '#008b45 underline', Name.Variable: '#00688B', Name.Constant: '#00688B', Name.Decorator: '#707a7c', Name.Tag: '#8B008B bold', Name.Attribute: '#658b00', Name.Builtin: '#658b00', Generic.Heading: 'bold #000080', Generic.Subheading: 'bold #800080', Generic.Deleted: '#aa0000', Generic.Inserted: '#00aa00', Generic.Error: '#aa0000', Generic.Emph: 'italic', Generic.Strong: 'bold', Generic.Prompt: '#555555', Generic.Output: '#888888', Generic.Traceback: '#aa0000', Error: 'bg:#e3d2d2 #a61717' }
try: import distutils.msvc9compiler except ImportError: pass unpatched = dict() def patch_for_specialized_compiler(): """ Patch functions in distutils.msvc9compiler to use the standalone compiler build for Python (Windows only). Fall back to original behavior when the standalone compiler is not available. """ if 'distutils' not in globals(): # The module isn't available to be patched return if unpatched: # Already patched return unpatched.update(vars(distutils.msvc9compiler)) distutils.msvc9compiler.find_vcvarsall = find_vcvarsall distutils.msvc9compiler.query_vcvarsall = query_vcvarsall def find_vcvarsall(version): Reg = distutils.msvc9compiler.Reg VC_BASE = r'Software\%sMicrosoft\DevDiv\VCForPython\%0.1f' key = VC_BASE % ('', version) try: # Per-user installs register the compiler path here productdir = Reg.get_value(key, "installdir") except KeyError: try: # All-user installs on a 64-bit system register here key = VC_BASE % ('Wow6432Node\\', version) productdir = Reg.get_value(key, "installdir") except KeyError: productdir = None if productdir: import os vcvarsall = os.path.join(productdir, "vcvarsall.bat") if os.path.isfile(vcvarsall): return vcvarsall return unpatched['find_vcvarsall'](version) def query_vcvarsall(version, *args, **kwargs): try: return unpatched['query_vcvarsall'](version, *args, **kwargs) except distutils.errors.DistutilsPlatformError as exc: if exc and "vcvarsall.bat" in exc.args[0]: message = 'Microsoft Visual C++ %0.1f is required (%s).' % (version, exc.args[0]) if int(version) == 9: # This redirection link is maintained by Microsoft. # Contact vspython@microsoft.com if it needs updating. raise distutils.errors.DistutilsPlatformError( message + ' Get it from http://aka.ms/vcpython27' ) raise distutils.errors.DistutilsPlatformError(message) raise
from __future__ import unicode_literals from django import forms from django.forms.util import flatatt from django.template import loader from django.utils.datastructures import SortedDict from django.utils.html import format_html, format_html_join from django.utils.http import int_to_base36 from django.utils.safestring import mark_safe from django.utils.text import capfirst from django.utils.translation import ugettext, ugettext_lazy as _ from django.contrib.auth import authenticate, get_user_model from django.contrib.auth.models import User from django.contrib.auth.hashers import UNUSABLE_PASSWORD, identify_hasher from django.contrib.auth.tokens import default_token_generator from django.contrib.sites.models import get_current_site UNMASKED_DIGITS_TO_SHOW = 6 mask_password = lambda p: "%s%s" % (p[:UNMASKED_DIGITS_TO_SHOW], "*" * max(len(p) - UNMASKED_DIGITS_TO_SHOW, 0)) class ReadOnlyPasswordHashWidget(forms.Widget): def render(self, name, value, attrs): encoded = value final_attrs = self.build_attrs(attrs) if encoded == '' or encoded == UNUSABLE_PASSWORD: summary = mark_safe("<strong>%s</strong>" % ugettext("No password set.")) else: try: hasher = identify_hasher(encoded) except ValueError: summary = mark_safe("<strong>%s</strong>" % ugettext( "Invalid password format or unknown hashing algorithm.")) else: summary = format_html_join('', "<strong>{0}</strong>: {1} ", ((ugettext(key), value) for key, value in hasher.safe_summary(encoded).items()) ) return format_html("<div{0}>{1}</div>", flatatt(final_attrs), summary) class ReadOnlyPasswordHashField(forms.Field): widget = ReadOnlyPasswordHashWidget def __init__(self, *args, **kwargs): kwargs.setdefault("required", False) super(ReadOnlyPasswordHashField, self).__init__(*args, **kwargs) class UserCreationForm(forms.ModelForm): """ A form that creates a user, with no privileges, from the given username and password. """ error_messages = { 'duplicate_username': _("A user with that username already exists."), 'password_mismatch': _("The two password fields didn't match."), } username = forms.RegexField(label=_("Username"), max_length=30, regex=r'^[\w.@+-]+$', help_text=_("Required. 30 characters or fewer. Letters, digits and " "@/./+/-/_ only."), error_messages={ 'invalid': _("This value may contain only letters, numbers and " "@/./+/-/_ characters.")}) password1 = forms.CharField(label=_("Password"), widget=forms.PasswordInput) password2 = forms.CharField(label=_("Password confirmation"), widget=forms.PasswordInput, help_text=_("Enter the same password as above, for verification.")) class Meta: model = User fields = ("username",) def clean_username(self): # Since User.username is unique, this check is redundant, # but it sets a nicer error message than the ORM. See #13147. username = self.cleaned_data["username"] try: User.objects.get(username=username) except User.DoesNotExist: return username raise forms.ValidationError(self.error_messages['duplicate_username']) def clean_password2(self): password1 = self.cleaned_data.get("password1") password2 = self.cleaned_data.get("password2") if password1 and password2 and password1 != password2: raise forms.ValidationError( self.error_messages['password_mismatch']) return password2 def save(self, commit=True): user = super(UserCreationForm, self).save(commit=False) user.set_password(self.cleaned_data["password1"]) if commit: user.save() return user class UserChangeForm(forms.ModelForm): username = forms.RegexField( label=_("Username"), max_length=30, regex=r"^[\w.@+-]+$", help_text=_("Required. 30 characters or fewer. Letters, digits and " "@/./+/-/_ only."), error_messages={ 'invalid': _("This value may contain only letters, numbers and " "@/./+/-/_ characters.")}) password = ReadOnlyPasswordHashField(label=_("Password"), help_text=_("Raw passwords are not stored, so there is no way to see " "this user's password, but you can change the password " "using <a href=\"password/\">this form</a>.")) def clean_password(self): return self.initial["password"] class Meta: model = User def __init__(self, *args, **kwargs): super(UserChangeForm, self).__init__(*args, **kwargs) f = self.fields.get('user_permissions', None) if f is not None: f.queryset = f.queryset.select_related('content_type') class AuthenticationForm(forms.Form): """ Base class for authenticating users. Extend this to get a form that accepts username/password logins. """ username = forms.CharField(max_length=30) password = forms.CharField(label=_("Password"), widget=forms.PasswordInput) error_messages = { 'invalid_login': _("Please enter a correct username and password. " "Note that both fields are case-sensitive."), 'no_cookies': _("Your Web browser doesn't appear to have cookies " "enabled. Cookies are required for logging in."), 'inactive': _("This account is inactive."), } def __init__(self, request=None, *args, **kwargs): """ If request is passed in, the form will validate that cookies are enabled. Note that the request (a HttpRequest object) must have set a cookie with the key TEST_COOKIE_NAME and value TEST_COOKIE_VALUE before running this validation. """ self.request = request self.user_cache = None super(AuthenticationForm, self).__init__(*args, **kwargs) # Set the label for the "username" field. UserModel = get_user_model() username_field = UserModel._meta.get_field(getattr(UserModel, 'USERNAME_FIELD', 'username')) self.fields['username'].label = capfirst(username_field.verbose_name) def clean(self): username = self.cleaned_data.get('username') password = self.cleaned_data.get('password') if username and password: self.user_cache = authenticate(username=username, password=password) if self.user_cache is None: raise forms.ValidationError( self.error_messages['invalid_login']) elif not self.user_cache.is_active: raise forms.ValidationError(self.error_messages['inactive']) self.check_for_test_cookie() return self.cleaned_data def check_for_test_cookie(self): if self.request and not self.request.session.test_cookie_worked(): raise forms.ValidationError(self.error_messages['no_cookies']) def get_user_id(self): if self.user_cache: return self.user_cache.id return None def get_user(self): return self.user_cache class PasswordResetForm(forms.Form): error_messages = { 'unknown': _("That email address doesn't have an associated " "user account. Are you sure you've registered?"), 'unusable': _("The user account associated with this email " "address cannot reset the password."), } email = forms.EmailField(label=_("Email"), max_length=75) def clean_email(self): """ Validates that an active user exists with the given email address. """ UserModel = get_user_model() email = self.cleaned_data["email"] self.users_cache = UserModel.objects.filter(email__iexact=email, is_active=True) if not len(self.users_cache): raise forms.ValidationError(self.error_messages['unknown']) if any((user.password == UNUSABLE_PASSWORD) for user in self.users_cache): raise forms.ValidationError(self.error_messages['unusable']) return email def save(self, domain_override=None, subject_template_name='registration/password_reset_subject.txt', email_template_name='registration/password_reset_email.html', use_https=False, token_generator=default_token_generator, from_email=None, request=None): """ Generates a one-use only link for resetting password and sends to the user. """ from django.core.mail import send_mail for user in self.users_cache: if not domain_override: current_site = get_current_site(request) site_name = current_site.name domain = current_site.domain else: site_name = domain = domain_override c = { 'email': user.email, 'domain': domain, 'site_name': site_name, 'uid': int_to_base36(user.id), 'user': user, 'token': token_generator.make_token(user), 'protocol': use_https and 'https' or 'http', } subject = loader.render_to_string(subject_template_name, c) # Email subject *must not* contain newlines subject = ''.join(subject.splitlines()) email = loader.render_to_string(email_template_name, c) send_mail(subject, email, from_email, [user.email]) class SetPasswordForm(forms.Form): """ A form that lets a user change set his/her password without entering the old password """ error_messages = { 'password_mismatch': _("The two password fields didn't match."), } new_password1 = forms.CharField(label=_("New password"), widget=forms.PasswordInput) new_password2 = forms.CharField(label=_("New password confirmation"), widget=forms.PasswordInput) def __init__(self, user, *args, **kwargs): self.user = user super(SetPasswordForm, self).__init__(*args, **kwargs) def clean_new_password2(self): password1 = self.cleaned_data.get('new_password1') password2 = self.cleaned_data.get('new_password2') if password1 and password2: if password1 != password2: raise forms.ValidationError( self.error_messages['password_mismatch']) return password2 def save(self, commit=True): self.user.set_password(self.cleaned_data['new_password1']) if commit: self.user.save() return self.user class PasswordChangeForm(SetPasswordForm): """ A form that lets a user change his/her password by entering their old password. """ error_messages = dict(SetPasswordForm.error_messages, **{ 'password_incorrect': _("Your old password was entered incorrectly. " "Please enter it again."), }) old_password = forms.CharField(label=_("Old password"), widget=forms.PasswordInput) def clean_old_password(self): """ Validates that the old_password field is correct. """ old_password = self.cleaned_data["old_password"] if not self.user.check_password(old_password): raise forms.ValidationError( self.error_messages['password_incorrect']) return old_password PasswordChangeForm.base_fields = SortedDict([ (k, PasswordChangeForm.base_fields[k]) for k in ['old_password', 'new_password1', 'new_password2'] ]) class AdminPasswordChangeForm(forms.Form): """ A form used to change the password of a user in the admin interface. """ error_messages = { 'password_mismatch': _("The two password fields didn't match."), } password1 = forms.CharField(label=_("Password"), widget=forms.PasswordInput) password2 = forms.CharField(label=_("Password (again)"), widget=forms.PasswordInput) def __init__(self, user, *args, **kwargs): self.user = user super(AdminPasswordChangeForm, self).__init__(*args, **kwargs) def clean_password2(self): password1 = self.cleaned_data.get('password1') password2 = self.cleaned_data.get('password2') if password1 and password2: if password1 != password2: raise forms.ValidationError( self.error_messages['password_mismatch']) return password2 def save(self, commit=True): """ Saves the new password. """ self.user.set_password(self.cleaned_data["password1"]) if commit: self.user.save() return self.user
# util/topological.py # Copyright (C) 2005-2013 the SQLAlchemy authors and contributors <see AUTHORS file> # # This module is part of SQLAlchemy and is released under # the MIT License: http://www.opensource.org/licenses/mit-license.php """Topological sorting algorithms.""" from ..exc import CircularDependencyError from .. import util __all__ = ['sort', 'sort_as_subsets', 'find_cycles'] def sort_as_subsets(tuples, allitems): edges = util.defaultdict(set) for parent, child in tuples: edges[child].add(parent) todo = set(allitems) while todo: output = set() for node in list(todo): if not todo.intersection(edges[node]): output.add(node) if not output: raise CircularDependencyError( "Circular dependency detected.", find_cycles(tuples, allitems), _gen_edges(edges) ) todo.difference_update(output) yield output def sort(tuples, allitems): """sort the given list of items by dependency. 'tuples' is a list of tuples representing a partial ordering. """ for set_ in sort_as_subsets(tuples, allitems): for s in set_: yield s def find_cycles(tuples, allitems): # adapted from: # http://neopythonic.blogspot.com/2009/01/detecting-cycles-in-directed-graph.html edges = util.defaultdict(set) for parent, child in tuples: edges[parent].add(child) nodes_to_test = set(edges) output = set() # we'd like to find all nodes that are # involved in cycles, so we do the full # pass through the whole thing for each # node in the original list. # we can go just through parent edge nodes. # if a node is only a child and never a parent, # by definition it can't be part of a cycle. same # if it's not in the edges at all. for node in nodes_to_test: stack = [node] todo = nodes_to_test.difference(stack) while stack: top = stack[-1] for node in edges[top]: if node in stack: cyc = stack[stack.index(node):] todo.difference_update(cyc) output.update(cyc) if node in todo: stack.append(node) todo.remove(node) break else: node = stack.pop() return output def _gen_edges(edges): return set([ (right, left) for left in edges for right in edges[left] ])
"""Running tests""" import sys import time from . import result from .signals import registerResult __unittest = True class _WritelnDecorator(object): """Used to decorate file-like objects with a handy 'writeln' method""" def __init__(self,stream): self.stream = stream def __getattr__(self, attr): if attr in ('stream', '__getstate__'): raise AttributeError(attr) return getattr(self.stream,attr) def writeln(self, arg=None): if arg: self.write(arg) self.write('\n') # text-mode streams translate to \r\n if needed class TextTestResult(result.TestResult): """A test result class that can print formatted text results to a stream. Used by TextTestRunner. """ separator1 = '=' * 70 separator2 = '-' * 70 def __init__(self, stream, descriptions, verbosity): super(TextTestResult, self).__init__() self.stream = stream self.showAll = verbosity > 1 self.dots = verbosity == 1 self.descriptions = descriptions def getDescription(self, test): doc_first_line = test.shortDescription() if self.descriptions and doc_first_line: return '\n'.join((str(test), doc_first_line)) else: return str(test) def startTest(self, test): super(TextTestResult, self).startTest(test) if self.showAll: self.stream.write(self.getDescription(test)) self.stream.write(" ... ") self.stream.flush() def addSuccess(self, test): super(TextTestResult, self).addSuccess(test) if self.showAll: self.stream.writeln("ok") elif self.dots: self.stream.write('.') self.stream.flush() def addError(self, test, err): super(TextTestResult, self).addError(test, err) if self.showAll: self.stream.writeln("ERROR") elif self.dots: self.stream.write('E') self.stream.flush() def addFailure(self, test, err): super(TextTestResult, self).addFailure(test, err) if self.showAll: self.stream.writeln("FAIL") elif self.dots: self.stream.write('F') self.stream.flush() def addSkip(self, test, reason): super(TextTestResult, self).addSkip(test, reason) if self.showAll: self.stream.writeln("skipped {0!r}".format(reason)) elif self.dots: self.stream.write("s") self.stream.flush() def addExpectedFailure(self, test, err): super(TextTestResult, self).addExpectedFailure(test, err) if self.showAll: self.stream.writeln("expected failure") elif self.dots: self.stream.write("x") self.stream.flush() def addUnexpectedSuccess(self, test): super(TextTestResult, self).addUnexpectedSuccess(test) if self.showAll: self.stream.writeln("unexpected success") elif self.dots: self.stream.write("u") self.stream.flush() def printErrors(self): if self.dots or self.showAll: self.stream.writeln() self.printErrorList('ERROR', self.errors) self.printErrorList('FAIL', self.failures) def printErrorList(self, flavour, errors): for test, err in errors: self.stream.writeln(self.separator1) self.stream.writeln("%s: %s" % (flavour,self.getDescription(test))) self.stream.writeln(self.separator2) self.stream.writeln("%s" % err) class TextTestRunner(object): """A test runner class that displays results in textual form. It prints out the names of tests as they are run, errors as they occur, and a summary of the results at the end of the test run. """ resultclass = TextTestResult def __init__(self, stream=sys.stderr, descriptions=True, verbosity=1, failfast=False, buffer=False, resultclass=None): self.stream = _WritelnDecorator(stream) self.descriptions = descriptions self.verbosity = verbosity self.failfast = failfast self.buffer = buffer if resultclass is not None: self.resultclass = resultclass def _makeResult(self): return self.resultclass(self.stream, self.descriptions, self.verbosity) def run(self, test): "Run the given test case or test suite." result = self._makeResult() registerResult(result) result.failfast = self.failfast result.buffer = self.buffer startTime = time.time() startTestRun = getattr(result, 'startTestRun', None) if startTestRun is not None: startTestRun() try: test(result) finally: stopTestRun = getattr(result, 'stopTestRun', None) if stopTestRun is not None: stopTestRun() stopTime = time.time() timeTaken = stopTime - startTime result.printErrors() if hasattr(result, 'separator2'): self.stream.writeln(result.separator2) run = result.testsRun self.stream.writeln("Ran %d test%s in %.3fs" % (run, run != 1 and "s" or "", timeTaken)) self.stream.writeln() expectedFails = unexpectedSuccesses = skipped = 0 try: results = map(len, (result.expectedFailures, result.unexpectedSuccesses, result.skipped)) except AttributeError: pass else: expectedFails, unexpectedSuccesses, skipped = results infos = [] if not result.wasSuccessful(): self.stream.write("FAILED") failed, errored = map(len, (result.failures, result.errors)) if failed: infos.append("failures=%d" % failed) if errored: infos.append("errors=%d" % errored) else: self.stream.write("OK") if skipped: infos.append("skipped=%d" % skipped) if expectedFails: infos.append("expected failures=%d" % expectedFails) if unexpectedSuccesses: infos.append("unexpected successes=%d" % unexpectedSuccesses) if infos: self.stream.writeln(" (%s)" % (", ".join(infos),)) else: self.stream.write("\n") return result
# # Licensed to the Apache Software Foundation (ASF) under one or more # contributor license agreements. See the NOTICE file distributed with # this work for additional information regarding copyright ownership. # The ASF licenses this file to You under the Apache License, Version 2.0 # (the "License"); you may not use this file except in compliance with # the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # import os import random import stat import sys import tempfile import time import unittest from pyspark import SparkConf, SparkContext, TaskContext, BarrierTaskContext from pyspark.testing.utils import PySparkTestCase class TaskContextTests(PySparkTestCase): def setUp(self): self._old_sys_path = list(sys.path) class_name = self.__class__.__name__ # Allow retries even though they are normally disabled in local mode self.sc = SparkContext('local[4, 2]', class_name) def test_stage_id(self): """Test the stage ids are available and incrementing as expected.""" rdd = self.sc.parallelize(range(10)) stage1 = rdd.map(lambda x: TaskContext.get().stageId()).take(1)[0] stage2 = rdd.map(lambda x: TaskContext.get().stageId()).take(1)[0] # Test using the constructor directly rather than the get() stage3 = rdd.map(lambda x: TaskContext().stageId()).take(1)[0] self.assertEqual(stage1 + 1, stage2) self.assertEqual(stage1 + 2, stage3) self.assertEqual(stage2 + 1, stage3) def test_resources(self): """Test the resources are empty by default.""" rdd = self.sc.parallelize(range(10)) resources1 = rdd.map(lambda x: TaskContext.get().resources()).take(1)[0] # Test using the constructor directly rather than the get() resources2 = rdd.map(lambda x: TaskContext().resources()).take(1)[0] self.assertEqual(len(resources1), 0) self.assertEqual(len(resources2), 0) def test_partition_id(self): """Test the partition id.""" rdd1 = self.sc.parallelize(range(10), 1) rdd2 = self.sc.parallelize(range(10), 2) pids1 = rdd1.map(lambda x: TaskContext.get().partitionId()).collect() pids2 = rdd2.map(lambda x: TaskContext.get().partitionId()).collect() self.assertEqual(0, pids1[0]) self.assertEqual(0, pids1[9]) self.assertEqual(0, pids2[0]) self.assertEqual(1, pids2[9]) def test_attempt_number(self): """Verify the attempt numbers are correctly reported.""" rdd = self.sc.parallelize(range(10)) # Verify a simple job with no failures attempt_numbers = rdd.map(lambda x: TaskContext.get().attemptNumber()).collect() map(lambda attempt: self.assertEqual(0, attempt), attempt_numbers) def fail_on_first(x): """Fail on the first attempt so we get a positive attempt number""" tc = TaskContext.get() attempt_number = tc.attemptNumber() partition_id = tc.partitionId() attempt_id = tc.taskAttemptId() if attempt_number == 0 and partition_id == 0: raise Exception("Failing on first attempt") else: return [x, partition_id, attempt_number, attempt_id] result = rdd.map(fail_on_first).collect() # We should re-submit the first partition to it but other partitions should be attempt 0 self.assertEqual([0, 0, 1], result[0][0:3]) self.assertEqual([9, 3, 0], result[9][0:3]) first_partition = filter(lambda x: x[1] == 0, result) map(lambda x: self.assertEqual(1, x[2]), first_partition) other_partitions = filter(lambda x: x[1] != 0, result) map(lambda x: self.assertEqual(0, x[2]), other_partitions) # The task attempt id should be different self.assertTrue(result[0][3] != result[9][3]) def test_tc_on_driver(self): """Verify that getting the TaskContext on the driver returns None.""" tc = TaskContext.get() self.assertTrue(tc is None) def test_get_local_property(self): """Verify that local properties set on the driver are available in TaskContext.""" key = "testkey" value = "testvalue" self.sc.setLocalProperty(key, value) try: rdd = self.sc.parallelize(range(1), 1) prop1 = rdd.map(lambda _: TaskContext.get().getLocalProperty(key)).collect()[0] self.assertEqual(prop1, value) prop2 = rdd.map(lambda _: TaskContext.get().getLocalProperty("otherkey")).collect()[0] self.assertTrue(prop2 is None) finally: self.sc.setLocalProperty(key, None) def test_barrier(self): """ Verify that BarrierTaskContext.barrier() performs global sync among all barrier tasks within a stage. """ rdd = self.sc.parallelize(range(10), 4) def f(iterator): yield sum(iterator) def context_barrier(x): tc = BarrierTaskContext.get() time.sleep(random.randint(1, 10)) tc.barrier() return time.time() times = rdd.barrier().mapPartitions(f).map(context_barrier).collect() self.assertTrue(max(times) - min(times) < 1) def test_barrier_infos(self): """ Verify that BarrierTaskContext.getTaskInfos() returns a list of all task infos in the barrier stage. """ rdd = self.sc.parallelize(range(10), 4) def f(iterator): yield sum(iterator) taskInfos = rdd.barrier().mapPartitions(f).map(lambda x: BarrierTaskContext.get() .getTaskInfos()).collect() self.assertTrue(len(taskInfos) == 4) self.assertTrue(len(taskInfos[0]) == 4) class TaskContextTestsWithWorkerReuse(unittest.TestCase): def setUp(self): class_name = self.__class__.__name__ conf = SparkConf().set("spark.python.worker.reuse", "true") self.sc = SparkContext('local[2]', class_name, conf=conf) def test_barrier_with_python_worker_reuse(self): """ Regression test for SPARK-25921: verify that BarrierTaskContext.barrier() with reused python worker. """ # start a normal job first to start all workers and get all worker pids worker_pids = self.sc.parallelize(range(2), 2).map(lambda x: os.getpid()).collect() # the worker will reuse in this barrier job rdd = self.sc.parallelize(range(10), 2) def f(iterator): yield sum(iterator) def context_barrier(x): tc = BarrierTaskContext.get() time.sleep(random.randint(1, 10)) tc.barrier() return (time.time(), os.getpid()) result = rdd.barrier().mapPartitions(f).map(context_barrier).collect() times = list(map(lambda x: x[0], result)) pids = list(map(lambda x: x[1], result)) # check both barrier and worker reuse effect self.assertTrue(max(times) - min(times) < 1) for pid in pids: self.assertTrue(pid in worker_pids) def tearDown(self): self.sc.stop() class TaskContextTestsWithResources(unittest.TestCase): def setUp(self): class_name = self.__class__.__name__ self.tempFile = tempfile.NamedTemporaryFile(delete=False) self.tempFile.write(b'echo {\\"name\\": \\"gpu\\", \\"addresses\\": [\\"0\\"]}') self.tempFile.close() os.chmod(self.tempFile.name, stat.S_IRWXU | stat.S_IXGRP | stat.S_IRGRP | stat.S_IROTH | stat.S_IXOTH) conf = SparkConf().set("spark.task.resource.gpu.amount", "1") conf = conf.set("spark.executor.resource.gpu.amount", "1") conf = conf.set("spark.executor.resource.gpu.discoveryScript", self.tempFile.name) self.sc = SparkContext('local-cluster[2,1,1024]', class_name, conf=conf) def test_resources(self): """Test the resources are available.""" rdd = self.sc.parallelize(range(10)) resources = rdd.map(lambda x: TaskContext.get().resources()).take(1)[0] self.assertEqual(len(resources), 1) self.assertTrue('gpu' in resources) self.assertEqual(resources['gpu'].name, 'gpu') self.assertEqual(resources['gpu'].addresses, ['0']) def tearDown(self): os.unlink(self.tempFile.name) self.sc.stop() if __name__ == "__main__": import unittest from pyspark.tests.test_taskcontext import * try: import xmlrunner testRunner = xmlrunner.XMLTestRunner(output='target/test-reports', verbosity=2) except ImportError: testRunner = None unittest.main(testRunner=testRunner, verbosity=2)
# Copyright (c) 2003-2013 LOGILAB S.A. (Paris, FRANCE). # This program is free software; you can redistribute it and/or modify it under # the terms of the GNU General Public License as published by the Free Software # Foundation; either version 2 of the License, or (at your option) any later # version. # # This program is distributed in the hope that it will be useful, but WITHOUT # ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS # FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. # # You should have received a copy of the GNU General Public License along with # this program; if not, write to the Free Software Foundation, Inc., # 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA. """HTML reporter""" import sys from cgi import escape from logilab.common.ureports import HTMLWriter, Section, Table from pylint.interfaces import IReporter from pylint.reporters import BaseReporter class HTMLReporter(BaseReporter): """report messages and layouts in HTML""" __implements__ = IReporter name = 'html' extension = 'html' def __init__(self, output=sys.stdout): BaseReporter.__init__(self, output) self.msgs = [] def handle_message(self, msg): """manage message of different type and in the context of path""" self.msgs += (msg.category, msg.module, msg.obj, str(msg.line), str(msg.column), escape(msg.msg)) def set_output(self, output=None): """set output stream messages buffered for old output is processed first""" if self.out and self.msgs: self._display(Section()) BaseReporter.set_output(self, output) def _display(self, layout): """launch layouts display overridden from BaseReporter to add insert the messages section (in add_message, message is not displayed, just collected so it can be displayed in an html table) """ if self.msgs: # add stored messages to the layout msgs = ['type', 'module', 'object', 'line', 'col_offset', 'message'] msgs += self.msgs sect = Section('Messages') layout.append(sect) sect.append(Table(cols=6, children=msgs, rheaders=1)) self.msgs = [] HTMLWriter().format(layout, self.out) def register(linter): """Register the reporter classes with the linter.""" linter.register_reporter(HTMLReporter)
# Copyright 2020 The HuggingFace Team. All rights reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. import unittest import pytest from transformers import pipeline from transformers.testing_utils import is_pipeline_test, is_torch_available, require_torch, slow from .test_pipelines_common import MonoInputPipelineCommonMixin if is_torch_available(): from transformers.models.mbart import MBart50TokenizerFast, MBartForConditionalGeneration class TranslationEnToDePipelineTests(MonoInputPipelineCommonMixin, unittest.TestCase): pipeline_task = "translation_en_to_de" small_models = ["patrickvonplaten/t5-tiny-random"] # Default model - Models tested without the @slow decorator large_models = [None] # Models tested with the @slow decorator invalid_inputs = [4, "<mask>"] mandatory_keys = ["translation_text"] class TranslationEnToRoPipelineTests(MonoInputPipelineCommonMixin, unittest.TestCase): pipeline_task = "translation_en_to_ro" small_models = ["patrickvonplaten/t5-tiny-random"] # Default model - Models tested without the @slow decorator large_models = [None] # Models tested with the @slow decorator invalid_inputs = [4, "<mask>"] mandatory_keys = ["translation_text"] @is_pipeline_test class TranslationNewFormatPipelineTests(unittest.TestCase): @require_torch @slow def test_default_translations(self): # We don't provide a default for this pair with self.assertRaises(ValueError): pipeline(task="translation_cn_to_ar") # but we do for this one translator = pipeline(task="translation_en_to_de") self.assertEquals(translator.src_lang, "en") self.assertEquals(translator.tgt_lang, "de") @require_torch @slow def test_multilingual_translation(self): model = MBartForConditionalGeneration.from_pretrained("facebook/mbart-large-50-many-to-many-mmt") tokenizer = MBart50TokenizerFast.from_pretrained("facebook/mbart-large-50-many-to-many-mmt") translator = pipeline(task="translation", model=model, tokenizer=tokenizer) # Missing src_lang, tgt_lang with self.assertRaises(ValueError): translator("This is a test") outputs = translator("This is a test", src_lang="en_XX", tgt_lang="ar_AR") self.assertEqual(outputs, [{"translation_text": " "}]) outputs = translator("This is a test", src_lang="en_XX", tgt_lang="hi_IN") self.assertEqual(outputs, [{"translation_text": "   "}]) # src_lang, tgt_lang can be defined at pipeline call time translator = pipeline(task="translation", model=model, tokenizer=tokenizer, src_lang="en_XX", tgt_lang="ar_AR") outputs = translator("This is a test") self.assertEqual(outputs, [{"translation_text": " "}]) @require_torch def test_translation_on_odd_language(self): model = "patrickvonplaten/t5-tiny-random" translator = pipeline(task="translation_cn_to_ar", model=model) self.assertEquals(translator.src_lang, "cn") self.assertEquals(translator.tgt_lang, "ar") @require_torch def test_translation_default_language_selection(self): model = "patrickvonplaten/t5-tiny-random" with pytest.warns(UserWarning, match=r".*translation_en_to_de.*"): translator = pipeline(task="translation", model=model) self.assertEqual(translator.task, "translation_en_to_de") self.assertEquals(translator.src_lang, "en") self.assertEquals(translator.tgt_lang, "de") @require_torch def test_translation_with_no_language_no_model_fails(self): with self.assertRaises(ValueError): pipeline(task="translation")
# Copyright 2014 The Chromium Authors. All rights reserved. # Use of this source code is governed by a BSD-style license that can be # found in the LICENSE file. import unittest from telemetry.timeline import inspector_importer from telemetry.timeline import model from telemetry.timeline import trace_data _BACKGROUND_MESSAGE = { 'data': {}, 'type': 'BeginFrame', 'thread': '2', 'startTime': 1352783525921.824} _SAMPLE_MESSAGE = { 'children': [ {'data': {}, 'startTime': 1352783525921.823, 'type': 'BeginFrame', 'usedHeapSize': 1870736}, {'children': [], 'data': {'height': 723, 'width': 1272, 'x': 0, 'y': 0}, 'endTime': 1352783525921.8992, 'frameId': '10.2', 'startTime': 1352783525921.8281, 'type': 'Layout', 'usedHeapSize': 1870736}, {'children': [ {'children': [], 'data': {'imageType': 'PNG'}, 'endTime': 1352783525927.7939, 'startTime': 1352783525922.4241, 'type': 'DecodeImage', 'usedHeapSize': 1870736} ], 'data': {'height': 432, 'width': 1272, 'x': 0, 'y': 8}, 'endTime': 1352783525927.9822, 'frameId': '10.2', 'startTime': 1352783525921.9292, 'type': 'Paint', 'usedHeapSize': 1870736} ], 'data': {}, 'endTime': 1352783525928.041, 'startTime': 1352783525921.8049, 'type': 'Program'} class InspectorEventParsingTest(unittest.TestCase): def testParsingWithSampleData(self): root_event = (inspector_importer.InspectorTimelineImporter .RawEventToTimelineEvent(_SAMPLE_MESSAGE)) self.assertTrue(root_event) decode_image_event = [ child for child in root_event.IterEventsInThisContainerRecrusively() if child.name == 'DecodeImage'][0] self.assertEquals(decode_image_event.args['data']['imageType'], 'PNG') self.assertTrue(decode_image_event.duration > 0) def testParsingWithSimpleData(self): raw_event = {'type': 'Foo', 'startTime': 1, 'endTime': 3, 'children': []} event = (inspector_importer.InspectorTimelineImporter .RawEventToTimelineEvent(raw_event)) self.assertEquals('Foo', event.name) self.assertEquals(1, event.start) self.assertEquals(3, event.end) self.assertEquals(2, event.duration) self.assertEquals([], event.sub_slices) def testParsingWithArgs(self): raw_event = {'type': 'Foo', 'startTime': 1, 'endTime': 3, 'foo': 7, 'bar': {'x': 1}} event = (inspector_importer.InspectorTimelineImporter .RawEventToTimelineEvent(raw_event)) self.assertEquals('Foo', event.name) self.assertEquals(1, event.start) self.assertEquals(3, event.end) self.assertEquals(2, event.duration) self.assertEquals([], event.sub_slices) self.assertEquals(7, event.args['foo']) self.assertEquals(1, event.args['bar']['x']) def testEventsWithNoStartTimeAreDropped(self): raw_event = {'type': 'Foo', 'endTime': 1, 'children': []} event = (inspector_importer.InspectorTimelineImporter. RawEventToTimelineEvent(raw_event)) self.assertEquals(None, event) def testEventsWithNoEndTimeAreOk(self): raw_event = {'type': 'Foo', 'startTime': 1, 'children': []} event = (inspector_importer.InspectorTimelineImporter. RawEventToTimelineEvent(raw_event)) self.assertEquals(1, event.start) self.assertEquals(1, event.end) def testOutOfOrderData(self): builder = trace_data.TraceDataBuilder() builder.AddEventsTo( trace_data.INSPECTOR_TRACE_PART, [{ 'startTime': 5295.004, 'endTime': 5305.004, 'data': {}, 'type': 'Program', 'children': [ {'startTime': 5295.004, 'data': {'id': 0}, 'type': 'BeginFrame', }, {'startTime': 4492.973, 'endTime': 4493.086, 'data': {'rootNode': -3}, 'type': 'PaintSetup'}, {'startTime': 5298.004, 'endTime': 5301.004, 'type': 'Paint', 'frameId': '53228.1', 'data': {'rootNode': -3, 'clip': [0, 0, 1018, 0, 1018, 764, 0, 764], 'layerId': 10}, 'children': []}, {'startTime': 5301.004, 'endTime': 5305.004, 'data': {}, 'type': 'CompositeLayers', 'children': []}, {'startTime': 5305.004, 'data': {}, 'type': 'MarkFirstPaint'} ]}]) model.TimelineModel(builder.AsData(), shift_world_to_zero=False) class InspectorImporterTest(unittest.TestCase): def testImport(self): builder = trace_data.TraceDataBuilder() builder.AddEventsTo(trace_data.INSPECTOR_TRACE_PART, [_BACKGROUND_MESSAGE, _SAMPLE_MESSAGE]) m = model.TimelineModel(builder.AsData(), shift_world_to_zero=False) self.assertEquals(1, len(m.processes)) process = m.processes.values()[0] threads = process.threads self.assertEquals(2, len(threads)) renderer_thread = threads[0] self.assertEquals(1, len(renderer_thread.toplevel_slices)) self.assertEquals('Program', renderer_thread.toplevel_slices[0].name) second_thread = threads['2'] self.assertEquals(1, len(second_thread.toplevel_slices)) self.assertEquals('BeginFrame', second_thread.toplevel_slices[0].name)
"""Tests for Rt - Python interface to Request Tracker :term:`API`""" __license__ = """ Copyright (C) 2013 CZ.NIC, z.s.p.o. This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see <http://www.gnu.org/licenses/>. """ __docformat__ = "reStructuredText en" __authors__ = [ '"Jiri Machalek" <jiri.machalek@nic.cz>' ] import unittest import random import string from six import iteritems from six.moves import range import rt class RtTestCase(unittest.TestCase): RT_VALID_CREDENTIALS = { 'RT3.8 stable': { 'url': 'http://rt.easter-eggs.org/demos/3.8/REST/1.0', 'admin': { 'default_login': 'admin', 'default_password': 'admin', }, 'john.foo': { 'default_login': 'john.foo', 'default_password': 'john.foo', } }, 'RT4.0 stable': { 'url': 'http://rt.easter-eggs.org/demos/4.0/REST/1.0', 'admin': { 'default_login': 'admin', 'default_password': 'admin', }, 'john.foo': { 'default_login': 'john.foo', 'default_password': 'john.foo', } }, } RT_INVALID_CREDENTIALS = { 'RT3.8 stable (bad credentials)': { 'url': 'http://rt.easter-eggs.org/demos/3.8/REST/1.0', 'default_login': 'idontexist', 'default_password': 'idonthavepassword', }, } RT_MISSING_CREDENTIALS = { 'RT4.0 stable (missing credentials)': { 'url': 'http://rt.easter-eggs.org/demos/4.0/REST/1.0', }, } RT_BAD_URL = { 'RT (bad url)': { 'url': 'http://httpbin.org/status/404', 'default_login': 'idontexist', 'default_password': 'idonthavepassword', }, } def test_login_and_logout(self): for name in self.RT_VALID_CREDENTIALS: tracker = rt.Rt(self.RT_VALID_CREDENTIALS[name]['url'], **self.RT_VALID_CREDENTIALS[name]['john.foo']) self.assertTrue(tracker.login(), 'Invalid login to RT demo site ' + name) self.assertTrue(tracker.logout(), 'Invalid logout from RT demo site ' + name) for name, params in iteritems(self.RT_INVALID_CREDENTIALS): tracker = rt.Rt(**params) self.assertFalse(tracker.login(), 'Login to RT demo site ' + name + ' should fail but did not') self.assertRaises(rt.AuthorizationError, lambda: tracker.search()) for name, params in iteritems(self.RT_MISSING_CREDENTIALS): tracker = rt.Rt(**params) self.assertRaises(rt.AuthorizationError, lambda: tracker.login()) for name, params in iteritems(self.RT_BAD_URL): tracker = rt.Rt(**params) self.assertRaises(rt.UnexpectedResponse, lambda: tracker.login()) def check_or_create_queue(self, name): tracker = rt.Rt(self.RT_VALID_CREDENTIALS[name]['url'], **self.RT_VALID_CREDENTIALS[name]['admin']) tracker.login() queue = tracker.get_queue('General') if 'Name' not in queue: queue_id = tracker.create_queue('General') tracker.logout() def test_ticket_operations(self): ticket_subject = 'Testing issue ' + "".join([random.choice(string.ascii_letters) for i in range(15)]) ticket_text = 'Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.' for name in ('RT4.0 stable', 'RT3.8 stable'): self.check_or_create_queue(name) url = self.RT_VALID_CREDENTIALS[name]['url'] default_login = self.RT_VALID_CREDENTIALS[name]['john.foo']['default_login'] default_password = self.RT_VALID_CREDENTIALS[name]['john.foo']['default_password'] tracker = rt.Rt(url, default_login=default_login, default_password=default_password) self.assertTrue(tracker.login(), 'Invalid login to RT demo site ' + name) # empty search result search_result = tracker.search(Subject=ticket_subject) self.assertEqual(search_result, [], 'Search for ticket with random subject returned non empty list.') # create ticket_id = tracker.create_ticket(Subject=ticket_subject, Text=ticket_text) self.assertTrue(ticket_id > -1, 'Creating ticket failed.') # search search_result = tracker.search(Subject=ticket_subject) self.assertEqual(len(search_result), 1, 'Created ticket is not found by the subject.') self.assertEqual(search_result[0]['id'], 'ticket/' + str(ticket_id), 'Bad id in search result of just created ticket.') self.assertEqual(search_result[0]['Status'], 'new', 'Bad status in search result of just created ticket.') # search all queues search_result = tracker.search(Queue=rt.ALL_QUEUES, Subject=ticket_subject) self.assertEqual(search_result[0]['id'], 'ticket/' + str(ticket_id), 'Bad id in search result of just created ticket.') # raw search search_result = tracker.search(raw_query='Subject="%s"' % ticket_subject) self.assertEqual(len(search_result), 1, 'Created ticket is not found by the subject.') self.assertEqual(search_result[0]['id'], 'ticket/' + str(ticket_id), 'Bad id in search result of just created ticket.') self.assertEqual(search_result[0]['Status'], 'new', 'Bad status in search result of just created ticket.') # raw search all queues search_result = tracker.search(Queue=rt.ALL_QUEUES, raw_query='Subject="%s"' % ticket_subject) self.assertEqual(search_result[0]['id'], 'ticket/' + str(ticket_id), 'Bad id in search result of just created ticket.') # get ticket ticket = tracker.get_ticket(ticket_id) self.assertEqual(ticket, search_result[0], 'Ticket get directly by its id is not equal to previous search result.') # edit ticket requestors = ['tester1@example.com', 'tester2@example.com'] tracker.edit_ticket(ticket_id, Status='open', Requestors=requestors) # get ticket (edited) ticket = tracker.get_ticket(ticket_id) self.assertEqual(ticket['Status'], 'open', 'Ticket status was not changed to open.') self.assertEqual(ticket['Requestors'], requestors, 'Ticket requestors were not added properly.') # get history hist = tracker.get_history(ticket_id) self.assertTrue(len(hist) > 0, 'Empty ticket history.') self.assertEqual(hist[0]['Content'], ticket_text, 'Ticket text was not receives is it was submited.') # get_short_history short_hist = tracker.get_short_history(ticket_id) self.assertTrue(len(short_hist) > 0, 'Empty ticket short history.') self.assertEqual(short_hist[0][1], 'Ticket created by john.foo') # create 2nd ticket ticket2_subject = 'Testing issue ' + "".join([random.choice(string.ascii_letters) for i in range(15)]) ticket2_id = tracker.create_ticket(Subject=ticket2_subject) self.assertTrue(ticket2_id > -1, 'Creating 2nd ticket failed.') # edit link self.assertTrue(tracker.edit_link(ticket_id, 'DependsOn', ticket2_id)) # get links links1 = tracker.get_links(ticket_id) self.assertTrue('DependsOn' in links1, 'Missing just created link DependsOn.') self.assertTrue(links1['DependsOn'][0].endswith('ticket/' + str(ticket2_id)), 'Unexpected value of link DependsOn.') links2 = tracker.get_links(ticket2_id) self.assertTrue('DependedOnBy' in links2, 'Missing just created link DependedOnBy.') self.assertTrue(links2['DependedOnBy'][0].endswith('ticket/' + str(ticket_id)), 'Unexpected value of link DependedOnBy.') # reply with attachment attachment_content = b'Content of attachment.' attachment_name = 'attachment-name.txt' reply_text = 'Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.' # should provide a content type as RT 4.0 type guessing is broken (missing use statement for guess_media_type in REST.pm) self.assertTrue(tracker.reply(ticket_id, text=reply_text, files=[(attachment_name, attachment_content, 'text/plain')]), 'Reply to ticket returned False indicating error.') at_ids = tracker.get_attachments_ids(ticket_id) self.assertTrue(at_ids, 'Emply list with attachment ids, something went wrong.') at_content = tracker.get_attachment_content(ticket_id, at_ids[-1]) self.assertEqual(at_content, attachment_content, 'Recorded attachment is not equal to the original file.') # attachments list at_list = tracker.get_attachments(ticket_id) at_names = [at[1] for at in at_list] self.assertTrue(attachment_name in at_names, 'Attachment name is not in the list of attachments.') # merge tickets self.assertTrue(tracker.merge_ticket(ticket2_id, ticket_id), 'Merging tickets failed.') # delete ticket self.assertTrue(tracker.edit_ticket(ticket_id, Status='deleted'), 'Ticket delete failed.') # get user self.assertEqual(tracker.get_user(default_login)['EmailAddress'], default_login + '@no.mail', 'Bad user email received.') if __name__ == '__main__': unittest.main()
# coding: utf-8 from __future__ import unicode_literals from django.template import TemplateSyntaxError from django.test import SimpleTestCase from ..utils import SomeClass, SomeOtherException, UTF8Class, setup class FilterSyntaxTests(SimpleTestCase): @setup({'filter-syntax01': '{{ var|upper }}'}) def test_filter_syntax01(self): """ Basic filter usage """ output = self.engine.render_to_string('filter-syntax01', {"var": "Django is the greatest!"}) self.assertEqual(output, "DJANGO IS THE GREATEST!") @setup({'filter-syntax02': '{{ var|upper|lower }}'}) def test_filter_syntax02(self): """ Chained filters """ output = self.engine.render_to_string('filter-syntax02', {"var": "Django is the greatest!"}) self.assertEqual(output, "django is the greatest!") @setup({'filter-syntax03': '{{ var |upper }}'}) def test_filter_syntax03(self): """ Allow spaces before the filter pipe """ output = self.engine.render_to_string('filter-syntax03', {'var': 'Django is the greatest!'}) self.assertEqual(output, 'DJANGO IS THE GREATEST!') @setup({'filter-syntax04': '{{ var| upper }}'}) def test_filter_syntax04(self): """ Allow spaces after the filter pipe """ output = self.engine.render_to_string('filter-syntax04', {'var': 'Django is the greatest!'}) self.assertEqual(output, 'DJANGO IS THE GREATEST!') @setup({'filter-syntax05': '{{ var|does_not_exist }}'}) def test_filter_syntax05(self): """ Raise TemplateSyntaxError for a nonexistent filter """ with self.assertRaises(TemplateSyntaxError): self.engine.get_template('filter-syntax05') @setup({'filter-syntax06': '{{ var|fil(ter) }}'}) def test_filter_syntax06(self): """ Raise TemplateSyntaxError when trying to access a filter containing an illegal character """ with self.assertRaises(TemplateSyntaxError): self.engine.get_template('filter-syntax06') @setup({'filter-syntax07': "{% nothing_to_see_here %}"}) def test_filter_syntax07(self): """ Raise TemplateSyntaxError for invalid block tags """ with self.assertRaises(TemplateSyntaxError): self.engine.get_template('filter-syntax07') @setup({'filter-syntax08': "{% %}"}) def test_filter_syntax08(self): """ Raise TemplateSyntaxError for empty block tags """ with self.assertRaises(TemplateSyntaxError): self.engine.get_template('filter-syntax08') @setup({'filter-syntax09': '{{ var|cut:"o"|upper|lower }}'}) def test_filter_syntax09(self): """ Chained filters, with an argument to the first one """ output = self.engine.render_to_string('filter-syntax09', {'var': 'Foo'}) self.assertEqual(output, 'f') @setup({'filter-syntax10': r'{{ var|default_if_none:" endquote\" hah" }}'}) def test_filter_syntax10(self): """ Literal string as argument is always "safe" from auto-escaping. """ output = self.engine.render_to_string('filter-syntax10', {"var": None}) self.assertEqual(output, ' endquote" hah') @setup({'filter-syntax11': r'{{ var|default_if_none:var2 }}'}) def test_filter_syntax11(self): """ Variable as argument """ output = self.engine.render_to_string('filter-syntax11', {"var": None, "var2": "happy"}) self.assertEqual(output, 'happy') @setup({'filter-syntax12': r'{{ var|yesno:"yup,nup,mup" }} {{ var|yesno }}'}) def test_filter_syntax12(self): """ Default argument testing """ output = self.engine.render_to_string('filter-syntax12', {"var": True}) self.assertEqual(output, 'yup yes') @setup({'filter-syntax13': r'1{{ var.method3 }}2'}) def test_filter_syntax13(self): """ Fail silently for methods that raise an exception with a `silent_variable_failure` attribute """ output = self.engine.render_to_string('filter-syntax13', {"var": SomeClass()}) if self.engine.string_if_invalid: self.assertEqual(output, "1INVALID2") else: self.assertEqual(output, "12") @setup({'filter-syntax14': r'1{{ var.method4 }}2'}) def test_filter_syntax14(self): """ In methods that raise an exception without a `silent_variable_attribute` set to True, the exception propagates """ with self.assertRaises(SomeOtherException): self.engine.render_to_string('filter-syntax14', {"var": SomeClass()}) @setup({'filter-syntax15': r'{{ var|default_if_none:"foo\bar" }}'}) def test_filter_syntax15(self): """ Escaped backslash in argument """ output = self.engine.render_to_string('filter-syntax15', {"var": None}) self.assertEqual(output, r'foo\bar') @setup({'filter-syntax16': r'{{ var|default_if_none:"foo\now" }}'}) def test_filter_syntax16(self): """ Escaped backslash using known escape char """ output = self.engine.render_to_string('filter-syntax16', {"var": None}) self.assertEqual(output, r'foo\now') @setup({'filter-syntax17': r'{{ var|join:"" }}'}) def test_filter_syntax17(self): """ Empty strings can be passed as arguments to filters """ output = self.engine.render_to_string('filter-syntax17', {'var': ['a', 'b', 'c']}) self.assertEqual(output, 'abc') @setup({'filter-syntax18': r'{{ var }}'}) def test_filter_syntax18(self): """ Make sure that any unicode strings are converted to bytestrings in the final output. """ output = self.engine.render_to_string('filter-syntax18', {'var': UTF8Class()}) self.assertEqual(output, '\u0160\u0110\u0106\u017d\u0107\u017e\u0161\u0111') @setup({'filter-syntax19': '{{ var|truncatewords:1 }}'}) def test_filter_syntax19(self): """ Numbers as filter arguments should work """ output = self.engine.render_to_string('filter-syntax19', {"var": "hello world"}) self.assertEqual(output, "hello ...") @setup({'filter-syntax20': '{{ ""|default_if_none:"was none" }}'}) def test_filter_syntax20(self): """ Filters should accept empty string constants """ output = self.engine.render_to_string('filter-syntax20') self.assertEqual(output, "") @setup({'filter-syntax21': r'1{{ var.silent_fail_key }}2'}) def test_filter_syntax21(self): """ Fail silently for non-callable attribute and dict lookups which raise an exception with a "silent_variable_failure" attribute """ output = self.engine.render_to_string('filter-syntax21', {"var": SomeClass()}) if self.engine.string_if_invalid: self.assertEqual(output, "1INVALID2") else: self.assertEqual(output, "12") @setup({'filter-syntax22': r'1{{ var.silent_fail_attribute }}2'}) def test_filter_syntax22(self): """ Fail silently for non-callable attribute and dict lookups which raise an exception with a `silent_variable_failure` attribute """ output = self.engine.render_to_string('filter-syntax22', {"var": SomeClass()}) if self.engine.string_if_invalid: self.assertEqual(output, "1INVALID2") else: self.assertEqual(output, "12") @setup({'filter-syntax23': r'1{{ var.noisy_fail_key }}2'}) def test_filter_syntax23(self): """ In attribute and dict lookups that raise an unexpected exception without a `silent_variable_attribute` set to True, the exception propagates """ with self.assertRaises(SomeOtherException): self.engine.render_to_string('filter-syntax23', {"var": SomeClass()}) @setup({'filter-syntax24': r'1{{ var.noisy_fail_attribute }}2'}) def test_filter_syntax24(self): """ In attribute and dict lookups that raise an unexpected exception without a `silent_variable_attribute` set to True, the exception propagates """ with self.assertRaises(SomeOtherException): self.engine.render_to_string('filter-syntax24', {"var": SomeClass()}) @setup({'filter-syntax25': '{{ var.attribute_error_attribute }}'}) def test_filter_syntax25(self): """ #16383 - Attribute errors from an @property value should be reraised. """ with self.assertRaises(AttributeError): self.engine.render_to_string('filter-syntax25', {'var': SomeClass()})
# -*- coding: utf-8 -*- # # py-crypto-params documentation build configuration file, created by # sphinx-quickstart on Tue Dec 29 20:30:34 2015. # # This file is execfile()d with the current directory set to its # containing dir. # # Note that not all possible configuration values are present in this # autogenerated file. # # All configuration values have a default; values that are commented out # serve to show the default. import datetime import sys import os import shlex import sphinx_rtd_theme def copyright_builder(): """ Simple function that build copyright information with correct year range :return: Copyright info that will be printed by Sphinx :rtype: str """ beginning_year = 2015 current_year = datetime.date.today().year if beginning_year == current_year: return u"{year}, Gian Luca Dalla Torre".format(year=beginning_year) else: return u"{start} - {end}, Gian Luca Dalla Torre".format(start=beginning_year, end=current_year) # If extensions (or modules to document with autodoc) are in another directory, # add these directories to sys.path here. If the directory is relative to the # documentation root, use os.path.abspath to make it absolute, like shown here. sys.path.insert(0, os.path.abspath('.')) sys.path.insert(0, os.path.abspath(os.path.join(os.path.abspath('.'), os.pardir))) # -- General configuration ------------------------------------------------ # If your documentation needs a minimal Sphinx version, state it here. #needs_sphinx = '1.0' # Add any Sphinx extension module names here, as strings. They can be # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom # ones. extensions = [ 'sphinx.ext.autodoc', 'sphinx.ext.intersphinx', 'sphinx.ext.viewcode', ] # Add any paths that contain templates here, relative to this directory. templates_path = ['_templates'] # The suffix(es) of source filenames. # You can specify multiple suffix as a list of string: # source_suffix = ['.rst', '.md'] source_suffix = '.rst' # The encoding of source files. #source_encoding = 'utf-8-sig' # The master toctree document. master_doc = 'index' # General information about the project. project = u'py-crypto-params' copyright = copyright_builder() author = u'Gian Luca Dalla Torre' # The version info for the project you're documenting, acts as replacement for # |version| and |release|, also used in various other places throughout the # built documents. # # The short X.Y version. version = u'1.0.0' # The full version, including alpha/beta/rc tags. release = u'1.0.0' # The language for content autogenerated by Sphinx. Refer to documentation # for a list of supported languages. # # This is also used if you do content translation via gettext catalogs. # Usually you set "language" from the command line for these cases. language = None # There are two options for replacing |today|: either, you set today to some # non-false value, then it is used: #today = '' # Else, today_fmt is used as the format for a strftime call. #today_fmt = '%B %d, %Y' # List of patterns, relative to source directory, that match files and # directories to ignore when looking for source files. exclude_patterns = ['_build'] # The reST default role (used for this markup: `text`) to use for all # documents. #default_role = None # If true, '()' will be appended to :func: etc. cross-reference text. #add_function_parentheses = True # If true, the current module name will be prepended to all description # unit titles (such as .. function::). #add_module_names = True # If true, sectionauthor and moduleauthor directives will be shown in the # output. They are ignored by default. #show_authors = False # The name of the Pygments (syntax highlighting) style to use. pygments_style = 'sphinx' # A list of ignored prefixes for module index sorting. #modindex_common_prefix = [] # If true, keep warnings as "system message" paragraphs in the built documents. #keep_warnings = False # If true, `todo` and `todoList` produce output, else they produce nothing. todo_include_todos = False # -- Options for HTML output ---------------------------------------------- # The theme to use for HTML and HTML Help pages. See the documentation for # a list of builtin themes. html_theme = 'sphinx_rtd_theme' # Theme options are theme-specific and customize the look and feel of a theme # further. For a list of options available for each theme, see the # documentation. #html_theme_options = {} # Add any paths that contain custom themes here, relative to this directory. html_theme_path = [sphinx_rtd_theme.get_html_theme_path()] # The name for this set of Sphinx documents. If None, it defaults to # "<project> v<release> documentation". #html_title = None # A shorter title for the navigation bar. Default is the same as html_title. #html_short_title = None # The name of an image file (relative to this directory) to place at the top # of the sidebar. #html_logo = None # The name of an image file (within the static path) to use as favicon of the # docs. This file should be a Windows icon file (.ico) being 16x16 or 32x32 # pixels large. #html_favicon = None # Add any paths that contain custom static files (such as style sheets) here, # relative to this directory. They are copied after the builtin static files, # so a file named "default.css" will overwrite the builtin "default.css". html_static_path = ['_static'] # Add any extra paths that contain custom files (such as robots.txt or # .htaccess) here, relative to this directory. These files are copied # directly to the root of the documentation. #html_extra_path = [] # If not '', a 'Last updated on:' timestamp is inserted at every page bottom, # using the given strftime format. #html_last_updated_fmt = '%b %d, %Y' # If true, SmartyPants will be used to convert quotes and dashes to # typographically correct entities. #html_use_smartypants = True # Custom sidebar templates, maps document names to template names. #html_sidebars = {} # Additional templates that should be rendered to pages, maps page names to # template names. #html_additional_pages = {} # If false, no module index is generated. #html_domain_indices = True # If false, no index is generated. #html_use_index = True # If true, the index is split into individual pages for each letter. #html_split_index = False # If true, links to the reST sources are added to the pages. #html_show_sourcelink = True # If true, "Created using Sphinx" is shown in the HTML footer. Default is True. #html_show_sphinx = True # If true, "(C) Copyright ..." is shown in the HTML footer. Default is True. #html_show_copyright = True # If true, an OpenSearch description file will be output, and all pages will # contain a <link> tag referring to it. The value of this option must be the # base URL from which the finished HTML is served. #html_use_opensearch = '' # This is the file name suffix for HTML files (e.g. ".xhtml"). #html_file_suffix = None # Language to be used for generating the HTML full-text search index. # Sphinx supports the following languages: # 'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja' # 'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr' #html_search_language = 'en' # A dictionary with options for the search language support, empty by default. # Now only 'ja' uses this config value #html_search_options = {'type': 'default'} # The name of a javascript file (relative to the configuration directory) that # implements a search results scorer. If empty, the default will be used. #html_search_scorer = 'scorer.js' # Output file base name for HTML help builder. htmlhelp_basename = 'py-crypto-paramsdoc' # -- Options for LaTeX output --------------------------------------------- latex_elements = { # The paper size ('letterpaper' or 'a4paper'). #'papersize': 'letterpaper', # The font size ('10pt', '11pt' or '12pt'). #'pointsize': '10pt', # Additional stuff for the LaTeX preamble. #'preamble': '', # Latex figure (float) alignment #'figure_align': 'htbp', } # Grouping the document tree into LaTeX files. List of tuples # (source start file, target name, title, # author, documentclass [howto, manual, or own class]). latex_documents = [ (master_doc, 'py-crypto-params.tex', u'py-crypto-params Documentation', u'Gian Luca Dalla Torre', 'manual'), ] # The name of an image file (relative to this directory) to place at the top of # the title page. #latex_logo = None # For "manual" documents, if this is true, then toplevel headings are parts, # not chapters. #latex_use_parts = False # If true, show page references after internal links. #latex_show_pagerefs = False # If true, show URL addresses after external links. #latex_show_urls = False # Documents to append as an appendix to all manuals. #latex_appendices = [] # If false, no module index is generated. #latex_domain_indices = True # -- Options for manual page output --------------------------------------- # One entry per manual page. List of tuples # (source start file, name, description, authors, manual section). man_pages = [ (master_doc, 'py-crypto-params', u'py-crypto-params Documentation', [author], 1) ] # If true, show URL addresses after external links. #man_show_urls = False # -- Options for Texinfo output ------------------------------------------- # Grouping the document tree into Texinfo files. List of tuples # (source start file, target name, title, author, # dir menu entry, description, category) texinfo_documents = [ (master_doc, 'py-crypto-params', u'py-crypto-params Documentation', author, 'py-crypto-params', 'One line description of project.', 'Miscellaneous'), ] # Documents to append as an appendix to all manuals. #texinfo_appendices = [] # If false, no module index is generated. #texinfo_domain_indices = True # How to display URL addresses: 'footnote', 'no', or 'inline'. #texinfo_show_urls = 'footnote' # If true, do not generate a @detailmenu in the "Top" node's menu. #texinfo_no_detailmenu = False # Example configuration for intersphinx: refer to the Python standard library. intersphinx_mapping = {'https://docs.python.org/': None}
# This file is part of Shoop. # # Copyright (c) 2012-2015, Shoop Ltd. All rights reserved. # # This source code is licensed under the AGPLv3 license found in the # LICENSE file in the root directory of this source tree. class Setup(object): def __init__(self, load_from=None): self.commit(load_from) def is_valid_key(self, key): return key == key.upper() and not key.startswith("_") def commit(self, source): if source: if not hasattr(source, "items"): # pragma: no cover source = vars(source) for key, value in source.items(): if self.is_valid_key(key): setattr(self, key, value) def values(self): for key, value in self.__dict__.items(): if self.is_valid_key(key): # pragma: no branch yield (key, value) def get(self, key, default=None): # pragma: no cover return getattr(self, key, default) def getlist(self, key, default=()): # pragma: no cover val = getattr(self, key, default) return list(val) @classmethod def configure(cls, configure): setup = cls() try: configure(setup) except: # pragma: no cover print("@" * 80) import traceback import sys traceback.print_exc() print("@" * 80) sys.exit(1) return setup.values()
####################################################################### # # Author: Gabi Roeger # Modified by: Silvia Richter (silvia.richter@nicta.com.au) # (C) Copyright 2008: Gabi Roeger and NICTA # # This file is part of LAMA. # # LAMA is free software; you can redistribute it and/or # modify it under the terms of the GNU General Public License # as published by the Free Software Foundation; either version 3 # of the license, or (at your option) any later version. # # LAMA is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program; if not, see <http://www.gnu.org/licenses/>. # ####################################################################### import string import conditions def parse_expression(exp): if isinstance(exp, list): functionsymbol = exp[0] return PrimitiveNumericExpression(functionsymbol, [conditions.parse_term(arg) for arg in exp[1:]]) elif exp.replace(".","").isdigit(): return NumericConstant(string.atof(exp)) else: return PrimitiveNumericExpression(exp,[]) def parse_assignment(alist): assert len(alist) == 3 op = alist[0] head = parse_expression(alist[1]) exp = parse_expression(alist[2]) if op == "=": return Assign(head, exp) elif op == "increase": return Increase(head, exp) else: assert False, "Assignment operator not supported." class FunctionalExpression(object): def __init__(self, parts): self.parts = tuple(parts) def dump(self, indent=" "): print "%s%s" % (indent, self._dump()) for part in self.parts: part.dump(indent + " ") def _dump(self): return self.__class__.__name__ def instantiate(self, var_mapping, init_facts): raise ValueError("Cannot instantiate condition: not normalized") class NumericConstant(FunctionalExpression): parts = () def __init__(self, value): self.value = value def __eq__(self, other): return (self.__class__ == other.__class__ and self.value == other.value) def __str__(self): return "%s %s" % (self.__class__.__name__, self.value) def _dump(self): return str(self) def instantiate(self, var_mapping, init_facts): return self class PrimitiveNumericExpression(FunctionalExpression): parts = () def __init__(self, symbol, args): self.symbol = symbol self.args = tuple(args) def __eq__(self, other): if not (self.__class__ == other.__class__ and self.symbol == other.symbol and len(self.args) == len(other.args)): return False else: for s,o in zip(self.args, other.args): if not s == o: return False return True def __str__(self): return "%s %s(%s)" % ("PNE", self.symbol, ", ".join(map(str, self.args))) def dump(self, indent=" "): print "%s%s" % (indent, self._dump()) for arg in self.args: arg.dump(indent + " ") def _dump(self): return str(self) def instantiate(self, var_mapping, init_facts): args = [conditions.ObjectTerm(var_mapping.get(arg.name, arg.name)) for arg in self.args] pne = PrimitiveNumericExpression(self.symbol, args) assert not self.symbol == "total-cost" # We know this expression is constant. Substitute it by corresponding # initialization from task. for fact in init_facts: if isinstance(fact, FunctionAssignment): if fact.fluent == pne: return fact.expression assert False, "Could not find instantiation for PNE!" class FunctionAssignment(object): def __init__(self, fluent, expression): self.fluent = fluent self.expression = expression def __str__(self): return "%s %s %s" % (self.__class__.__name__, self.fluent, self.expression) def dump(self, indent=" "): print "%s%s" % (indent, self._dump()) self.fluent.dump(indent + " ") self.expression.dump(indent + " ") def _dump(self): return self.__class__.__name__ def instantiate(self, var_mapping, init_facts): if not (isinstance(self.expression, PrimitiveNumericExpression) or isinstance(self.expression, NumericConstant)): raise ValueError("Cannot instantiate assignment: not normalized") # We know that this assignment is a cost effect of an action (for initial state # assignments, "instantiate" is not called). Hence, we know that the fluent is # the 0-ary "total-cost" which does not need to be instantiated assert self.fluent.symbol == "total-cost" fluent = self.fluent expression = self.expression.instantiate(var_mapping, init_facts) return self.__class__(fluent, expression) class Assign(FunctionAssignment): def __str__(self): return "%s := %s" % (self.fluent, self.expression) class Increase(FunctionAssignment): pass
# -*- coding: utf-8 -*- # <nbformat>3.0</nbformat> # <headingcell level=2> # Usage of IC 4069 # <codecell> from __future__ import print_function from BinPy import * # <codecell> # Usage of IC 4069: ic = IC_4069() print(ic.__doc__) # <codecell> # The Pin configuration is: inp = {2: 0, 3: 1, 4: 0, 5: 1, 7: 0, 9: 1, 10: 1, 11: 1, 12: 1, 14: 1} # Pin initinalization # Powering up the IC - using -- ic.setIC({14: 1, 7: 0}) -- \n ic.setIC({14: 1, 7: 0}) # Setting the inputs of the ic ic.setIC(inp) # Draw the IC with the current configuration\n ic.drawIC() # <codecell> # Run the IC with the current configuration using -- print ic.run() -- # Note that the ic.run() returns a dict of pin configuration similar to print (ic.run()) # <codecell> # Seting the outputs to the current IC configuration using -- # ic.setIC(ic.run()) --\n ic.setIC(ic.run()) # Draw the final configuration ic.drawIC() # <codecell> # Seting the outputs to the current IC configuration using -- # ic.setIC(ic.run()) -- ic.setIC(ic.run()) # Draw the final configuration ic.drawIC() # Run the IC print (ic.run()) # <codecell> # Connector Outputs c = Connector() # Set the output connector to a particular pin of the ic ic.setOutput(2, c) print(c)
"""Module docstring. Generic Library. Many are for reference, make them "inline" """ __author__ = "Zacharias El Banna" ################################# Generics #################################### def debug_decorator(func_name): def decorator(func): def decorated(*args,**kwargs): res = func(*args,**kwargs) print("DEBUGGER: %s(%s,%s) => %s"%(func_name, args, kwargs, res)) return res return decorated return decorator # # Basic Auth header generator for base64 authentication # def basic_auth(aUsername,aPassword): from base64 import b64encode return {'Authorization':'Basic %s'%(b64encode(("%s:%s"%(aUsername,aPassword)).encode('utf-8')).decode()) } def random_string(aLength): import string import random return ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(aLength)) def get_host_name(aIP): from socket import gethostbyaddr try: return gethostbyaddr(aIP)[0].partition('.')[0] except: return None def ip2int(addr): from struct import unpack from socket import inet_aton return unpack("!I", inet_aton(addr))[0] def int2ip(addr): from struct import pack from socket import inet_ntoa return inet_ntoa(pack("!I", addr)) def ips2range(addr1,addr2): from struct import pack, unpack from socket import inet_ntoa, inet_aton return [inet_ntoa(pack("!I", addr)) for addr in range(unpack("!I", inet_aton(addr1))[0], unpack("!I", inet_aton(addr2))[0] + 1)] def ipint2range(start,end): from struct import pack from socket import inet_ntoa return [inet_ntoa(pack("!I", addr)) for addr in range(start,end + 1)] def ip2ptr(addr): octets = addr.split('.') octets.reverse() octets.append("in-addr.arpa") return ".".join(octets) def ip2arpa(addr): octets = addr.split('.')[:3] octets.reverse() octets.append("in-addr.arpa") return ".".join(octets) def int2mac(aInt): return ':'.join("%s%s"%x for x in zip(*[iter("{:012x}".format(aInt))]*2)) def mac2int(aMAC): try: return int(aMAC.replace(":",""),16) except: return 0 def ping_os(ip): from os import system return system("ping -c 1 -w 1 " + ip + " > /dev/null 2>&1") == 0 def external_ip(): from dns import resolver from socket import gethostbyname try: opendns = resolver.Resolver() opendns.nameservers = [gethostbyname('resolver1.opendns.com')] res = str(opendns.query("myip.opendns.com",'A').response.answer[0]) return res.split()[4] except: return None def get_quote(aString): from urllib.parse import quote_plus return quote_plus(aString) def str2hex(arg): try: return '0x{0:02x}'.format(int(arg)) except: return '0x00' def pidfile_write(pidfname): from os import getpid pidfile = open(pidfname,'w') pidfile.write(str(getpid())) pidfile.close() def pidfile_read(pidfname): pid = -1 from os import path as ospath if ospath.isfile(pidfname): pidfile = open(pidfname) pid = pidfile.readline().strip('\n') pidfile.close() return int(pid) def pidfile_release(pidfname): from os import path as ospath if ospath.isfile(pidfname): from os import remove remove(pidfname) def pidfile_lock(pidfname, sleeptime = 1): from time import sleep from os import path as ospath while ospath.isfile(pidfname): sleep(sleeptime) pidfile_write(pidfname) def file_replace(afile,old,new): if afile == "" or new == "" or old == "": return False with open(afile, 'r') as f: filedata = f.read() filedata = filedata.replace(old,new) with open(afile, 'w') as f: f.write(filedata) return True
# Copyright 2018 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== """Defines trials for parameter exploration.""" from __future__ import absolute_import from __future__ import division from __future__ import print_function import time from REDACTED.tensorflow_models.mlperf.models.rough.transformer_lingvo.lingvo.core import hyperparams class Trial(object): """Base class for a trial.""" @classmethod def Params(cls): """Default parameters for a trial.""" p = hyperparams.Params() p.Define( 'report_interval_seconds', 600, 'Interval between reporting trial results and checking for early ' 'stopping.') p.Define('vizier_objective_metric_key', 'loss', 'Which eval metric to use as the "objective value" for tuning.') p.Define( 'report_during_training', False, 'Whether to report objective metrics during the training process.') return p def __init__(self, params): self._params = params.Copy() self._next_report_time = time.time() @property def report_interval_seconds(self): return self._params.report_interval_seconds @property def objective_metric_key(self): return self._params.vizier_objective_metric_key def Name(self): raise NotImplementedError('Abstract method') def OverrideModelParams(self, model_params): """Modifies `model_params` according to trial params. Through this method a `Trial` may tweak model hyperparams (e.g., learning rate, shape, depth, or width of networks). Args: model_params: the original model hyperparams. Returns: The modified `model_params`. """ raise NotImplementedError('Abstract method') def ShouldStop(self): """Returns whether the trial should stop.""" raise NotImplementedError('Abstract method') def ReportDone(self, infeasible=False, infeasible_reason=''): """Report that the trial is completed.""" raise NotImplementedError('Abstract method') def ShouldStopAndMaybeReport(self, global_step, metrics_dict): """Returns whether the trial should stop. Args: global_step: The global step counter. metrics_dict: If not None, contains the metric should be reported. If None, do nothing but returns whether the trial should stop. """ if not metrics_dict or not self._params.report_during_training: return self.ShouldStop() if time.time() < self._next_report_time: return False self._next_report_time = time.time() + self.report_interval_seconds return self._DoReportTrainingProgress(global_step, metrics_dict) def _DoReportTrainingProgress(self, global_step, metrics_dict): raise NotImplementedError('Abstract method') def ReportEvalMeasure(self, global_step, metrics_dict, checkpoint_path): """Reports eval measurement and returns whether the trial should stop.""" raise NotImplementedError('Abstract method') class NoOpTrial(Trial): """A Trial implementation that does nothing.""" def __init__(self): super(NoOpTrial, self).__init__(Trial.Params()) def Name(self): return '' def OverrideModelParams(self, model_params): return model_params def ShouldStop(self): return False def ReportDone(self, infeasible=False, infeasible_reason=''): return False def ShouldStopAndMaybeReport(self, global_step, metrics_dict): del global_step, metrics_dict # Unused return False def ReportEvalMeasure(self, global_step, metrics_dict, checkpoint_path): del global_step, metrics_dict, checkpoint_path # Unused return False
# Copyright 2009 Google Inc. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. from django.conf.urls.defaults import * urlpatterns = patterns('', (r'^$', 'django.views.generic.simple.redirect_to', {'url': '/docs'}), (r'^api(?P<the_rest>/.*)$', 'django.views.generic.simple.redirect_to', {'url': '%(the_rest)s'}), (r'^keys$', 'api.views.api_keys'), (r'^keys/(?P<consumer_key>\w+)$', 'api.views.api_key'), (r'^key$', 'api.views.api_key_legacy'), (r'^tokens', 'api.views.api_tokens'), (r'^docs$', 'api.views.api_docs'), (r'^docs/(?P<doc>\w+)$', 'api.views.api_doc'), (r'^json', 'api.views.api_call'), (r'^loaddata', 'api.views.api_loaddata'), (r'^cleardata', 'api.views.api_cleardata'), (r'^request_token', 'api.views.api_request_token'), (r'^authorize', 'api.views.api_authorize'), (r'^access_token', 'api.views.api_access_token'), (r'^sms_receive/(?P<vendor_secret>.*)$', 'api.views.api_vendor_sms_receive'), (r'^process_queue$', 'api.views.api_vendor_queue_process'), (r'^xmlrpc', 'api.views.api_xmlrpc'), ) handler404 = 'common.views.common_404' handler500 = 'common.views.common_500'
#!/usr/bin/python # # @author: Gaurav Rastogi (grastogi@avinetworks.com) # Eric Anderson (eanderson@avinetworks.com) # module_check: supported # Avi Version: 17.1.2 # # Copyright: (c) 2017 Gaurav Rastogi, <grastogi@avinetworks.com> # GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt) # ANSIBLE_METADATA = {'metadata_version': '1.1', 'status': ['preview'], 'supported_by': 'community'} DOCUMENTATION = ''' --- module: avi_vsvip author: Gaurav Rastogi (@grastogi23) <grastogi@avinetworks.com> short_description: Module for setup of VsVip Avi RESTful Object description: - This module is used to configure VsVip object - more examples at U(https://github.com/avinetworks/devops) requirements: [ avisdk ] version_added: "2.4" options: state: description: - The state that should be applied on the entity. default: present choices: ["absent", "present"] avi_api_update_method: description: - Default method for object update is HTTP PUT. - Setting to patch will override that behavior to use HTTP PATCH. version_added: "2.5" default: put choices: ["put", "patch"] avi_api_patch_op: description: - Patch operation to use when using avi_api_update_method as patch. version_added: "2.5" choices: ["add", "replace", "delete"] cloud_ref: description: - It is a reference to an object of type cloud. - Field introduced in 17.1.1. dns_info: description: - Service discovery specific data including fully qualified domain name, type and time-to-live of the dns record. - Field introduced in 17.1.1. east_west_placement: description: - Force placement on all service engines in the service engine group (container clouds only). - Field introduced in 17.1.1. - Default value when not specified in API or module is interpreted by Avi Controller as False. type: bool name: description: - Name for the vsvip object. - Field introduced in 17.1.1. required: true tenant_ref: description: - It is a reference to an object of type tenant. - Field introduced in 17.1.1. url: description: - Avi controller URL of the object. use_standard_alb: description: - This overrides the cloud level default and needs to match the se group value in which it will be used if the se group use_standard_alb value is - set. - This is only used when fip is used for vs on azure cloud. - Field introduced in 18.2.3. version_added: "2.9" type: bool uuid: description: - Uuid of the vsvip object. - Field introduced in 17.1.1. vip: description: - List of virtual service ips and other shareable entities. - Field introduced in 17.1.1. vrf_context_ref: description: - Virtual routing context that the virtual service is bound to. - This is used to provide the isolation of the set of networks the application is attached to. - It is a reference to an object of type vrfcontext. - Field introduced in 17.1.1. vsvip_cloud_config_cksum: description: - Checksum of cloud configuration for vsvip. - Internally set by cloud connector. - Field introduced in 17.2.9, 18.1.2. version_added: "2.9" extends_documentation_fragment: - avi ''' EXAMPLES = """ - name: Example to create VsVip object avi_vsvip: controller: 10.10.25.42 username: admin password: something state: present name: sample_vsvip """ RETURN = ''' obj: description: VsVip (api/vsvip) object returned: success, changed type: dict ''' from ansible.module_utils.basic import AnsibleModule try: from ansible.module_utils.network.avi.avi import ( avi_common_argument_spec, avi_ansible_api, HAS_AVI) except ImportError: HAS_AVI = False def main(): argument_specs = dict( state=dict(default='present', choices=['absent', 'present']), avi_api_update_method=dict(default='put', choices=['put', 'patch']), avi_api_patch_op=dict(choices=['add', 'replace', 'delete']), cloud_ref=dict(type='str',), dns_info=dict(type='list',), east_west_placement=dict(type='bool',), name=dict(type='str', required=True), tenant_ref=dict(type='str',), url=dict(type='str',), use_standard_alb=dict(type='bool',), uuid=dict(type='str',), vip=dict(type='list',), vrf_context_ref=dict(type='str',), vsvip_cloud_config_cksum=dict(type='str',), ) argument_specs.update(avi_common_argument_spec()) module = AnsibleModule( argument_spec=argument_specs, supports_check_mode=True) if not HAS_AVI: return module.fail_json(msg=( 'Avi python API SDK (avisdk>=17.1) or requests is not installed. ' 'For more details visit https://github.com/avinetworks/sdk.')) return avi_ansible_api(module, 'vsvip', set([])) if __name__ == '__main__': main()
#!BPY import struct import bpy import Blender def newFileName(ext): return '.'.join(Blender.Get('filename').split('.')[:-1] + [ext]) def saveAllMeshes(filename): for object in Blender.Object.Get(): if object.getType() == 'Mesh': mesh = object.getData() if (len(mesh.verts) > 0): saveMesh(filename, mesh) return def saveMesh(filename, mesh): # First, write the header. KSS. # vertex count, triangle count, texture count file = open(filename, "w") file.write(struct.pack("<I", len(mesh.verts))) file.write(struct.pack("<H", len(mesh.faces))) file.write(struct.pack("<H", len(mesh.materials))) # Get the textures we need. Store the file names in the data files # as simple null terminated strings. for mt in mesh.materials: mtexList = mt.getTextures() img = mtexList[0].tex.getImage() file.write(struct.pack("s", img.getName())) # Write the camera co-ordinates. This is our stating postiion in the map for object in Blender.Object.Get(): if object.getType() == 'Camera': location = object.getLocation(); file.write(struct.pack("<fff", *location)) # Write an interleaved array containing vertex co-ordinate, normal, and UV co-ord for face in mesh.faces: for i in range(0, 3): file.write(struct.pack("fff", *face.v[i].co)) file.write(struct.pack("fff", *face.v[i].no)) file.write(struct.pack("ff", *face.uv[i])) # Write out an index array of materials (textures) for the triangls for face in mesh.faces: file.write(struct.pack("<H", face.mat)) # Write the vertex index array. Currently not used but still here for the moment #for face in mesh.faces: # assert len(face.v) == 3 # for vertex in face.v: # file.write(struct.pack("<H", vertex.index)) inEditMode = Blender.Window.EditMode() if inEditMode: Blender.Window.EditMode(0) Blender.Window.FileSelector(saveAllMeshes, "Export for iPhone", newFileName("gldata")) if inEditMode: Blender.Window.EditMode(1)
import logging from scrapy.exceptions import NotConfigured from scrapy import signals logger = logging.getLogger(__name__) class AutoThrottle(object): def __init__(self, crawler): self.crawler = crawler if not crawler.settings.getbool('AUTOTHROTTLE_ENABLED'): raise NotConfigured self.debug = crawler.settings.getbool("AUTOTHROTTLE_DEBUG") self.target_concurrency = crawler.settings.getfloat("AUTOTHROTTLE_TARGET_CONCURRENCY") crawler.signals.connect(self._spider_opened, signal=signals.spider_opened) crawler.signals.connect(self._response_downloaded, signal=signals.response_downloaded) @classmethod def from_crawler(cls, crawler): return cls(crawler) def _spider_opened(self, spider): self.mindelay = self._min_delay(spider) self.maxdelay = self._max_delay(spider) spider.download_delay = self._start_delay(spider) def _min_delay(self, spider): s = self.crawler.settings return getattr(spider, 'download_delay', s.getfloat('DOWNLOAD_DELAY')) def _max_delay(self, spider): return self.crawler.settings.getfloat('AUTOTHROTTLE_MAX_DELAY') def _start_delay(self, spider): return max(self.mindelay, self.crawler.settings.getfloat('AUTOTHROTTLE_START_DELAY')) def _response_downloaded(self, response, request, spider): key, slot = self._get_slot(request, spider) latency = request.meta.get('download_latency') if latency is None or slot is None: return olddelay = slot.delay self._adjust_delay(slot, latency, response) if self.debug: diff = slot.delay - olddelay size = len(response.body) conc = len(slot.transferring) logger.info( "slot: %(slot)s | conc:%(concurrency)2d | " "delay:%(delay)5d ms (%(delaydiff)+d) | " "latency:%(latency)5d ms | size:%(size)6d bytes", { 'slot': key, 'concurrency': conc, 'delay': slot.delay * 1000, 'delaydiff': diff * 1000, 'latency': latency * 1000, 'size': size }, extra={'spider': spider} ) def _get_slot(self, request, spider): key = request.meta.get('download_slot') return key, self.crawler.engine.downloader.slots.get(key) def _adjust_delay(self, slot, latency, response): """Define delay adjustment policy""" # If a server needs `latency` seconds to respond then # we should send a request each `latency/N` seconds # to have N requests processed in parallel target_delay = latency / self.target_concurrency # Adjust the delay to make it closer to target_delay new_delay = (slot.delay + target_delay) / 2.0 # If target delay is bigger than old delay, then use it instead of mean. # It works better with problematic sites. new_delay = max(target_delay, new_delay) # Make sure self.mindelay <= new_delay <= self.max_delay new_delay = min(max(self.mindelay, new_delay), self.maxdelay) # Dont adjust delay if response status != 200 and new delay is smaller # than old one, as error pages (and redirections) are usually small and # so tend to reduce latency, thus provoking a positive feedback by # reducing delay instead of increase. if response.status != 200 and new_delay <= slot.delay: return slot.delay = new_delay
# -*- coding: utf-8 -*- import re import os import sys import xbmc import urllib import xbmcvfs import xbmcaddon import xbmcgui,xbmcplugin from bs4 import BeautifulSoup import requests import simplejson __addon__ = xbmcaddon.Addon() __author__ = __addon__.getAddonInfo('author') __scriptid__ = __addon__.getAddonInfo('id') __scriptname__ = __addon__.getAddonInfo('name') __version__ = __addon__.getAddonInfo('version') __language__ = __addon__.getLocalizedString __cwd__ = xbmc.translatePath( __addon__.getAddonInfo('path') ).decode("utf-8") __profile__ = xbmc.translatePath( __addon__.getAddonInfo('profile') ).decode("utf-8") __resource__ = xbmc.translatePath( os.path.join( __cwd__, 'resources', 'lib' ) ).decode("utf-8") __temp__ = xbmc.translatePath( os.path.join( __profile__, 'temp') ).decode("utf-8") sys.path.append (__resource__) SUBHD_API = 'http://subhd.com/search/%s' SUBHD_BASE = 'http://subhd.com' UserAgent = 'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)' def log(module, msg): xbmc.log((u"%s::%s - %s" % (__scriptname__,module,msg,)).encode('utf-8'),level=xbmc.LOGDEBUG ) def normalizeString(str): return str def session_get(url, id='', referer=''): if id: HEADERS={'Accept': 'application/json, text/javascript, */*; q=0.01', 'Accept-Encoding': 'gzip, deflate', 'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2', 'Host': 'subhd.com', 'Origin': 'http://subhd.com', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:58.0) Gecko/20100101 Firefox/58.0'} s = requests.Session() s.headers.update(HEADERS) r = s.get(referer) s.headers.update({'Referer': referer}) r = s.post(url, data={'sub_id': id}) return r.content else: HEADERS={'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8', 'Accept-Encoding': 'gzip, deflate, sdch', 'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:58.0) Gecko/20100101 Firefox/58.0'} s = requests.Session() s.headers.update(HEADERS) r = s.get(url) return r.content def Search( item ): subtitles_list = [] log(sys._getframe().f_code.co_name, "Search for [%s] by name" % (os.path.basename( item['file_original_path'] ),)) if item['mansearch']: search_string = item['mansearchstr'] elif len(item['tvshow']) > 0: search_string = "%s S%.2dE%.2d" % (item['tvshow'], int(item['season']), int(item['episode'])) else: search_string = item['title'] url = SUBHD_API % (urllib.quote(search_string)) data = session_get(url) try: soup = BeautifulSoup(data, "html.parser") except: return results = soup.find_all("div", class_="box") # if can't find subtitle for the specified episode, try the whole season instead if (len(results) == 0) and (len(item['tvshow']) > 0): search_string = "%s S%.2d" % (item['tvshow'], int(item['season'])) url = SUBHD_API % (urllib.quote(search_string)) data = session_get(url) try: soup = BeautifulSoup(data, "html.parser") except: return results = [x for x in soup.find_all("div", class_="box") if x.find('div', class_='tvlist')] for it in results: link = SUBHD_BASE + it.find("div", class_="d_title").a.get('href').encode('utf-8') version = it.find("div", class_="d_title").a.get('title').encode('utf-8') if version.find(' ') == 0: version = version.split()[1] try: group = it.find("div", class_="d_zu").text.encode('utf-8') if group.isspace(): group = '' except: group = '' if group and (version.find(group) == -1): version += ' ' + group try: r2 = it.find_all("span", class_="label") langs = [x.text.encode('utf-8') for x in r2][:-1] except: langs = '' name = '%s (%s)' % (version, ",".join(langs)) if ('' in langs) and not(('' in langs) or ('' in langs)): subtitles_list.append({"language_name":"English", "filename":name, "link":link, "language_flag":'en', "rating":"0", "lang":langs}) else: subtitles_list.append({"language_name":"Chinese", "filename":name, "link":link, "language_flag":'zh', "rating":"0", "lang":langs}) if subtitles_list: for it in subtitles_list: listitem = xbmcgui.ListItem(label=it["language_name"], label2=it["filename"], iconImage=it["rating"], thumbnailImage=it["language_flag"] ) listitem.setProperty( "sync", "false" ) listitem.setProperty( "hearing_imp", "false" ) url = "plugin://%s/?action=download&link=%s&lang=%s" % (__scriptid__, it["link"], it["lang"] ) xbmcplugin.addDirectoryItem(handle=int(sys.argv[1]),url=url,listitem=listitem,isFolder=False) def rmtree(path): if isinstance(path, unicode): path = path.encode('utf-8') dirs, files = xbmcvfs.listdir(path) for dir in dirs: rmtree(os.path.join(path, dir)) for file in files: xbmcvfs.delete(os.path.join(path, file)) xbmcvfs.rmdir(path) def Download(url,lang): try: rmtree(__temp__) except: pass try: os.makedirs(__temp__) except: pass referer = url subtitle_list = [] exts = [".srt", ".sub", ".txt", ".smi", ".ssa", ".ass" ] try: data = session_get(url) soup = BeautifulSoup(data, "html.parser") id = soup.find("button", class_="btn btn-danger btn-sm").get("sid").encode('utf-8') url = "http://subhd.com/ajax/down_ajax" data = session_get(url, id=id, referer=referer) json_response = simplejson.loads(data) if json_response['success']: url = json_response['url'].replace(r'\/','/').decode("unicode-escape").encode('utf-8') if url[:4] <> 'http': url = 'http://subhd.com%s' % (url) log(sys._getframe().f_code.co_name, "Downloading %s" % (url.decode('utf-8'))) data = session_get(url) else: msg = json_response['msg'].decode("unicode-escape") xbmc.executebuiltin((u'XBMC.Notification("subhd","%s")' % (msg)).encode('utf-8'), True) data = '' except: log(sys._getframe().f_code.co_name, "%s (%d) [%s]" % ( sys.exc_info()[2].tb_frame.f_code.co_name, sys.exc_info()[2].tb_lineno, sys.exc_info()[1] )) return [] if len(data) < 1024: return [] zip = os.path.join(__temp__, "subtitles%s" % os.path.splitext(url)[1]) with open(zip, "wb") as subFile: subFile.write(data) subFile.close() xbmc.sleep(500) if data[:4] == 'Rar!' or data[:2] == 'PK': xbmc.executebuiltin(('XBMC.Extract("%s","%s")' % (zip,__temp__,)).encode('utf-8'), True) path = __temp__ dirs, files = xbmcvfs.listdir(path) if ('__MACOSX') in dirs: dirs.remove('__MACOSX') if len(dirs) > 0: path = os.path.join(__temp__, dirs[0].decode('utf-8')) dirs, files = xbmcvfs.listdir(path) list = [] for subfile in files: if (os.path.splitext( subfile )[1] in exts): list.append(subfile.decode('utf-8')) if len(list) == 1: subtitle_list.append(os.path.join(path, list[0])) elif len(list) > 1: sel = xbmcgui.Dialog().select('', list) if sel == -1: sel = 0 subtitle_list.append(os.path.join(path, list[sel])) return subtitle_list def get_params(): param=[] paramstring=sys.argv[2] if len(paramstring)>=2: params=paramstring cleanedparams=params.replace('?','') if (params[len(params)-1]=='/'): params=params[0:len(params)-2] pairsofparams=cleanedparams.split('&') param={} for i in range(len(pairsofparams)): splitparams={} splitparams=pairsofparams[i].split('=') if (len(splitparams))==2: param[splitparams[0]]=splitparams[1] return param params = get_params() if params['action'] == 'search' or params['action'] == 'manualsearch': item = {} item['temp'] = False item['rar'] = False item['mansearch'] = False item['year'] = xbmc.getInfoLabel("VideoPlayer.Year") # Year item['season'] = str(xbmc.getInfoLabel("VideoPlayer.Season")) # Season item['episode'] = str(xbmc.getInfoLabel("VideoPlayer.Episode")) # Episode item['tvshow'] = normalizeString(xbmc.getInfoLabel("VideoPlayer.TVshowtitle")) # Show item['title'] = normalizeString(xbmc.getInfoLabel("VideoPlayer.OriginalTitle")) # try to get original title item['file_original_path'] = urllib.unquote(xbmc.Player().getPlayingFile().decode('utf-8')) # Full path of a playing file item['3let_language'] = [] if 'searchstring' in params: item['mansearch'] = True item['mansearchstr'] = params['searchstring'] for lang in urllib.unquote(params['languages']).decode('utf-8').split(","): item['3let_language'].append(xbmc.convertLanguage(lang,xbmc.ISO_639_2)) if item['title'] == "": item['title'] = xbmc.getInfoLabel("VideoPlayer.Title") # no original title, get just Title if item['title'] == os.path.basename(xbmc.Player().getPlayingFile()): # get movie title and year if is filename title, year = xbmc.getCleanMovieTitle(item['title']) item['title'] = normalizeString(title.replace('[','').replace(']','')) item['year'] = year if item['episode'].lower().find("s") > -1: # Check if season is "Special" item['season'] = "0" # item['episode'] = item['episode'][-1:] if ( item['file_original_path'].find("http") > -1 ): item['temp'] = True elif ( item['file_original_path'].find("rar://") > -1 ): item['rar'] = True item['file_original_path'] = os.path.dirname(item['file_original_path'][6:]) elif ( item['file_original_path'].find("stack://") > -1 ): stackPath = item['file_original_path'].split(" , ") item['file_original_path'] = stackPath[0][8:] Search(item) elif params['action'] == 'download': subs = Download(params["link"], params["lang"]) for sub in subs: listitem = xbmcgui.ListItem(label=sub) xbmcplugin.addDirectoryItem(handle=int(sys.argv[1]),url=sub,listitem=listitem,isFolder=False) xbmcplugin.endOfDirectory(int(sys.argv[1]))
from __future__ import absolute_import from base64 import b64encode from ..packages.six import b ACCEPT_ENCODING = 'gzip,deflate' def make_headers(keep_alive=None, accept_encoding=None, user_agent=None, basic_auth=None, proxy_basic_auth=None, disable_cache=None): """ Shortcuts for generating request headers. :param keep_alive: If ``True``, adds 'connection: keep-alive' header. :param accept_encoding: Can be a boolean, list, or string. ``True`` translates to 'gzip,deflate'. List will get joined by comma. String will be used as provided. :param user_agent: String representing the user-agent you want, such as "python-urllib3/0.6" :param basic_auth: Colon-separated username:password string for 'authorization: basic ...' auth header. :param proxy_basic_auth: Colon-separated username:password string for 'proxy-authorization: basic ...' auth header. :param disable_cache: If ``True``, adds 'cache-control: no-cache' header. Example:: >>> make_headers(keep_alive=True, user_agent="Batman/1.0") {'connection': 'keep-alive', 'user-agent': 'Batman/1.0'} >>> make_headers(accept_encoding=True) {'accept-encoding': 'gzip,deflate'} """ headers = {} if accept_encoding: if isinstance(accept_encoding, str): pass elif isinstance(accept_encoding, list): accept_encoding = ','.join(accept_encoding) else: accept_encoding = ACCEPT_ENCODING headers['accept-encoding'] = accept_encoding if user_agent: headers['user-agent'] = user_agent if keep_alive: headers['connection'] = 'keep-alive' if basic_auth: headers['authorization'] = 'Basic ' + \ b64encode(b(basic_auth)).decode('utf-8') if proxy_basic_auth: headers['proxy-authorization'] = 'Basic ' + \ b64encode(b(proxy_basic_auth)).decode('utf-8') if disable_cache: headers['cache-control'] = 'no-cache' return headers
from testbase import * class SimpleRemoveTests(OperationsTests): @staticmethod def buildPkgs(pkgs, *args): pkgs.leaf = FakePackage('foo', '2.5', '1.1', '0', 'noarch') pkgs.leaf.addFile('/bin/foo') pkgs.requires_leaf = FakePackage('bar', '4') pkgs.requires_leaf.addRequires('foo') pkgs.requires_file = FakePackage('barkeeper', '0.8') pkgs.requires_file.addRequires('/bin/foo') pkgs.rr_leaf = FakePackage('baz', '5.3') pkgs.rr_leaf.addRequires('bar') pkgs.provides_leaf = FakePackage('foo-ng', '2.5') pkgs.provides_leaf.addProvides('foo') def testRemoveSingle(self): p = self.pkgs res, msg = self.runOperation(['remove', 'foo'], [p.leaf], []) self.assert_(res=='ok', msg) self.assertResult( () ) def testRemoveRequired(self): p = self.pkgs res, msg = self.runOperation(['remove', 'foo'], [p.leaf, p.requires_leaf], []) self.assert_(res=='ok', msg) self.assertResult( () ) def testRemoveRequiredMissing(self): p = self.pkgs res, msg = self.runOperation(['remove', 'bar'], [p.requires_leaf], []) self.assert_(res=='ok', msg) self.assertResult( () ) def testRemoveRequiredProvided(self): p = self.pkgs res, msg = self.runOperation(['remove', 'foo'], [p.leaf, p.requires_leaf, p.provides_leaf], []) self.assert_(res=='ok', msg) self.assertResult( (p.requires_leaf, p.provides_leaf) ) def testRemoveRequiredAvailable(self): p = self.pkgs res, msg = self.runOperation(['remove', 'foo'], [p.leaf, p.requires_leaf], [p.provides_leaf]) self.assert_(res=='ok', msg) self.assertResult( () ) def testRemoveRequiredChain(self): p = self.pkgs res, msg = self.runOperation(['remove', 'foo'], [p.leaf, p.requires_leaf, p.rr_leaf], []) self.assert_(res=='ok', msg) self.assertResult( () ) def testRemoveRequiredFile(self): p = self.pkgs res, msg = self.runOperation(['remove', 'foo'], [p.leaf, p.requires_file], []) self.assert_(res=='ok', msg) self.assertResult( () ) def testShellUpRm1(self): """ Do an update for a package, and then rm it. """ pi1 = FakePackage('foo', '1', '1', '0', 'x86_64') pa1 = FakePackage('foo', '2', '1', '0', 'x86_64') res, msg = self.runOperation((['update', 'foo'], ['remove', 'foo'], ), [pi1], [pa1], multi_cmds=True) self.assert_(res=='ok', msg) self.assertResult(()) def testShellUpRm2(self): """ Do an update for a package, and then rm it. """ pi1 = FakePackage('foo', '1', '1', '0', 'x86_64') pi2 = FakePackage('foo', '1', '1', '0', 'i686') pa1 = FakePackage('foo', '2', '1', '0', 'x86_64') pa2 = FakePackage('foo', '2', '1', '0', 'i686') res, msg = self.runOperation((['update', 'foo'], ['remove', 'foo.i686'], ), [pi1, pi2], [pa1, pa2], multi_cmds=True) self.assert_(res=='ok', msg) self.assertResult((pi1, )) def testShellUpRm3(self): """ Do an update for a package, and then rm it. """ pi1 = FakePackage('foo', '1', '1', '0', 'x86_64') pi2 = FakePackage('foo', '1', '1', '0', 'i686') pa1 = FakePackage('foo', '2', '1', '0', 'x86_64') pa2 = FakePackage('foo', '2', '1', '0', 'i686') res, msg = self.runOperation((['update', 'foo'], ['remove', 'foo.x86_64'], ), [pi1, pi2], [pa1, pa2], multi_cmds=True) self.assert_(res=='ok', msg) self.assertResult((pi2, )) def testShellUpRm4(self): """ Do an update for a package, and then rm it. """ pi1 = FakePackage('foo', '1', '1', '0', 'x86_64') pi2 = FakePackage('foo', '1', '1', '0', 'i686') pa1 = FakePackage('foo', '2', '1', '0', 'x86_64') pa2 = FakePackage('foo', '2', '1', '0', 'i686') res, msg = self.runOperation((['update', 'foo-2-1'], ['remove', 'foo.i686'], ), [pi1, pi2], [pa1, pa2], multi_cmds=True) self.assert_(res=='ok', msg) self.assertResult((pi1,))
from django.db import models # from simple_history.models import HistoricalRecords #to store history from django.db.models.fields import * from django.core.mail import send_mail from django.template.loader import get_template from django.template import Context from django.conf import settings import caching.base # cache data #to validate min and max value from django.core.validators import MaxValueValidator, MinValueValidator from smart_selects.db_fields import ChainedForeignKey """ Model for Aeeo """ class Aeeo(models.Model): block_code=models.PositiveIntegerField() username=models.CharField(max_length=11) emis_blocks=models.CharField(max_length=22) emis_district=models.CharField(max_length=15) block_id=models.ForeignKey('Block') district_id=models.ForeignKey('District') def __unicode__(self): return u'%s' % (self.username) """ Model for Teacher App Staff_Category """ class Staff_Category(models.Model): staff_category_code = models.CharField(max_length=100) staff_category_name = models.CharField(max_length=100) def __unicode__(self): return u'%s' % (self. staff_category_name) """ Model for Designation """ class Designation(models.Model): designation_code = models.CharField(max_length=10) designation_name = models.CharField(max_length=1000) stafs = models.ForeignKey('Staff_Category') def __unicode__(self): return u'%s%s' % (self.designation_name,self.stafs) class Subject(models.Model): subject_code = models.CharField(max_length=10) subject_name = models.CharField(max_length=1000) designation = models.ForeignKey('Designation') def __unicode__(self): return u'%s' % (self.subject_name) """ Model for Assembly constituencies """ class Assembly(models.Model): assembly_name = models.CharField(max_length=100) district = models.ForeignKey('District') def __unicode__(self): return u'%s' % (self.assembly_name) """ Model for Parliamentary constituencies """ class Parliamentary(models.Model): parliamentary_name = models.CharField(max_length=100) def __unicode__(self): return u'%s' % (self.parliamentary_name) """ Model for State """ class State(caching.base.CachingMixin, models.Model): state_name = models.CharField(max_length=100) objects = caching.base.CachingManager() def __unicode__(self): return u'%s' % (self.state_name) """ Model for District """ class District(caching.base.CachingMixin, models.Model): district_code = models.PositiveIntegerField( unique=True, validators=[MinValueValidator(3300), MaxValueValidator(3399)]) district_name = models.CharField(max_length=100) objects = caching.base.CachingManager() def __unicode__(self): return u'%s' % (self.district_name) """ Model for Block """ class Block(caching.base.CachingMixin, models.Model): block_code = models.PositiveIntegerField( unique=True, validators=[MinValueValidator(330000), MaxValueValidator(339999)]) block_name = models.CharField(max_length=100) block_type = models.CharField(max_length=50) district = models.ForeignKey('District') objects = caching.base.CachingManager() def __unicode__(self): return u'%s %s %s' % (self.block_code, self.block_name, self.block_type) """ Model for Zone Type """ class Zone_type(models.Model): zone_type = models.CharField(max_length=25) def __unicode__(self): return u'%s' % (self.zone_type) """ Model for zone """ class Zone (models.Model): zone_type = models.ForeignKey(Zone_type) code = models.PositiveIntegerField(unique=True, validators=[ MinValueValidator(330000000), MaxValueValidator(339999999)]) name = models.CharField(max_length=100) block = models.ForeignKey('Block') def __unicode__(self): return u'%s %s' % (self.name, self.zone_type) """ Model for Habitation """ class Habitation(caching.base.CachingMixin, models.Model): code = models.PositiveIntegerField(unique=True) name = models.CharField(max_length=100) #ward_number = models.CharField(max_length=100) block = models.ForeignKey('Block') zone = ChainedForeignKey( Zone, chained_field='block', chained_model_field='block', auto_choose=True) objects = caching.base.CachingManager() def __unicode__(self): return u'%s' % (self.name) """ Model for School """ class School(models.Model): school_code = BigIntegerField() school_name = models.CharField(max_length=100) district = models.ForeignKey('District') block = ChainedForeignKey( Block, chained_field='district', chained_model_field='district', auto_choose=True) #block = models.ForeignKey('Block') habitation = ChainedForeignKey( Habitation, chained_field='block', chained_model_field='block', auto_choose=True) management = models.ForeignKey('Management') category = models.ForeignKey('Category') student_id_count = models.PositiveIntegerField() def __unicode__(self): return u'%s %s %s %s %s %s %s %s' % (self.school_code, self.school_name, self.habitation.name, self.district.district_name, self.block.block_name, self.management.management_name, self.category.category_name, self.student_id_count) """ Model for Taluk """ class Taluk(caching.base.CachingMixin, models.Model): taluk_name = models.CharField(max_length=100) district = models.ForeignKey('District') objects = caching.base.CachingManager() def __unicode__(self): return u'%s' % (self.taluk_name) """ Model for Educational District : """ class Educational_district(models.Model): educational_district = models.CharField(max_length=100) def __unicode__(self): return u'%s' % (self.educational_district) """ Model for Educational Block/ Mandal/ Taluk Name """ class Educational_block(models.Model): educational_block = models.CharField(max_length=100) district = models.ForeignKey('District') def __unicode__(self): return u'%s' % (self.educational_block) """ Model for Revenue Block/ Mandal / Taluk name : """ class Revenue_block(models.Model): revenue_block = models.CharField(max_length=100) district = models.ForeignKey('District') def __unicode__(self): return u'%s' % (self.revenue_block) """ Model for Community """ class Community(models.Model): community_code = models.CharField(max_length=100) community_name = models.CharField(max_length=100) religion = models.ForeignKey('Religion') def __unicode__(self): return u'%s' % (self.community_name) """ Model for Sub Castes """ class Sub_Castes(models.Model): caste_code = models.CharField(max_length=10) caste_name = models.CharField(max_length=1000) community = models.ForeignKey('Community') def __unicode__(self): return u'%s %s %s' % (self.caste_name,self.caste_code, self.community.community_name) """ Model for Religion """ class Religion(models.Model): religion_name = models.CharField(max_length=100) def __unicode__(self): return u'%s' % (self.religion_name) """ Model for Language """ class Language(models.Model): language_name = models.CharField(max_length=100) def __unicode__(self): return u'%s' % (self.language_name) """ Model for Differently Abled """ class Differently_abled(models.Model): da_code = models.CharField(max_length=100) da_name = models.CharField(max_length=100) def __unicode__(self): return u'%s %s' % (self.da_code, self.da_name) """ Model for Disadvantaged Group """ class Disadvantaged_group(models.Model): dis_group_name = models.CharField(max_length=100) def __unicode__(self): return u'%s' % (self.dis_group_name) """ Model for Government Schemes """ class Schemes(models.Model): scheme_code = models.CharField(max_length=100) scheme_name = models.CharField(max_length=100) def __unicode__(self): return u'%s %s' % (self.scheme_code, self.scheme_name) """ Model for School Management """ class Management(models.Model): management_code = models.CharField(max_length=100) management_name = models.CharField(max_length=100) def __unicode__(self): return u'%s %s' % (self.management_code, self.management_name) """ Model for School Category """ class Category(models.Model): category_code = models.CharField(max_length=100) category_name = models.CharField(max_length=100) def __unicode__(self): return u'%s %s' % (self.category_code, self.category_name) """ Model for Nationality """ class Nationality(models.Model): nationality = models.CharField(max_length=50) def __unicode__(self): return u'%s' % (self.nationality) """ Model for class studying """ class Class_Studying(models.Model): class_studying = models.CharField(max_length=10) def __unicode__(self): return u'%s' % (self.class_studying) """ Model for group code for hsc """ class Group_code(models.Model): group_code = models.PositiveIntegerField() group_name = models.CharField(max_length=100) group_description = models.TextField(max_length=500) def __unicode__(self): return u'%s %s %s' % (self.group_code, self.group_name, self.group_description) """ Model for Education Medium """ class Education_medium(models.Model): education_medium = models.CharField(max_length=15) def __unicode__(self): return u'%s' % (self.education_medium) """ Model for bank """ class Bank(models.Model): bank = models.CharField(max_length=30) def __unicode__(self): return u'%s' % (self.bank) """ Model for academic unique_for_year """ class Academic_Year(models.Model): academic_year = models.CharField(max_length=9) def __unicode__(self): return u'%s' % (self.academic_year) """ Model for pool database """ class Child_detail_pool_database(models.Model): district = models.ForeignKey('District') block = ChainedForeignKey( Block, chained_field='district', chained_model_field='district', auto_choose=True) school = ChainedForeignKey( School, chained_field='block', chained_model_field='block', auto_choose=True) class_last_studied = models.CharField(max_length=20,blank=True,null=True) unique_id_no = models.BigIntegerField(blank=True, null=True) class_studying = models.ForeignKey(Class_Studying) migrated_school = models.CharField(max_length=100,blank=True,null=True) def __unicode__(self): return u'%s %s %s %s %s %s %s' % (self.district.district_name, self.block.block_name, self.school.school_name, self.class_last_studied, self.unique_id_no, self.class_studying, self.migrated_school)
#!/usr/bin/env python #coding:utf-8 # Author: mozman # Purpose: svg examples # Created: 07.11.2010 # Copyright (C) 2010, Manfred Moitzi # License: MIT License try: import svgwrite except ImportError: # if svgwrite is not 'installed' append parent dir of __file__ to sys.path import sys, os sys.path.insert(0, os.path.abspath(os.path.split(os.path.abspath(__file__))[0]+'/..')) import svgwrite from svgwrite import cm, mm def basic_shapes(name): dwg = svgwrite.Drawing(filename=name, debug=True) hlines = dwg.add(dwg.g(id='hlines', stroke='green')) for y in range(20): hlines.add(dwg.line(start=(2*cm, (2+y)*cm), end=(18*cm, (2+y)*cm))) vlines = dwg.add(dwg.g(id='vline', stroke='blue')) for x in range(17): vlines.add(dwg.line(start=((2+x)*cm, 2*cm), end=((2+x)*cm, 21*cm))) shapes = dwg.add(dwg.g(id='shapes', fill='red')) # set presentation attributes at object creation as SVG-Attributes shapes.add(dwg.circle(center=(15*cm, 8*cm), r='2.5cm', stroke='blue', stroke_width=3)) # override the 'fill' attribute of the parent group 'shapes' shapes.add(dwg.rect(insert=(5*cm, 5*cm), size=(45*mm, 45*mm), fill='blue', stroke='red', stroke_width=3)) # or set presentation attributes by helper functions of the Presentation-Mixin ellipse = shapes.add(dwg.ellipse(center=(10*cm, 15*cm), r=('5cm', '10mm'))) ellipse.fill('green', opacity=0.5).stroke('black', width=5).dasharray([20, 20]) dwg.save() if __name__ == '__main__': basic_shapes('basic_shapes.svg')
# ##### BEGIN GPL LICENSE BLOCK ##### # # This program is free software; you can redistribute it and/or # modify it under the terms of the GNU General Public License # as published by the Free Software Foundation; either version 2 # of the License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program; if not, write to the Free Software Foundation, # Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA. # # ##### END GPL LICENSE BLOCK ##### # Project Name: MakeHuman # Product Home Page: http://www.makehuman.org/ # Code Home Page: http://code.google.com/p/makehuman/ # Authors: Thomas Larsson # Script copyright (C) MakeHuman Team 2001-2014 # Coding Standards: See http://www.makehuman.org/node/165 import bpy from bpy.props import BoolProperty from mathutils import Matrix, Vector from .utils import * from . import fkik #------------------------------------------------------------- # Plane #------------------------------------------------------------- def getRigAndPlane(scn): rig = None plane = None for ob in scn.objects: if ob.select: if ob.type == 'ARMATURE': if rig: raise MocapError("Two armatures selected: %s and %s" % (rig.name, ob.name)) else: rig = ob elif ob.type == 'MESH': if plane: raise MocapError("Two meshes selected: %s and %s" % (plane.name, ob.name)) else: plane = ob if rig is None: raise MocapError("No rig selected") return rig,plane def getPlaneInfo(plane): if plane is None: ez = Vector((0,0,1)) origin = Vector((0,0,0)) rot = Matrix() else: mat = plane.matrix_world.to_3x3().normalized() ez = mat.col[2] origin = plane.location rot = mat.to_4x4() return ez,origin,rot #------------------------------------------------------------- # Offset and projection #------------------------------------------------------------- def getProjection(vec, ez): return ez.dot(Vector(vec[:3])) def getOffset(point, ez, origin): vec = Vector(point[:3]) - origin offset = -ez.dot(vec) return offset def getHeadOffset(pb, ez, origin): head = pb.matrix.col[3] return getOffset(head, ez, origin) def getTailOffset(pb, ez, origin): head = pb.matrix.col[3] y = pb.matrix.col[1] tail = head + y*pb.length return getOffset(tail, ez, origin) def addOffset(pb, offset, ez): gmat = pb.matrix.copy() x,y,z = offset*ez gmat.col[3] += Vector((x,y,z,0)) pmat = fkik.getPoseMatrix(gmat, pb) fkik.insertLocation(pb, pmat) #------------------------------------------------------------- # Toe below ball #------------------------------------------------------------- def toesBelowBall(context): scn = context.scene rig,plane = getRigAndPlane(scn) try: useIk = rig["MhaLegIk_L"] or rig["MhaLegIk_R"] except KeyError: useIk = False if useIk: raise MocapError("Toe Below Ball only for FK feet") layers = list(rig.data.layers) startProgress("Keep toes down") frames = getActiveFramesBetweenMarkers(rig, scn) print("Left toe") toeBelowBall(scn, frames, rig, plane, ".L") print("Right toe") toeBelowBall(scn, frames, rig, plane, ".R") endProgress("Toes kept down") rig.data.layers = layers def toeBelowBall(scn, frames, rig, plane, suffix): from .retarget import getLocks foot,toe,mBall,mToe,mHeel = getFkFeetBones(rig, suffix) ez,origin,rot = getPlaneInfo(plane) order,lock = getLocks(toe, scn) factor = 1.0/toe.length nFrames = len(frames) if mBall: for n,frame in enumerate(frames): scn.frame_set(frame) showProgress(n, frame, nFrames) zToe = getProjection(mToe.matrix.col[3], ez) zBall = getProjection(mBall.matrix.col[3], ez) if zToe > zBall: pmat = offsetToeRotation(toe, ez, factor, order, lock, scn) else: pmat = fkik.getPoseMatrix(toe.matrix, toe) pmat = keepToeRotationNegative(pmat, scn) fkik.insertRotation(toe, pmat) else: for n,frame in enumerate(frames): scn.frame_set(frame) showProgress(n, frame, nFrames) dzToe = getProjection(toe.matrix.col[1], ez) if dzToe > 0: pmat = offsetToeRotation(toe, ez, factor, order, lock, scn) else: pmat = fkik.getPoseMatrix(toe.matrix, toe) pmat = keepToeRotationNegative(pmat, scn) fkik.insertRotation(toe, pmat) def offsetToeRotation(toe, ez, factor, order, lock, scn): from .retarget import correctMatrixForLocks mat = toe.matrix.to_3x3() y = mat.col[1] y -= ez.dot(y)*ez y.normalize() x = mat.col[0] x -= x.dot(y)*y x.normalize() z = x.cross(y) mat.col[0] = x mat.col[1] = y mat.col[2] = z gmat = mat.to_4x4() gmat.col[3] = toe.matrix.col[3] pmat = fkik.getPoseMatrix(gmat, toe) return correctMatrixForLocks(pmat, order, lock, toe, scn.McpUseLimits) def keepToeRotationNegative(pmat, scn): euler = pmat.to_3x3().to_euler('YZX') if euler.x > 0: pmat0 = pmat euler.x = 0 pmat = euler.to_matrix().to_4x4() pmat.col[3] = pmat0.col[3] return pmat class VIEW3D_OT_McpOffsetToeButton(bpy.types.Operator): bl_idname = "mcp.offset_toe" bl_label = "Offset Toes" bl_description = "Keep toes below balls" bl_options = {'UNDO'} def execute(self, context): from .target import getTargetArmature getTargetArmature(context.object, context.scene) try: toesBelowBall(context) except MocapError: bpy.ops.mcp.error('INVOKE_DEFAULT') return{'FINISHED'} #------------------------------------------------------------- # Floor #------------------------------------------------------------- def floorFoot(context): startProgress("Keep feet above floor") scn = context.scene rig,plane = getRigAndPlane(scn) try: useIk = rig["MhaLegIk_L"] or rig["MhaLegIk_R"] except KeyError: useIk = False frames = getActiveFramesBetweenMarkers(rig, scn) if useIk: floorIkFoot(rig, plane, scn, frames) else: floorFkFoot(rig, plane, scn, frames) endProgress("Feet kept above floor") def getFkFeetBones(rig, suffix): foot = getTrgBone("foot" + suffix, rig) toe = getTrgBone("toe" + suffix, rig) try: mBall = rig.pose.bones["ball.marker" + suffix] mToe = rig.pose.bones["toe.marker" + suffix] mHeel = rig.pose.bones["heel.marker" + suffix] except KeyError: mBall = mToe = mHeel = None return foot,toe,mBall,mToe,mHeel def floorFkFoot(rig, plane, scn, frames): hips = getTrgBone("hips", rig) lFoot,lToe,lmBall,lmToe,lmHeel = getFkFeetBones(rig, ".L") rFoot,rToe,rmBall,rmToe,rmHeel = getFkFeetBones(rig, ".R") ez,origin,rot = getPlaneInfo(plane) nFrames = len(frames) for n,frame in enumerate(frames): scn.frame_set(frame) fkik.updateScene() offset = 0 if scn.McpFloorLeft: offset = getFkOffset(rig, ez, origin, lFoot, lToe, lmBall, lmToe, lmHeel) if scn.McpFloorRight: rOffset = getFkOffset(rig, ez, origin, rFoot, rToe, rmBall, rmToe, rmHeel) if rOffset > offset: offset = rOffset showProgress(n, frame, nFrames) if offset > 0: addOffset(hips, offset, ez) def getFkOffset(rig, ez, origin, foot, toe, mBall, mToe, mHeel): if mBall: offset = toeOffset = getHeadOffset(mToe, ez, origin) ballOffset = getHeadOffset(mBall, ez, origin) if ballOffset > offset: offset = ballOffset heelOffset = getHeadOffset(mHeel, ez, origin) if heelOffset > offset: offset = heelOffset elif toe: offset = getTailOffset(toe, ez, origin) ballOffset = getHeadOffset(toe, ez, origin) if ballOffset > offset: offset = ballOffset ball = toe.matrix.col[3] y = toe.matrix.col[1] heel = ball - y*foot.length heelOffset = getOffset(heel, ez, origin) if heelOffset > offset: offset = heelOffset else: offset = 0 return offset def floorIkFoot(rig, plane, scn, frames): root = rig.pose.bones["root"] lleg = rig.pose.bones["foot.ik.L"] rleg = rig.pose.bones["foot.ik.R"] ez,origin,rot = getPlaneInfo(plane) fillKeyFrames(lleg, rig, frames, 3, mode='location') fillKeyFrames(rleg, rig, frames, 3, mode='location') if scn.McpFloorHips: fillKeyFrames(root, rig, frames, 3, mode='location') nFrames = len(frames) for n,frame in enumerate(frames): scn.frame_set(frame) showProgress(n, frame, nFrames) if scn.McpFloorLeft: lOffset = getIkOffset(rig, ez, origin, lleg) if lOffset > 0: addOffset(lleg, lOffset, ez) else: lOffset = 0 if scn.McpFloorRight: rOffset = getIkOffset(rig, ez, origin, rleg) if rOffset > 0: addOffset(rleg, rOffset, ez) else: rOffset = 0 hOffset = min(lOffset,rOffset) if hOffset > 0 and scn.McpFloorHips: addOffset(root, hOffset, ez) def getIkOffset(rig, ez, origin, leg): offset = getHeadOffset(leg, ez, origin) tailOffset = getTailOffset(leg, ez, origin) if tailOffset > offset: offset = tailOffset return offset foot = rig.pose.bones["foot.rev" + suffix] toe = rig.pose.bones["toe.rev" + suffix] ballOffset = getTailOffset(toe, ez, origin) if ballOffset > offset: offset = ballOffset ball = foot.matrix.col[3] y = toe.matrix.col[1] heel = ball + y*foot.length heelOffset = getOffset(heel, ez, origin) if heelOffset > offset: offset = heelOffset return offset class VIEW3D_OT_McpFloorFootButton(bpy.types.Operator): bl_idname = "mcp.floor_foot" bl_label = "Keep Feet Above Floor" bl_description = "Keep Feet Above Plane" bl_options = {'UNDO'} def execute(self, context): from .target import getTargetArmature getTargetArmature(context.object, context.scene) try: floorFoot(context) except MocapError: bpy.ops.mcp.error('INVOKE_DEFAULT') return{'FINISHED'}
# -*- test-case-name: twisted.conch.test.test_session -*- # Copyright (c) Twisted Matrix Laboratories. # See LICENSE for details. """ This module contains the implementation of SSHSession, which (by default) allows access to a shell and a python interpreter over SSH. Maintainer: Paul Swartz """ import struct import signal import sys import os from zope.interface import implements from twisted.internet import interfaces, protocol from twisted.python import log from twisted.conch.interfaces import ISession from twisted.conch.ssh import common, channel class SSHSession(channel.SSHChannel): name = 'session' def __init__(self, *args, **kw): channel.SSHChannel.__init__(self, *args, **kw) self.buf = '' self.client = None self.session = None def request_subsystem(self, data): subsystem, ignored= common.getNS(data) log.msg('asking for subsystem "%s"' % subsystem) client = self.avatar.lookupSubsystem(subsystem, data) if client: pp = SSHSessionProcessProtocol(self) proto = wrapProcessProtocol(pp) client.makeConnection(proto) pp.makeConnection(wrapProtocol(client)) self.client = pp return 1 else: log.msg('failed to get subsystem') return 0 def request_shell(self, data): log.msg('getting shell') if not self.session: self.session = ISession(self.avatar) try: pp = SSHSessionProcessProtocol(self) self.session.openShell(pp) except: log.deferr() return 0 else: self.client = pp return 1 def request_exec(self, data): if not self.session: self.session = ISession(self.avatar) f,data = common.getNS(data) log.msg('executing command "%s"' % f) try: pp = SSHSessionProcessProtocol(self) self.session.execCommand(pp, f) except: log.deferr() return 0 else: self.client = pp return 1 def request_pty_req(self, data): if not self.session: self.session = ISession(self.avatar) term, windowSize, modes = parseRequest_pty_req(data) log.msg('pty request: %s %s' % (term, windowSize)) try: self.session.getPty(term, windowSize, modes) except: log.err() return 0 else: return 1 def request_window_change(self, data): if not self.session: self.session = ISession(self.avatar) winSize = parseRequest_window_change(data) try: self.session.windowChanged(winSize) except: log.msg('error changing window size') log.err() return 0 else: return 1 def dataReceived(self, data): if not self.client: #self.conn.sendClose(self) self.buf += data return self.client.transport.write(data) def extReceived(self, dataType, data): if dataType == connection.EXTENDED_DATA_STDERR: if self.client and hasattr(self.client.transport, 'writeErr'): self.client.transport.writeErr(data) else: log.msg('weird extended data: %s'%dataType) def eofReceived(self): if self.session: self.session.eofReceived() elif self.client: self.conn.sendClose(self) def closed(self): if self.session: self.session.closed() elif self.client: self.client.transport.loseConnection() #def closeReceived(self): # self.loseConnection() # don't know what to do with this def loseConnection(self): if self.client: self.client.transport.loseConnection() channel.SSHChannel.loseConnection(self) class _ProtocolWrapper(protocol.ProcessProtocol): """ This class wraps a L{Protocol} instance in a L{ProcessProtocol} instance. """ def __init__(self, proto): self.proto = proto def connectionMade(self): self.proto.connectionMade() def outReceived(self, data): self.proto.dataReceived(data) def processEnded(self, reason): self.proto.connectionLost(reason) class _DummyTransport: def __init__(self, proto): self.proto = proto def dataReceived(self, data): self.proto.transport.write(data) def write(self, data): self.proto.dataReceived(data) def writeSequence(self, seq): self.write(''.join(seq)) def loseConnection(self): self.proto.connectionLost(protocol.connectionDone) def wrapProcessProtocol(inst): if isinstance(inst, protocol.Protocol): return _ProtocolWrapper(inst) else: return inst def wrapProtocol(proto): return _DummyTransport(proto) # SUPPORTED_SIGNALS is a list of signals that every session channel is supposed # to accept. See RFC 4254 SUPPORTED_SIGNALS = ["ABRT", "ALRM", "FPE", "HUP", "ILL", "INT", "KILL", "PIPE", "QUIT", "SEGV", "TERM", "USR1", "USR2"] class SSHSessionProcessProtocol(protocol.ProcessProtocol): """I am both an L{IProcessProtocol} and an L{ITransport}. I am a transport to the remote endpoint and a process protocol to the local subsystem. """ implements(interfaces.ITransport) # once initialized, a dictionary mapping signal values to strings # that follow RFC 4254. _signalValuesToNames = None def __init__(self, session): self.session = session self.lostOutOrErrFlag = False def connectionMade(self): if self.session.buf: self.transport.write(self.session.buf) self.session.buf = None def outReceived(self, data): self.session.write(data) def errReceived(self, err): self.session.writeExtended(connection.EXTENDED_DATA_STDERR, err) def outConnectionLost(self): """ EOF should only be sent when both STDOUT and STDERR have been closed. """ if self.lostOutOrErrFlag: self.session.conn.sendEOF(self.session) else: self.lostOutOrErrFlag = True def errConnectionLost(self): """ See outConnectionLost(). """ self.outConnectionLost() def connectionLost(self, reason = None): self.session.loseConnection() def _getSignalName(self, signum): """ Get a signal name given a signal number. """ if self._signalValuesToNames is None: self._signalValuesToNames = {} # make sure that the POSIX ones are the defaults for signame in SUPPORTED_SIGNALS: signame = 'SIG' + signame sigvalue = getattr(signal, signame, None) if sigvalue is not None: self._signalValuesToNames[sigvalue] = signame for k, v in signal.__dict__.items(): # Check for platform specific signals, ignoring Python specific # SIG_DFL and SIG_IGN if k.startswith('SIG') and not k.startswith('SIG_'): if v not in self._signalValuesToNames: self._signalValuesToNames[v] = k + '@' + sys.platform return self._signalValuesToNames[signum] def processEnded(self, reason=None): """ When we are told the process ended, try to notify the other side about how the process ended using the exit-signal or exit-status requests. Also, close the channel. """ if reason is not None: err = reason.value if err.signal is not None: signame = self._getSignalName(err.signal) if (getattr(os, 'WCOREDUMP', None) is not None and os.WCOREDUMP(err.status)): log.msg('exitSignal: %s (core dumped)' % (signame,)) coreDumped = 1 else: log.msg('exitSignal: %s' % (signame,)) coreDumped = 0 self.session.conn.sendRequest(self.session, 'exit-signal', common.NS(signame[3:]) + chr(coreDumped) + common.NS('') + common.NS('')) elif err.exitCode is not None: log.msg('exitCode: %r' % (err.exitCode,)) self.session.conn.sendRequest(self.session, 'exit-status', struct.pack('>L', err.exitCode)) self.session.loseConnection() def getHost(self): """ Return the host from my session's transport. """ return self.session.conn.transport.getHost() def getPeer(self): """ Return the peer from my session's transport. """ return self.session.conn.transport.getPeer() def write(self, data): self.session.write(data) def writeSequence(self, seq): self.session.write(''.join(seq)) def loseConnection(self): self.session.loseConnection() class SSHSessionClient(protocol.Protocol): def dataReceived(self, data): if self.transport: self.transport.write(data) # methods factored out to make live easier on server writers def parseRequest_pty_req(data): """Parse the data from a pty-req request into usable data. @returns: a tuple of (terminal type, (rows, cols, xpixel, ypixel), modes) """ term, rest = common.getNS(data) cols, rows, xpixel, ypixel = struct.unpack('>4L', rest[: 16]) modes, ignored= common.getNS(rest[16:]) winSize = (rows, cols, xpixel, ypixel) modes = [(ord(modes[i]), struct.unpack('>L', modes[i+1: i+5])[0]) for i in range(0, len(modes)-1, 5)] return term, winSize, modes def packRequest_pty_req(term, (rows, cols, xpixel, ypixel), modes): """Pack a pty-req request so that it is suitable for sending. NOTE: modes must be packed before being sent here. """ termPacked = common.NS(term) winSizePacked = struct.pack('>4L', cols, rows, xpixel, ypixel) modesPacked = common.NS(modes) # depend on the client packing modes return termPacked + winSizePacked + modesPacked def parseRequest_window_change(data): """Parse the data from a window-change request into usuable data. @returns: a tuple of (rows, cols, xpixel, ypixel) """ cols, rows, xpixel, ypixel = struct.unpack('>4L', data) return rows, cols, xpixel, ypixel def packRequest_window_change((rows, cols, xpixel, ypixel)): """Pack a window-change request so that it is suitable for sending. """ return struct.pack('>4L', cols, rows, xpixel, ypixel) import connection
""" Opal file management. """ import sys import pycurl import opal.core class OpalFile: """ File on Opal file system """ def __init__(self, path): self.path = path def get_meta_ws(self): return '/files/meta' + self.path def get_ws(self): return '/files' + self.path def add_arguments(parser): """ Add file command specific options """ parser.add_argument('path', help='File path in Opal file system.') parser.add_argument('--download', '-dl', action='store_true', help='Download file, or folder (as a zip file).') parser.add_argument('--upload', '-up', required=False, help='Upload a local file to a folder in Opal file system.') parser.add_argument('--delete', '-dt', action='store_true', help='Delete a file on Opal file system.') parser.add_argument('--force', '-f', action='store_true', help='Skip confirmation.') parser.add_argument('--json', '-j', action='store_true', help='Pretty JSON formatting of the response') def do_command(args): """ Execute file command """ # Build and send request try: request = opal.core.OpalClient.build(opal.core.OpalClient.LoginInfo.parse(args)).new_request() request.fail_on_error().accept_json() if args.verbose: request.verbose() # buildWithAuthentication opal file file = OpalFile(args.path) # send request if args.download: response = request.get().resource(file.get_ws()).send() elif args.upload: #boundary = 'OpalPythonClient' #request.post().content_type('multipart/form-data; boundary=' + boundary).accept('text/html') #content = '--' + boundary + '\n' #content = content + 'Content-Disposition: form-data; name="fileToUpload"; filename="'+args.upload+'"\n\n' #content = content + open(args.upload,'rb').read() #content = content + '\n--' + boundary #request.content(content) request.content_upload(args.upload).accept('text/html').content_type('multipart/form-data') response = request.post().resource(file.get_ws()).send() elif args.delete: # confirm if args.force: response = request.delete().resource(file.get_ws()).send() else: print 'Delete the file "' + args.path + '"? [y/N]: ', confirmed = sys.stdin.readline().rstrip().strip() if confirmed == 'y': response = request.delete().resource(file.get_ws()).send() else: print 'Aborted.' sys.exit(0) else: response = request.get().resource(file.get_meta_ws()).send() # format response res = response.content if args.json and not args.download and not args.upload: res = response.pretty_json() # output to stdout print res except Exception, e: print >> sys.stderr, e sys.exit(2) except pycurl.error, error: print response errno, errstr = error print >> sys.stderr, 'An error occurred: ', errstr sys.exit(2)
from mock import patch import matplotlib matplotlib.use('Agg') class TestConfig: def setup(self): pass # setup() before each test method def teardown(self): pass # teardown() after each test method @classmethod def setup_class(cls): pass # setup_class() before any methods in this class @classmethod def teardown_class(cls): pass # teardown_class() after any methods in this class def test_all(self): with patch('lantern.plotting.plot_matplotlib.in_ipynb', create=True) as mock1: import lantern as l mock1.return_value = True df = l.bar() l.plot(df, 'line', 'matplotlib') def test_list(self): with patch('lantern.plotting.plot_matplotlib.in_ipynb', create=True) as mock1: import lantern as l mock1.return_value = True df = l.bar() l.plot(df, ['line' for _ in df], 'matplotlib') def test_dict(self): with patch('lantern.plotting.plot_matplotlib.in_ipynb', create=True) as mock1: import lantern as l mock1.return_value = True df = l.bar() l.plot(df, {c: 'line' for c in df.columns}, 'matplotlib')
#! /usr/bin/env python """ This script adds a license file to a DMG. Requires Xcode and a plain ascii text license file. Obviously only runs on a Mac. Copyright (C) 2011-2013 Jared Hobbs Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. """ import os import sys import tempfile import optparse class Path(str): def __enter__(self): return self def __exit__(self, type, value, traceback): os.unlink(self) def mktemp(dir=None, suffix=''): (fd, filename) = tempfile.mkstemp(dir=dir, suffix=suffix) os.close(fd) return Path(filename) def main(options, args): dmgFile, license = args with mktemp('.') as tmpFile: with open(tmpFile, 'w') as f: f.write("""data 'TMPL' (128, "LPic") { $"1344 6566 6175 6C74 204C 616E 6775 6167" $"6520 4944 4457 5244 0543 6F75 6E74 4F43" $"4E54 042A 2A2A 2A4C 5354 430B 7379 7320" $"6C61 6E67 2049 4444 5752 441E 6C6F 6361" $"6C20 7265 7320 4944 2028 6F66 6673 6574" $"2066 726F 6D20 3530 3030 4457 5244 1032" $"2D62 7974 6520 6C61 6E67 7561 6765 3F44" $"5752 4404 2A2A 2A2A 4C53 5445" }; data 'LPic' (5000) { $"0000 0002 0000 0000 0000 0000 0004 0000" }; data 'STR#' (5000, "English buttons") { $"0006 0D45 6E67 6C69 7368 2074 6573 7431" $"0541 6772 6565 0844 6973 6167 7265 6505" $"5072 696E 7407 5361 7665 2E2E 2E7A 4966" $"2079 6F75 2061 6772 6565 2077 6974 6820" $"7468 6520 7465 726D 7320 6F66 2074 6869" $"7320 6C69 6365 6E73 652C 2063 6C69 636B" $"2022 4167 7265 6522 2074 6F20 6163 6365" $"7373 2074 6865 2073 6F66 7477 6172 652E" $"2020 4966 2079 6F75 2064 6F20 6E6F 7420" $"6167 7265 652C 2070 7265 7373 2022 4469" $"7361 6772 6565 2E22" }; data 'STR#' (5002, "English") { $"0006 0745 6E67 6C69 7368 0541 6772 6565" $"0844 6973 6167 7265 6505 5072 696E 7407" $"5361 7665 2E2E 2E7B 4966 2079 6F75 2061" $"6772 6565 2077 6974 6820 7468 6520 7465" $"726D 7320 6F66 2074 6869 7320 6C69 6365" $"6E73 652C 2070 7265 7373 2022 4167 7265" $"6522 2074 6F20 696E 7374 616C 6C20 7468" $"6520 736F 6674 7761 7265 2E20 2049 6620" $"796F 7520 646F 206E 6F74 2061 6772 6565" $"2C20 7072 6573 7320 2244 6973 6167 7265" $"6522 2E" };\n\n""") with open(license, 'r') as l: kind = 'RTF ' if license.lower().endswith('.rtf') else 'TEXT' f.write('data \'%s\' (5000, "English") {\n' % kind) def escape(s): return s.strip().replace('\\', '\\\\').replace('"', '\\"') for line in l: if len(line) < 1000: f.write(' "' + escape(line) + '\\n"\n') else: for liner in line.split('.'): f.write(' "' + escape(liner) + '. \\n"\n') f.write('};\n\n') f.write("""data 'styl' (5000, "English") { $"0003 0000 0000 000C 0009 0014 0000 0000" $"0000 0000 0000 0000 0027 000C 0009 0014" $"0100 0000 0000 0000 0000 0000 002A 000C" $"0009 0014 0000 0000 0000 0000 0000" };\n""") os.system('hdiutil unflatten -quiet "%s"' % dmgFile) ret = os.system('%s -a %s -o "%s"' % (options.rez, tmpFile, dmgFile)) os.system('hdiutil flatten -quiet "%s"' % dmgFile) if options.compression is not None: os.system('cp %s %s.temp.dmg' % (dmgFile, dmgFile)) os.remove(dmgFile) if options.compression == "bz2": os.system('hdiutil convert %s.temp.dmg -format UDBZ -o %s' % (dmgFile, dmgFile)) elif options.compression == "gz": os.system('hdiutil convert %s.temp.dmg -format ' % dmgFile + 'UDZO -imagekey zlib-devel=9 -o %s' % dmgFile) os.remove('%s.temp.dmg' % dmgFile) if ret == 0: print "Successfully added license to '%s'" % dmgFile else: print "Failed to add license to '%s'" % dmgFile if __name__ == '__main__': parser = optparse.OptionParser() parser.set_usage("""%prog <dmgFile> <licenseFile> [OPTIONS] This program adds a software license agreement to a DMG file. It requires Xcode and either a plain ascii text <licenseFile> or a <licenseFile.rtf> with the RTF contents. See --help for more details.""") parser.add_option( '--rez', '-r', action='store', default='/Applications/Xcode.app/Contents/Developer/Tools/Rez', help='The path to the Rez tool. Defaults to %default' ) parser.add_option( '--compression', '-c', action='store', choices=['bz2', 'gz'], default=None, help='Optionally compress dmg using specified compression type. ' 'Choices are bz2 and gz.' ) options, args = parser.parse_args() cond = len(args) != 2 if not os.path.exists(options.rez): print 'Failed to find Rez at "%s"!\n' % options.rez cond = True if cond: parser.print_usage() sys.exit(1) main(options, args)
# Copyright (C) 2010 Google Inc. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. """OAuth 2.0 utilities for Django. Utilities for using OAuth 2.0 in conjunction with the Django datastore. """ __author__ = 'jcgregorio@google.com (Joe Gregorio)' import oauth2client import base64 import pickle from django.db import models from oauth2client.client import Storage as BaseStorage class CredentialsField(models.Field): __metaclass__ = models.SubfieldBase def get_internal_type(self): return "TextField" def to_python(self, value): if not value: return None if isinstance(value, oauth2client.client.Credentials): return value return pickle.loads(base64.b64decode(value)) def get_db_prep_value(self, value, connection, prepared=False): return base64.b64encode(pickle.dumps(value)) class FlowField(models.Field): __metaclass__ = models.SubfieldBase def get_internal_type(self): return "TextField" def to_python(self, value): if value is None: return None if isinstance(value, oauth2client.client.Flow): return value return pickle.loads(base64.b64decode(value)) def get_db_prep_value(self, value, connection, prepared=False): return base64.b64encode(pickle.dumps(value)) class Storage(BaseStorage): """Store and retrieve a single credential to and from the datastore. This Storage helper presumes the Credentials have been stored as a CredenialsField on a db model class. """ def __init__(self, model_class, key_name, key_value, property_name): """Constructor for Storage. Args: model: db.Model, model class key_name: string, key name for the entity that has the credentials key_value: string, key value for the entity that has the credentials property_name: string, name of the property that is an CredentialsProperty """ self.model_class = model_class self.key_name = key_name self.key_value = key_value self.property_name = property_name def locked_get(self): """Retrieve Credential from datastore. Returns: oauth2client.Credentials """ credential = None query = {self.key_name: self.key_value} entities = self.model_class.objects.filter(**query) if len(entities) > 0: credential = getattr(entities[0], self.property_name) if credential and hasattr(credential, 'set_store'): credential.set_store(self) return credential def locked_put(self, credentials): """Write a Credentials to the datastore. Args: credentials: Credentials, the credentials to store. """ args = {self.key_name: self.key_value} entity = self.model_class(**args) setattr(entity, self.property_name, credentials) entity.save()
from share.transform.chain import ChainTransformer, Parser, Delegate, RunPython, ParseDate, ParseName, Map, ctx, Try, Subjects, IRI, Concat class Subject(Parser): name = ctx class ThroughSubjects(Parser): subject = Delegate(Subject, ctx) class Tag(Parser): name = ctx class ThroughTags(Parser): tag = Delegate(Tag, ctx) class Organization(Parser): name = ctx class Publisher(Parser): agent = Delegate(Organization, ctx) class Person(Parser): given_name = ParseName(ctx).first family_name = ParseName(ctx).last class Creator(Parser): agent = Delegate(Person, ctx) cited_as = ctx order_cited = ctx('index') class WorkIdentifier(Parser): uri = IRI(ctx) class Article(Parser): title = ctx.title description = Try(ctx.description) language = ctx.language date_published = ParseDate(ctx.date) date_updated = ParseDate(ctx.date) identifiers = Map( Delegate(WorkIdentifier), ctx.doi, ctx.pdf_url, ctx.fulltext_html_url, RunPython(lambda x: 'https://www.ncbi.nlm.nih.gov/pubmed/{}'.format(x) if x else None, Try(ctx.identifiers.pubmed)), RunPython(lambda x: 'https://www.ncbi.nlm.nih.gov/pmc/articles/{}'.format(x) if x else None, Try(ctx.identifiers.pmc)), ) subjects = Map(Delegate(ThroughSubjects), Subjects(ctx.subjects)) tags = Map(Delegate(ThroughTags), Try(ctx.keywords), Try(ctx.subjects)) related_agents = Concat( Map(Delegate(Creator), ctx.author), Map(Delegate(Publisher), ctx.publisher), ) class Extra: volume = Try(ctx.volume) journal_title = Try(ctx.journal_title) journal_abbrev = Try(ctx.journal_abbrev) description_html = Try(ctx['description-html']) issn = Try(ctx.issn) class Preprint(Article): class Extra: modified = ParseDate(ctx.date) subjects = ctx.subjects identifiers = Try(ctx.identifiers) emails = Try(ctx.author_email) description_html = Try(ctx['description-html']) class PeerJTransformer(ChainTransformer): VERSION = 1 def get_root_parser(self, unwrapped, emitted_type=None, **kwargs): if emitted_type == 'preprint': return Preprint return Article
""" ============================ Faces dataset decompositions ============================ This example applies to :ref:`olivetti_faces` different unsupervised matrix decomposition (dimension reduction) methods from the module :py:mod:`sklearn.decomposition` (see the documentation chapter :ref:`decompositions`) . """ print(__doc__) # Authors: Vlad Niculae, Alexandre Gramfort # License: BSD 3 clause import logging from time import time from numpy.random import RandomState import matplotlib.pyplot as plt from sklearn.datasets import fetch_olivetti_faces from sklearn.cluster import MiniBatchKMeans from sklearn import decomposition # Display progress logs on stdout logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s') n_row, n_col = 2, 3 n_components = n_row * n_col image_shape = (64, 64) rng = RandomState(0) ############################################################################### # Load faces data dataset = fetch_olivetti_faces(shuffle=True, random_state=rng) faces = dataset.data n_samples, n_features = faces.shape # global centering faces_centered = faces - faces.mean(axis=0) # local centering faces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1) print("Dataset consists of %d faces" % n_samples) ############################################################################### def plot_gallery(title, images, n_col=n_col, n_row=n_row): plt.figure(figsize=(2. * n_col, 2.26 * n_row)) plt.suptitle(title, size=16) for i, comp in enumerate(images): plt.subplot(n_row, n_col, i + 1) vmax = max(comp.max(), -comp.min()) plt.imshow(comp.reshape(image_shape), cmap=plt.cm.gray, interpolation='nearest', vmin=-vmax, vmax=vmax) plt.xticks(()) plt.yticks(()) plt.subplots_adjust(0.01, 0.05, 0.99, 0.93, 0.04, 0.) ############################################################################### # List of the different estimators, whether to center and transpose the # problem, and whether the transformer uses the clustering API. estimators = [ ('Eigenfaces - RandomizedPCA', decomposition.RandomizedPCA(n_components=n_components, whiten=True), True), ('Non-negative components - NMF', decomposition.NMF(n_components=n_components, init='nndsvda', tol=5e-3), False), ('Independent components - FastICA', decomposition.FastICA(n_components=n_components, whiten=True), True), ('Sparse comp. - MiniBatchSparsePCA', decomposition.MiniBatchSparsePCA(n_components=n_components, alpha=0.8, n_iter=100, batch_size=3, random_state=rng), True), ('MiniBatchDictionaryLearning', decomposition.MiniBatchDictionaryLearning(n_components=15, alpha=0.1, n_iter=50, batch_size=3, random_state=rng), True), ('Cluster centers - MiniBatchKMeans', MiniBatchKMeans(n_clusters=n_components, tol=1e-3, batch_size=20, max_iter=50, random_state=rng), True), ('Factor Analysis components - FA', decomposition.FactorAnalysis(n_components=n_components, max_iter=2), True), ] ############################################################################### # Plot a sample of the input data plot_gallery("First centered Olivetti faces", faces_centered[:n_components]) ############################################################################### # Do the estimation and plot it for name, estimator, center in estimators: print("Extracting the top %d %s..." % (n_components, name)) t0 = time() data = faces if center: data = faces_centered estimator.fit(data) train_time = (time() - t0) print("done in %0.3fs" % train_time) if hasattr(estimator, 'cluster_centers_'): components_ = estimator.cluster_centers_ else: components_ = estimator.components_ if hasattr(estimator, 'noise_variance_'): plot_gallery("Pixelwise variance", estimator.noise_variance_.reshape(1, -1), n_col=1, n_row=1) plot_gallery('%s - Train time %.1fs' % (name, train_time), components_[:n_components]) plt.show()
#!/usr/bin/env python # -*- coding: utf-8 -*- from core import FeatureExtractorRegistry from twinkle.connectors.core import ConnectorRegistry class FeatureExtractorPipelineFactory(object): """ Factory object for creating a pipeline from a file """ def __init__(self): """ """ pass def buildInput(self, config_data): """ builds an input from the ConnectorRegistry """ input_name = config_data["name"] input_config = config_data["config"] return ConnectorRegistry.buildConnector(input_name, input_config) def buildOutput(self, config_data): """ builds na output from the connectorRegister """ output_name = config_data["name"] output_config = config_data["config"] return ConnectorRegistry.buildConnector(output_name, output_config) def buildExtractor(self, config_data): """ """ extractor_name = config_data["name"] extractor_config = config_data["config"] return FeatureExtractorRegistry.buildExtractor(extractor_name, extractor_config) def buildFromDictionary(self,config_data): """ """ if "input" not in config_data: raise Exception("No input source was specified in the configuration data") if "output" not in config_data: raise Exception("No output source was specified in the configuration data") #build input input_data = config_data["input"] input = self.buildInput(input_data) # build output output_data = config_data["output"] output = self.buildOutput(output_data) # create the pipeline pipeline = FeatureExtractorPipeline(input, output) # get feature extractors extractors = config_data["extractors"] # add each extractor for extractor_config in extractors: extractor = self.buildExtractor(extractor_config) pipeline.addExtractor(extractor) return pipeline class FeatureExtractorPipeline(object): """ Simple feature extractor pipeline. Needs a lot of features in the future such as dependency graphs to resolve some of the intermediates and the ability to do second passes for items which need to be normalized. """ def __init__(self, input, output): self.feature_extractors = [] self.input = input self.output = output def addExtractor(self, extractor): """ add Extractor to the pipeline """ self.feature_extractors.append(extractor) def run(self): """ runs the pipeline """ processed_items = [] # iterate through each item for item in self.input: item_cookie = { "tweet": item, "text": item.text} output = {} # first do preprossing for extractor in self.feature_extractors: extractor.extract(item, item_cookie, output) print output # write output self.output.write(output)
# -*- encoding: utf-8 -*- from __future__ import unicode_literals import re from unittest import skipUnless from django.core.management import call_command from django.db import connection from django.test import TestCase, skipUnlessDBFeature from django.utils.six import PY3, StringIO class InspectDBTestCase(TestCase): def test_stealth_table_name_filter_option(self): out = StringIO() # Lets limit the introspection to tables created for models of this # application call_command('inspectdb', table_name_filter=lambda tn: tn.startswith('inspectdb_'), stdout=out) error_message = "inspectdb has examined a table that should have been filtered out." # contrib.contenttypes is one of the apps always installed when running # the Django test suite, check that one of its tables hasn't been # inspected self.assertNotIn("class DjangoContentType(models.Model):", out.getvalue(), msg=error_message) def make_field_type_asserter(self): """Call inspectdb and return a function to validate a field type in its output""" out = StringIO() call_command('inspectdb', table_name_filter=lambda tn: tn.startswith('inspectdb_columntypes'), stdout=out) output = out.getvalue() def assertFieldType(name, definition): out_def = re.search(r'^\s*%s = (models.*)$' % name, output, re.MULTILINE).groups()[0] self.assertEqual(definition, out_def) return assertFieldType def test_field_types(self): """Test introspection of various Django field types""" assertFieldType = self.make_field_type_asserter() # Inspecting Oracle DB doesn't produce correct results (#19884): # - it gets max_length wrong: it returns a number of bytes. # - it reports fields as blank=True when they aren't. if (connection.features.can_introspect_max_length and not connection.features.interprets_empty_strings_as_nulls): assertFieldType('char_field', "models.CharField(max_length=10)") assertFieldType('comma_separated_int_field', "models.CharField(max_length=99)") assertFieldType('date_field', "models.DateField()") assertFieldType('date_time_field', "models.DateTimeField()") if (connection.features.can_introspect_max_length and not connection.features.interprets_empty_strings_as_nulls): assertFieldType('email_field', "models.CharField(max_length=75)") assertFieldType('file_field', "models.CharField(max_length=100)") assertFieldType('file_path_field', "models.CharField(max_length=100)") if connection.features.can_introspect_ip_address_field: assertFieldType('ip_address_field', "models.GenericIPAddressField()") assertFieldType('gen_ip_adress_field', "models.GenericIPAddressField()") elif (connection.features.can_introspect_max_length and not connection.features.interprets_empty_strings_as_nulls): assertFieldType('ip_address_field', "models.CharField(max_length=15)") assertFieldType('gen_ip_adress_field', "models.CharField(max_length=39)") if (connection.features.can_introspect_max_length and not connection.features.interprets_empty_strings_as_nulls): assertFieldType('slug_field', "models.CharField(max_length=50)") if not connection.features.interprets_empty_strings_as_nulls: assertFieldType('text_field', "models.TextField()") if connection.features.can_introspect_time_field: assertFieldType('time_field', "models.TimeField()") if (connection.features.can_introspect_max_length and not connection.features.interprets_empty_strings_as_nulls): assertFieldType('url_field', "models.CharField(max_length=200)") def test_number_field_types(self): """Test introspection of various Django field types""" assertFieldType = self.make_field_type_asserter() if not connection.features.can_introspect_autofield: assertFieldType('id', "models.IntegerField(primary_key=True) # AutoField?") if connection.features.can_introspect_big_integer_field: assertFieldType('big_int_field', "models.BigIntegerField()") else: assertFieldType('big_int_field', "models.IntegerField()") if connection.features.can_introspect_boolean_field: assertFieldType('bool_field', "models.BooleanField()") if connection.features.can_introspect_null: assertFieldType('null_bool_field', "models.NullBooleanField()") else: assertFieldType('null_bool_field', "models.BooleanField()") else: assertFieldType('bool_field', "models.IntegerField()") if connection.features.can_introspect_null: assertFieldType('null_bool_field', "models.IntegerField(blank=True, null=True)") else: assertFieldType('null_bool_field', "models.IntegerField()") if connection.features.can_introspect_decimal_field: assertFieldType('decimal_field', "models.DecimalField(max_digits=6, decimal_places=1)") else: # Guessed arguments on SQLite, see #5014 assertFieldType('decimal_field', "models.DecimalField(max_digits=10, decimal_places=5) " "# max_digits and decimal_places have been guessed, " "as this database handles decimal fields as float") assertFieldType('float_field', "models.FloatField()") assertFieldType('int_field', "models.IntegerField()") if connection.features.can_introspect_positive_integer_field: assertFieldType('pos_int_field', "models.PositiveIntegerField()") else: assertFieldType('pos_int_field', "models.IntegerField()") if connection.features.can_introspect_positive_integer_field: if connection.features.can_introspect_small_integer_field: assertFieldType('pos_small_int_field', "models.PositiveSmallIntegerField()") else: assertFieldType('pos_small_int_field', "models.PositiveIntegerField()") else: if connection.features.can_introspect_small_integer_field: assertFieldType('pos_small_int_field', "models.SmallIntegerField()") else: assertFieldType('pos_small_int_field', "models.IntegerField()") if connection.features.can_introspect_small_integer_field: assertFieldType('small_int_field', "models.SmallIntegerField()") else: assertFieldType('small_int_field', "models.IntegerField()") @skipUnlessDBFeature('can_introspect_foreign_keys') def test_attribute_name_not_python_keyword(self): out = StringIO() # Lets limit the introspection to tables created for models of this # application call_command('inspectdb', table_name_filter=lambda tn: tn.startswith('inspectdb_'), stdout=out) output = out.getvalue() error_message = "inspectdb generated an attribute name which is a python keyword" # Recursive foreign keys should be set to 'self' self.assertIn("parent = models.ForeignKey('self')", output) self.assertNotIn("from = models.ForeignKey(InspectdbPeople)", output, msg=error_message) # As InspectdbPeople model is defined after InspectdbMessage, it should be quoted self.assertIn("from_field = models.ForeignKey('InspectdbPeople', db_column='from_id')", output) self.assertIn("people_pk = models.ForeignKey(InspectdbPeople, primary_key=True)", output) self.assertIn("people_unique = models.ForeignKey(InspectdbPeople, unique=True)", output) def test_digits_column_name_introspection(self): """Introspection of column names consist/start with digits (#16536/#17676)""" out = StringIO() # Lets limit the introspection to tables created for models of this # application call_command('inspectdb', table_name_filter=lambda tn: tn.startswith('inspectdb_'), stdout=out) output = out.getvalue() error_message = "inspectdb generated a model field name which is a number" self.assertNotIn(" 123 = models.CharField", output, msg=error_message) self.assertIn("number_123 = models.CharField", output) error_message = "inspectdb generated a model field name which starts with a digit" self.assertNotIn(" 4extra = models.CharField", output, msg=error_message) self.assertIn("number_4extra = models.CharField", output) self.assertNotIn(" 45extra = models.CharField", output, msg=error_message) self.assertIn("number_45extra = models.CharField", output) def test_special_column_name_introspection(self): """ Introspection of column names containing special characters, unsuitable for Python identifiers """ out = StringIO() call_command('inspectdb', stdout=out) output = out.getvalue() base_name = 'Field' if not connection.features.uppercases_column_names else 'field' self.assertIn("field = models.IntegerField()", output) self.assertIn("field_field = models.IntegerField(db_column='%s_')" % base_name, output) self.assertIn("field_field_0 = models.IntegerField(db_column='%s__')" % base_name, output) self.assertIn("field_field_1 = models.IntegerField(db_column='__field')", output) self.assertIn("prc_x = models.IntegerField(db_column='prc(%) x')", output) if PY3: # Python 3 allows non-ASCII identifiers self.assertIn("tamao = models.IntegerField()", output) else: self.assertIn("tama_o = models.IntegerField(db_column='tama\\xf1o')", output) def test_managed_models(self): """Test that by default the command generates models with `Meta.managed = False` (#14305)""" out = StringIO() call_command('inspectdb', table_name_filter=lambda tn: tn.startswith('inspectdb_columntypes'), stdout=out) output = out.getvalue() self.longMessage = False self.assertIn(" managed = False", output, msg='inspectdb should generate unmanaged models.') @skipUnless(connection.vendor == 'sqlite', "Only patched sqlite's DatabaseIntrospection.data_types_reverse for this test") def test_custom_fields(self): """ Introspection of columns with a custom field (#21090) """ out = StringIO() orig_data_types_reverse = connection.introspection.data_types_reverse try: connection.introspection.data_types_reverse = { 'text': 'myfields.TextField', 'bigint': 'BigIntegerField', } call_command('inspectdb', table_name_filter=lambda tn: tn.startswith('inspectdb_columntypes'), stdout=out) output = out.getvalue() self.assertIn("text_field = myfields.TextField()", output) self.assertIn("big_int_field = models.BigIntegerField()", output) finally: connection.introspection.data_types_reverse = orig_data_types_reverse
# -*- coding: utf-8 -*- ############################################################################## # # OpenERP, Open Source Management Solution # Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>). # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## import crm_helpdesk_report # vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
from django.test import TestCase import pytest from django.test import Client from django.core.urlresolvers import reverse from webapp.apps.register.models import Subscriber # run this with `py.test --pdb -s -m register` # actually no. `py.test -q webapp/apps/register/tests.py` @pytest.mark.register class RegisterTestCase(TestCase): def test_initial_subscribe(self): subscriber = Subscriber.objects.create( email='test@example.com', ) self.assertFalse(subscriber.active) def test_post_registration_email(self): c = Client() response = c.post(reverse('about'), {'email': 'afarrell+test1@continuum.io'}) subscriber = Subscriber.objects.get( email='afarrell+test1@continuum.io') self.assertFalse(subscriber.active) def test_confirm_link_correct(self): subscriber = Subscriber.objects.create( email='test@example.com', ) self.assertEqual(subscriber.confirm_url("http://ospc-taxes.org"), ("http://ospc-taxes.org/register/?k={}" .format(subscriber.confirm_key))) # Tests to write: # User enters in a username that already exists, then changes it and # resubmits # User enters an email that is the same as an existing email. # def test_patching_works(self): # with patch('django.core.mail.send_mail') as mocked_send_mail: # from django.core.mail import send_mail # send_mail(subject="foo", message="bar", # from_email="andrew <amfarrell@mit.edu>", # recipient_list = ['farrell <afarrell@mit.edu>',]) # self.assertTrue(mocked_send_mail.called) # # def test_mail_is_sent(self): # with patch('django.core.mail.send_mail') as mocked_send_mail: # from webapp.apps.register.models import Subscriber # subscriber = Subscriber.objects.create( # email = 'test@example.com', # ) # subscriber.save() # self.assertFalse(subscriber.active) # self.assertTrue(mocked_send_mail.called) # self.assertEqual(mocked_send_mail.call_args['recipient_list'], # 'test@example.com') # self.assertIn(mocked_send_mail.call_args['message'], # subscriber.confirm_key)
# This Source Code Form is subject to the terms of the Mozilla Public # License, v. 2.0. If a copy of the MPL was not distributed with this # file, You can obtain one at http://mozilla.org/MPL/2.0/. import os import sys here = os.path.split(__file__)[0] def wpt_path(*args): return os.path.join(here, *args) # Imports sys.path.append(wpt_path("harness")) from wptrunner import wptcommandline def update_tests(**kwargs): from wptrunner import update set_defaults(kwargs) logger = update.setup_logging(kwargs, {"mach": sys.stdout}) rv = update.run_update(logger, **kwargs) return 0 if rv is update.update.exit_clean else 1 def set_defaults(kwargs): if kwargs["product"] is None: kwargs["product"] = "servo" if kwargs["config"] is None: kwargs["config"] = wpt_path('config_css.ini') wptcommandline.set_from_config(kwargs) def main(): parser = wptcommandline.create_parser_update() kwargs = vars(parser.parse_args()) return update_tests(**kwargs) if __name__ == "__main__": sys.exit(0 if main() else 1)
# Copyright (c) 2012 The Chromium Embedded Framework Authors. # Portions copyright (c) 2011 The Chromium Authors. All rights reserved. # Use of this source code is governed by a BSD-style license that can be # found in the LICENSE file. import os, re, string, sys from file_util import * import git_util as git # script directory script_dir = os.path.dirname(__file__) # CEF root directory cef_dir = os.path.abspath(os.path.join(script_dir, os.pardir)) # Valid extensions for files we want to lint. DEFAULT_LINT_WHITELIST_REGEX = r"(.*\.cpp|.*\.cc|.*\.h)" DEFAULT_LINT_BLACKLIST_REGEX = r"$^" try: # depot_tools may already be in the import path. import cpplint import cpplint_chromium except ImportError, e: # Search the PATH environment variable to find the depot_tools folder. depot_tools = None; paths = os.environ.get('PATH').split(os.pathsep) for path in paths: if os.path.exists(os.path.join(path, 'cpplint_chromium.py')): depot_tools = path break if depot_tools is None: print >> sys.stderr, 'Error: could not find depot_tools in PATH.' sys.exit(2) # Add depot_tools to import path. sys.path.append(depot_tools) import cpplint import cpplint_chromium # The default implementation of FileInfo.RepositoryName looks for the top-most # directory that contains a .git folder. This is a problem for CEF because the # CEF root folder (which may have an arbitrary name) lives inside the Chromium # src folder. Reimplement in a dumb but sane way. def patch_RepositoryName(self): fullname = self.FullName() project_dir = os.path.dirname(fullname) if os.path.exists(fullname): root_dir = project_dir while os.path.basename(project_dir) != "src": project_dir = os.path.dirname(project_dir) prefix = os.path.commonprefix([root_dir, project_dir]) components = fullname[len(prefix) + 1:].split('/') return string.join(["cef"] + components[1:], '/') return fullname def check_style(args, white_list = None, black_list = None): """ Execute cpplint with the specified arguments. """ # Apply patches. cpplint.FileInfo.RepositoryName = patch_RepositoryName # Process cpplint arguments. filenames = cpplint.ParseArguments(args) if not white_list: white_list = DEFAULT_LINT_WHITELIST_REGEX white_regex = re.compile(white_list) if not black_list: black_list = DEFAULT_LINT_BLACKLIST_REGEX black_regex = re.compile(black_list) extra_check_functions = [cpplint_chromium.CheckPointerDeclarationWhitespace] for filename in filenames: if white_regex.match(filename): if black_regex.match(filename): print "Ignoring file %s" % filename else: cpplint.ProcessFile(filename, cpplint._cpplint_state.verbose_level, extra_check_functions) else: print "Skipping file %s" % filename print "Total errors found: %d\n" % cpplint._cpplint_state.error_count return 1 if __name__ == "__main__": # Start with the default parameters. args = [ # * Disable the 'build/class' test because it errors uselessly with C # structure pointers and template declarations. # * Disable the 'runtime/references' test because CEF allows non-const # arguments passed by reference. # * Disable the 'runtime/sizeof' test because it has a high number of # false positives and adds marginal value. '--filter=-build/class,-runtime/references,-runtime/sizeof', ] # Add anything passed on the command-line. args += sys.argv[1:] # Pre-process the arguments before passing to the linter. new_args = [] changed = [] for arg in args: if arg == '--changed': # Add any changed files. changed = git.get_changed_files(cef_dir) elif arg[:2] == '--' or not os.path.isdir(arg): # Pass argument unchanged. new_args.append(arg) else: # Add all files in the directory. new_args += get_files(os.path.join(arg, '*')) if len(changed) > 0: new_args += changed check_style(new_args)
# -*- coding: utf-8 -*- """ Created on Sun May 06 05:32:15 2012 Author: Josef Perktold editted by: Paul Hobson (2012-08-19) """ from scipy import stats from matplotlib import pyplot as plt import statsmodels.api as sm #example from docstring data = sm.datasets.longley.load() data.exog = sm.add_constant(data.exog, prepend=True) mod_fit = sm.OLS(data.endog, data.exog).fit() res = mod_fit.resid left = -1.8 #x coordinate for text insert fig = plt.figure() ax = fig.add_subplot(2, 2, 1) sm.graphics.qqplot(res, ax=ax) top = ax.get_ylim()[1] * 0.75 txt = ax.text(left, top, 'no keywords', verticalalignment='top') txt.set_bbox(dict(facecolor='k', alpha=0.1)) ax = fig.add_subplot(2, 2, 2) sm.graphics.qqplot(res, line='s', ax=ax) top = ax.get_ylim()[1] * 0.75 txt = ax.text(left, top, "line='s'", verticalalignment='top') txt.set_bbox(dict(facecolor='k', alpha=0.1)) ax = fig.add_subplot(2, 2, 3) sm.graphics.qqplot(res, line='45', fit=True, ax=ax) ax.set_xlim(-2, 2) top = ax.get_ylim()[1] * 0.75 txt = ax.text(left, top, "line='45', \nfit=True", verticalalignment='top') txt.set_bbox(dict(facecolor='k', alpha=0.1)) ax = fig.add_subplot(2, 2, 4) sm.graphics.qqplot(res, dist=stats.t, line='45', fit=True, ax=ax) ax.set_xlim(-2, 2) top = ax.get_ylim()[1] * 0.75 txt = ax.text(left, top, "dist=stats.t, \nline='45', \nfit=True", verticalalignment='top') txt.set_bbox(dict(facecolor='k', alpha=0.1)) fig.tight_layout() plt.gcf() # example with the new ProbPlot class import numpy as np x = np.random.normal(loc=8.25, scale=3.5, size=37) y = np.random.normal(loc=8.00, scale=3.25, size=37) pp_x = sm.ProbPlot(x, fit=True) pp_y = sm.ProbPlot(y, fit=True) # probability of exceedance fig2 = pp_x.probplot(exceed=True) # compare x quantiles to y quantiles fig3 = pp_x.qqplot(other=pp_y, line='45') # same as above with probabilities/percentiles fig4 = pp_x.ppplot(other=pp_y, line='45')
"""Tests for distutils.command.bdist_rpm.""" import unittest import sys import os import tempfile import shutil from test.support import run_unittest from distutils.core import Distribution from distutils.command.bdist_rpm import bdist_rpm from distutils.tests import support from distutils.spawn import find_executable from distutils import spawn from distutils.errors import DistutilsExecError SETUP_PY = """\ from distutils.core import setup import foo setup(name='foo', version='0.1', py_modules=['foo'], url='xxx', author='xxx', author_email='xxx') """ class BuildRpmTestCase(support.TempdirManager, support.LoggingSilencer, unittest.TestCase): def setUp(self): try: sys.executable.encode("UTF-8") except UnicodeEncodeError: raise unittest.SkipTest("sys.executable is not encodable to UTF-8") super(BuildRpmTestCase, self).setUp() self.old_location = os.getcwd() self.old_sys_argv = sys.argv, sys.argv[:] def tearDown(self): os.chdir(self.old_location) sys.argv = self.old_sys_argv[0] sys.argv[:] = self.old_sys_argv[1] super(BuildRpmTestCase, self).tearDown() # XXX I am unable yet to make this test work without # spurious sdtout/stderr output under Mac OS X @unittest.skipUnless(sys.platform.startswith('linux'), 'spurious sdtout/stderr output under Mac OS X') @unittest.skipIf(find_executable('rpm') is None, 'the rpm command is not found') @unittest.skipIf(find_executable('rpmbuild') is None, 'the rpmbuild command is not found') def test_quiet(self): # let's create a package tmp_dir = self.mkdtemp() pkg_dir = os.path.join(tmp_dir, 'foo') os.mkdir(pkg_dir) self.write_file((pkg_dir, 'setup.py'), SETUP_PY) self.write_file((pkg_dir, 'foo.py'), '#') self.write_file((pkg_dir, 'MANIFEST.in'), 'include foo.py') self.write_file((pkg_dir, 'README'), '') dist = Distribution({'name': 'foo', 'version': '0.1', 'py_modules': ['foo'], 'url': 'xxx', 'author': 'xxx', 'author_email': 'xxx'}) dist.script_name = 'setup.py' os.chdir(pkg_dir) sys.argv = ['setup.py'] cmd = bdist_rpm(dist) cmd.fix_python = True # running in quiet mode cmd.quiet = 1 cmd.ensure_finalized() cmd.run() dist_created = os.listdir(os.path.join(pkg_dir, 'dist')) self.assertIn('foo-0.1-1.noarch.rpm', dist_created) # bug #2945: upload ignores bdist_rpm files self.assertIn(('bdist_rpm', 'any', 'dist/foo-0.1-1.src.rpm'), dist.dist_files) self.assertIn(('bdist_rpm', 'any', 'dist/foo-0.1-1.noarch.rpm'), dist.dist_files) # XXX I am unable yet to make this test work without # spurious sdtout/stderr output under Mac OS X @unittest.skipUnless(sys.platform.startswith('linux'), 'spurious sdtout/stderr output under Mac OS X') # http://bugs.python.org/issue1533164 @unittest.skipIf(find_executable('rpm') is None, 'the rpm command is not found') @unittest.skipIf(find_executable('rpmbuild') is None, 'the rpmbuild command is not found') def test_no_optimize_flag(self): # let's create a package that brakes bdist_rpm tmp_dir = self.mkdtemp() pkg_dir = os.path.join(tmp_dir, 'foo') os.mkdir(pkg_dir) self.write_file((pkg_dir, 'setup.py'), SETUP_PY) self.write_file((pkg_dir, 'foo.py'), '#') self.write_file((pkg_dir, 'MANIFEST.in'), 'include foo.py') self.write_file((pkg_dir, 'README'), '') dist = Distribution({'name': 'foo', 'version': '0.1', 'py_modules': ['foo'], 'url': 'xxx', 'author': 'xxx', 'author_email': 'xxx'}) dist.script_name = 'setup.py' os.chdir(pkg_dir) sys.argv = ['setup.py'] cmd = bdist_rpm(dist) cmd.fix_python = True cmd.quiet = 1 cmd.ensure_finalized() cmd.run() dist_created = os.listdir(os.path.join(pkg_dir, 'dist')) self.assertIn('foo-0.1-1.noarch.rpm', dist_created) # bug #2945: upload ignores bdist_rpm files self.assertIn(('bdist_rpm', 'any', 'dist/foo-0.1-1.src.rpm'), dist.dist_files) self.assertIn(('bdist_rpm', 'any', 'dist/foo-0.1-1.noarch.rpm'), dist.dist_files) os.remove(os.path.join(pkg_dir, 'dist', 'foo-0.1-1.noarch.rpm')) def test_suite(): return unittest.makeSuite(BuildRpmTestCase) if __name__ == '__main__': run_unittest(test_suite())
#!/usr/bin/python # # This file is part of Ansible # # Ansible is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Ansible is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Ansible. If not, see <http://www.gnu.org/licenses/>. # ANSIBLE_METADATA = {'metadata_version': '1.1', 'status': ['preview'], 'supported_by': 'network'} DOCUMENTATION = ''' --- module: nxos_udld extends_documentation_fragment: nxos version_added: "2.2" short_description: Manages UDLD global configuration params. description: - Manages UDLD global configuration params. author: - Jason Edelman (@jedelman8) notes: - Tested against NXOSv 7.3.(0)D1(1) on VIRL - Module will fail if the udld feature has not been previously enabled. options: aggressive: description: - Toggles aggressive mode. choices: ['enabled','disabled'] msg_time: description: - Message time in seconds for UDLD packets or keyword 'default'. reset: description: - Ability to reset all ports shut down by UDLD. 'state' parameter cannot be 'absent' when this is present. type: bool default: 'no' state: description: - Manage the state of the resource. When set to 'absent', aggressive and msg_time are set to their default values. default: present choices: ['present','absent'] ''' EXAMPLES = ''' # ensure udld aggressive mode is globally disabled and se global message interval is 20 - nxos_udld: aggressive: disabled msg_time: 20 host: "{{ inventory_hostname }}" username: "{{ un }}" password: "{{ pwd }}" # Ensure agg mode is globally enabled and msg time is 15 - nxos_udld: aggressive: enabled msg_time: 15 host: "{{ inventory_hostname }}" username: "{{ un }}" password: "{{ pwd }}" ''' RETURN = ''' proposed: description: k/v pairs of parameters passed into module returned: always type: dict sample: {"aggressive": "enabled", "msg_time": "40"} existing: description: - k/v pairs of existing udld configuration returned: always type: dict sample: {"aggressive": "disabled", "msg_time": "15"} end_state: description: k/v pairs of udld configuration after module execution returned: always type: dict sample: {"aggressive": "enabled", "msg_time": "40"} updates: description: command sent to the device returned: always type: list sample: ["udld message-time 40", "udld aggressive"] changed: description: check to see if a change was made on the device returned: always type: boolean sample: true ''' import re from ansible.module_utils.network.nxos.nxos import get_config, load_config, run_commands from ansible.module_utils.network.nxos.nxos import get_capabilities, nxos_argument_spec from ansible.module_utils.basic import AnsibleModule PARAM_TO_DEFAULT_KEYMAP = { 'msg_time': '15', } def execute_show_command(command, module, command_type='cli_show'): device_info = get_capabilities(module) network_api = device_info.get('network_api', 'nxapi') if network_api == 'cliconf': if 'show run' not in command: command += ' | json' cmds = [command] body = run_commands(module, cmds) elif network_api == 'nxapi': cmds = [command] body = run_commands(module, cmds) return body def flatten_list(command_lists): flat_command_list = [] for command in command_lists: if isinstance(command, list): flat_command_list.extend(command) else: flat_command_list.append(command) return flat_command_list def apply_key_map(key_map, table): new_dict = {} for key, value in table.items(): new_key = key_map.get(key) if new_key: value = table.get(key) if value: new_dict[new_key] = str(value) else: new_dict[new_key] = value return new_dict def get_commands_config_udld_global(delta, reset, existing): commands = [] for param, value in delta.items(): if param == 'aggressive': command = 'udld aggressive' if value == 'enabled' else 'no udld aggressive' commands.append(command) elif param == 'msg_time': if value == 'default': if existing.get('msg_time') != PARAM_TO_DEFAULT_KEYMAP.get('msg_time'): commands.append('no udld message-time') else: commands.append('udld message-time ' + value) if reset: command = 'udld reset' commands.append(command) return commands def get_commands_remove_udld_global(existing): commands = [] if existing.get('aggressive') == 'enabled': command = 'no udld aggressive' commands.append(command) if existing.get('msg_time') != PARAM_TO_DEFAULT_KEYMAP.get('msg_time'): command = 'no udld message-time' commands.append(command) return commands def get_udld_global(module): command = 'show udld global' udld_table = execute_show_command(command, module)[0] status = str(udld_table.get('udld-global-mode', None)) if status == 'enabled-aggressive': aggressive = 'enabled' else: aggressive = 'disabled' interval = str(udld_table.get('message-interval', None)) udld = dict(msg_time=interval, aggressive=aggressive) return udld def main(): argument_spec = dict( aggressive=dict(required=False, choices=['enabled', 'disabled']), msg_time=dict(required=False, type='str'), reset=dict(required=False, type='bool'), state=dict(choices=['absent', 'present'], default='present'), ) argument_spec.update(nxos_argument_spec) module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=True) warnings = list() aggressive = module.params['aggressive'] msg_time = module.params['msg_time'] reset = module.params['reset'] state = module.params['state'] if reset and state == 'absent': module.fail_json(msg="state must be present when using reset flag.") args = dict(aggressive=aggressive, msg_time=msg_time, reset=reset) proposed = dict((k, v) for k, v in args.items() if v is not None) existing = get_udld_global(module) end_state = existing delta = set(proposed.items()).difference(existing.items()) changed = False commands = [] if state == 'present': if delta: command = get_commands_config_udld_global(dict(delta), reset, existing) commands.append(command) elif state == 'absent': command = get_commands_remove_udld_global(existing) if command: commands.append(command) cmds = flatten_list(commands) if cmds: if module.check_mode: module.exit_json(changed=True, commands=cmds) else: changed = True load_config(module, cmds) end_state = get_udld_global(module) if 'configure' in cmds: cmds.pop(0) results = {} results['proposed'] = proposed results['existing'] = existing results['end_state'] = end_state results['updates'] = cmds results['changed'] = changed results['warnings'] = warnings module.exit_json(**results) if __name__ == '__main__': main()
# pylint: disable=missing-docstring # pylint: disable=redefined-outer-name import os from lettuce import world, step from nose.tools import assert_true, assert_in # pylint: disable=no-name-in-module from django.conf import settings from student.roles import CourseStaffRole, CourseInstructorRole, GlobalStaff from student.models import get_user from selenium.webdriver.common.keys import Keys from logging import getLogger from student.tests.factories import AdminFactory from student import auth logger = getLogger(__name__) from terrain.browser import reset_data TEST_ROOT = settings.COMMON_TEST_DATA_ROOT @step('I (?:visit|access|open) the Studio homepage$') def i_visit_the_studio_homepage(_step): # To make this go to port 8001, put # LETTUCE_SERVER_PORT = 8001 # in your settings.py file. world.visit('/') signin_css = 'a.action-signin' assert world.is_css_present(signin_css) @step('I am logged into Studio$') def i_am_logged_into_studio(_step): log_into_studio() @step('I confirm the alert$') def i_confirm_with_ok(_step): world.browser.get_alert().accept() @step(u'I press the "([^"]*)" delete icon$') def i_press_the_category_delete_icon(_step, category): if category == 'section': css = 'a.action.delete-section-button' elif category == 'subsection': css = 'a.action.delete-subsection-button' else: assert False, 'Invalid category: %s' % category world.css_click(css) @step('I have opened a new course in Studio$') def i_have_opened_a_new_course(_step): open_new_course() @step('I have populated a new course in Studio$') def i_have_populated_a_new_course(_step): world.clear_courses() course = world.CourseFactory.create() world.scenario_dict['COURSE'] = course section = world.ItemFactory.create(parent_location=course.location) world.ItemFactory.create( parent_location=section.location, category='sequential', display_name='Subsection One', ) user = create_studio_user(is_staff=False) add_course_author(user, course) log_into_studio() world.css_click('a.course-link') world.wait_for_js_to_load() @step('(I select|s?he selects) the new course') def select_new_course(_step, whom): course_link_css = 'a.course-link' world.css_click(course_link_css) @step(u'I press the "([^"]*)" notification button$') def press_the_notification_button(_step, name): # Because the notification uses a CSS transition, # Selenium will always report it as being visible. # This makes it very difficult to successfully click # the "Save" button at the UI level. # Instead, we use JavaScript to reliably click # the button. btn_css = 'div#page-notification a.action-%s' % name.lower() world.trigger_event(btn_css, event='focus') world.browser.execute_script("$('{}').click()".format(btn_css)) world.wait_for_ajax_complete() @step('I change the "(.*)" field to "(.*)"$') def i_change_field_to_value(_step, field, value): field_css = '#%s' % '-'.join([s.lower() for s in field.split()]) ele = world.css_find(field_css).first ele.fill(value) ele._element.send_keys(Keys.ENTER) @step('I reset the database') def reset_the_db(_step): """ When running Lettuce tests using examples (i.e. "Confirmation is shown on save" in course-settings.feature), the normal hooks aren't called between examples. reset_data should run before each scenario to flush the test database. When this doesn't happen we get errors due to trying to insert a non-unique entry. So instead, we delete the database manually. This has the effect of removing any users and courses that have been created during the test run. """ reset_data(None) @step('I see a confirmation that my changes have been saved') def i_see_a_confirmation(step): confirmation_css = '#alert-confirmation' assert world.is_css_present(confirmation_css) def open_new_course(): world.clear_courses() create_studio_user() log_into_studio() create_a_course() def create_studio_user( uname='robot', email='robot+studio@edx.org', password='test', is_staff=False): studio_user = world.UserFactory( username=uname, email=email, password=password, is_staff=is_staff) registration = world.RegistrationFactory(user=studio_user) registration.register(studio_user) registration.activate() return studio_user def fill_in_course_info( name='Robot Super Course', org='MITx', num='101', run='2013_Spring'): world.css_fill('.new-course-name', name) world.css_fill('.new-course-org', org) world.css_fill('.new-course-number', num) world.css_fill('.new-course-run', run) def log_into_studio( uname='robot', email='robot+studio@edx.org', password='test', name='Robot Studio'): world.log_in(username=uname, password=password, email=email, name=name) # Navigate to the studio dashboard world.visit('/') assert_in(uname, world.css_text('h2.title', timeout=10)) def add_course_author(user, course): """ Add the user to the instructor group of the course so they will have the permissions to see it in studio """ global_admin = AdminFactory() for role in (CourseStaffRole, CourseInstructorRole): auth.add_users(global_admin, role(course.id), user) def create_a_course(): course = world.CourseFactory.create(org='MITx', course='999', display_name='Robot Super Course') world.scenario_dict['COURSE'] = course user = world.scenario_dict.get("USER") if not user: user = get_user('robot+studio@edx.org') add_course_author(user, course) # Navigate to the studio dashboard world.visit('/') course_link_css = 'a.course-link' world.css_click(course_link_css) course_title_css = 'span.course-title' assert_true(world.is_css_present(course_title_css)) def add_section(): world.css_click('.outline .button-new') assert_true(world.is_css_present('.outline-section .xblock-field-value')) def set_date_and_time(date_css, desired_date, time_css, desired_time, key=None): set_element_value(date_css, desired_date, key) world.wait_for_ajax_complete() set_element_value(time_css, desired_time, key) world.wait_for_ajax_complete() def set_element_value(element_css, element_value, key=None): element = world.css_find(element_css).first element.fill(element_value) # hit TAB or provided key to trigger save content if key is not None: element._element.send_keys(getattr(Keys, key)) # pylint: disable=protected-access else: element._element.send_keys(Keys.TAB) # pylint: disable=protected-access @step('I have enabled the (.*) advanced module$') def i_enabled_the_advanced_module(step, module): step.given('I have opened a new course section in Studio') world.css_click('.nav-course-settings') world.css_click('.nav-course-settings-advanced a') type_in_codemirror(0, '["%s"]' % module) press_the_notification_button(step, 'Save') @world.absorb def create_unit_from_course_outline(): """ Expands the section and clicks on the New Unit link. The end result is the page where the user is editing the new unit. """ css_selectors = [ '.outline-subsection .expand-collapse', '.outline-subsection .button-new' ] for selector in css_selectors: world.css_click(selector) world.wait_for_mathjax() world.wait_for_xmodule() world.wait_for_loading() assert world.is_css_present('ul.new-component-type') @world.absorb def wait_for_loading(): """ Waits for the loading indicator to be hidden. """ world.wait_for(lambda _driver: len(world.browser.find_by_css('div.ui-loading.is-hidden')) > 0) @step('I have clicked the new unit button$') @step(u'I am in Studio editing a new unit$') def edit_new_unit(step): step.given('I have populated a new course in Studio') create_unit_from_course_outline() @step('the save notification button is disabled') def save_button_disabled(step): button_css = '.action-save' disabled = 'is-disabled' assert world.css_has_class(button_css, disabled) @step('the "([^"]*)" button is disabled') def button_disabled(step, value): button_css = 'input[value="%s"]' % value assert world.css_has_class(button_css, 'is-disabled') def _do_studio_prompt_action(intent, action): """ Wait for a studio prompt to appear and press the specified action button See cms/static/js/views/feedback_prompt.js for implementation """ assert intent in [ 'warning', 'error', 'confirmation', 'announcement', 'step-required', 'help', 'mini', ] assert action in ['primary', 'secondary'] world.wait_for_present('div.wrapper-prompt.is-shown#prompt-{}'.format(intent)) action_css = 'li.nav-item > a.action-{}'.format(action) world.trigger_event(action_css, event='focus') world.browser.execute_script("$('{}').click()".format(action_css)) world.wait_for_ajax_complete() world.wait_for_present('div.wrapper-prompt.is-hiding#prompt-{}'.format(intent)) @world.absorb def confirm_studio_prompt(): _do_studio_prompt_action('warning', 'primary') @step('I confirm the prompt') def confirm_the_prompt(step): confirm_studio_prompt() @step(u'I am shown a prompt$') def i_am_shown_a_notification(step): assert world.is_css_present('.wrapper-prompt') def type_in_codemirror(index, text, find_prefix="$"): script = """ var cm = {find_prefix}('div.CodeMirror:eq({index})').get(0).CodeMirror; cm.getInputField().focus(); cm.setValue(arguments[0]); cm.getInputField().blur();""".format(index=index, find_prefix=find_prefix) world.browser.driver.execute_script(script, str(text)) world.wait_for_ajax_complete() def get_codemirror_value(index=0, find_prefix="$"): return world.browser.driver.execute_script( """ return {find_prefix}('div.CodeMirror:eq({index})').get(0).CodeMirror.getValue(); """.format(index=index, find_prefix=find_prefix) ) def attach_file(filename, sub_path): path = os.path.join(TEST_ROOT, sub_path, filename) world.browser.execute_script("$('input.file-input').css('display', 'block')") assert_true(os.path.exists(path)) world.browser.attach_file('file', os.path.abspath(path)) def upload_file(filename, sub_path=''): attach_file(filename, sub_path) button_css = '.wrapper-modal-window-assetupload .action-upload' world.css_click(button_css) @step(u'"([^"]*)" logs in$') def other_user_login(step, name): step.given('I log out') world.visit('/') signin_css = 'a.action-signin' world.is_css_present(signin_css) world.css_click(signin_css) def fill_login_form(): login_form = world.browser.find_by_css('form#login_form') login_form.find_by_name('email').fill(name + '@edx.org') login_form.find_by_name('password').fill("test") login_form.find_by_name('submit').click() world.retry_on_exception(fill_login_form) assert_true(world.is_css_present('.new-course-button')) world.scenario_dict['USER'] = get_user(name + '@edx.org') @step(u'the user "([^"]*)" exists( as a course (admin|staff member|is_staff))?$') def create_other_user(_step, name, has_extra_perms, role_name): email = name + '@edx.org' user = create_studio_user(uname=name, password="test", email=email) if has_extra_perms: if role_name == "is_staff": GlobalStaff().add_users(user) else: if role_name == "admin": # admins get staff privileges, as well roles = (CourseStaffRole, CourseInstructorRole) else: roles = (CourseStaffRole,) course_key = world.scenario_dict["COURSE"].id global_admin = AdminFactory() for role in roles: auth.add_users(global_admin, role(course_key), user) @step('I log out') def log_out(_step): world.visit('logout')
# pyOCD debugger # Copyright (c) 2018-2019 Arm Limited # Copyright (c) 2017 NXP # Copyright (c) 2016 Freescale Semiconductor, Inc. # SPDX-License-Identifier: Apache-2.0 # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. from ..family.target_kinetis import Kinetis from ..family.flash_kinetis import Flash_Kinetis from ...core.memory_map import (FlashRegion, RamRegion, MemoryMap) from ...debug.svd.loader import SVDFile RCM_MR = 0x4007f010 RCM_MR_BOOTROM_MASK = 0x6 FLASH_ALGO = { 'load_address' : 0x20000000, 'instructions' : [ 0xE00ABE00, 0x062D780D, 0x24084068, 0xD3000040, 0x1E644058, 0x1C49D1FA, 0x2A001E52, 0x4770D1F2, 0x4829b510, 0x60414927, 0x60814928, 0x22806801, 0x22204391, 0x60014311, 0x44484825, 0xf84cf000, 0xd0002800, 0xbd102001, 0x47702000, 0xb5104820, 0x44484920, 0xf88ef000, 0xd1042800, 0x2100481c, 0xf0004448, 0xbd10f948, 0x4c19b570, 0x444c4605, 0x4b184601, 0x68e24620, 0xf8b5f000, 0xd1052800, 0x46292300, 0x68e24620, 0xf93ff000, 0xb570bd70, 0x460b460c, 0x46014606, 0xb084480d, 0x44484615, 0xf8e4f000, 0xd10a2800, 0x90029001, 0x48082101, 0x462b9100, 0x46314622, 0xf0004448, 0xb004f96d, 0x0000bd70, 0xd928c520, 0x40052000, 0x0000ffff, 0x00000004, 0x6b65666b, 0xd00b2800, 0x68c949dd, 0x0f090109, 0xd007290f, 0x00494adb, 0x5a51447a, 0xe0030289, 0x47702004, 0x04892101, 0x2300b430, 0x60416003, 0x02cc2101, 0x608160c4, 0x7a0d49d3, 0x40aa158a, 0x7ac96142, 0x61816103, 0x06892105, 0x62016244, 0x2000bc30, 0x28004770, 0x6101d002, 0x47702000, 0x47702004, 0x48c94602, 0x210168c0, 0x43080289, 0x60c849c6, 0x48c64770, 0x70012170, 0x70012180, 0x06097801, 0x7800d5fc, 0xd5010681, 0x47702067, 0xd50106c1, 0x47702068, 0xd0fc07c0, 0x47702069, 0xd1012800, 0x47702004, 0x4604b510, 0x48b94ab8, 0x48b96050, 0xd0014281, 0xe000206b, 0x28002000, 0x4620d107, 0xffd7f7ff, 0x46204603, 0xffcaf7ff, 0xbd104618, 0xd1012800, 0x47702004, 0x4614b510, 0x60622200, 0x60e260a2, 0x61626122, 0x61e261a2, 0x68c16021, 0x68816061, 0xf0006840, 0x60a0f953, 0x60e02008, 0x61606120, 0x200461a0, 0x200061e0, 0xb5ffbd10, 0x4615b089, 0x466a460c, 0xf7ff9809, 0x462affd9, 0x9b044621, 0xf0009809, 0x0007f90c, 0x9c00d130, 0x19659e01, 0x46311e6d, 0xf0004628, 0x2900f931, 0x1c40d002, 0x1e454370, 0xd81d42ac, 0x20090221, 0x06000a09, 0x488d1809, 0x498e6041, 0x4288980c, 0x206bd001, 0x2000e000, 0xd1112800, 0xf7ff9809, 0x4607ff80, 0x69009809, 0xd0002800, 0x2f004780, 0x19a4d102, 0xd9e142ac, 0xf7ff9809, 0x4638ff69, 0xbdf0b00d, 0xd1012a00, 0x47702004, 0xb089b5ff, 0x461e4614, 0x466a460d, 0xf7ff9809, 0x4632ff91, 0x9b034629, 0xf0009809, 0x0007f8c4, 0x9d00d12d, 0xd0262e00, 0x4871cc02, 0x99036081, 0xd0022904, 0xd0072908, 0x022ae00e, 0x0a122103, 0x18510649, 0xe0076041, 0x60c1cc02, 0x2107022a, 0x06090a12, 0x60411851, 0xf7ff9809, 0x4607ff3c, 0x69009809, 0xd0002800, 0x2f004780, 0x9803d103, 0x1a361945, 0x9809d1d8, 0xff24f7ff, 0xb00d4638, 0x2800bdf0, 0x4a5dd005, 0x18890409, 0x60514a58, 0x2004e721, 0xb5ff4770, 0x4614b08b, 0x460d461e, 0x980b466a, 0xff46f7ff, 0x46294622, 0x980b9b05, 0xf879f000, 0xd1332800, 0x4629466a, 0xf7ff980b, 0x9d00ff39, 0x90089802, 0x42404269, 0x424f4001, 0xd10142af, 0x183f9808, 0xd0202c00, 0x90090230, 0x42a61b7e, 0x4626d900, 0x99054630, 0xf88af000, 0x2101022a, 0x06090a12, 0x493d1852, 0x9a09604a, 0x43100400, 0x608830ff, 0xf7ff980b, 0x2800fee4, 0x9808d106, 0x19ad1ba4, 0x2c00183f, 0x2000d1e0, 0xbdf0b00f, 0xd1012b00, 0x47702004, 0xb089b5ff, 0x461d4616, 0x466a460c, 0x98099f12, 0xfefaf7ff, 0x46214632, 0x98099b07, 0xf82df000, 0xd11d2800, 0x2e009c00, 0x492ad01a, 0x18470638, 0x20010221, 0x06400a09, 0x48221809, 0x60876041, 0x60c16829, 0xf7ff9809, 0x2800feb0, 0x9913d00a, 0xd0002900, 0x9914600c, 0xd0012900, 0x600a2200, 0xbdf0b00d, 0x1a769907, 0x00890889, 0x9907194d, 0x2e00190c, 0xb00dd1dc, 0x2800bdf0, 0x2004d101, 0xb4104770, 0x460c1e5b, 0xd101421c, 0xd002421a, 0x2065bc10, 0x68034770, 0xd804428b, 0x18896840, 0x42881818, 0xbc10d202, 0x47702066, 0x2000bc10, 0x00004770, 0x40048040, 0x000003bc, 0x40020020, 0xf0003000, 0x40020000, 0x44ffffff, 0x6b65666b, 0x4000ffff, 0x00ffffff, 0x460bb530, 0x20004601, 0x24012220, 0x460de009, 0x429d40d5, 0x461dd305, 0x1b494095, 0x40954625, 0x46151940, 0x2d001e52, 0xbd30dcf1, 0x40020004, 0x40020010, 0x00100008, 0x00200018, 0x00400030, 0x00800060, 0x010000c0, 0x02000180, 0x04000300, 0x00000600, 0x00000000, ], 'pc_init' : 0x20000021, 'pc_unInit': 0x20000049, 'pc_program_page': 0x2000008F, 'pc_erase_sector': 0x20000069, 'pc_eraseAll' : 0x2000004D, 'static_base' : 0x20000000 + 0x00000020 + 0x000004ac, 'begin_stack' : 0x20000000 + 0x00000800, 'begin_data' : 0x20000000 + 0x00000A00, 'page_size' : 0x00000800, 'analyzer_supported' : True, 'analyzer_address' : 0x1ffff000, # Analyzer 0x1ffff000..0x1ffff600 'page_buffers' : [0x20003000, 0x20004000], # Enable double buffering 'min_program_length' : 8, } class KE15Z7(Kinetis): MEMORY_MAP = MemoryMap( FlashRegion( start=0, length=0x40000, blocksize=0x800, is_boot_memory=True, algo=FLASH_ALGO, flash_class=Flash_Kinetis), RamRegion( start=0x1fffe000, length=0x8000) ) def __init__(self, session): super(KE15Z7, self).__init__(session, self.MEMORY_MAP) self._svd_location = SVDFile.from_builtin("MKE15Z7.svd") def post_connect_hook(self): # Disable ROM vector table remapping. self.write32(RCM_MR, RCM_MR_BOOTROM_MASK)
from django.forms.models import ModelFormMetaclass, ModelForm from django.template import RequestContext, loader from django.http import Http404, HttpResponse, HttpResponseRedirect from django.core.xheaders import populate_xheaders from django.core.exceptions import ObjectDoesNotExist, ImproperlyConfigured from django.utils.translation import ugettext from django.contrib.auth.views import redirect_to_login from django.views.generic import GenericViewError from django.contrib import messages import warnings warnings.warn( 'Function-based generic views have been deprecated; use class-based views instead.', DeprecationWarning ) def apply_extra_context(extra_context, context): """ Adds items from extra_context dict to context. If a value in extra_context is callable, then it is called and the result is added to context. """ for key, value in extra_context.iteritems(): if callable(value): context[key] = value() else: context[key] = value def get_model_and_form_class(model, form_class): """ Returns a model and form class based on the model and form_class parameters that were passed to the generic view. If ``form_class`` is given then its associated model will be returned along with ``form_class`` itself. Otherwise, if ``model`` is given, ``model`` itself will be returned along with a ``ModelForm`` class created from ``model``. """ if form_class: return form_class._meta.model, form_class if model: # The inner Meta class fails if model = model is used for some reason. tmp_model = model # TODO: we should be able to construct a ModelForm without creating # and passing in a temporary inner class. class Meta: model = tmp_model class_name = model.__name__ + 'Form' form_class = ModelFormMetaclass(class_name, (ModelForm,), {'Meta': Meta}) return model, form_class raise GenericViewError("Generic view must be called with either a model or" " form_class argument.") def redirect(post_save_redirect, obj): """ Returns a HttpResponseRedirect to ``post_save_redirect``. ``post_save_redirect`` should be a string, and can contain named string- substitution place holders of ``obj`` field names. If ``post_save_redirect`` is None, then redirect to ``obj``'s URL returned by ``get_absolute_url()``. If ``obj`` has no ``get_absolute_url`` method, then raise ImproperlyConfigured. This function is meant to handle the post_save_redirect parameter to the ``create_object`` and ``update_object`` views. """ if post_save_redirect: return HttpResponseRedirect(post_save_redirect % obj.__dict__) elif hasattr(obj, 'get_absolute_url'): return HttpResponseRedirect(obj.get_absolute_url()) else: raise ImproperlyConfigured( "No URL to redirect to. Either pass a post_save_redirect" " parameter to the generic view or define a get_absolute_url" " method on the Model.") def lookup_object(model, object_id, slug, slug_field): """ Return the ``model`` object with the passed ``object_id``. If ``object_id`` is None, then return the object whose ``slug_field`` equals the passed ``slug``. If ``slug`` and ``slug_field`` are not passed, then raise Http404 exception. """ lookup_kwargs = {} if object_id: lookup_kwargs['%s__exact' % model._meta.pk.name] = object_id elif slug and slug_field: lookup_kwargs['%s__exact' % slug_field] = slug else: raise GenericViewError( "Generic view must be called with either an object_id or a" " slug/slug_field.") try: return model.objects.get(**lookup_kwargs) except ObjectDoesNotExist: raise Http404("No %s found for %s" % (model._meta.verbose_name, lookup_kwargs)) def create_object(request, model=None, template_name=None, template_loader=loader, extra_context=None, post_save_redirect=None, login_required=False, context_processors=None, form_class=None): """ Generic object-creation function. Templates: ``<app_label>/<model_name>_form.html`` Context: form the form for the object """ if extra_context is None: extra_context = {} if login_required and not request.user.is_authenticated(): return redirect_to_login(request.path) model, form_class = get_model_and_form_class(model, form_class) if request.method == 'POST': form = form_class(request.POST, request.FILES) if form.is_valid(): new_object = form.save() msg = ugettext("The %(verbose_name)s was created successfully.") %\ {"verbose_name": model._meta.verbose_name} messages.success(request, msg, fail_silently=True) return redirect(post_save_redirect, new_object) else: form = form_class() # Create the template, context, response if not template_name: template_name = "%s/%s_form.html" % (model._meta.app_label, model._meta.object_name.lower()) t = template_loader.get_template(template_name) c = RequestContext(request, { 'form': form, }, context_processors) apply_extra_context(extra_context, c) return HttpResponse(t.render(c)) def update_object(request, model=None, object_id=None, slug=None, slug_field='slug', template_name=None, template_loader=loader, extra_context=None, post_save_redirect=None, login_required=False, context_processors=None, template_object_name='object', form_class=None): """ Generic object-update function. Templates: ``<app_label>/<model_name>_form.html`` Context: form the form for the object object the original object being edited """ if extra_context is None: extra_context = {} if login_required and not request.user.is_authenticated(): return redirect_to_login(request.path) model, form_class = get_model_and_form_class(model, form_class) obj = lookup_object(model, object_id, slug, slug_field) if request.method == 'POST': form = form_class(request.POST, request.FILES, instance=obj) if form.is_valid(): obj = form.save() msg = ugettext("The %(verbose_name)s was updated successfully.") %\ {"verbose_name": model._meta.verbose_name} messages.success(request, msg, fail_silently=True) return redirect(post_save_redirect, obj) else: form = form_class(instance=obj) if not template_name: template_name = "%s/%s_form.html" % (model._meta.app_label, model._meta.object_name.lower()) t = template_loader.get_template(template_name) c = RequestContext(request, { 'form': form, template_object_name: obj, }, context_processors) apply_extra_context(extra_context, c) response = HttpResponse(t.render(c)) populate_xheaders(request, response, model, getattr(obj, obj._meta.pk.attname)) return response def delete_object(request, model, post_delete_redirect, object_id=None, slug=None, slug_field='slug', template_name=None, template_loader=loader, extra_context=None, login_required=False, context_processors=None, template_object_name='object'): """ Generic object-delete function. The given template will be used to confirm deletetion if this view is fetched using GET; for safty, deletion will only be performed if this view is POSTed. Templates: ``<app_label>/<model_name>_confirm_delete.html`` Context: object the original object being deleted """ if extra_context is None: extra_context = {} if login_required and not request.user.is_authenticated(): return redirect_to_login(request.path) obj = lookup_object(model, object_id, slug, slug_field) if request.method == 'POST': obj.delete() msg = ugettext("The %(verbose_name)s was deleted.") %\ {"verbose_name": model._meta.verbose_name} messages.success(request, msg, fail_silently=True) return HttpResponseRedirect(post_delete_redirect) else: if not template_name: template_name = "%s/%s_confirm_delete.html" % (model._meta.app_label, model._meta.object_name.lower()) t = template_loader.get_template(template_name) c = RequestContext(request, { template_object_name: obj, }, context_processors) apply_extra_context(extra_context, c) response = HttpResponse(t.render(c)) populate_xheaders(request, response, model, getattr(obj, obj._meta.pk.attname)) return response
from django.core.exceptions import ImproperlyConfigured from django.forms import models as model_forms from django.http import HttpResponseRedirect from django.views.generic.base import ContextMixin, TemplateResponseMixin, View from django.views.generic.detail import ( BaseDetailView, SingleObjectMixin, SingleObjectTemplateResponseMixin, ) class FormMixin(ContextMixin): """Provide a way to show and handle a form in a request.""" initial = {} form_class = None success_url = None prefix = None def get_initial(self): """Return the initial data to use for forms on this view.""" return self.initial.copy() def get_prefix(self): """Return the prefix to use for forms.""" return self.prefix def get_form_class(self): """Return the form class to use.""" return self.form_class def get_form(self, form_class=None): """Return an instance of the form to be used in this view.""" if form_class is None: form_class = self.get_form_class() return form_class(**self.get_form_kwargs()) def get_form_kwargs(self): """Return the keyword arguments for instantiating the form.""" kwargs = { 'initial': self.get_initial(), 'prefix': self.get_prefix(), } if self.request.method in ('POST', 'PUT'): kwargs.update({ 'data': self.request.POST, 'files': self.request.FILES, }) return kwargs def get_success_url(self): """Return the URL to redirect to after processing a valid form.""" if not self.success_url: raise ImproperlyConfigured("No URL to redirect to. Provide a success_url.") return str(self.success_url) # success_url may be lazy def form_valid(self, form): """If the form is valid, redirect to the supplied URL.""" return HttpResponseRedirect(self.get_success_url()) def form_invalid(self, form): """If the form is invalid, render the invalid form.""" return self.render_to_response(self.get_context_data(form=form)) def get_context_data(self, **kwargs): """Insert the form into the context dict.""" if 'form' not in kwargs: kwargs['form'] = self.get_form() return super().get_context_data(**kwargs) class ModelFormMixin(FormMixin, SingleObjectMixin): """Provide a way to show and handle a ModelForm in a request.""" fields = None def get_form_class(self): """Return the form class to use in this view.""" if self.fields is not None and self.form_class: raise ImproperlyConfigured( "Specifying both 'fields' and 'form_class' is not permitted." ) if self.form_class: return self.form_class else: if self.model is not None: # If a model has been explicitly provided, use it model = self.model elif hasattr(self, 'object') and self.object is not None: # If this view is operating on a single object, use # the class of that object model = self.object.__class__ else: # Try to get a queryset and extract the model class # from that model = self.get_queryset().model if self.fields is None: raise ImproperlyConfigured( "Using ModelFormMixin (base class of %s) without " "the 'fields' attribute is prohibited." % self.__class__.__name__ ) return model_forms.modelform_factory(model, fields=self.fields) def get_form_kwargs(self): """Return the keyword arguments for instantiating the form.""" kwargs = super().get_form_kwargs() if hasattr(self, 'object'): kwargs.update({'instance': self.object}) return kwargs def get_success_url(self): """Return the URL to redirect to after processing a valid form.""" if self.success_url: url = self.success_url.format(**self.object.__dict__) else: try: url = self.object.get_absolute_url() except AttributeError: raise ImproperlyConfigured( "No URL to redirect to. Either provide a url or define" " a get_absolute_url method on the Model.") return url def form_valid(self, form): """If the form is valid, save the associated model.""" self.object = form.save() return super().form_valid(form) class ProcessFormView(View): """Render a form on GET and processes it on POST.""" def get(self, request, *args, **kwargs): """Handle GET requests: instantiate a blank version of the form.""" return self.render_to_response(self.get_context_data()) def post(self, request, *args, **kwargs): """ Handle POST requests: instantiate a form instance with the passed POST variables and then check if it's valid. """ form = self.get_form() if form.is_valid(): return self.form_valid(form) else: return self.form_invalid(form) # PUT is a valid HTTP verb for creating (with a known URL) or editing an # object, note that browsers only support POST for now. def put(self, *args, **kwargs): return self.post(*args, **kwargs) class BaseFormView(FormMixin, ProcessFormView): """A base view for displaying a form.""" class FormView(TemplateResponseMixin, BaseFormView): """A view for displaying a form and rendering a template response.""" class BaseCreateView(ModelFormMixin, ProcessFormView): """ Base view for creating an new object instance. Using this base class requires subclassing to provide a response mixin. """ def get(self, request, *args, **kwargs): self.object = None return super().get(request, *args, **kwargs) def post(self, request, *args, **kwargs): self.object = None return super().post(request, *args, **kwargs) class CreateView(SingleObjectTemplateResponseMixin, BaseCreateView): """ View for creating a new object, with a response rendered by a template. """ template_name_suffix = '_form' class BaseUpdateView(ModelFormMixin, ProcessFormView): """ Base view for updating an existing object. Using this base class requires subclassing to provide a response mixin. """ def get(self, request, *args, **kwargs): self.object = self.get_object() return super().get(request, *args, **kwargs) def post(self, request, *args, **kwargs): self.object = self.get_object() return super().post(request, *args, **kwargs) class UpdateView(SingleObjectTemplateResponseMixin, BaseUpdateView): """View for updating an object, with a response rendered by a template.""" template_name_suffix = '_form' class DeletionMixin: """Provide the ability to delete objects.""" success_url = None def delete(self, request, *args, **kwargs): """ Call the delete() method on the fetched object and then redirect to the success URL. """ self.object = self.get_object() success_url = self.get_success_url() self.object.delete() return HttpResponseRedirect(success_url) # Add support for browsers which only accept GET and POST for now. def post(self, request, *args, **kwargs): return self.delete(request, *args, **kwargs) def get_success_url(self): if self.success_url: return self.success_url.format(**self.object.__dict__) else: raise ImproperlyConfigured( "No URL to redirect to. Provide a success_url.") class BaseDeleteView(DeletionMixin, BaseDetailView): """ Base view for deleting an object. Using this base class requires subclassing to provide a response mixin. """ class DeleteView(SingleObjectTemplateResponseMixin, BaseDeleteView): """ View for deleting an object retrieved with self.get_object(), with a response rendered by a template. """ template_name_suffix = '_confirm_delete'
#!/usr/bin/env python import unittest from PySide2.QtCore import QObject, Slot, SIGNAL, SLOT class MyObject(QObject): def __init__(self, parent=None): QObject.__init__(self, parent) self._slotCalledCount = 0 @Slot() def mySlot(self): self._slotCalledCount = self._slotCalledCount + 1 @Slot(int) @Slot('QString') def mySlot2(self, arg0): self._slotCalledCount = self._slotCalledCount + 1 @Slot(name='mySlot3') def foo(self): self._slotCalledCount = self._slotCalledCount + 1 @Slot(str, int) def mySlot4(self, a, b): self._slotCalledCount = self._slotCalledCount + 1 @Slot(result=int) def mySlot5(self): self._slotCalledCount = self._slotCalledCount + 1 @Slot(result=QObject) def mySlot6(self): self._slotCalledCount = self._slotCalledCount + 1 class StaticMetaObjectTest(unittest.TestCase): def testSignalPropagation(self): o = MyObject() m = o.metaObject() self.assertTrue(m.indexOfSlot('mySlot()') > 0) self.assertTrue(m.indexOfSlot('mySlot2(int)') > 0) self.assertTrue(m.indexOfSlot('mySlot2(QString)') > 0) self.assertTrue(m.indexOfSlot('mySlot3()') > 0) self.assertTrue(m.indexOfSlot('mySlot4(QString,int)') > 0) def testEmission(self): o = MyObject() o.connect(SIGNAL("mySignal()"), o, SLOT("mySlot()")) o.emit(SIGNAL("mySignal()")) self.assertTrue(o._slotCalledCount == 1) def testResult(self): o = MyObject() mo = o.metaObject() i = mo.indexOfSlot('mySlot5()') m = mo.method(i) self.assertEqual(m.typeName(), "int") def testResultObject(self): o = MyObject() mo = o.metaObject() i = mo.indexOfSlot('mySlot6()') m = mo.method(i) self.assertEqual(m.typeName(), "QObject*") class SlotWithoutArgs(unittest.TestCase): def testError(self): # It should be an error to call the slot without the # arguments, as just @Slot would end up in a slot # accepting argument functions self.assertRaises(TypeError, Slot, lambda: 3) if __name__ == '__main__': unittest.main()
# Licensed to the Apache Software Foundation (ASF) under one # or more contributor license agreements. See the NOTICE file # distributed with this work for additional information # regarding copyright ownership. The ASF licenses this file # to you under the Apache License, Version 2.0 (the # "License"); you may not use this file except in compliance # with the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, # software distributed under the License is distributed on an # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY # KIND, either express or implied. See the License for the # specific language governing permissions and limitations # under the License. # pylint: skip-file import mxnet as mx import numpy as np import logging from solver import Solver, Monitor try: import cPickle as pickle except: import pickle def extract_feature(sym, args, auxs, data_iter, N, xpu=mx.cpu()): input_buffs = [mx.nd.empty(shape, ctx=xpu) for k, shape in data_iter.provide_data] input_names = [k for k, shape in data_iter.provide_data] args = dict(args, **dict(zip(input_names, input_buffs))) exe = sym.bind(xpu, args=args, aux_states=auxs) outputs = [[] for i in exe.outputs] output_buffs = None data_iter.hard_reset() for batch in data_iter: for data, buff in zip(batch.data, input_buffs): data.copyto(buff) exe.forward(is_train=False) if output_buffs is None: output_buffs = [mx.nd.empty(i.shape, ctx=mx.cpu()) for i in exe.outputs] else: for out, buff in zip(outputs, output_buffs): out.append(buff.asnumpy()) for out, buff in zip(exe.outputs, output_buffs): out.copyto(buff) for out, buff in zip(outputs, output_buffs): out.append(buff.asnumpy()) outputs = [np.concatenate(i, axis=0)[:N] for i in outputs] return dict(zip(sym.list_outputs(), outputs)) class MXModel(object): def __init__(self, xpu=mx.cpu(), *args, **kwargs): self.xpu = xpu self.loss = None self.args = {} self.args_grad = {} self.args_mult = {} self.auxs = {} self.setup(*args, **kwargs) def save(self, fname): args_save = {key: v.asnumpy() for key, v in self.args.items()} with open(fname, 'wb') as fout: pickle.dump(args_save, fout) def load(self, fname): with open(fname, 'rb') as fin: args_save = pickle.load(fin) for key, v in args_save.items(): if key in self.args: self.args[key][:] = v def setup(self, *args, **kwargs): raise NotImplementedError("must override this")
#!/usr/bin/python # -*- coding: utf-8 -*- # (c) 2017, Jon Hawkesworth (@jhawkesworth) <figs@unity.demon.co.uk> # # This file is part of Ansible # # Ansible is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Ansible is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Ansible. If not, see <http://www.gnu.org/licenses/>. # this is a windows documentation stub. actual code lives in the .ps1 # file of the same name ANSIBLE_METADATA = {'metadata_version': '1.0', 'status': ['preview'], 'supported_by': 'community'} DOCUMENTATION = r''' --- module: win_msg version_added: "2.3" short_description: Sends a message to logged in users on Windows hosts. description: - Wraps the msg.exe command in order to send messages to Windows hosts. options: to: description: - Who to send the message to. Can be a username, sessionname or sessionid. default: '*' display_seconds: description: - How long to wait for receiver to acknowledge message, in seconds. default: 10 wait: description: - Whether to wait for users to respond. Module will only wait for the number of seconds specified in display_seconds or 10 seconds if not specified. However, if I(wait) is true, the message is sent to each logged on user in turn, waiting for the user to either press 'ok' or for the timeout to elapse before moving on to the next user. type: bool default: 'no' msg: description: - The text of the message to be displayed. default: Hello world! author: - Jon Hawkesworth (@jhawkesworth) notes: - This module must run on a windows host, so ensure your play targets windows hosts, or delegates to a windows host. - Messages are only sent to the local host where the module is run. - The module does not support sending to users listed in a file. - Setting wait to true can result in long run times on systems with many logged in users. ''' EXAMPLES = r''' - name: Warn logged in users of impending upgrade win_msg: display_seconds: 60 msg: Automated upgrade about to start. Please save your work and log off before {{ deployment_start_time }} ''' RETURN = r''' msg: description: Test of the message that was sent. returned: changed type: string sample: Automated upgrade about to start. Please save your work and log off before 22 July 2016 18:00:00 display_seconds: description: Value of display_seconds module parameter. returned: success type: string sample: 10 rc: description: The return code of the API call returned: always type: int sample: 0 runtime_seconds: description: How long the module took to run on the remote windows host. returned: success type: string sample: 22 July 2016 17:45:51 sent_localtime: description: local time from windows host when the message was sent. returned: success type: string sample: 22 July 2016 17:45:51 wait: description: Value of wait module parameter. returned: success type: boolean sample: false '''
# Copyright 2013 The Chromium Authors. All rights reserved. # Use of this source code is governed by a BSD-style license that can be # found in the LICENSE file. import json from telemetry.core import web_contents from telemetry.core.backends.chrome import inspector_backend class MiscWebContentsBackend(object): """Provides acccess to chrome://oobe/login page, which is neither an extension nor a tab.""" def __init__(self, browser_backend): self._browser_backend = browser_backend def GetOobe(self): oobe_web_contents_info = self._FindWebContentsInfo() if oobe_web_contents_info: debugger_url = oobe_web_contents_info.get('webSocketDebuggerUrl') if debugger_url: inspector = self._CreateInspectorBackend(debugger_url) return web_contents.WebContents(inspector) return None def _CreateInspectorBackend(self, debugger_url): return inspector_backend.InspectorBackend(self._browser_backend.browser, self._browser_backend, debugger_url) def _ListWebContents(self, timeout=None): data = self._browser_backend.Request('', timeout=timeout) return json.loads(data) def _FindWebContentsInfo(self): for web_contents_info in self._ListWebContents(): # Prior to crrev.com/203152, url was chrome://oobe/login. if (web_contents_info.get('url').startswith('chrome://oobe')): return web_contents_info return None
# http://www.absoft.com/literature/osxuserguide.pdf # http://www.absoft.com/documentation.html # Notes: # - when using -g77 then use -DUNDERSCORE_G77 to compile f2py # generated extension modules (works for f2py v2.45.241_1936 and up) import os from numpy.distutils.cpuinfo import cpu from numpy.distutils.fcompiler import FCompiler, dummy_fortran_file from numpy.distutils.misc_util import cyg2win32 compilers = ['AbsoftFCompiler'] class AbsoftFCompiler(FCompiler): compiler_type = 'absoft' description = 'Absoft Corp Fortran Compiler' #version_pattern = r'FORTRAN 77 Compiler (?P<version>[^\s*,]*).*?Absoft Corp' version_pattern = r'(f90:.*?(Absoft Pro FORTRAN Version|FORTRAN 77 Compiler|Absoft Fortran Compiler Version|Copyright Absoft Corporation.*?Version))'+\ r' (?P<version>[^\s*,]*)(.*?Absoft Corp|)' # on windows: f90 -V -c dummy.f # f90: Copyright Absoft Corporation 1994-1998 mV2; Cray Research, Inc. 1994-1996 CF90 (2.x.x.x f36t87) Version 2.3 Wed Apr 19, 2006 13:05:16 # samt5735(8)$ f90 -V -c dummy.f # f90: Copyright Absoft Corporation 1994-2002; Absoft Pro FORTRAN Version 8.0 # Note that fink installs g77 as f77, so need to use f90 for detection. executables = { 'version_cmd' : None, # set by update_executables 'compiler_f77' : ["f77"], 'compiler_fix' : ["f90"], 'compiler_f90' : ["f90"], 'linker_so' : ["<F90>"], 'archiver' : ["ar", "-cr"], 'ranlib' : ["ranlib"] } if os.name=='nt': library_switch = '/out:' #No space after /out:! module_dir_switch = None module_include_switch = '-p' def update_executables(self): f = cyg2win32(dummy_fortran_file()) self.executables['version_cmd'] = ['<F90>', '-V', '-c', f+'.f', '-o', f+'.o'] def get_flags_linker_so(self): if os.name=='nt': opt = ['/dll'] # The "-K shared" switches are being left in for pre-9.0 versions # of Absoft though I don't think versions earlier than 9 can # actually be used to build shared libraries. In fact, version # 8 of Absoft doesn't recognize "-K shared" and will fail. elif self.get_version() >= '9.0': opt = ['-shared'] else: opt = ["-K", "shared"] return opt def library_dir_option(self, dir): if os.name=='nt': return ['-link', '/PATH:%s' % (dir)] return "-L" + dir def library_option(self, lib): if os.name=='nt': return '%s.lib' % (lib) return "-l" + lib def get_library_dirs(self): opt = FCompiler.get_library_dirs(self) d = os.environ.get('ABSOFT') if d: if self.get_version() >= '10.0': # use shared libraries, the static libraries were not compiled -fPIC prefix = 'sh' else: prefix = '' if cpu.is_64bit(): suffix = '64' else: suffix = '' opt.append(os.path.join(d, '%slib%s' % (prefix, suffix))) return opt def get_libraries(self): opt = FCompiler.get_libraries(self) if self.get_version() >= '11.0': opt.extend(['af90math', 'afio', 'af77math', 'amisc']) elif self.get_version() >= '10.0': opt.extend(['af90math', 'afio', 'af77math', 'U77']) elif self.get_version() >= '8.0': opt.extend(['f90math', 'fio', 'f77math', 'U77']) else: opt.extend(['fio', 'f90math', 'fmath', 'U77']) if os.name =='nt': opt.append('COMDLG32') return opt def get_flags(self): opt = FCompiler.get_flags(self) if os.name != 'nt': opt.extend(['-s']) if self.get_version(): if self.get_version()>='8.2': opt.append('-fpic') return opt def get_flags_f77(self): opt = FCompiler.get_flags_f77(self) opt.extend(['-N22', '-N90', '-N110']) v = self.get_version() if os.name == 'nt': if v and v>='8.0': opt.extend(['-f', '-N15']) else: opt.append('-f') if v: if v<='4.6': opt.append('-B108') else: # Though -N15 is undocumented, it works with # Absoft 8.0 on Linux opt.append('-N15') return opt def get_flags_f90(self): opt = FCompiler.get_flags_f90(self) opt.extend(["-YCFRL=1", "-YCOM_NAMES=LCS", "-YCOM_PFX", "-YEXT_PFX", "-YCOM_SFX=_", "-YEXT_SFX=_", "-YEXT_NAMES=LCS"]) if self.get_version(): if self.get_version()>'4.6': opt.extend(["-YDEALLOC=ALL"]) return opt def get_flags_fix(self): opt = FCompiler.get_flags_fix(self) opt.extend(["-YCFRL=1", "-YCOM_NAMES=LCS", "-YCOM_PFX", "-YEXT_PFX", "-YCOM_SFX=_", "-YEXT_SFX=_", "-YEXT_NAMES=LCS"]) opt.extend(["-f", "fixed"]) return opt def get_flags_opt(self): opt = ['-O'] return opt if __name__ == '__main__': from distutils import log log.set_verbosity(2) from numpy.distutils import customized_fcompiler print(customized_fcompiler(compiler='absoft').get_version())
# Copyright (C) 2003-2005 Peter J. Verveer # # Redistribution and use in source and binary forms, with or without # modification, are permitted provided that the following conditions # are met: # # 1. Redistributions of source code must retain the above copyright # notice, this list of conditions and the following disclaimer. # # 2. Redistributions in binary form must reproduce the above # copyright notice, this list of conditions and the following # disclaimer in the documentation and/or other materials provided # with the distribution. # # 3. The name of the author may not be used to endorse or promote # products derived from this software without specific prior # written permission. # # THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS # OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED # WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE # ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY # DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL # DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE # GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, # WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING # NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS # SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. import types import numpy def _extend_mode_to_code(mode): """Convert an extension mode to the corresponding integer code. """ if mode == 'nearest': return 0 elif mode == 'wrap': return 1 elif mode == 'reflect': return 2 elif mode == 'mirror': return 3 elif mode == 'constant': return 4 else: raise RuntimeError('boundary mode not supported') def _normalize_sequence(input, rank, array_type=None): """If input is a scalar, create a sequence of length equal to the rank by duplicating the input. If input is a sequence, check if its length is equal to the length of array. """ if (isinstance(input, (types.IntType, types.LongType, types.FloatType))): normalized = [input] * rank else: normalized = list(input) if len(normalized) != rank: err = "sequence argument must have length equal to input rank" raise RuntimeError(err) return normalized def _get_output(output, input, shape=None): if shape is None: shape = input.shape if output is None: output = numpy.zeros(shape, dtype = input.dtype.name) return_value = output elif type(output) in [type(types.TypeType), type(numpy.zeros((4,)).dtype)]: output = numpy.zeros(shape, dtype = output) return_value = output elif type(output) is types.StringType: output = numpy.typeDict[output] output = numpy.zeros(shape, dtype = output) return_value = output else: if output.shape != shape: raise RuntimeError("output shape not correct") return_value = None return output, return_value def _check_axis(axis, rank): if axis < 0: axis += rank if axis < 0 or axis >= rank: raise ValueError('invalid axis') return axis
# Copyright 2012 Red Hat, Inc. # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. # W0603: Using the global statement # W0621: Redefining name %s from outer scope # pylint: disable=W0603,W0621 from __future__ import print_function import getpass import inspect import os import sys import textwrap from oslo_utils import encodeutils from oslo_utils import strutils import prettytable import six from six import moves from nova.openstack.common._i18n import _ class MissingArgs(Exception): """Supplied arguments are not sufficient for calling a function.""" def __init__(self, missing): self.missing = missing msg = _("Missing arguments: %s") % ", ".join(missing) super(MissingArgs, self).__init__(msg) def validate_args(fn, *args, **kwargs): """Check that the supplied args are sufficient for calling a function. >>> validate_args(lambda a: None) Traceback (most recent call last): ... MissingArgs: Missing argument(s): a >>> validate_args(lambda a, b, c, d: None, 0, c=1) Traceback (most recent call last): ... MissingArgs: Missing argument(s): b, d :param fn: the function to check :param arg: the positional arguments supplied :param kwargs: the keyword arguments supplied """ argspec = inspect.getargspec(fn) num_defaults = len(argspec.defaults or []) required_args = argspec.args[:len(argspec.args) - num_defaults] def isbound(method): return getattr(method, '__self__', None) is not None if isbound(fn): required_args.pop(0) missing = [arg for arg in required_args if arg not in kwargs] missing = missing[len(args):] if missing: raise MissingArgs(missing) def arg(*args, **kwargs): """Decorator for CLI args. Example: >>> @arg("name", help="Name of the new entity") ... def entity_create(args): ... pass """ def _decorator(func): add_arg(func, *args, **kwargs) return func return _decorator def env(*args, **kwargs): """Returns the first environment variable set. If all are empty, defaults to '' or keyword arg `default`. """ for arg in args: value = os.environ.get(arg) if value: return value return kwargs.get('default', '') def add_arg(func, *args, **kwargs): """Bind CLI arguments to a shell.py `do_foo` function.""" if not hasattr(func, 'arguments'): func.arguments = [] # NOTE(sirp): avoid dups that can occur when the module is shared across # tests. if (args, kwargs) not in func.arguments: # Because of the semantics of decorator composition if we just append # to the options list positional options will appear to be backwards. func.arguments.insert(0, (args, kwargs)) def unauthenticated(func): """Adds 'unauthenticated' attribute to decorated function. Usage: >>> @unauthenticated ... def mymethod(f): ... pass """ func.unauthenticated = True return func def isunauthenticated(func): """Checks if the function does not require authentication. Mark such functions with the `@unauthenticated` decorator. :returns: bool """ return getattr(func, 'unauthenticated', False) def print_list(objs, fields, formatters=None, sortby_index=0, mixed_case_fields=None, field_labels=None): """Print a list or objects as a table, one row per object. :param objs: iterable of :class:`Resource` :param fields: attributes that correspond to columns, in order :param formatters: `dict` of callables for field formatting :param sortby_index: index of the field for sorting table rows :param mixed_case_fields: fields corresponding to object attributes that have mixed case names (e.g., 'serverId') :param field_labels: Labels to use in the heading of the table, default to fields. """ formatters = formatters or {} mixed_case_fields = mixed_case_fields or [] field_labels = field_labels or fields if len(field_labels) != len(fields): raise ValueError(_("Field labels list %(labels)s has different number " "of elements than fields list %(fields)s"), {'labels': field_labels, 'fields': fields}) if sortby_index is None: kwargs = {} else: kwargs = {'sortby': field_labels[sortby_index]} pt = prettytable.PrettyTable(field_labels) pt.align = 'l' for o in objs: row = [] for field in fields: if field in formatters: row.append(formatters[field](o)) else: if field in mixed_case_fields: field_name = field.replace(' ', '_') else: field_name = field.lower().replace(' ', '_') data = getattr(o, field_name, '') row.append(data) pt.add_row(row) if six.PY3: print(encodeutils.safe_encode(pt.get_string(**kwargs)).decode()) else: print(encodeutils.safe_encode(pt.get_string(**kwargs))) def print_dict(dct, dict_property="Property", wrap=0): """Print a `dict` as a table of two columns. :param dct: `dict` to print :param dict_property: name of the first column :param wrap: wrapping for the second column """ pt = prettytable.PrettyTable([dict_property, 'Value']) pt.align = 'l' for k, v in six.iteritems(dct): # convert dict to str to check length if isinstance(v, dict): v = six.text_type(v) if wrap > 0: v = textwrap.fill(six.text_type(v), wrap) # if value has a newline, add in multiple rows # e.g. fault with stacktrace if v and isinstance(v, six.string_types) and r'\n' in v: lines = v.strip().split(r'\n') col1 = k for line in lines: pt.add_row([col1, line]) col1 = '' else: pt.add_row([k, v]) if six.PY3: print(encodeutils.safe_encode(pt.get_string()).decode()) else: print(encodeutils.safe_encode(pt.get_string())) def get_password(max_password_prompts=3): """Read password from TTY.""" verify = strutils.bool_from_string(env("OS_VERIFY_PASSWORD")) pw = None if hasattr(sys.stdin, "isatty") and sys.stdin.isatty(): # Check for Ctrl-D try: for __ in moves.range(max_password_prompts): pw1 = getpass.getpass("OS Password: ") if verify: pw2 = getpass.getpass("Please verify: ") else: pw2 = pw1 if pw1 == pw2 and pw1: pw = pw1 break except EOFError: pass return pw def service_type(stype): """Adds 'service_type' attribute to decorated function. Usage: .. code-block:: python @service_type('volume') def mymethod(f): ... """ def inner(f): f.service_type = stype return f return inner def get_service_type(f): """Retrieves service type from function.""" return getattr(f, 'service_type', None) def pretty_choice_list(l): return ', '.join("'%s'" % i for i in l) def exit(msg=''): if msg: print (msg, file=sys.stderr) sys.exit(1)
""" Tests for logout for enterprise flow """ import ddt import mock from django.test.utils import override_settings from django.urls import reverse from openedx.core.djangolib.testing.utils import CacheIsolationTestCase, skip_unless_lms from openedx.features.enterprise_support.api import enterprise_enabled from openedx.features.enterprise_support.tests import ( FAKE_ENTERPRISE_CUSTOMER, FEATURES_WITH_ENTERPRISE_ENABLED, factories ) from openedx.features.enterprise_support.tests.mixins.enterprise import EnterpriseServiceMockMixin from student.tests.factories import UserFactory from util.testing import UrlResetMixin @ddt.ddt @override_settings(FEATURES=FEATURES_WITH_ENTERPRISE_ENABLED) @skip_unless_lms class EnterpriseLogoutTests(EnterpriseServiceMockMixin, CacheIsolationTestCase, UrlResetMixin): """ Tests for the enterprise logout functionality. """ def setUp(self): super(EnterpriseLogoutTests, self).setUp() self.user = UserFactory() self.enterprise_customer = FAKE_ENTERPRISE_CUSTOMER self.enterprise_learner = factories.EnterpriseCustomerUserFactory(user_id=self.user.id) self.client.login(username=self.user.username, password='test') patcher = mock.patch('openedx.features.enterprise_support.api.enterprise_customer_from_api') self.mock_enterprise_customer_from_api = patcher.start() self.mock_enterprise_customer_from_api.return_value = self.enterprise_customer self.addCleanup(patcher.stop) @ddt.data( ('https%3A%2F%2Ftest.edx.org%2Fcourses', False), ('/courses/course-v1:ARTS+D1+2018_T/course/', False), ('invalid-url', False), ('/enterprise/c5dad9a7-741c-4841-868f-850aca3ff848/course/Microsoft+DAT206x/enroll/', True), ('%2Fenterprise%2Fc5dad9a7-741c-4841-868f-850aca3ff848%2Fcourse%2FMicrosoft%2BDAT206x%2Fenroll%2F', True), ('/enterprise/handle_consent_enrollment/efd91463-dc40-4882-aeb9-38202131e7b2/course', True), ('%2Fenterprise%2Fhandle_consent_enrollment%2Fefd91463-dc40-4882-aeb9-38202131e7b2%2Fcourse', True), ) @ddt.unpack def test_logout_enterprise_target(self, redirect_url, enterprise_target): url = '{logout_path}?redirect_url={redirect_url}'.format( logout_path=reverse('logout'), redirect_url=redirect_url ) self.assertTrue(enterprise_enabled()) response = self.client.get(url, HTTP_HOST='testserver') expected = { 'enterprise_target': enterprise_target, } self.assertDictContainsSubset(expected, response.context_data) if enterprise_target: self.assertContains(response, 'We are signing you in.')
#-*- coding:utf-8 -*- """ This file is part of PyGaze. PyGaze is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. PyGaze is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with PyGaze. If not, see <http://www.gnu.org/licenses/>. """ import inspect from openexp.canvas import canvas from libopensesame.item import item from libqtopensesame.items.qtautoplugin import qtautoplugin from pygaze.display import Display class pygaze_drift_correct(item): """Plug-in runtime definition.""" description = u'Perform eye-tracker drift correction' def reset(self): """ desc: Resets plug-in settings. """ self.var.xpos = 0 self.var.ypos = 0 self.var.fixation_triggered = u'no' self.var.target_color = u'[foreground]' self.var.target_style = u'default' self.var.draw_target = u'yes' def prepare_drift_correction_canvas(self): """A hook to prepare the canvas with the drift-correction target.""" if self.var.draw_target == u'yes': self.dc_canvas = canvas(self.experiment) self.dc_canvas.fixdot(self.var.xpos, self.var.ypos, color=self.var.target_color, style=self.var.target_style) else: self.dc_canvas = None def draw_drift_correction_canvas(self, x, y): """ A hook to show the canvas with the drift-correction target. Arguments: x -- The X coordinate (unused). y -- The Y coordinate (unused). """ if self.dc_canvas is not None: self.dc_canvas.show() def prepare(self): """The preparation phase of the plug-in goes here.""" item.prepare(self) self.prepare_drift_correction_canvas() self.experiment.pygaze_eyetracker.set_draw_drift_correction_target_func( self.draw_drift_correction_canvas) def run(self): """The run phase of the plug-in goes here.""" self.set_item_onset() if self.var.uniform_coordinates == u'yes': xpos = self.var.width / 2 + self.var.xpos ypos = self.var.height / 2 + self.var.ypos else: xpos = self.var.xpos ypos = self.var.ypos while True: success = self.experiment.pygaze_eyetracker.drift_correction( pos=(xpos, ypos), fix_triggered=self.var.fixation_triggered==u'yes') if success: break class qtpygaze_drift_correct(pygaze_drift_correct, qtautoplugin): """Plug-in GUI definition.""" def __init__(self, name, experiment, script=None): """ Constructor. Arguments: name -- The name of the plug-in. experiment -- The experiment object. Keyword arguments: script -- A definition script. (default=None) """ pygaze_drift_correct.__init__(self, name, experiment, script) qtautoplugin.__init__(self, __file__) self.custom_interactions() def apply_edit_changes(self): """Apply the controls""" if not qtautoplugin.apply_edit_changes(self) or self.lock: return False self.custom_interactions() def custom_interactions(self): """ Disables the target-style combobox if no target display should be drawn. """ draw_target = self.var.draw_target == u'yes' self.combobox_target_style.setEnabled(draw_target) self.line_edit_target_color.setEnabled(draw_target)
#!/usr/bin/env python import vtk from vtk.test import Testing from vtk.util.misc import vtkGetDataRoot VTK_DATA_ROOT = vtkGetDataRoot() # Create the RenderWindow, Renderer and both Actors ren1 = vtk.vtkRenderer() renWin = vtk.vtkRenderWindow() renWin.AddRenderer(ren1) iren = vtk.vtkRenderWindowInteractor() iren.SetRenderWindow(renWin) # load in the texture map # pngReader = vtk.vtkPNGReader() pngReader.SetFileName(VTK_DATA_ROOT + "/Data/camscene.png") pngReader.Update() xWidth = pngReader.GetOutput().GetDimensions()[0] yHeight = pngReader.GetOutput().GetDimensions()[1] wl = vtk.vtkWarpLens() wl.SetInputConnection(pngReader.GetOutputPort()) wl.SetPrincipalPoint(2.4507, 1.7733) wl.SetFormatWidth(4.792) wl.SetFormatHeight(3.6) wl.SetImageWidth(xWidth) wl.SetImageHeight(yHeight) wl.SetK1(0.01307) wl.SetK2(0.0003102) wl.SetP1(1.953e-005) wl.SetP2(-9.655e-005) gf = vtk.vtkGeometryFilter() gf.SetInputConnection(wl.GetOutputPort()) tf = vtk.vtkTriangleFilter() tf.SetInputConnection(gf.GetOutputPort()) strip = vtk.vtkStripper() strip.SetInputConnection(tf.GetOutputPort()) strip.SetMaximumLength(250) dsm = vtk.vtkPolyDataMapper() dsm.SetInputConnection(strip.GetOutputPort()) planeActor = vtk.vtkActor() planeActor.SetMapper(dsm) # Add the actors to the renderer, set the background and size ren1.AddActor(planeActor) ren1.SetBackground(0.1, 0.2, 0.4) renWin.SetSize(300, 300) # render the image iren.Initialize() renWin.Render() ren1.GetActiveCamera().Zoom(1.4) renWin.Render() #iren.Start()
import os import synapse.exc as s_exc import synapse.common as s_common import synapse.lib.rstorm as s_rstorm import synapse.tests.utils as s_test rst_in = ''' HI ## .. storm-cortex:: default .. storm-cortex:: default .. storm-opts:: {"vars": {"foo": 10, "bar": "baz"}} .. storm-pre:: [ inet:asn=$foo ] .. storm:: $lib.print($bar) $lib.warn(omgomgomg) .. storm-expect:: baz .. storm-pre:: [ inet:ipv6=0 ] .. storm-pkg:: synapse/tests/files/stormpkg/testpkg.yaml .. storm:: --hide-props testpkgcmd foo .. storm:: --hide-query $lib.print(secret) .. storm:: --hide-query file:bytes .. storm-svc:: synapse.tests.files.rstorm.testsvc.Testsvc test {"secret": "jupiter"} .. storm:: testsvc.test ''' rst_out = ''' HI ## :: > $lib.print($bar) $lib.warn(omgomgomg) baz WARNING: omgomgomg :: > testpkgcmd foo inet:ipv6=::ffff:0 :: secret :: :: > testsvc.test jupiter ''' rst_in_debug = ''' HI ## .. storm-cortex:: default .. storm:: --debug [ inet:ipv4=0 ] ''' rst_in_props = ''' HI ## .. storm-cortex:: default .. storm:: [ inet:ipv4=0 ] ''' rst_out_props = ''' HI ## :: > [ inet:ipv4=0 ] inet:ipv4=0.0.0.0 :type = private ''' rst_in_http = ''' HI ## .. storm-cortex:: default .. storm:: $resp=$lib.inet.http.get("http://foo.com") $d=$resp.json() $lib.print($d) .. storm-mock-http:: synapse/tests/files/rstorm/httpresp1.json .. storm:: $resp=$lib.inet.http.get("http://foo.com") $d=$resp.json() [ inet:ipv4=$d.data ] .. storm-mock-http:: synapse/tests/files/rstorm/httpresp2.json .. storm:: $resp=$lib.inet.http.get("http://foo.com") $d=$resp.json() [ inet:ipv4=$d.data ] .. storm-mock-http:: synapse/tests/files/rstorm/httpresp3.json .. storm:: $resp=$lib.inet.http.get("http://foo.com") $d=$resp.body.decode() [ it:dev:str=$d ] ''' boom1 = ''' .. storm:: $lib.print(newp) ''' boom2 = ''' .. storm-pre:: $lib.print(newp) ''' boom3 = ''' .. storm-cortex:: default .. storm:: $x = (10 + "foo") ''' boom4 = ''' .. storm-pkg:: synapse/tests/files/stormpkg/testpkg.yaml ''' boom5 = ''' .. storm-svc:: synapse.tests.files.rstorm.testsvc.Testsvc test {"secret": "jupiter"} ''' boom6 = ''' .. storm-cortex:: default .. storm-svc:: synapse.tests.files.rstorm.testsvc.Testsvc test ''' boom7 = ''' .. storm-cortex:: default .. storm-pkg:: synapse/tests/files/stormpkg/newp.newp ''' boom8 = ''' .. storm-cortex:: default .. storm-mock-http:: synapse/tests/files/rstorm/newp.newp ''' boom9 = ''' .. storm-newp:: newp ''' async def get_rst_text(rstfile): async with await s_rstorm.StormRst.anit(rstfile) as rstorm: lines = await rstorm.run() return ''.join(lines) class RStormLibTest(s_test.SynTest): async def test_lib_rstorm(self): with self.getTestDir() as dirn: path = s_common.genpath(dirn, 'test.rst') with s_common.genfile(path) as fd: fd.write(rst_in.encode()) text = await get_rst_text(path) self.eq(text, rst_out) # debug output path = s_common.genpath(dirn, 'test2.rst') with s_common.genfile(path) as fd: fd.write(rst_in_debug.encode()) text = await get_rst_text(path) self.isin('node:edits', text) self.isin('inet:ipv4', text) # props output path = s_common.genpath(dirn, 'test3.rst') with s_common.genfile(path) as fd: fd.write(rst_in_props.encode()) text = await get_rst_text(path) text_nocrt = '\n'.join(line for line in text.split('\n') if '.created =' not in line) self.eq(text_nocrt, rst_out_props) # http path = s_common.genpath(dirn, 'http.rst') with s_common.genfile(path) as fd: fd.write(rst_in_http.encode()) text = await get_rst_text(path) self.isin('{}', text) # no mock gives empty response self.isin('inet:ipv4=1.2.3.4', text) # first mock self.isin('inet:ipv4=5.6.7.8', text) # one mock at a time self.isin('it:dev:str=notjson', text) # one mock at a time # boom1 test path = s_common.genpath(dirn, 'boom1.rst') with s_common.genfile(path) as fd: fd.write(boom1.encode()) with self.raises(s_exc.NoSuchVar): await get_rst_text(path) # boom2 test path = s_common.genpath(dirn, 'boom2.rst') with s_common.genfile(path) as fd: fd.write(boom2.encode()) with self.raises(s_exc.NoSuchVar): await get_rst_text(path) # boom3 test path_boom3 = s_common.genpath(dirn, 'boom3.rst') with s_common.genfile(path_boom3) as fd: fd.write(boom3.encode()) with self.raises(s_exc.StormRuntimeError): await get_rst_text(path_boom3) # boom4 test path = s_common.genpath(dirn, 'boom4.rst') with s_common.genfile(path) as fd: fd.write(boom4.encode()) with self.raises(s_exc.NoSuchVar): await get_rst_text(path) # boom5 test path = s_common.genpath(dirn, 'boom5.rst') with s_common.genfile(path) as fd: fd.write(boom5.encode()) with self.raises(s_exc.NoSuchVar): await get_rst_text(path) # boom6 test path = s_common.genpath(dirn, 'boom6.rst') with s_common.genfile(path) as fd: fd.write(boom6.encode()) with self.raises(s_exc.NeedConfValu): await get_rst_text(path) # boom7 test path = s_common.genpath(dirn, 'boom7.rst') with s_common.genfile(path) as fd: fd.write(boom7.encode()) with self.raises(s_exc.NoSuchFile): await get_rst_text(path) # boom8 test path = s_common.genpath(dirn, 'boom8.rst') with s_common.genfile(path) as fd: fd.write(boom8.encode()) with self.raises(s_exc.NoSuchFile): await get_rst_text(path) # boom9 test path = s_common.genpath(dirn, 'boom9.rst') with s_common.genfile(path) as fd: fd.write(boom9.encode()) with self.raises(s_exc.NoSuchName): await get_rst_text(path) # make sure things get cleaned up async with await s_rstorm.StormRst.anit(path_boom3) as rstorm: try: await rstorm.run() self.fail('This must raise') except s_exc.StormRuntimeError: pass self.true(rstorm.core.isfini) self.true(rstorm.isfini) self.false(os.path.exists(rstorm.core.dirn)) # bad path path = s_common.genpath(dirn, 'newp.newp') with self.raises(s_exc.BadConfValu): await get_rst_text(path)
#!/usr/bin/env python """ Which - locate a command * adapted from Brian Curtin's http://bugs.python.org/file15381/shutil_which.patch * see http://bugs.python.org/issue444582 * uses ``PATHEXT`` on Windows * searches current directory before ``PATH`` on Windows, but not before an explicitly passed path * accepts both string or iterable for an explicitly passed path, or pathext * accepts an explicitly passed empty path, or pathext (either '' or []) * does not search ``PATH`` for files that have a path specified in their name already * moved defpath and defpathext lists initialization to module level, instead of initializing them on each function call * changed interface: which_files() returns generator, which() returns first match, or raises IOError(errno.ENOENT) .. function:: which_files(file [, mode=os.F_OK | os.X_OK[, path=None[, pathext=None]]]) Return a generator which yields full paths in which the *file* name exists in a directory that is part of the file name, or on *path*, and has the given *mode*. By default, *mode* matches an inclusive OR of os.F_OK and os.X_OK - an existing executable file. The *path* is, by default, the ``PATH`` variable on the platform, or the string/iterable passed in as *path*. In the event that a ``PATH`` variable is not found, :const:`os.defpath` is used. On Windows, a current directory is searched before using the ``PATH`` variable, but not before an explicitly passed *path*. The *pathext* is only used on Windows to match files with given extensions appended as well. It defaults to the ``PATHEXT`` variable, or the string/iterable passed in as *pathext*. In the event that a ``PATHEXT`` variable is not found, default value for Windows XP/Vista is used. The command is always searched without extension first, even when *pathext* is explicitly passed. .. function:: which(file [, mode=os.F_OK | os.X_OK[, path=None[, pathext=None]]]) Return first match generated by which_files(file, mode, path, pathext), or raise IOError(errno.ENOENT). """ __docformat__ = 'restructuredtext en' __all__ = 'which which_files pathsep defpath defpathext F_OK R_OK W_OK X_OK'.split() import sys from os import access, defpath, pathsep, environ, F_OK, R_OK, W_OK, X_OK from os.path import exists, dirname, split, join windows = sys.platform.startswith('win') defpath = environ.get('PATH', defpath).split(pathsep) if windows: defpath.insert(0, '.') # can insert without checking, when duplicates are removed # given the quite usual mess in PATH on Windows, let's rather remove duplicates seen = set() defpath = [dir for dir in defpath if dir.lower() not in seen and not seen.add(dir.lower())] del seen defpathext = [''] + environ.get('PATHEXT', '.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC').lower().split(pathsep) else: defpathext = [''] def which_files(file, mode=F_OK | X_OK, path=None, pathext=None): """ Locate a file in a path supplied as a part of the file name, or the user's path, or a supplied path. The function yields full paths (not necessarily absolute paths), in which the given file name matches an existing file in a directory on the path. >>> def test_which(expected, *args, **argd): ... result = list(which_files(*args, **argd)) ... assert result == expected, 'which_files: %s != %s' % (result, expected) ... ... try: ... result = [ which(*args, **argd) ] ... except IOError: ... result = [] ... assert result[:1] == expected[:1], 'which: %s != %s' % (result[:1], expected[:1]) >>> if windows: cmd = environ['COMSPEC'] >>> if windows: test_which([cmd], 'cmd') >>> if windows: test_which([cmd], 'cmd.exe') >>> if windows: test_which([cmd], 'cmd', path=dirname(cmd)) >>> if windows: test_which([cmd], 'cmd', pathext='.exe') >>> if windows: test_which([cmd], cmd) >>> if windows: test_which([cmd], cmd, path='<nonexistent>') >>> if windows: test_which([cmd], cmd, pathext='<nonexistent>') >>> if windows: test_which([cmd], cmd[:-4]) >>> if windows: test_which([cmd], cmd[:-4], path='<nonexistent>') >>> if windows: test_which([], 'cmd', path='<nonexistent>') >>> if windows: test_which([], 'cmd', pathext='<nonexistent>') >>> if windows: test_which([], '<nonexistent>/cmd') >>> if windows: test_which([], cmd[:-4], pathext='<nonexistent>') >>> if not windows: sh = '/bin/sh' >>> if not windows: test_which([sh], 'sh') >>> if not windows: test_which([sh], 'sh', path=dirname(sh)) >>> if not windows: test_which([sh], 'sh', pathext='<nonexistent>') >>> if not windows: test_which([sh], sh) >>> if not windows: test_which([sh], sh, path='<nonexistent>') >>> if not windows: test_which([sh], sh, pathext='<nonexistent>') >>> if not windows: test_which([], 'sh', mode=W_OK) # not running as root, are you? >>> if not windows: test_which([], 'sh', path='<nonexistent>') >>> if not windows: test_which([], '<nonexistent>/sh') """ filepath, file = split(file) if filepath: path = (filepath,) elif path is None: path = defpath elif isinstance(path, str): path = path.split(pathsep) if pathext is None: pathext = defpathext elif isinstance(pathext, str): pathext = pathext.split(pathsep) if not '' in pathext: pathext.insert(0, '') # always check command without extension, even for custom pathext for dir in path: basepath = join(dir, file) for ext in pathext: fullpath = basepath + ext if exists(fullpath) and access(fullpath, mode): yield fullpath def which(file, mode=F_OK | X_OK, path=None, pathext=None): """ Locate a file in a path supplied as a part of the file name, or the user's path, or a supplied path. The function returns full path (not necessarily absolute path), in which the given file name matches an existing file in a directory on the path, or raises IOError(errno.ENOENT). >>> # for doctest see which_files() """ try: return iter(which_files(file, mode, path, pathext)).next() except StopIteration: try: from errno import ENOENT except ImportError: ENOENT = 2 raise IOError(ENOENT, '%s not found' % (mode & X_OK and 'command' or 'file'), file) if __name__ == '__main__': import doctest doctest.testmod() # vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
"""Windows service. Requires pywin32.""" import os import win32api import win32con import win32event import win32service import win32serviceutil from cherrypy.process import wspbus, plugins class ConsoleCtrlHandler(plugins.SimplePlugin): """A WSPBus plugin for handling Win32 console events (like Ctrl-C).""" def __init__(self, bus): self.is_set = False plugins.SimplePlugin.__init__(self, bus) def start(self): if self.is_set: self.bus.log('Handler for console events already set.', level=40) return result = win32api.SetConsoleCtrlHandler(self.handle, 1) if result == 0: self.bus.log('Could not SetConsoleCtrlHandler (error %r)' % win32api.GetLastError(), level=40) else: self.bus.log('Set handler for console events.', level=40) self.is_set = True def stop(self): if not self.is_set: self.bus.log('Handler for console events already off.', level=40) return try: result = win32api.SetConsoleCtrlHandler(self.handle, 0) except ValueError: # "ValueError: The object has not been registered" result = 1 if result == 0: self.bus.log('Could not remove SetConsoleCtrlHandler (error %r)' % win32api.GetLastError(), level=40) else: self.bus.log('Removed handler for console events.', level=40) self.is_set = False def handle(self, event): """Handle console control events (like Ctrl-C).""" if event in (win32con.CTRL_C_EVENT, win32con.CTRL_LOGOFF_EVENT, win32con.CTRL_BREAK_EVENT, win32con.CTRL_SHUTDOWN_EVENT, win32con.CTRL_CLOSE_EVENT): self.bus.log('Console event %s: shutting down bus' % event) # Remove self immediately so repeated Ctrl-C doesn't re-call it. try: self.stop() except ValueError: pass self.bus.exit() # 'First to return True stops the calls' return 1 return 0 class Win32Bus(wspbus.Bus): """A Web Site Process Bus implementation for Win32. Instead of time.sleep, this bus blocks using native win32event objects. """ def __init__(self): self.events = {} wspbus.Bus.__init__(self) def _get_state_event(self, state): """Return a win32event for the given state (creating it if needed).""" try: return self.events[state] except KeyError: event = win32event.CreateEvent(None, 0, 0, "WSPBus %s Event (pid=%r)" % (state.name, os.getpid())) self.events[state] = event return event def _get_state(self): return self._state def _set_state(self, value): self._state = value event = self._get_state_event(value) win32event.PulseEvent(event) state = property(_get_state, _set_state) def wait(self, state, interval=0.1, channel=None): """Wait for the given state(s), KeyboardInterrupt or SystemExit. Since this class uses native win32event objects, the interval argument is ignored. """ if isinstance(state, (tuple, list)): # Don't wait for an event that beat us to the punch ;) if self.state not in state: events = tuple([self._get_state_event(s) for s in state]) win32event.WaitForMultipleObjects(events, 0, win32event.INFINITE) else: # Don't wait for an event that beat us to the punch ;) if self.state != state: event = self._get_state_event(state) win32event.WaitForSingleObject(event, win32event.INFINITE) class _ControlCodes(dict): """Control codes used to "signal" a service via ControlService. User-defined control codes are in the range 128-255. We generally use the standard Python value for the Linux signal and add 128. Example: >>> signal.SIGUSR1 10 control_codes['graceful'] = 128 + 10 """ def key_for(self, obj): """For the given value, return its corresponding key.""" for key, val in self.items(): if val is obj: return key raise ValueError("The given object could not be found: %r" % obj) control_codes = _ControlCodes({'graceful': 138}) def signal_child(service, command): if command == 'stop': win32serviceutil.StopService(service) elif command == 'restart': win32serviceutil.RestartService(service) else: win32serviceutil.ControlService(service, control_codes[command]) class PyWebService(win32serviceutil.ServiceFramework): """Python Web Service.""" _svc_name_ = "Python Web Service" _svc_display_name_ = "Python Web Service" _svc_deps_ = None # sequence of service names on which this depends _exe_name_ = "pywebsvc" _exe_args_ = None # Default to no arguments # Only exists on Windows 2000 or later, ignored on windows NT _svc_description_ = "Python Web Service" def SvcDoRun(self): from cherrypy import process process.bus.start() process.bus.block() def SvcStop(self): from cherrypy import process self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING) process.bus.exit() def SvcOther(self, control): process.bus.publish(control_codes.key_for(control)) if __name__ == '__main__': win32serviceutil.HandleCommandLine(PyWebService)
#!/usr/bin/python # Copyright: (c) 2018, Sebastian Schenzel <sebastian.schenzel@mailbox.org> # GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt) from __future__ import absolute_import, division, print_function __metaclass__ = type ANSIBLE_METADATA = { 'metadata_version': '1.1', 'status': ['preview'], 'supported_by': 'community' } DOCUMENTATION = """ --- module: utm_proxy_exception author: - Sebastian Schenzel (@RickS-C137) short_description: Create, update or destroy reverse_proxy exception entry in Sophos UTM description: - Create, update or destroy a reverse_proxy exception entry in SOPHOS UTM. - This module needs to have the REST Ability of the UTM to be activated. version_added: "2.8" options: name: description: - The name of the object. Will be used to identify the entry required: True type: str op: description: - The operand to be used with the entries of the path parameter default: 'AND' choices: - 'AND' - 'OR' required: False type: str path: description: - The paths the exception in the reverse proxy is defined for type: list default: [] required: False skip_custom_threats_filters: description: - A list of threats to be skipped type: list default: [] required: False skip_threats_filter_categories: description: - Define which categories of threats are skipped type: list default: [] required: False skipav: description: - Skip the Antivirus Scanning default: False type: bool required: False skipbadclients: description: - Block clients with bad reputation default: False type: bool required: False skipcookie: description: - Skip the Cookie Signing check default: False type: bool required: False skipform: description: - Enable form hardening default: False type: bool required: False skipform_missingtoken: description: - Enable form hardening with missing tokens default: False type: bool required: False skiphtmlrewrite: description: - Protection against SQL default: False type: bool required: False skiptft: description: - Enable true file type control default: False type: bool required: False skipurl: description: - Enable static URL hardening default: False type: bool required: False source: description: - Define which categories of threats are skipped type: list default: [] required: False status: description: - Status of the exception rule set default: True type: bool required: False extends_documentation_fragment: - utm """ EXAMPLES = """ - name: Create UTM proxy_exception utm_proxy_exception: utm_host: sophos.host.name utm_token: abcdefghijklmno1234 name: TestExceptionEntry backend: REF_OBJECT_STRING state: present - name: Remove UTM proxy_exception utm_proxy_exception: utm_host: sophos.host.name utm_token: abcdefghijklmno1234 name: TestExceptionEntry state: absent """ RETURN = """ result: description: The utm object that was created returned: success type: complex contains: _ref: description: The reference name of the object type: string _locked: description: Whether or not the object is currently locked type: boolean _type: description: The type of the object type: string name: description: The name of the object type: string comment: description: The optional comment string op: description: The operand to be used with the entries of the path parameter type: string path: description: The paths the exception in the reverse proxy is defined for type: array skip_custom_threats_filters: description: A list of threats to be skipped type: array skip_threats_filter_categories: description: Define which categories of threats are skipped type: array skipav: description: Skip the Antivirus Scanning type: bool skipbadclients: description: Block clients with bad reputation type: bool skipcookie: description: Skip the Cookie Signing check type: bool skipform: description: Enable form hardening type: bool skipform_missingtoken: description: Enable form hardening with missing tokens type: bool skiphtmlrewrite: description: Protection against SQL type: bool skiptft: description: Enable true file type control type: bool skipurl: description: Enable static URL hardening type: bool source: description: Define which categories of threats are skipped type: array """ from ansible.module_utils.utm_utils import UTM, UTMModule from ansible.module_utils._text import to_native def main(): endpoint = "reverse_proxy/exception" key_to_check_for_changes = ["op", "path", "skip_custom_threats_filters", "skip_threats_filter_categories", "skipav", "comment", "skipbadclients", "skipcookie", "skipform", "status", "skipform_missingtoken", "skiphtmlrewrite", "skiptft", "skipurl", "source"] module = UTMModule( argument_spec=dict( name=dict(type='str', required=True), op=dict(type='str', required=False, default='AND', choices=['AND', 'OR']), path=dict(type='list', elements='string', required=False, default=[]), skip_custom_threats_filters=dict(type='list', elements='string', required=False, default=[]), skip_threats_filter_categories=dict(type='list', elements='string', required=False, default=[]), skipav=dict(type='bool', required=False, default=False), skipbadclients=dict(type='bool', required=False, default=False), skipcookie=dict(type='bool', required=False, default=False), skipform=dict(type='bool', required=False, default=False), skipform_missingtoken=dict(type='bool', required=False, default=False), skiphtmlrewrite=dict(type='bool', required=False, default=False), skiptft=dict(type='bool', required=False, default=False), skipurl=dict(type='bool', required=False, default=False), source=dict(type='list', elements='string', required=False, default=[]), status=dict(type='bool', required=False, default=True), ) ) try: UTM(module, endpoint, key_to_check_for_changes).execute() except Exception as e: module.fail_json(msg=to_native(e)) if __name__ == '__main__': main()
# -*- coding: utf-8 -*- ############################################################################## # # OpenERP, Open Source Management Solution # Copyright (C) 2004-2009 Tiny SPRL (<http://tiny.be>). # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## import logging from translate import _ _logger = logging.getLogger(__name__) #------------------------------------------------------------- #ENGLISH #------------------------------------------------------------- to_19 = ( 'Zero', 'One', 'Two', 'Three', 'Four', 'Five', 'Six', 'Seven', 'Eight', 'Nine', 'Ten', 'Eleven', 'Twelve', 'Thirteen', 'Fourteen', 'Fifteen', 'Sixteen', 'Seventeen', 'Eighteen', 'Nineteen' ) tens = ( 'Twenty', 'Thirty', 'Forty', 'Fifty', 'Sixty', 'Seventy', 'Eighty', 'Ninety') denom = ( '', 'Thousand', 'Million', 'Billion', 'Trillion', 'Quadrillion', 'Quintillion', 'Sextillion', 'Septillion', 'Octillion', 'Nonillion', 'Decillion', 'Undecillion', 'Duodecillion', 'Tredecillion', 'Quattuordecillion', 'Sexdecillion', 'Septendecillion', 'Octodecillion', 'Novemdecillion', 'Vigintillion' ) def _convert_nn(val): """convert a value < 100 to English. """ if val < 20: return to_19[val] for (dcap, dval) in ((k, 20 + (10 * v)) for (v, k) in enumerate(tens)): if dval + 10 > val: if val % 10: return dcap + '-' + to_19[val % 10] return dcap def _convert_nnn(val): """ convert a value < 1000 to english, special cased because it is the level that kicks off the < 100 special case. The rest are more general. This also allows you to get strings in the form of 'forty-five hundred' if called directly. """ word = '' (mod, rem) = (val % 100, val // 100) if rem > 0: word = to_19[rem] + ' Hundred' if mod > 0: word += ' ' if mod > 0: word += _convert_nn(mod) return word def english_number(val): if val < 100: return _convert_nn(val) if val < 1000: return _convert_nnn(val) for (didx, dval) in ((v - 1, 1000 ** v) for v in range(len(denom))): if dval > val: mod = 1000 ** didx l = val // mod r = val - (l * mod) ret = _convert_nnn(l) + ' ' + denom[didx] if r > 0: ret = ret + ', ' + english_number(r) return ret def amount_to_text(number, currency): number = '%.2f' % number units_name = currency list = str(number).split('.') start_word = english_number(int(list[0])) end_word = english_number(int(list[1])) cents_number = int(list[1]) cents_name = (cents_number > 1) and 'Cents' or 'Cent' return ' '.join(filter(None, [start_word, units_name, (start_word or units_name) and (end_word or cents_name) and 'and', end_word, cents_name])) #------------------------------------------------------------- # Generic functions #------------------------------------------------------------- _translate_funcs = {'en' : amount_to_text} #TODO: we should use the country AND language (ex: septante VS soixante dix) #TODO: we should use en by default, but the translation func is yet to be implemented def amount_to_text(nbr, lang='en', currency='euro'): """ Converts an integer to its textual representation, using the language set in the context if any. Example:: 1654: thousands six cent cinquante-quatre. """ import openerp.loglevels as loglevels # if nbr > 10000000: # _logger.warning(_("Number too large '%d', can not translate it")) # return str(nbr) if not _translate_funcs.has_key(lang): _logger.warning(_("no translation function found for lang: '%s'"), lang) #TODO: (default should be en) same as above lang = 'en' return _translate_funcs[lang](abs(nbr), currency) if __name__=='__main__': from sys import argv lang = 'nl' if len(argv) < 2: for i in range(1,200): print i, ">>", int_to_text(i, lang) for i in range(200,999999,139): print i, ">>", int_to_text(i, lang) else: print int_to_text(int(argv[1]), lang) # vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
# Copyright (c) 2013 - 2016 EMC Corporation. # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import json import mock from cinder import context from cinder.tests.unit.consistencygroup import fake_consistencygroup from cinder.tests.unit import fake_constants as fake from cinder.tests.unit import fake_snapshot from cinder.tests.unit import fake_volume from cinder.tests.unit.volume.drivers.emc import scaleio from cinder.tests.unit.volume.drivers.emc.scaleio import mocks class TestConsistencyGroups(scaleio.TestScaleIODriver): """Test cases for ``ScaleIODriver consistency groups support``""" def setUp(self): """Setup a test case environment. Creates a fake volume object and sets up the required API responses. """ super(TestConsistencyGroups, self).setUp() self.ctx = context.RequestContext('fake', 'fake', auth_token=True) self.consistency_group = ( fake_consistencygroup.fake_consistencyobject_obj( self.ctx, **{'id': fake.CONSISTENCY_GROUP_ID})) fake_volume1 = fake_volume.fake_volume_obj( self.ctx, **{'id': fake.VOLUME_ID, 'provider_id': fake.PROVIDER_ID}) fake_volume2 = fake_volume.fake_volume_obj( self.ctx, **{'id': fake.VOLUME2_ID, 'provider_id': fake.PROVIDER2_ID}) fake_volume3 = fake_volume.fake_volume_obj( self.ctx, **{'id': fake.VOLUME3_ID, 'provider_id': fake.PROVIDER3_ID}) fake_volume4 = fake_volume.fake_volume_obj( self.ctx, **{'id': fake.VOLUME4_ID, 'provider_id': fake.PROVIDER4_ID}) self.volumes = [fake_volume1, fake_volume2] self.volumes2 = [fake_volume3, fake_volume4] fake_snapshot1 = fake_snapshot.fake_snapshot_obj( self.ctx, **{'id': fake.SNAPSHOT_ID, 'volume_id': fake.VOLUME_ID, 'volume': fake_volume1}) fake_snapshot2 = fake_snapshot.fake_snapshot_obj( self.ctx, **{'id': fake.SNAPSHOT2_ID, 'volume_id': fake.VOLUME2_ID, 'volume': fake_volume2}) self.snapshots = [fake_snapshot1, fake_snapshot2] self.snapshot_reply = json.dumps({ 'volumeIdList': ['sid1', 'sid2'], 'snapshotGroupId': 'sgid1'}) self.HTTPS_MOCK_RESPONSES = { self.RESPONSE_MODE.Valid: { 'instances/Volume::{}/action/removeVolume'.format( fake_volume1['provider_id'] ): fake_volume1['provider_id'], 'instances/Volume::{}/action/removeVolume'.format( fake_volume2['provider_id'] ): fake_volume2['provider_id'], 'instances/Volume::{}/action/removeMappedSdc'.format( fake_volume1['provider_id'] ): fake_volume1['provider_id'], 'instances/Volume::{}/action/removeMappedSdc'.format( fake_volume2['provider_id'] ): fake_volume2['provider_id'], 'instances/System/action/snapshotVolumes': self.snapshot_reply, }, self.RESPONSE_MODE.BadStatus: { 'instances/Volume::{}/action/removeVolume'.format( fake_volume1['provider_id'] ): mocks.MockHTTPSResponse( { 'errorCode': 401, 'message': 'BadStatus Volume Test', }, 401 ), 'instances/Volume::{}/action/removeVolume'.format( fake_volume2['provider_id'] ): mocks.MockHTTPSResponse( { 'errorCode': 401, 'message': 'BadStatus Volume Test', }, 401 ), 'instances/System/action/snapshotVolumes': self.BAD_STATUS_RESPONSE }, } def _fake_cgsnapshot(self): cgsnap = {'id': 'cgsid', 'name': 'testsnap', 'consistencygroup_id': fake.CONSISTENCY_GROUP_ID, 'status': 'available'} return cgsnap def test_create_consistencygroup(self): result = self.driver.create_consistencygroup(self.ctx, self.consistency_group) self.assertEqual('available', result['status']) def test_delete_consistencygroup_valid(self): self.set_https_response_mode(self.RESPONSE_MODE.Valid) self.driver.configuration.set_override( 'sio_unmap_volume_before_deletion', override=True) result_model_update, result_volumes_update = ( self.driver.delete_consistencygroup(self.ctx, self.consistency_group, self.volumes)) self.assertTrue(all(volume['status'] == 'deleted' for volume in result_volumes_update)) self.assertEqual('deleted', result_model_update['status']) def test_delete_consistency_group_fail(self): self.set_https_response_mode(self.RESPONSE_MODE.BadStatus) result_model_update, result_volumes_update = ( self.driver.delete_consistencygroup(self.ctx, self.consistency_group, self.volumes)) self.assertTrue(any(volume['status'] == 'error_deleting' for volume in result_volumes_update)) self.assertIn(result_model_update['status'], ['error_deleting', 'error']) def test_create_consistencygroup_from_cg(self): self.set_https_response_mode(self.RESPONSE_MODE.Valid) result_model_update, result_volumes_model_update = ( self.driver.create_consistencygroup_from_src( self.ctx, self.consistency_group, self.volumes2, source_cg=self.consistency_group, source_vols=self.volumes)) self.assertEqual('available', result_model_update['status']) get_pid = lambda snapshot: snapshot['provider_id'] volume_provider_list = list(map(get_pid, result_volumes_model_update)) self.assertListEqual(volume_provider_list, ['sid1', 'sid2']) def test_create_consistencygroup_from_cgs(self): self.snapshots[0]['provider_id'] = fake.PROVIDER_ID self.snapshots[1]['provider_id'] = fake.PROVIDER2_ID self.set_https_response_mode(self.RESPONSE_MODE.Valid) result_model_update, result_volumes_model_update = ( self.driver.create_consistencygroup_from_src( self.ctx, self.consistency_group, self.volumes2, cgsnapshot=self._fake_cgsnapshot(), snapshots=self.snapshots)) self.assertEqual('available', result_model_update['status']) get_pid = lambda snapshot: snapshot['provider_id'] volume_provider_list = list(map(get_pid, result_volumes_model_update)) self.assertListEqual(['sid1', 'sid2'], volume_provider_list) @mock.patch('cinder.objects.snapshot') @mock.patch('cinder.objects.snapshot') def test_create_cgsnapshots(self, snapshot1, snapshot2): type(snapshot1).volume = mock.PropertyMock( return_value=self.volumes[0]) type(snapshot2).volume = mock.PropertyMock( return_value=self.volumes[1]) snapshots = [snapshot1, snapshot2] self.set_https_response_mode(self.RESPONSE_MODE.Valid) result_model_update, result_snapshot_model_update = ( self.driver.create_cgsnapshot( self.ctx, self._fake_cgsnapshot(), snapshots )) self.assertEqual('available', result_model_update['status']) self.assertTrue(all(snapshot['status'] == 'available' for snapshot in result_snapshot_model_update)) get_pid = lambda snapshot: snapshot['provider_id'] snapshot_provider_list = list(map(get_pid, result_snapshot_model_update)) self.assertListEqual(['sid1', 'sid2'], snapshot_provider_list) @mock.patch('cinder.objects.snapshot') @mock.patch('cinder.objects.snapshot') def test_delete_cgsnapshots(self, snapshot1, snapshot2): type(snapshot1).volume = mock.PropertyMock( return_value=self.volumes[0]) type(snapshot2).volume = mock.PropertyMock( return_value=self.volumes[1]) type(snapshot1).provider_id = mock.PropertyMock( return_value=fake.PROVIDER_ID) type(snapshot2).provider_id = mock.PropertyMock( return_value=fake.PROVIDER2_ID) snapshots = [snapshot1, snapshot2] self.set_https_response_mode(self.RESPONSE_MODE.Valid) result_model_update, result_snapshot_model_update = ( self.driver.delete_cgsnapshot( self.ctx, self._fake_cgsnapshot(), snapshots )) self.assertEqual('deleted', result_model_update['status']) self.assertTrue(all(snapshot['status'] == 'deleted' for snapshot in result_snapshot_model_update))
#!/usr/bin/env python # -*- coding: utf-8 -*- """ .. module:: tcpiface.py :platform: Unix, Windows :synopsis: Ulyxes - an open source project to drive total stations and publish observation results. GPL v2.0 license Copyright (C) 2010- Zoltan Siki <siki.zoltan@epito.bme.hu>. .. moduleauthor:: Bence Turak <bence.turak@gmail.com> """ import sys import socket import logging import re from iface import Iface class TCPIface(Iface): """Interface to communicate on TCP/IP protocol. This class requires socket. :param name: name of tcp interface (str) :param address: address of server (tuple) (ip(str), port(int)) :param bufSize: size of buffer in case of file (int) :param timeout: communication timeout seconds (int), default 15 """ def __init__(self, name, address, bufSize = 1024, timeout=15): """ Constructor for TCP socket interface """ super(TCPIface, self).__init__(name) self.sock = None self.bufSize = None # open socket self.Open(address, bufSize, timeout) def __del__(self): """ Destructor for TCP socket interface """ self.Close() def Open(self, address, bufSize = 1024, timeout=15): """ Open TCP socket """ try: self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) self.sock.connect(address) self.sock.settimeout(timeout) self.bufSize = bufSize self.opened = True self.state = self.IF_OK except Exception: self.opened = False self.state = self.IF_ERROR logging.error(" cannot open TCP socket") def Close(self): """ Close TCP socket """ try: self.sock.close() self.opened = False self.state = self.IF_OK except Exception: self.state = self.IF_ERROR logging.error(" cannot close TCP socet") def GetLine(self, fileSize = None): """ read from TCP interface until end of line :param fileSize: the size of the expected file (int) :returns: line read from TCP (str) or empty string on timeout or error, state is set also """ if self.sock is None or not self.opened or self.state != self.IF_OK: logging.error(" TCP socket not opened") return None # read answer till end of line ans = b'' a = b'' try: if fileSize != None: a = self.sock.recv(self.bufSize) ans += a while sys.getsizeof(ans) < fileSize + 17: l = sys.getsizeof(ans) a = self.sock.recv(self.bufSize) ans += a else: a = self.sock.recv(1) ans += a while a != b'\n': a = self.sock.recv(1) ans += a except Exception as e: #self.state = self.IF_READ logging.error(" cannot read TCP socket") if ans == b'': # timeout exit loop #self.state = self.IF_TIMEOUT logging.error(" timeout on TCP socket") # remove end of line logging.debug(" message got: %s", ans) ans = ans.strip(b'\n') return ans def PutLine(self, msg): """ send message through the TCP socket :param msg: message to send (str) :returns: 0 - on OK, -1 on error or interface is in error state """ # do nothing if interface is in error state if self.sock is None or not self.opened or self.state != self.IF_OK: logging.error(" TCP socket not opened or in error state") return -1 # add CR/LF to message end if (msg[-1:] != '\n'): msg += '\n' # remove special characters msg = msg.encode('ascii', 'ignore') # send message to socket logging.debug(" message sent: %s", msg) try: self.sock.send(msg) except Exception: self.state = self.IF_WRITE logging.error(" cannot write tcp") return -1 return 0 def Send(self, msg): """ send message to TCP socket and read answer :param msg: message to send, it can be multipart message separated by '|' (str) :returns: answer from server (str) """ msglist = re.split("\|", msg) res = b'' # sending for m in msglist: if self.PutLine(m) == 0: res += self.GetLine() + b"|" if res.endswith(b"|"): res = res[:-1] res = res.decode('ascii') return res if __name__ == "__main__": a = TCPIface('test', ('127.0.0.1', 80), 1024, 15) print (a.GetName()) print (a.GetState()) print (a.Send('GET /index.html HTTP/1.1'))
import itertools import os import json from collections import OrderedDict, defaultdict import sqlparse import dbt.project import dbt.utils import dbt.include import dbt.tracking from dbt.utils import get_materialization, NodeType, is_type from dbt.linker import Linker import dbt.compat import dbt.context.runtime import dbt.contracts.project import dbt.exceptions import dbt.flags import dbt.loader from dbt.contracts.graph.compiled import CompiledNode, CompiledGraph from dbt.clients.system import write_json from dbt.logger import GLOBAL_LOGGER as logger graph_file_name = 'graph.gpickle' manifest_file_name = 'manifest.json' def print_compile_stats(stats): names = { NodeType.Model: 'models', NodeType.Test: 'tests', NodeType.Archive: 'archives', NodeType.Analysis: 'analyses', NodeType.Macro: 'macros', NodeType.Operation: 'operations', NodeType.Seed: 'seed files', } results = {k: 0 for k in names.keys()} results.update(stats) stat_line = ", ".join( ["{} {}".format(ct, names.get(t)) for t, ct in results.items()]) logger.info("Found {}".format(stat_line)) def _add_prepended_cte(prepended_ctes, new_cte): for dct in prepended_ctes: if dct['id'] == new_cte['id']: dct['sql'] = new_cte['sql'] return prepended_ctes.append(new_cte) def _extend_prepended_ctes(prepended_ctes, new_prepended_ctes): for new_cte in new_prepended_ctes: _add_prepended_cte(prepended_ctes, new_cte) def prepend_ctes(model, manifest): model, _, manifest = recursively_prepend_ctes(model, manifest) return (model, manifest) def recursively_prepend_ctes(model, manifest): if model.extra_ctes_injected: return (model, model.extra_ctes, manifest) if dbt.flags.STRICT_MODE: # ensure that all the nodes in this manifest are compiled CompiledGraph(**manifest.to_flat_graph()) prepended_ctes = [] for cte in model.extra_ctes: cte_id = cte['id'] cte_to_add = manifest.nodes.get(cte_id) cte_to_add, new_prepended_ctes, manifest = recursively_prepend_ctes( cte_to_add, manifest) _extend_prepended_ctes(prepended_ctes, new_prepended_ctes) new_cte_name = '__dbt__CTE__{}'.format(cte_to_add.get('name')) sql = ' {} as (\n{}\n)'.format(new_cte_name, cte_to_add.compiled_sql) _add_prepended_cte(prepended_ctes, {'id': cte_id, 'sql': sql}) model.prepend_ctes(prepended_ctes) manifest.nodes[model.unique_id] = model return (model, prepended_ctes, manifest) class Compiler(object): def __init__(self, project): self.project = project def initialize(self): dbt.clients.system.make_directory(self.project['target-path']) dbt.clients.system.make_directory(self.project['modules-path']) def compile_node(self, node, manifest): logger.debug("Compiling {}".format(node.get('unique_id'))) data = node.to_dict() data.update({ 'compiled': False, 'compiled_sql': None, 'extra_ctes_injected': False, 'extra_ctes': [], 'injected_sql': None, }) compiled_node = CompiledNode(**data) context = dbt.context.runtime.generate( compiled_node, self.project, manifest) compiled_node.compiled_sql = dbt.clients.jinja.get_rendered( node.get('raw_sql'), context, node) compiled_node.compiled = True injected_node, _ = prepend_ctes(compiled_node, manifest) should_wrap = {NodeType.Test, NodeType.Analysis, NodeType.Operation} if injected_node.resource_type in should_wrap: # data tests get wrapped in count(*) # TODO : move this somewhere more reasonable if 'data' in injected_node.tags and \ is_type(injected_node, NodeType.Test): injected_node.wrapped_sql = ( "select count(*) from (\n{test_sql}\n) sbq").format( test_sql=injected_node.injected_sql) else: # don't wrap schema tests or analyses. injected_node.wrapped_sql = injected_node.injected_sql elif is_type(injected_node, NodeType.Archive): # unfortunately we do everything automagically for # archives. in the future it'd be nice to generate # the SQL at the parser level. pass elif(is_type(injected_node, NodeType.Model) and get_materialization(injected_node) == 'ephemeral'): pass else: injected_node.wrapped_sql = None return injected_node def write_manifest_file(self, manifest): """Write the manifest file to disk. manifest should be a Manifest. """ filename = manifest_file_name manifest_path = os.path.join(self.project['target-path'], filename) write_json(manifest_path, manifest.serialize()) def write_graph_file(self, linker): filename = graph_file_name graph_path = os.path.join(self.project['target-path'], filename) linker.write_graph(graph_path) def link_node(self, linker, node, manifest): linker.add_node(node.unique_id) linker.update_node_data( node.unique_id, node.to_dict()) for dependency in node.depends_on_nodes: if manifest.nodes.get(dependency): linker.dependency( node.unique_id, (manifest.nodes.get(dependency).unique_id)) else: dbt.exceptions.dependency_not_found(node, dependency) def link_graph(self, linker, manifest): for node in manifest.nodes.values(): self.link_node(linker, node, manifest) cycle = linker.find_cycles() if cycle: raise RuntimeError("Found a cycle: {}".format(cycle)) def get_all_projects(self): root_project = self.project.cfg all_projects = {root_project.get('name'): root_project} dependency_projects = dbt.utils.dependency_projects(self.project) for project in dependency_projects: name = project.cfg.get('name', 'unknown') all_projects[name] = project.cfg if dbt.flags.STRICT_MODE: dbt.contracts.project.ProjectList(**all_projects) return all_projects def _check_resource_uniqueness(cls, manifest): names_resources = {} alias_resources = {} for resource, node in manifest.nodes.items(): if node.resource_type not in NodeType.refable(): continue name = node.name alias = "{}.{}".format(node.schema, node.alias) existing_node = names_resources.get(name) if existing_node is not None: dbt.exceptions.raise_duplicate_resource_name( existing_node, node) existing_alias = alias_resources.get(alias) if existing_alias is not None: dbt.exceptions.raise_ambiguous_alias( existing_alias, node) names_resources[name] = node alias_resources[alias] = node def compile(self): linker = Linker() all_projects = self.get_all_projects() manifest = dbt.loader.GraphLoader.load_all(self.project, all_projects) self.write_manifest_file(manifest) self._check_resource_uniqueness(manifest) self.link_graph(linker, manifest) stats = defaultdict(int) for node_name, node in itertools.chain( manifest.nodes.items(), manifest.macros.items()): stats[node.resource_type] += 1 self.write_graph_file(linker) print_compile_stats(stats) return manifest, linker
#!/usr/bin/env python # -*- coding: utf-8 -*- from runner.koan import * import random class DiceSet: def __init__(self): self._values = None @property def values(self): return self._values def roll(self, n): # Needs implementing! # Tip: random.randint(min, max) can be used to generate random numbers pass class AboutDiceProject(Koan): def test_can_create_a_dice_set(self): dice = DiceSet() self.assertTrue(dice) def test_rolling_the_dice_returns_a_set_of_integers_between_1_and_6(self): dice = DiceSet() dice.roll(5) self.assertTrue(isinstance(dice.values, list), "should be a list") self.assertEqual(5, len(dice.values)) for value in dice.values: self.assertTrue(value >= 1 and value <= 6, "value " + str(value) + " must be between 1 and 6") def test_dice_values_do_not_change_unless_explicitly_rolled(self): dice = DiceSet() dice.roll(5) first_time = dice.values second_time = dice.values self.assertEqual(first_time, second_time) def test_dice_values_should_change_between_rolls(self): dice = DiceSet() dice.roll(5) first_time = dice.values dice.roll(5) second_time = dice.values self.assertNotEqual(first_time, second_time, \ "Two rolls should not be equal") # THINK ABOUT IT: # # If the rolls are random, then it is possible (although not # likely) that two consecutive rolls are equal. What would be a # better way to test this? def test_you_can_roll_different_numbers_of_dice(self): dice = DiceSet() dice.roll(3) self.assertEqual(3, len(dice.values)) dice.roll(1) self.assertEqual(1, len(dice.values))
#!/usr/bin/env python """ @file statistics.py @author Michael Behrisch @author Daniel Krajzewicz @date 2008-10-17 @version $Id: statistics.py 12898 2012-10-26 08:58:14Z behrisch $ Collecting statistics for the CityMobil parking lot SUMO, Simulation of Urban MObility; see http://sumo.sourceforge.net/ Copyright (C) 2008-2012 DLR (http://www.dlr.de/) and contributors All rights reserved """ persons = {} personsRunning = 0 class Person: def __init__(self, id, source, target, step): self.id = id self.source = source self.target = target self.waitStart = step self.depart = None self.arrive = None def personArrived(personID, edge, target, step): global personsRunning persons[personID] = Person(personID, edge, target, step) personsRunning += 1 def personLoaded(personID, step): persons[personID].depart = step def personUnloaded(personID, step): global personsRunning persons[personID].arrive = step personsRunning -= 1 def evaluate(forTest=False): try: import numpy, math except ImportError: print "No numpy available, skipping statistics" return waitTimes = [] routeTimes = {} for person in persons.itervalues(): waitTimes.append(person.depart - person.waitStart) route = (person.source, person.target) if not route in routeTimes: routeTimes[route] = [] routeTimes[route].append(person.arrive - person.depart) waitArray = numpy.array(waitTimes) if forTest: print "waiting time (max, mean, dev):", waitArray.max() < 1000, waitArray.mean() < 1000, math.sqrt(waitArray.var()) < 100 else: print "waiting time (max, mean, dev):", waitArray.max(), waitArray.mean(), math.sqrt(waitArray.var()) for route, times in sorted(routeTimes.iteritems()): timeArray = numpy.array(times) if forTest: print route, timeArray.max() < 1000, timeArray.mean() < 1000, math.sqrt(timeArray.var()) < 100 else: print route, timeArray.max(), timeArray.mean(), math.sqrt(timeArray.var()) co2 = 0. for line in open("aggregated.xml"): if "cyber" in line: pos = line.find('CO2_abs="') + 9 if pos >= 9: endpos = line.find('"', pos) co2 += float(line[pos:endpos]) if forTest: print "CO2:", co2 < 10000000 else: print "CO2:", co2 if __name__ == "__main__": from pylab import * stats = open(sys.argv[1]) demand = [] simpleWaitMean = [] agentWaitMean = [] simpleWaitDev = [] agentWaitDev = [] simpleRouteMean = [] agentRouteMean = [] simpleRouteDev = [] agentRouteDev = [] for line in stats: if "simple" in line: mean = simpleWaitMean dev = simpleWaitDev rmean = simpleRouteMean rdev = simpleRouteDev demand.append(int(line.split()[-1])) if "agent" in line: mean = agentWaitMean dev = agentWaitDev rmean = agentRouteMean rdev = agentRouteDev if "waiting" in line: mean.append(float(line.split()[-2])) dev.append(float(line.split()[-1])) if line.startswith("('footmain0to1'"): rmean.append(float(line.split()[-2])) rdev.append(float(line.split()[-1])) stats.close() figure() errorbar(demand, simpleWaitMean, simpleWaitDev, lw=2, ms=10, fmt='o', label='standard bus scenario') errorbar(demand, agentWaitMean, agentWaitDev, lw=2, ms=10, color="red", fmt='o', label='agent controlled cyber cars') xlim(0, 50) ylim(0, 3300) xlabel('Repeater interval (s)') ylabel('Waiting time (s)') title('Mean and standard deviation of waiting time') legend(numpoints=1) savefig("waitingtime.png") figure() errorbar(demand, simpleRouteMean, simpleRouteDev, lw=2, ms=10, fmt='o', label='standard bus scenario') errorbar(demand, agentRouteMean, agentRouteDev, lw=2, ms=10, color="red", fmt='o', label='agent controlled cyber cars') xlim(0, 50) ylim(0, 300) xlabel('Repeater interval (s)') ylabel('Travel time (s)') title('Mean and standard deviation of travel time on the longest route') legend(numpoints=1) savefig("traveltime.png") show()
import os import re from django.conf import settings from django.core.management.base import CommandError from django.db import models from django.db.models import get_models def sql_create(app, style, connection): "Returns a list of the CREATE TABLE SQL statements for the given app." if connection.settings_dict['ENGINE'] == 'django.db.backends.dummy': # This must be the "dummy" database backend, which means the user # hasn't set ENGINE for the databse. raise CommandError("Django doesn't know which syntax to use for your SQL statements,\n" + "because you haven't specified the ENGINE setting for the database.\n" + "Edit your settings file and change DATBASES['default']['ENGINE'] to something like\n" + "'django.db.backends.postgresql' or 'django.db.backends.mysql'.") # Get installed models, so we generate REFERENCES right. # We trim models from the current app so that the sqlreset command does not # generate invalid SQL (leaving models out of known_models is harmless, so # we can be conservative). app_models = models.get_models(app, include_auto_created=True) final_output = [] tables = connection.introspection.table_names() known_models = set([model for model in connection.introspection.installed_models(tables) if model not in app_models]) pending_references = {} for model in app_models: output, references = connection.creation.sql_create_model(model, style, known_models) final_output.extend(output) for refto, refs in references.items(): pending_references.setdefault(refto, []).extend(refs) if refto in known_models: final_output.extend(connection.creation.sql_for_pending_references(refto, style, pending_references)) final_output.extend(connection.creation.sql_for_pending_references(model, style, pending_references)) # Keep track of the fact that we've created the table for this model. known_models.add(model) # Handle references to tables that are from other apps # but don't exist physically. not_installed_models = set(pending_references.keys()) if not_installed_models: alter_sql = [] for model in not_installed_models: alter_sql.extend(['-- ' + sql for sql in connection.creation.sql_for_pending_references(model, style, pending_references)]) if alter_sql: final_output.append('-- The following references should be added but depend on non-existent tables:') final_output.extend(alter_sql) return final_output def sql_delete(app, style, connection): "Returns a list of the DROP TABLE SQL statements for the given app." # This should work even if a connection isn't available try: cursor = connection.cursor() except: cursor = None # Figure out which tables already exist if cursor: table_names = connection.introspection.get_table_list(cursor) else: table_names = [] output = [] # Output DROP TABLE statements for standard application tables. to_delete = set() references_to_delete = {} app_models = models.get_models(app, include_auto_created=True) for model in app_models: if cursor and connection.introspection.table_name_converter(model._meta.db_table) in table_names: # The table exists, so it needs to be dropped opts = model._meta for f in opts.local_fields: if f.rel and f.rel.to not in to_delete: references_to_delete.setdefault(f.rel.to, []).append( (model, f) ) to_delete.add(model) for model in app_models: if connection.introspection.table_name_converter(model._meta.db_table) in table_names: output.extend(connection.creation.sql_destroy_model(model, references_to_delete, style)) # Close database connection explicitly, in case this output is being piped # directly into a database client, to avoid locking issues. if cursor: cursor.close() connection.close() return output[::-1] # Reverse it, to deal with table dependencies. def sql_reset(app, style, connection): "Returns a list of the DROP TABLE SQL, then the CREATE TABLE SQL, for the given module." # This command breaks a lot and should be deprecated import warnings warnings.warn( 'This command has been deprecated. The command ``sqlflush`` can be used to delete everything. You can also use ALTER TABLE or DROP TABLE statements manually.', PendingDeprecationWarning ) return sql_delete(app, style, connection) + sql_all(app, style, connection) def sql_flush(style, connection, only_django=False): """ Returns a list of the SQL statements used to flush the database. If only_django is True, then only table names that have associated Django models and are in INSTALLED_APPS will be included. """ if only_django: tables = connection.introspection.django_table_names(only_existing=True) else: tables = connection.introspection.table_names() statements = connection.ops.sql_flush( style, tables, connection.introspection.sequence_list() ) return statements def sql_custom(app, style, connection): "Returns a list of the custom table modifying SQL statements for the given app." output = [] app_models = get_models(app) app_dir = os.path.normpath(os.path.join(os.path.dirname(app.__file__), 'sql')) for model in app_models: output.extend(custom_sql_for_model(model, style, connection)) return output def sql_indexes(app, style, connection): "Returns a list of the CREATE INDEX SQL statements for all models in the given app." output = [] for model in models.get_models(app): output.extend(connection.creation.sql_indexes_for_model(model, style)) return output def sql_all(app, style, connection): "Returns a list of CREATE TABLE SQL, initial-data inserts, and CREATE INDEX SQL for the given module." return sql_create(app, style, connection) + sql_custom(app, style, connection) + sql_indexes(app, style, connection) def custom_sql_for_model(model, style, connection): opts = model._meta app_dir = os.path.normpath(os.path.join(os.path.dirname(models.get_app(model._meta.app_label).__file__), 'sql')) output = [] # Post-creation SQL should come before any initial SQL data is loaded. # However, this should not be done for models that are unmanaged or # for fields that are part of a parent model (via model inheritance). if opts.managed: post_sql_fields = [f for f in opts.local_fields if hasattr(f, 'post_create_sql')] for f in post_sql_fields: output.extend(f.post_create_sql(style, model._meta.db_table)) # Some backends can't execute more than one SQL statement at a time, # so split into separate statements. statements = re.compile(r";[ \t]*$", re.M) # Find custom SQL, if it's available. backend_name = connection.settings_dict['ENGINE'].split('.')[-1] sql_files = [os.path.join(app_dir, "%s.%s.sql" % (opts.object_name.lower(), backend_name)), os.path.join(app_dir, "%s.sql" % opts.object_name.lower())] for sql_file in sql_files: if os.path.exists(sql_file): fp = open(sql_file, 'U') for statement in statements.split(fp.read().decode(settings.FILE_CHARSET)): # Remove any comments from the file statement = re.sub(ur"--.*([\n\Z]|$)", "", statement) if statement.strip(): output.append(statement + u";") fp.close() return output def emit_post_sync_signal(created_models, verbosity, interactive, db): # Emit the post_sync signal for every application. for app in models.get_apps(): app_name = app.__name__.split('.')[-2] if verbosity >= 2: print "Running post-sync handlers for application", app_name models.signals.post_syncdb.send(sender=app, app=app, created_models=created_models, verbosity=verbosity, interactive=interactive, db=db)
# (C) 2017 Red Hat Inc. # Copyright (C) 2017 Lenovo. # # GNU General Public License v3.0+ # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt) # # Contains terminal Plugin methods for ENOS Config Module # Lenovo Networking # from __future__ import (absolute_import, division, print_function) __metaclass__ = type import json import re from ansible.errors import AnsibleConnectionFailure from ansible.module_utils._text import to_text, to_bytes from ansible.plugins.terminal import TerminalBase class TerminalModule(TerminalBase): terminal_stdout_re = [ re.compile(br"[\r\n]?[\w+\-\.:\/\[\]]+(?:\([^\)]+\)){,3}(?:>|#) ?$"), re.compile(br"\[\w+\@[\w\-\.]+(?: [^\]])\] ?[>#\$] ?$"), re.compile(br">[\r\n]?") ] terminal_stderr_re = [ re.compile(br"% ?Error"), re.compile(br"% ?Bad secret"), re.compile(br"invalid input", re.I), re.compile(br"(?:incomplete|ambiguous) command", re.I), re.compile(br"connection timed out", re.I), re.compile(br"[^\r\n]+ not found"), re.compile(br"'[^']' +returned error code: ?\d+"), ] def on_open_shell(self): try: for cmd in (b'\n', b'terminal-length 0\n'): self._exec_cli_command(cmd) except AnsibleConnectionFailure: raise AnsibleConnectionFailure('unable to set terminal parameters') def on_become(self, passwd=None): if self._get_prompt().endswith(b'#'): return cmd = {u'command': u'enable'} if passwd: # Note: python-3.5 cannot combine u"" and r"" together. Thus make # an r string and use to_text to ensure it's text # on both py2 and py3. cmd[u'prompt'] = to_text(r"[\r\n]?password: $", errors='surrogate_or_strict') cmd[u'answer'] = passwd try: self._exec_cli_command(to_bytes(json.dumps(cmd), errors='surrogate_or_strict')) except AnsibleConnectionFailure: msg = 'unable to elevate privilege to enable mode' raise AnsibleConnectionFailure(msg) def on_unbecome(self): prompt = self._get_prompt() if prompt is None: # if prompt is None most likely the terminal is hung up at a prompt return if b'(config' in prompt: self._exec_cli_command(b'end') self._exec_cli_command(b'disable') elif prompt.endswith(b'#'): self._exec_cli_command(b'disable')
from __future__ import with_statement from contextlib import contextmanager from django.conf import settings from django.contrib.auth.models import AnonymousUser, User from django.http import HttpRequest from django_lean.experiments.models import (AnonymousVisitor, Experiment, GoalRecord, GoalType, Participant) from django_lean.experiments.tests.utils import get_session, patch, TestCase from django_lean.experiments.utils import StaticUser, WebUser from django_lean.lean_analytics import (get_all_analytics, get_all_analytics_names, reset_caches, IdentificationError) from django_lean.lean_analytics.base import BaseAnalytics import mox class TestAnalytics(TestCase): def test_get_all_analytics_names(self): with patch(settings, 'LEAN_ANALYTICS', NotImplemented): reset_caches() self.assertEqual(get_all_analytics_names(), ()) with patch(settings, 'LEAN_ANALYTICS', []): reset_caches() self.assertEqual(get_all_analytics_names(), []) base_name = '%s.%s' % (BaseAnalytics.__module__, BaseAnalytics.__name__) with patch(settings, 'LEAN_ANALYTICS', [base_name]): reset_caches() self.assertEqual(get_all_analytics_names(), [base_name]) def test_get_all_analytics(self): with patch(settings, 'LEAN_ANALYTICS', NotImplemented): reset_caches() self.assertEqual(get_all_analytics(), []) with patch(settings, 'LEAN_ANALYTICS', []): reset_caches() self.assertEqual(get_all_analytics(), []) base_name = '%s.%s' % (BaseAnalytics.__module__, BaseAnalytics.__name__) with patch(settings, 'LEAN_ANALYTICS', [base_name]): reset_caches() self.assertEqual([a.__class__.__name__ for a in get_all_analytics()], [BaseAnalytics.__name__]) ############# # KISSMETRICS ############# try: import django_kissmetrics except ImportError: if 'django_lean.lean_analytics.kissmetrics.KissMetrics' in \ get_all_analytics_names(): traceback.print_exc() else: from django_lean.lean_analytics.kissmetrics import KissMetrics class TestKissMetrics(TestCase): def setUp(self): self.mox = mox.Mox() self.analytics = KissMetrics() def test_id_from_user(self): user = User.objects.create_user('user', 'user@example.com', 'user') self.assertEqual(self.analytics._id_from_user(user), 'User %d' % user.pk) self.assertRaises(IdentificationError, self.analytics._id_from_user, None) def test_id_from_session(self): # With real session with self.web_user(AnonymousUser()) as experiment_user: self.mox.ReplayAll() session = experiment_user.session self.assertEqual( self.analytics._id_from_session(experiment_user.session), 'Session %s' % session.session_key ) self.mox.VerifyAll() # With dict as session experiment_user = StaticUser() self.assertRaises(IdentificationError, self.analytics._id_from_session, experiment_user.session) def test_compute_id(self): # With anonymous WebUser with self.web_user(AnonymousUser()) as experiment_user: session = experiment_user.session self.mox.ReplayAll() self.assertEqual(self.analytics._compute_id(experiment_user), 'Session %s' % session.session_key) self.mox.VerifyAll() # With authenticated WebUser user = User.objects.create_user('user', 'user@example.com', 'user') with self.web_user(user) as experiment_user: self.mox.ReplayAll() self.assertEqual(self.analytics._compute_id(experiment_user), 'User %d' % user.id) self.mox.VerifyAll() # With StaticUser experiment_user = StaticUser() self.assertRaises(IdentificationError, self.analytics._compute_id, experiment_user) def test_identify(self): # With anonymous WebUser with self.web_user(AnonymousUser()) as experiment_user: self.mox.ReplayAll() self.assertTrue(self.analytics._identify(experiment_user)) self.mox.VerifyAll() # With authenticated WebUser user = User.objects.create_user('user', 'user@example.com', 'user') with self.web_user(user) as experiment_user: self.mox.ReplayAll() self.assertTrue(self.analytics._identify(experiment_user)) self.mox.VerifyAll() # With StaticUser experiment_user = StaticUser() self.assertFalse(self.analytics._identify(experiment_user)) def test_enroll(self): experiment = Experiment.objects.create(name='Experiment') user = User.objects.create_user('user', 'user@example.com', 'user') KM = self.mox.CreateMockAnything() analytics = KissMetrics(KM=KM) with self.web_user(user) as experiment_user: KM.identify(analytics._compute_id(experiment_user)) KM.record(action='Enrolled In Experiment', props={'Experiment': experiment.name, 'Group': 'Test'}) self.mox.ReplayAll() analytics.enroll(experiment=experiment, experiment_user=experiment_user, group_id=Participant.TEST_GROUP) self.mox.VerifyAll() def test_record(self): KM = self.mox.CreateMockAnything() analytics = KissMetrics(KM=KM) with self.web_user(AnonymousUser()) as experiment_user: KM.identify(analytics._id_from_session(experiment_user.session)) KM.record(action='Goal Recorded', props={'Goal Type': 'Goal Type'}) self.mox.ReplayAll() goal_type = GoalType.objects.create(name='Goal Type') goal_record = GoalRecord.record(goal_name=goal_type.name, experiment_user=experiment_user) analytics.record(goal_record=goal_record, experiment_user=experiment_user) self.mox.VerifyAll() def test_event(self): KM = self.mox.CreateMockAnything() analytics = KissMetrics(KM=KM) with self.web_user(AnonymousUser()) as experiment_user: KM.identify(analytics._id_from_session(experiment_user.session)) KM.record(action='Event', props={'Foo': 'Bar'}) self.mox.ReplayAll() analytics.event(name='Event', properties={'Foo': 'Bar'}, request=experiment_user.request) self.mox.VerifyAll() @contextmanager def web_user(self, user): session = get_session(None) request = self.mox.CreateMock(HttpRequest) request.user = user request.session = session experiment_user = WebUser(request) experiment_user.get_or_create_anonymous_visitor() yield experiment_user ########## # MIXPANEL ########## try: import mixpanel except ImportError: if 'django_lean.lean_analytics.mixpanel.Mixpanel' in \ get_all_analytics_names(): traceback.print_exc() else: from django_lean.lean_analytics.mixpanel import Mixpanel class TestMixpanel(TestCase): def setUp(self): self.mox = mox.Mox() self.analytics = Mixpanel() def tearDown(self): self.mox.UnsetStubs() def test_id_from_user(self): user = User.objects.create_user('user', 'user@example.com', 'user') self.assertEqual(self.analytics._id_from_user(user), 'User %d' % user.pk) self.assertRaises(IdentificationError, self.analytics._id_from_user, None) def test_id_from_session(self): # With real session with self.web_user(AnonymousUser()) as experiment_user: self.mox.ReplayAll() session = experiment_user.session self.assertEqual( self.analytics._id_from_session(experiment_user.session), 'Session %s' % session.session_key ) self.mox.VerifyAll() # With dict as session experiment_user = StaticUser() self.assertRaises(IdentificationError, self.analytics._id_from_session, experiment_user.session) def test_compute_id(self): # With anonymous WebUser with self.web_user(AnonymousUser()) as experiment_user: session = experiment_user.session self.mox.ReplayAll() self.assertEqual(self.analytics._compute_id(experiment_user), 'Session %s' % session.session_key) self.mox.VerifyAll() # With authenticated WebUser user = User.objects.create_user('user', 'user@example.com', 'user') with self.web_user(user) as experiment_user: self.mox.ReplayAll() self.assertEqual(self.analytics._compute_id(experiment_user), 'User %d' % user.id) self.mox.VerifyAll() # With StaticUser experiment_user = StaticUser() self.assertRaises(IdentificationError, self.analytics._compute_id, experiment_user) def test_identify(self): # With anonymous WebUser with self.web_user(AnonymousUser()) as experiment_user: self.mox.ReplayAll() self.assertTrue(self.analytics._identify(experiment_user)) self.assertEqual( self.analytics.identity, 'Session %s' % experiment_user.session.session_key ) self.mox.VerifyAll() # With authenticated WebUser user = User.objects.create_user('user', 'user@example.com', 'user') with self.web_user(user) as experiment_user: self.mox.ReplayAll() self.assertTrue(self.analytics._identify(experiment_user)) self.assertEqual(self.analytics.identity, 'User %s' % experiment_user.user.pk) self.mox.VerifyAll() # With StaticUser experiment_user = StaticUser() self.assertFalse(self.analytics._identify(experiment_user)) self.assertEqual(self.analytics.identity, None) def test_enroll(self): import time experiment = Experiment.objects.create(name='Experiment') user = User.objects.create_user('user', 'user@example.com', 'user') tracker = self.mox.CreateMockAnything() analytics = Mixpanel(tracker=tracker) now = time.gmtime() self.mox.StubOutWithMock(time, 'gmtime') time.gmtime().AndReturn(now) with self.web_user(user) as experiment_user: properties = {'time': '%d' % time.mktime(now), 'distinct_id': 'User %d' % user.pk, 'Experiment': experiment.name, 'Group': 'Test'} tracker.run(event_name='Enrolled In Experiment', properties=properties) self.mox.ReplayAll() analytics.enroll(experiment=experiment, experiment_user=experiment_user, group_id=Participant.TEST_GROUP) self.mox.VerifyAll() def test_record(self): import time tracker = self.mox.CreateMockAnything() analytics = Mixpanel(tracker=tracker) now = time.gmtime() self.mox.StubOutWithMock(time, 'gmtime') time.gmtime().AndReturn(now) with self.web_user(AnonymousUser()) as experiment_user: properties = { 'time': '%d' % time.mktime(now), 'distinct_id': ('Session %s' % experiment_user.session.session_key), 'Goal Type': 'Goal Type' } tracker.run(event_name='Goal Recorded', properties=properties) self.mox.ReplayAll() goal_type = GoalType.objects.create(name='Goal Type') goal_record = GoalRecord.record(goal_name=goal_type.name, experiment_user=experiment_user) analytics.record(goal_record=goal_record, experiment_user=experiment_user) self.mox.VerifyAll() def test_event(self): import time tracker = self.mox.CreateMockAnything() analytics = Mixpanel(tracker=tracker) now = time.gmtime() self.mox.StubOutWithMock(time, 'gmtime') time.gmtime().AndReturn(now) with self.web_user(AnonymousUser()) as experiment_user: properties = { 'time': '%d' % time.mktime(now), 'distinct_id': ('Session %s' % experiment_user.session.session_key), 'Foo': 'Bar' } tracker.run(event_name='Event', properties=properties) self.mox.ReplayAll() analytics.event(name='Event', properties={'Foo': 'Bar'}, request=experiment_user.request) self.mox.VerifyAll() @contextmanager def web_user(self, user): session = get_session(None) request = self.mox.CreateMock(HttpRequest) request.user = user request.session = session experiment_user = WebUser(request) experiment_user.get_or_create_anonymous_visitor() yield experiment_user
from os import listdir, path from lxml import etree, objectify from pickle import load from sys import argv from StringIO import StringIO from collections import OrderedDict import time from utilities.norm_arxiv import norm_arxiv from utilities.norm_attribute import norm_attribute from utilities.norm_mrow import norm_mrow from utilities.norm_outer_fence import norm_outer_fence from utilities.norm_splitter import norm_splitter from utilities.norm_tag import norm_tag from utilities.utils import Link_Types, Matching_Methods, utils from utilities.depgraph_heur import depgraph_heur __dtd = '<!DOCTYPE math SYSTEM "resources/xhtml-math11-f.dtd">' __xmlns = ' xmlns="http://www.w3.org/1998/Math/MathML"' __relation_fl = 'resources/math_symbols_unicode.dump' __xml_parser = etree.XMLParser(remove_blank_text = True, load_dtd = True, resolve_entities = True) def __get_clean_mathml(mt_string): mt_tree = etree.parse(StringIO(__dtd + mt_string), __xml_parser).getroot() objectify.deannotate(mt_tree, cleanup_namespaces=True) return mt_tree def __extract_math_line_arxiv(line): cells = line.strip().split('\t') latexml_id = cells[0] para_id = cells[1] kmcs_id = cells[2] gmid = '#'.join([para_id, kmcs_id, latexml_id]) mt_string = '\t'.join(cells[3:]).replace(__xmlns, "") mt = __get_clean_mathml(mt_string) return gmid, mt def __extract_math_line_acl(line): cells = line.strip().split('\t') gmid = cells[0] mt_string = '\t'.join(cells[1:]).replace(__xmlns, "") mt = __get_clean_mathml(mt_string) return gmid, mt def __write_edges(edges, toflname): lns = [] for gmid, nodes in edges.iteritems(): lns.append( '\t'.join([gmid, ' '.join([node[0] for node in nodes])]) + '\n') f = open(toflname, 'w') f.writelines(lns) f.close() def __get_dep_graph(math_dir, dep_dir, fl, matching_method): ''' input: file from math_new output: 1. edges: {gumid1:[(gumid2, linktype)]} --> component list 2. gumidmappings: {gmid:gumid} ''' #useful utilities classes n_arxiv = norm_arxiv() n_attribute = norm_attribute() n_mrow = norm_mrow(__dtd) n_outer_fence = norm_outer_fence() n_tag = norm_tag(__dtd) n_splitter = norm_splitter(__dtd, __relation_fl) u = utils() depgraph = depgraph_heur(matching_method) lns = open(path.join(math_dir, fl)).readlines() #enumerate if there is no id in the <math> tag mts = OrderedDict() #for xhtml, enumerate mathtag; for xml, enumerate expressiontag; for math_new, enumerate the lines for ln in lns: if ln.strip() == '': continue gmid, mt = __extract_math_line_arxiv(ln) #replace <m:math> with <math> mt_string_initial = n_arxiv.remove_math_prefix(etree.tostring(mt)) #remove annotation, attributes, and finally get rid the <math> tag mt_string_formatted = n_arxiv.remove_annotation(etree.parse(StringIO(__dtd + mt_string_initial)).getroot()) mt_string_formatted = n_attribute.normalize(mt_string_formatted) #normalize mrow mt_string_formatted = n_mrow.normalize(mt_string_formatted) #remove fences mt_string_formatted = etree.tostring(n_outer_fence.remove_outer_fence(etree.parse(StringIO(__dtd + mt_string_formatted)).getroot()))[6:-7] #expand maths (normalize tags and/or case) expanded = n_tag.normalize_tags('<math>%s</math>' % mt_string_formatted) if len(expanded) > 0: expanded[-1] = n_mrow.normalize('<math>%s</math>' % expanded[-1])[6:-7] expanded.extend([etree.tostring(n_outer_fence.remove_outer_fence(etree.parse(StringIO(__dtd + '<math>%s</math>' % exp)).getroot()))[6:-7] for exp in expanded]) else: expanded = [mt_string_formatted] mts[gmid] = expanded #split around the equality and get the left side subexpressions left_subexp = n_splitter.split('<math>%s</math>' % expanded[-1]) if left_subexp is None: continue left_subexp = n_mrow.normalize(left_subexp)[6:-7] if not u.is_empty_tag(left_subexp): expanded_left = n_tag.normalize_tags(left_subexp) expanded_left = [n_mrow.normalize('<math>%s</math>' % exp)[6:-7] for exp in expanded_left] mts[gmid].append(left_subexp) mts[gmid].extend(expanded_left) mts[gmid] = list(set(mts[gmid])) edges = depgraph.create_edges(mts) __write_edges(edges, path.join(dep_dir, fl)) if __name__ == '__main__': #Preparation math_path = "../mathmlandextra/math_new/5/0704.0005.txt"#argv[1] dep_dir = "./"#argv[2] math_dir = path.dirname(math_path) #path to math_new directory math_fl = path.basename(math_path) #./1/0704.0097.txt # try: __get_dep_graph(math_dir, dep_dir, math_fl, Matching_Methods.heur) # except: # print math_path
# Copyright (c) 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved # # Permission is hereby granted, free of charge, to any person obtaining a # copy of this software and associated documentation files (the # "Software"), to deal in the Software without restriction, including # without limitation the rights to use, copy, modify, merge, publish, dis- # tribute, sublicense, and/or sell copies of the Software, and to permit # persons to whom the Software is furnished to do so, subject to the fol- # lowing conditions: # # The above copyright notice and this permission notice shall be included # in all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS # OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL- # ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT # SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, # WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS # IN THE SOFTWARE. # from boto.exception import JSONResponseError class CaseIdNotFound(JSONResponseError): pass class CaseCreationLimitExceeded(JSONResponseError): pass class InternalServerError(JSONResponseError): pass class AttachmentLimitExceeded(JSONResponseError): pass class DescribeAttachmentLimitExceeded(JSONResponseError): pass class AttachmentSetIdNotFound(JSONResponseError): pass class AttachmentSetExpired(JSONResponseError): pass class AttachmentIdNotFound(JSONResponseError): pass class AttachmentSetSizeLimitExceeded(JSONResponseError): pass
"""\ A library of useful helper classes to the SAX classes, for the convenience of application and driver writers. """ import os, urllib.parse, urllib.request import io from . import handler from . import xmlreader def __dict_replace(s, d): """Replace substrings of a string using a dictionary.""" for key, value in d.items(): s = s.replace(key, value) return s def escape(data, entities={}): """Escape &, <, and > in a string of data. You can escape other strings of data by passing a dictionary as the optional entities parameter. The keys and values must all be strings; each key will be replaced with its corresponding value. """ # must do ampersand first data = data.replace("&", "&amp;") data = data.replace(">", "&gt;") data = data.replace("<", "&lt;") if entities: data = __dict_replace(data, entities) return data def unescape(data, entities={}): """Unescape &amp;, &lt;, and &gt; in a string of data. You can unescape other strings of data by passing a dictionary as the optional entities parameter. The keys and values must all be strings; each key will be replaced with its corresponding value. """ data = data.replace("&lt;", "<") data = data.replace("&gt;", ">") if entities: data = __dict_replace(data, entities) # must do ampersand last return data.replace("&amp;", "&") def quoteattr(data, entities={}): """Escape and quote an attribute value. Escape &, <, and > in a string of data, then quote it for use as an attribute value. The \" character will be escaped as well, if necessary. You can escape other strings of data by passing a dictionary as the optional entities parameter. The keys and values must all be strings; each key will be replaced with its corresponding value. """ entities = entities.copy() entities.update({'\n': '&#10;', '\r': '&#13;', '\t':'&#9;'}) data = escape(data, entities) if '"' in data: if "'" in data: data = '"%s"' % data.replace('"', "&quot;") else: data = "'%s'" % data else: data = '"%s"' % data return data def _gettextwriter(out, encoding): if out is None: import sys return sys.stdout if isinstance(out, io.TextIOBase): # use a text writer as is return out # wrap a binary writer with TextIOWrapper if isinstance(out, io.RawIOBase): # Keep the original file open when the TextIOWrapper is # destroyed class _wrapper: __class__ = out.__class__ def __getattr__(self, name): return getattr(out, name) buffer = _wrapper() buffer.close = lambda: None else: # This is to handle passed objects that aren't in the # IOBase hierarchy, but just have a write method buffer = io.BufferedIOBase() buffer.writable = lambda: True buffer.write = out.write try: # TextIOWrapper uses this methods to determine # if BOM (for UTF-16, etc) should be added buffer.seekable = out.seekable buffer.tell = out.tell except AttributeError: pass return io.TextIOWrapper(buffer, encoding=encoding, errors='xmlcharrefreplace', newline='\n', write_through=True) class XMLGenerator(handler.ContentHandler): def __init__(self, out=None, encoding="iso-8859-1", short_empty_elements=False): handler.ContentHandler.__init__(self) out = _gettextwriter(out, encoding) self._write = out.write self._flush = out.flush self._ns_contexts = [{}] # contains uri -> prefix dicts self._current_context = self._ns_contexts[-1] self._undeclared_ns_maps = [] self._encoding = encoding self._short_empty_elements = short_empty_elements self._pending_start_element = False def _qname(self, name): """Builds a qualified name from a (ns_url, localname) pair""" if name[0]: # Per http://www.w3.org/XML/1998/namespace, The 'xml' prefix is # bound by definition to http://www.w3.org/XML/1998/namespace. It # does not need to be declared and will not usually be found in # self._current_context. if 'http://www.w3.org/XML/1998/namespace' == name[0]: return 'xml:' + name[1] # The name is in a non-empty namespace prefix = self._current_context[name[0]] if prefix: # If it is not the default namespace, prepend the prefix return prefix + ":" + name[1] # Return the unqualified name return name[1] def _finish_pending_start_element(self,endElement=False): if self._pending_start_element: self._write('>') self._pending_start_element = False # ContentHandler methods def startDocument(self): self._write('<?xml version="1.0" encoding="%s"?>\n' % self._encoding) def endDocument(self): self._flush() def startPrefixMapping(self, prefix, uri): self._ns_contexts.append(self._current_context.copy()) self._current_context[uri] = prefix self._undeclared_ns_maps.append((prefix, uri)) def endPrefixMapping(self, prefix): self._current_context = self._ns_contexts[-1] del self._ns_contexts[-1] def startElement(self, name, attrs): self._finish_pending_start_element() self._write('<' + name) for (name, value) in attrs.items(): self._write(' %s=%s' % (name, quoteattr(value))) if self._short_empty_elements: self._pending_start_element = True else: self._write(">") def endElement(self, name): if self._pending_start_element: self._write('/>') self._pending_start_element = False else: self._write('</%s>' % name) def startElementNS(self, name, qname, attrs): self._finish_pending_start_element() self._write('<' + self._qname(name)) for prefix, uri in self._undeclared_ns_maps: if prefix: self._write(' xmlns:%s="%s"' % (prefix, uri)) else: self._write(' xmlns="%s"' % uri) self._undeclared_ns_maps = [] for (name, value) in attrs.items(): self._write(' %s=%s' % (self._qname(name), quoteattr(value))) if self._short_empty_elements: self._pending_start_element = True else: self._write(">") def endElementNS(self, name, qname): if self._pending_start_element: self._write('/>') self._pending_start_element = False else: self._write('</%s>' % self._qname(name)) def characters(self, content): if content: self._finish_pending_start_element() self._write(escape(content)) def ignorableWhitespace(self, content): if content: self._finish_pending_start_element() self._write(content) def processingInstruction(self, target, data): self._finish_pending_start_element() self._write('<?%s %s?>' % (target, data)) class XMLFilterBase(xmlreader.XMLReader): """This class is designed to sit between an XMLReader and the client application's event handlers. By default, it does nothing but pass requests up to the reader and events on to the handlers unmodified, but subclasses can override specific methods to modify the event stream or the configuration requests as they pass through.""" def __init__(self, parent = None): xmlreader.XMLReader.__init__(self) self._parent = parent # ErrorHandler methods def error(self, exception): self._err_handler.error(exception) def fatalError(self, exception): self._err_handler.fatalError(exception) def warning(self, exception): self._err_handler.warning(exception) # ContentHandler methods def setDocumentLocator(self, locator): self._cont_handler.setDocumentLocator(locator) def startDocument(self): self._cont_handler.startDocument() def endDocument(self): self._cont_handler.endDocument() def startPrefixMapping(self, prefix, uri): self._cont_handler.startPrefixMapping(prefix, uri) def endPrefixMapping(self, prefix): self._cont_handler.endPrefixMapping(prefix) def startElement(self, name, attrs): self._cont_handler.startElement(name, attrs) def endElement(self, name): self._cont_handler.endElement(name) def startElementNS(self, name, qname, attrs): self._cont_handler.startElementNS(name, qname, attrs) def endElementNS(self, name, qname): self._cont_handler.endElementNS(name, qname) def characters(self, content): self._cont_handler.characters(content) def ignorableWhitespace(self, chars): self._cont_handler.ignorableWhitespace(chars) def processingInstruction(self, target, data): self._cont_handler.processingInstruction(target, data) def skippedEntity(self, name): self._cont_handler.skippedEntity(name) # DTDHandler methods def notationDecl(self, name, publicId, systemId): self._dtd_handler.notationDecl(name, publicId, systemId) def unparsedEntityDecl(self, name, publicId, systemId, ndata): self._dtd_handler.unparsedEntityDecl(name, publicId, systemId, ndata) # EntityResolver methods def resolveEntity(self, publicId, systemId): return self._ent_handler.resolveEntity(publicId, systemId) # XMLReader methods def parse(self, source): self._parent.setContentHandler(self) self._parent.setErrorHandler(self) self._parent.setEntityResolver(self) self._parent.setDTDHandler(self) self._parent.parse(source) def setLocale(self, locale): self._parent.setLocale(locale) def getFeature(self, name): return self._parent.getFeature(name) def setFeature(self, name, state): self._parent.setFeature(name, state) def getProperty(self, name): return self._parent.getProperty(name) def setProperty(self, name, value): self._parent.setProperty(name, value) # XMLFilter methods def getParent(self): return self._parent def setParent(self, parent): self._parent = parent # --- Utility functions def prepare_input_source(source, base=""): """This function takes an InputSource and an optional base URL and returns a fully resolved InputSource object ready for reading.""" if isinstance(source, str): source = xmlreader.InputSource(source) elif hasattr(source, "read"): f = source source = xmlreader.InputSource() source.setByteStream(f) if hasattr(f, "name"): source.setSystemId(f.name) if source.getByteStream() is None: sysid = source.getSystemId() basehead = os.path.dirname(os.path.normpath(base)) sysidfilename = os.path.join(basehead, sysid) if os.path.isfile(sysidfilename): source.setSystemId(sysidfilename) f = open(sysidfilename, "rb") else: source.setSystemId(urllib.parse.urljoin(base, sysid)) f = urllib.request.urlopen(source.getSystemId()) source.setByteStream(f) return source
# -*- coding: UTF-8 -*- """ Test libpython.py. This is already partly tested by test_libcython_in_gdb and Lib/test/test_gdb.py in the Python source. These tests are run in gdb and called from test_libcython_in_gdb.main() """ import os import sys import gdb from Cython.Debugger import libcython from Cython.Debugger import libpython import test_libcython_in_gdb from test_libcython_in_gdb import _debug, inferior_python_version class TestPrettyPrinters(test_libcython_in_gdb.DebugTestCase): """ Test whether types of Python objects are correctly inferred and that the right libpython.PySomeTypeObjectPtr classes are instantiated. Also test whether values are appropriately formatted (don't be too laborious as Lib/test/test_gdb.py already covers this extensively). Don't take care of decreffing newly allocated objects as a new interpreter is started for every test anyway. """ def setUp(self): super(TestPrettyPrinters, self).setUp() self.break_and_run('b = c = d = 0') def get_pyobject(self, code): value = gdb.parse_and_eval(code) assert libpython.pointervalue(value) != 0 return value def pyobject_fromcode(self, code, gdbvar=None): if gdbvar is not None: d = {'varname':gdbvar, 'code':code} gdb.execute('set $%(varname)s = %(code)s' % d) code = '$' + gdbvar return libpython.PyObjectPtr.from_pyobject_ptr(self.get_pyobject(code)) def get_repr(self, pyobject): return pyobject.get_truncated_repr(libpython.MAX_OUTPUT_LEN) def alloc_bytestring(self, string, gdbvar=None): if inferior_python_version < (3, 0): funcname = 'PyString_FromStringAndSize' else: funcname = 'PyBytes_FromStringAndSize' assert '"' not in string # ensure double quotes code = '(PyObject *) %s("%s", %d)' % (funcname, string, len(string)) return self.pyobject_fromcode(code, gdbvar=gdbvar) def alloc_unicodestring(self, string, gdbvar=None): self.alloc_bytestring(string.encode('UTF-8'), gdbvar='_temp') postfix = libpython.get_inferior_unicode_postfix() funcname = 'PyUnicode%s_FromEncodedObject' % (postfix,) return self.pyobject_fromcode( '(PyObject *) %s($_temp, "UTF-8", "strict")' % funcname, gdbvar=gdbvar) def test_bytestring(self): bytestring = self.alloc_bytestring("spam") if inferior_python_version < (3, 0): bytestring_class = libpython.PyStringObjectPtr expected = repr("spam") else: bytestring_class = libpython.PyBytesObjectPtr expected = "b'spam'" self.assertEqual(type(bytestring), bytestring_class) self.assertEqual(self.get_repr(bytestring), expected) def test_unicode(self): unicode_string = self.alloc_unicodestring(u"spam ") expected = "'spam '" if inferior_python_version < (3, 0): expected = 'u' + expected self.assertEqual(type(unicode_string), libpython.PyUnicodeObjectPtr) self.assertEqual(self.get_repr(unicode_string), expected) def test_int(self): if inferior_python_version < (3, 0): intval = self.pyobject_fromcode('PyInt_FromLong(100)') self.assertEqual(type(intval), libpython.PyIntObjectPtr) self.assertEqual(self.get_repr(intval), '100') def test_long(self): longval = self.pyobject_fromcode('PyLong_FromLong(200)', gdbvar='longval') assert gdb.parse_and_eval('$longval->ob_type == &PyLong_Type') self.assertEqual(type(longval), libpython.PyLongObjectPtr) self.assertEqual(self.get_repr(longval), '200') def test_frame_type(self): frame = self.pyobject_fromcode('PyEval_GetFrame()') self.assertEqual(type(frame), libpython.PyFrameObjectPtr)
from django.shortcuts import render, render_to_response from django.http import HttpResponse, HttpResponseRedirect from django.views import generic from django.core.context_processors import csrf from django.views.decorators.csrf import csrf_protect from django.contrib import auth from django.contrib import messages from django.template import RequestContext from django.contrib.auth.decorators import login_required from django.contrib.admin.views.decorators import staff_member_required from django.shortcuts import get_object_or_404 from .forms import TheftForm, StudentVehicleForm, SuspiciousVehicleForm from .models import TheftReport, StudentVehicle, BusTiming, EmployeeVehicle, SuspiciousVehicle, Guard, ParkingSlot, StudentCycle, OnDutyGuard, PersonPass from datetime import datetime import requests, threading from vms import pdf def login(request): """ Displays login page at the start """ c = {} c.update(csrf(request)) if request.method == 'POST': return render_to_response('vms/login.html', { 'form_errors': form_errors, }) else: return render_to_response('vms/login.html', c) @login_required(login_url="/vms") def logout(request): """ Logs the user out, if he is logged in. """ auth.logout(request) return HttpResponseRedirect('/vms/', { 'form_errors': "You've succesfully logged out." }) def auth_view(request): """ Authenticates user from the username and password from POST -- REQUIRES CHANGES DEPENDING ON MODEL """ username = request.POST.get('username', '') password = request.POST.get('password', '') user = auth.authenticate(username=username, password=password) if user is not None: auth.login(request, user) return HttpResponseRedirect('/vms/users/dashboard') #CHANGE THIS!! -- SHOULD WORK ACCORDING TO USER else: return HttpResponseRedirect('/vms/') #------------------------------------------------------------ # Theft Reporting for User #------------------------------------------------------------ # @login_required(login_url="/vms/") # def home(request): # """ # Home page for user, with his previous tasks # """ # today = str.lower(datetime.now().strftime("%A")) # buses = sorted(j for j in BusTiming.objects.all() if (j.from_time >= datetime.now().time() and filter(lambda x: str(x).lower() == today, j.availability.all()) )) # return render(request, 'vms/dashboard.html',{ # 'username': request.user.first_name, # 'is_user': True, # 'user': request.user, # 'buses': buses[0:3], # }) @login_required(login_url="/vms/") def home(request): """ Home page for user, with his previous tasks """ today = str.lower(datetime.now().strftime("%A")) # buses = sorted(j for j in BusTiming.objects.all() if (j.from_time >= datetime.now().time() and filter(lambda x: str(x).lower() == today, j.availability.all()) )) if not request.user.is_superuser == True: num_suspicious = len(SuspiciousVehicle.objects.filter(reporter=request.user)) x1 = [j for j in StudentVehicle.objects.all() if (j.user == request.user and j.registered_with_security_section==None)] num_pending = len(x1) x2 = [j.available_slots for j in ParkingSlot.objects.all()] num_guards = sum(x2) x3 = [j for j in TheftReport.objects.all() if j.reporter==request.user] num_thefts = len(x3) return render(request, 'vms/dashboard.html',{ 'username': request.user.first_name, 'user': request.user, # 'buses': buses[0:3], 'num_suspicious': num_suspicious, 'num_pending': num_pending, 'num_guards': num_guards, 'num_thefts': num_thefts, 'user_thefts': x3, }) else: num_suspicious = len(SuspiciousVehicle.objects.all()) num_pending = len(StudentVehicle.objects.filter(registered_with_security_section=None)) + len(EmployeeVehicle.objects.filter(registered_with_security_section=None)) num_approved = len(StudentVehicle.objects.filter(registered_with_security_section=True)) + len(EmployeeVehicle.objects.filter(registered_with_security_section=True)) num_denied = len(StudentVehicle.objects.filter(registered_with_security_section=False)) + len(EmployeeVehicle.objects.filter(registered_with_security_section=False)) num_guards = len(OnDutyGuard.objects.all()) num_thefts = len(TheftReport.objects.filter(status="Submitted")) passes=PersonPass.objects.all() total_blocked = len(passes.filter(is_blocked=True)) total_issued = len(passes.filter(is_blocked=False)) x = [j for j in passes if j.expiry_date < datetime.now().date()] total_expired = len(x) return render(request, 'vms/dashboard.html',{ 'username': request.user.first_name, 'is_user': True, 'user': request.user, # 'buses': buses[0:3], 'num_suspicious': num_suspicious, 'num_pending': num_pending, 'num_guards': num_guards, 'num_thefts': num_thefts, 'num_approved': num_approved, 'num_denied': num_denied, 'total_issued': total_issued, 'total_expired': total_expired, 'total_blocked': total_blocked, }) @login_required(login_url="/vms/") def busdetails(request): return render(request, 'vms/busdetails.html') #Ayush Mananiya #----------thread function for sending sms--------------------------------------------- def send_sms(message, numbers): proxy = "http://sumeet.ranka:weh,hftg@202.141.80.24:3128" #change the username and password status1='' for i in numbers: response = requests.get("https://site2sms.p.mashape.com/index.php?msg="+message+"&phone="+str(i)+"&pwd=CS243iitg&uid=8011035945",headers={"X-Mashape-Key": "CW4gX5MRw2mshX6uxzLHMxEVoB0Op1v4cMrjsnZoeRXbk3LD46", "Accept": "application/json"},proxies={"http":proxy,"https":proxy,"ftp":proxy},) #-------------------------end----------------------------------------------------- @login_required(login_url="/vms/") def theft_report_form(request): """ Displays theft report form for user -- NOTE: This form is common to admin and user """ if request.method == 'POST': form = TheftForm(request.POST) if form.is_valid(): task = form.save(commit = False) task.reporter = request.user if request.user.user.is_student: vehicles=StudentVehicle.objects.filter(user=request.user) cycles=StudentCycle.objects.filter(user=request.user) try: vehicle = StudentVehicle.objects.get(vehicle_pass_no=task.vehicle_pass_no) cycle=0 except: message = "Vehicle does not belong to you." vehicle = None try: vehicle = StudentCycle.objects.get(cycle_pass_no=task.vehicle_pass_no) cycle=1 except: message = "Vehicle does not belong to you." vehicle = None return render(request, "vms/theft.html",{ 'message':message, 'user':request.user, 'form':form, }) else: vehicles=EmployeeVehicle.objects.filter(user=request.user) try: vehicle = EmployeeVehicle.objects.get(vehicle_pass_no=task.vehicle_pass_no) cycle=0 except: vehicle = None message = "Vehicle does not belong to you." return render(request, "vms/theft.html",{ 'message':message, 'user':request.user, 'form':form, }) if vehicle != None and vehicle in vehicles: if request.user.user.is_student: task.stud_vehicle=vehicle else: task.emp_vehicle=vehicle #ayush Mananiya #my funct started-------------------------------------------------------------------------- if cycle == 0: message = vehicle.make_and_model +' '+ task.vehicle_pass_no + ' is stolen from ' + task.theft_place +' at '+ str(task.theft_time.strftime('%d-%b-%Y %H:%M')) #extract the form fields and generate message text else: message = vehicle.cycle_model+ ' '+vehicle.cycle_color+' '+' '+'is stolen from '+task.theft_place+' at '+ str(task.theft_time) numbers = list(Guard.objects.values_list('guard_phone_number', flat=True)) #retrieves the phone numbers of all the guards sms_thread = threading.Thread(target=send_sms, args=(message, numbers)) #threading sms_thread.start() #ended here-------------------------------------------------------------------------------------------------------------------------- task.save() messages.success(request, 'Your theft report is submitted.') return render(request, "vms/theft.html",{ 'message':"Theft Report successfully submitted.", 'user':request.user, 'form':form, 'success':True, 'id':task.id, }) else: form = TheftForm() return render(request, "vms/theft.html", { 'form':form, 'user':request.user, }) @login_required(login_url="/vms/") def generate_report(request, report_id): rep = TheftReport.objects.filter(id=report_id) if len(rep) > 0: # print rep[0].theft_time return pdf.pdf_gen(rep[0]) return HttpResponse("done") @login_required(login_url="/vms/") def vehicles_missing(request): """ Displays to users their theft reports """ reports = TheftReport.objects.all() return render(request, "vms/theft_reports.html", { 'reports': reports, }) @login_required(login_url="/vms/") def parking_slot_availability(request): """ Function to serve the parking spaces that are available """ return render(request, 'users/parking.html', { 'pslots': ParkingSlot.objects.all(), }) @login_required(login_url="/vms/") def suspicious_vehicle_report_form(request): """ Function to report suspicious vehicles """ if request.method == 'POST': form = SuspiciousVehicleForm(request.POST, request.FILES) if form.is_valid(): task = form.save(commit = False) task.reporter=request.user task.save() return render(request, 'vms/suspicious.html',{ 'user':request.user, 'form':form, 'message':"Vehicle has been reported. Thanks for the caution." }) else: form=SuspiciousVehicleForm() return render(request, 'vms/suspicious.html', { 'user': request.user, 'form':form, }) @login_required(login_url="/vms/") def suspicious_vehicles(request): """ Function to allow users to view all suspicious reported activity """ str1="" if request.POST: SuspiciousVehicle.objects.get(id=request.POST['Delete']).delete() vehicles = SuspiciousVehicle.objects.all() messages.success(request,"Report for suspicious activity is deleted") return render(request, 'vms/suspicious_vehicles.html',{ 'user':request.user, 'vehicles':vehicles, }) else: vehicles = SuspiciousVehicle.objects.all() return render(request, 'vms/suspicious_vehicles.html', { 'user': request.user, 'vehicles':vehicles, }) @login_required(login_url="/vms/") def delete_suspicious_vehicles(request, suspicious_vehicle_id): SuspiciousVehicle.objects.get(id=suspicious_vehicle_id).delete() pass
# -*- coding: utf-8 -*- from __future__ import unicode_literals from datetime import date from django.db import migrations, models POSTS = [ { "title": "Django 1.0 Release", "slug": "django-10-released", "pub_date": date(2008, 9, 3), "startups": [], "tags": ["django", "python", "web"], "text": "THE Web Framework.", }, { "title": "Simple Robots for Sale", "slug": "simple-robots-for-sale", "pub_date": date(2011, 2, 21), "startups": ["simple-robots"], "tags": ["augmented-reality", "python"], "text": "If only they would make " "spider bots.", }, { "title": "Django Training", "slug": "django-training", "pub_date": date(2013, 1, 18), "startups": ["jambon-software"], "tags": ["django"], "text": "Want to learn Django in a class " "setting? JamBon Software offers " "hands-on courses in the web " "framework. Just looking for help? " "They'll consult on your web and " "mobile products and can also be " "hired for end-to-end development.", }, { "title": "Django 1.8 Release", "slug": "django-18-released", "pub_date": date(2015, 4, 1), "startups": [], "tags": ["django", "python", "web"], "text": "Django 1.8 is Django's newest " "version, and the next version " "slated for Long-Term Support " "(LTS). LTS means that Django 1.8 " "will be supported for longer than " "regular versions: Django core " "developers will specify a single " "release as LTS, and then continue " "to update that version regardless " "of the usual release cycle. This " "will last until they pick a new " "LTS version, which typically " "happens every 3 to 4 years. The " "last LTS version was 1.4, " "released in March 2012, which " "will stop being supported in " "October 2015.\n\n" "For more information: \n" "http://andrewsforge.com/article/" "upgrading-django-to-17/part-1-" "introduction-and-django-releases/", }, { "title": "More Django Info", "slug": "more-django-info", "pub_date": date(2015, 4, 8), "startups": ["jambon-software"], "tags": ["django", "web"], "text": "Remember that the official websites " "for Django and this book contain a " "number of extra resources.\n\n" "https://djangoproject.com\n" "https://django-unleashed.com\n\n" "Want more Django info? " "There's always my personal blog!\n\n" "https://AndrewsForge.com", }, { "title": "New Django Version", "slug": "new-django-version", "pub_date": date(2020, 5, 15), "startups": [], "tags": ["django", "python", "web"], "text": "Better integration with " "HTML Boilerstrap 9.", }, ] def add_post_data(apps, schema_editor): Post = apps.get_model('blog', 'Post') Startup = apps.get_model( 'organizer', 'Startup') Tag = apps.get_model('organizer', 'Tag') for post_dict in POSTS: post = Post.objects.create( title=post_dict['title'], slug=post_dict['slug'], text=post_dict['text']) post.pub_date = post_dict['pub_date'] post.save() for tag_slug in post_dict['tags']: post.tags.add( Tag.objects.get( slug=tag_slug)) for startup_slug in post_dict['startups']: post.startups.add( Startup.objects.get( slug=startup_slug)) def remove_post_data(apps, schema_editor): Post = apps.get_model('blog', 'Post') for post_dict in POSTS: post = Post.objects.get( slug=post_dict['slug']) post.delete() class Migration(migrations.Migration): dependencies = [ ('blog', '0001_initial'), ('organizer', '0003_startup_data'), ] operations = [ migrations.RunPython( add_post_data, remove_post_data) ]
""" =========== Zoom Window =========== This example shows how to connect events in one window, for example, a mouse press, to another figure window. If you click on a point in the first window, the z and y limits of the second will be adjusted so that the center of the zoom in the second window will be the x,y coordinates of the clicked point. Note the diameter of the circles in the scatter are defined in points**2, so their size is independent of the zoom """ import matplotlib.pyplot as plt #import figure, show import numpy as np # nodebox section if __name__ == '__builtin__': # were in nodebox import os import tempfile W = 800 inset = 20 size(W, 600) plt.cla() plt.clf() plt.close('all') def tempimage(): fob = tempfile.NamedTemporaryFile(mode='w+b', suffix='.png', delete=False) fname = fob.name fob.close() return fname imgx = 20 imgy = 0 def pltshow(plt, dpi=150): global imgx, imgy temppath = tempimage() plt.savefig(temppath, dpi=dpi) dx,dy = imagesize(temppath) w = min(W,dx) image(temppath,imgx,imgy,width=w) imgy = imgy + dy + 20 os.remove(temppath) size(W, HEIGHT+dy+40) else: def pltshow(mplpyplot): mplpyplot.show() # nodebox section end figsrc = plt.figure() figzoom = plt.figure() axsrc = figsrc.add_subplot(111, xlim=(0, 1), ylim=(0, 1), autoscale_on=False) axzoom = figzoom.add_subplot(111, xlim=(0.45, 0.55), ylim=(0.4, .6), autoscale_on=False) axsrc.set_title('Click to zoom') axzoom.set_title('zoom window') x, y, s, c = np.random.rand(4, 200) s *= 200 axsrc.scatter(x, y, s, c) axzoom.scatter(x, y, s, c) def onpress(event): if event.button != 1: return x, y = event.xdata, event.ydata axzoom.set_xlim(x - 0.1, x + 0.1) axzoom.set_ylim(y - 0.1, y + 0.1) figzoom.canvas.draw() figsrc.canvas.mpl_connect('button_press_event', onpress) pltshow(plt)
#!/usr/bin/env python # Copyright (c) 2011 The Chromium Authors. All rights reserved. # Use of this source code is governed by a BSD-style license that can be # found in the LICENSE file. import cStringIO import os import pickle import socket import sys import pyauto class RemoteHost(object): """Class used as a host for tests that use the PyAuto RemoteProxy. This class fires up a listener which waits for a connection from a RemoteProxy and receives method call requests. Run python remote_host.py remote_host.RemoteHost.RunHost to start up a PyAuto remote instance that you can connect to and automate using pyauto.RemoteProxy. """ def __init__(self, host, *args, **kwargs): self.StartSocketServer(host) def StartSocketServer(self, host): listening_socket = socket.socket() listening_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) listening_socket.bind(host) listening_socket.listen(1) print 'Listening for incoming connections on port %d.' % host[1] self._socket, address = listening_socket.accept() print 'Accepted connection from %s:%d.' % address while self.Connected(): self._HandleRPC() def StopSocketServer(self): if self._socket: try: self._socket.shutdown(socket.SHUT_RDWR) self._socket.close() except socket.error: pass self._socket = None def Connected(self): return self._socket def CreateTarget(self, target_class): """Creates an instance of the specified class to serve as the RPC target. RPC calls can be made on the target. """ self.target = target_class() def _HandleRPC(self): """Receives a method call request over the socket and executes the method. This method captures stdout and stderr for the duration of the method call, and sends those, the return value, and any thrown exceptions back to the RemoteProxy. """ # Receive request. request = self._socket.recv(4096) if not request: self.StopSocketServer() return request = pickle.loads(request) # Redirect output to strings. old_stdout = sys.stdout old_stderr = sys.stderr sys.stdout = stdout = cStringIO.StringIO() sys.stderr = stderr = cStringIO.StringIO() # Make requested method call. result = None exception = None try: if getattr(self, request[0], None): result = getattr(self, request[0])(*request[1], **request[2]) else: result = getattr(self.target, request[0])(*request[1], **request[2]) except BaseException, e: exception = (e.__class__.__name__, str(e)) # Put output back to the way it was before. sys.stdout = old_stdout sys.stderr = old_stderr # Package up and send the result of the method call. response = pickle.dumps((result, stdout.getvalue(), stderr.getvalue(), exception)) if self._socket.send(response) != len(response): self.StopSocketServer() if __name__ == '__main__': pyauto_suite = pyauto.PyUITestSuite(sys.argv) RemoteHost(('', 7410)) del pyauto_suite
# Copyright (c) 2013 The Johns Hopkins University/Applied Physics Laboratory # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. """ An implementation of a key manager that reads its key from the project's configuration options. This key manager implementation provides limited security, assuming that the key remains secret. Using the volume encryption feature as an example, encryption provides protection against a lost or stolen disk, assuming that the configuration file that contains the key is not stored on the disk. Encryption also protects the confidentiality of data as it is transmitted via iSCSI from the compute host to the storage host (again assuming that an attacker who intercepts the data does not know the secret key). Because this implementation uses a single, fixed key, it proffers no protection once that key is compromised. In particular, different volumes encrypted with a key provided by this key manager actually share the same encryption key so *any* volume can be decrypted once the fixed key is known. """ import array import binascii from oslo_config import cfg from oslo_log import log as logging from jacket.storage import exception from jacket.storage.i18n import _, _LW from jacket.storage.keymgr import key from jacket.storage.keymgr import key_mgr key_mgr_opts = [ cfg.StrOpt('fixed_key', help='Fixed key returned by key manager, specified in hex'), ] CONF = cfg.CONF CONF.register_opts(key_mgr_opts, group='storage_keymgr') LOG = logging.getLogger(__name__) class ConfKeyManager(key_mgr.KeyManager): """Key Manager that supports one key defined by the fixed_key conf option. This key manager implementation supports all the methods specified by the key manager interface. This implementation creates a single key in response to all invocations of create_key. Side effects (e.g., raising exceptions) for each method are handled as specified by the key manager interface. """ def __init__(self): super(ConfKeyManager, self).__init__() self.key_id = '00000000-0000-0000-0000-000000000000' def _generate_key(self, **kwargs): _hex = self._generate_hex_key(**kwargs) key_list = array.array('B', binascii.unhexlify(_hex)).tolist() return key.SymmetricKey('AES', key_list) def _generate_hex_key(self, **kwargs): if CONF.storage_keymgr.fixed_key is None: LOG.warning( _LW('config option storage_keymgr.fixed_key has not been defined:' ' some operations may fail unexpectedly')) raise ValueError(_('storage_keymgr.fixed_key not defined')) return CONF.storage_keymgr.fixed_key def create_key(self, ctxt, **kwargs): """Creates a key. This implementation returns a UUID for the created key. A NotAuthorized exception is raised if the specified context is None. """ if ctxt is None: raise exception.NotAuthorized() return self.key_id def store_key(self, ctxt, key, **kwargs): """Stores (i.e., registers) a key with the key manager.""" if ctxt is None: raise exception.NotAuthorized() if key != self._generate_key(): raise exception.KeyManagerError( reason="cannot store arbitrary keys") return self.key_id def copy_key(self, ctxt, key_id, **kwargs): if ctxt is None: raise exception.NotAuthorized() return self.key_id def get_key(self, ctxt, key_id, **kwargs): """Retrieves the key identified by the specified id. This implementation returns the key that is associated with the specified UUID. A NotAuthorized exception is raised if the specified context is None; a KeyError is raised if the UUID is invalid. """ if ctxt is None: raise exception.NotAuthorized() if key_id != self.key_id: raise KeyError(key_id) return self._generate_key() def delete_key(self, ctxt, key_id, **kwargs): if ctxt is None: raise exception.NotAuthorized() if key_id != self.key_id: raise exception.KeyManagerError( reason="cannot delete non-existent key") LOG.warning(_LW("Not deleting key %s"), key_id)
from test import test_support import unittest import nis class NisTests(unittest.TestCase): def test_maps(self): try: maps = nis.maps() except nis.error, msg: # NIS is probably not active, so this test isn't useful if test_support.verbose: print "Test Skipped:", msg # Can't raise TestSkipped as regrtest only recognizes the exception # import time. return try: # On some systems, this map is only accessible to the # super user maps.remove("passwd.adjunct.byname") except ValueError: pass done = 0 for nismap in maps: mapping = nis.cat(nismap) for k, v in mapping.items(): if not k: continue if nis.match(k, nismap) != v: self.fail("NIS match failed for key `%s' in map `%s'" % (k, nismap)) else: # just test the one key, otherwise this test could take a # very long time done = 1 break if done: break def test_main(): test_support.run_unittest(NisTests) if __name__ == '__main__': test_main()
#!/usr/bin/python # -*- coding:utf-8 -*- """ A script to create the "Terrain Table" on the TerrainCodeTableWML wiki page. Add the output to the wiki whenever a new terrain is added to mainline. """ from __future__ import with_statement # For python < 2.6 import os import sys import re try: import argparse except ImportError: print('Please install argparse by running "easy_install argparse"') sys.exit(1) # Where to get terrain images terrain_url = "https://raw.github.com/wesnoth/wesnoth/master/data/core/images/terrain/%s.png" def parse_terrain(data): """ Parses the terrains. Input looks like this: [terrain_type] symbol_image=water/ocean-grey-tile id=deep_water_gray editor_name= _ "Gray Deep Water" string=Wog aliasof=Wo submerge=0.5 editor_group=water [/terrain_type] Output is a text in wiki format. """ # Remove all comments. data = "\n".join([i for i in data.split("\n") if not i.startswith("#")]) terrains = re.compile("\[terrain_type\](.*?)\[\/terrain_type\]", re.DOTALL).findall(data) data = """{{AutogeneratedWML}}{| border="1" !terrain !name !string !alias of !editor group """ for i in terrains: # Strip unneeded things. i = i[5:] i = i.split("\n ") # Don't parse special files that are hacks. They shouldn't be used # directly. (They're only there to make aliasing work.) if i[0].startswith(" "): continue # This avoids problems due to additional = in strings. Exact string # removal does not matter as long as we do not print help_topic_text # in the wiki page. removeus = ("<italic>text='", "'</italic>", "<ref>dst='", "text='", "'</ref>") for text in removeus: i = [a.replace(text, "") for a in i] # Create a dictionary of key and values content = dict([v.strip().split("=") for v in i]) # Hidden things shouldn't be displayed if 'hidden' in content: continue data += """|- | %s | %s | <code>%s</code> | <code>%s</code> | %s """ % ( terrain_url % (content['editor_image'] if 'editor_image' in content else content['symbol_image']), content['editor_name'][4:-1] if 'editor_name' in content else content['name'][4:-1], content['string'].replace("# wmllint: ignore", "").replace("|", "&#124;"), content['aliasof'].replace("|", "&#124;") if 'aliasof' in content else "", content['editor_group']) data += "|}" return data if __name__ == "__main__": parser = argparse.ArgumentParser(description='terrain2wiki is a tool to\ convert the terrain codes located in terrain.cfg to wiki formatted text.') parser.add_argument('-f', '--file', default='data/core/terrain.cfg', dest='path', help="The location of the terrain.cfg file.") parser.add_argument('-o', '--output', default='/tmp/TerrainCodeTableWML', dest='output_path', help="The location of the output file.") args = parser.parse_args() path = args.path output_path = args.output_path if not os.path.exists(path) or not path.endswith('.cfg'): print("Invalid path: '%s' does not exist or not a .cfg file.") % path sys.exit(1) with open(path, "r") as input_file: data = input_file.read() data = parse_terrain(data) with open(output_path, "w") as output: output.write(data)
#!/usr/bin/env python """A class representing entity property range.""" # pylint: disable=g-bad-name # pylint: disable=g-import-not-at-top import datetime from google.appengine.ext import ndb from google.appengine.ext import db from mapreduce import errors from mapreduce import util __all__ = [ "should_shard_by_property_range", "PropertyRange"] def should_shard_by_property_range(filters): """Returns whether these filters suggests sharding by property range. Args: filters: user supplied filters. Each filter should be a list or tuple of format (<property_name_as_str>, <query_operator_as_str>, <value_of_certain_type>). Value type is up to the property's type. Returns: True if these filters suggests sharding by property range. False Otherwise. """ if not filters: return False for f in filters: if f[1] != "=": return True return False class PropertyRange(object): """A class that represents a range on a db.Model's property. It supports splitting the range into n shards and generating a query that returns entities within that range. """ def __init__(self, filters, model_class_path): """Init. Args: filters: user supplied filters. Each filter should be a list or tuple of format (<property_name_as_str>, <query_operator_as_str>, <value_of_certain_type>). Value type should satisfy the property's type. model_class_path: full path to the model class in str. """ self.filters = filters self.model_class_path = model_class_path self.model_class = util.for_name(self.model_class_path) self.prop, self.start, self.end = self._get_range_from_filters( self.filters, self.model_class) @classmethod def _get_range_from_filters(cls, filters, model_class): """Get property range from filters user provided. This method also validates there is one and only one closed range on a single property. Args: filters: user supplied filters. Each filter should be a list or tuple of format (<property_name_as_str>, <query_operator_as_str>, <value_of_certain_type>). Value type should satisfy the property's type. model_class: the model class for the entity type to apply filters on. Returns: a tuple of (property, start_filter, end_filter). property is the model's field that the range is about. start_filter and end_filter define the start and the end of the range. (None, None, None) if no range is found. Raises: BadReaderParamsError: if any filter is invalid in any way. """ if not filters: return None, None, None range_property = None start_val = None end_val = None start_filter = None end_filter = None for f in filters: prop, op, val = f if op in [">", ">=", "<", "<="]: if range_property and range_property != prop: raise errors.BadReaderParamsError( "Range on only one property is supported.") range_property = prop if val is None: raise errors.BadReaderParamsError( "Range can't be None in filter %s", f) if op in [">", ">="]: if start_val is not None: raise errors.BadReaderParamsError( "Operation %s is specified more than once.", op) start_val = val start_filter = f else: if end_val is not None: raise errors.BadReaderParamsError( "Operation %s is specified more than once.", op) end_val = val end_filter = f elif op != "=": raise errors.BadReaderParamsError( "Only < <= > >= = are supported as operation. Got %s", op) if not range_property: return None, None, None if start_val is None or end_val is None: raise errors.BadReaderParamsError( "Filter should contains a complete range on property %s", range_property) if issubclass(model_class, db.Model): property_obj = model_class.properties()[range_property] else: property_obj = ( model_class._properties[ # pylint: disable=protected-access range_property]) supported_properties = ( _DISCRETE_PROPERTY_SPLIT_FUNCTIONS.keys() + _CONTINUOUS_PROPERTY_SPLIT_FUNCTIONS.keys()) if not isinstance(property_obj, tuple(supported_properties)): raise errors.BadReaderParamsError( "Filtered property %s is not supported by sharding.", range_property) if not start_val < end_val: raise errors.BadReaderParamsError( "Start value %s should be smaller than end value %s", start_val, end_val) return property_obj, start_filter, end_filter def split(self, n): """Evenly split this range into contiguous, non overlapping subranges. Args: n: number of splits. Returns: a list of contiguous, non overlapping sub PropertyRanges. Maybe less than n when not enough subranges. """ new_range_filters = [] name = self.start[0] prop_cls = self.prop.__class__ if prop_cls in _DISCRETE_PROPERTY_SPLIT_FUNCTIONS: splitpoints = _DISCRETE_PROPERTY_SPLIT_FUNCTIONS[prop_cls]( self.start[2], self.end[2], n, self.start[1] == ">=", self.end[1] == "<=") start_filter = (name, ">=", splitpoints[0]) for p in splitpoints[1:]: end_filter = (name, "<", p) new_range_filters.append([start_filter, end_filter]) start_filter = (name, ">=", p) else: splitpoints = _CONTINUOUS_PROPERTY_SPLIT_FUNCTIONS[prop_cls]( self.start[2], self.end[2], n) start_filter = self.start for p in splitpoints: end_filter = (name, "<", p) new_range_filters.append([start_filter, end_filter]) start_filter = (name, ">=", p) new_range_filters.append([start_filter, self.end]) for f in new_range_filters: f.extend(self._equality_filters) return [self.__class__(f, self.model_class_path) for f in new_range_filters] def make_query(self, ns): """Make a query of entities within this range. Query options are not supported. They should be specified when the query is run. Args: ns: namespace of this query. Returns: a db.Query or ndb.Query, depends on the model class's type. """ if issubclass(self.model_class, db.Model): query = db.Query(self.model_class, namespace=ns) for f in self.filters: query.filter("%s %s" % (f[0], f[1]), f[2]) else: query = self.model_class.query(namespace=ns) for f in self.filters: query = query.filter(ndb.FilterNode(*f)) return query @property def _equality_filters(self): return [f for f in self.filters if f[1] == "="] def to_json(self): return {"filters": self.filters, "model_class_path": self.model_class_path} @classmethod def from_json(cls, json): return cls(json["filters"], json["model_class_path"]) def _split_datetime_property(start, end, n, include_start, include_end): # datastore stored datetime precision is microsecond. if not include_start: start += datetime.timedelta(microseconds=1) if include_end: end += datetime.timedelta(microseconds=1) delta = end - start stride = delta // n if stride <= datetime.timedelta(): raise ValueError("Range too small to split: start %r end %r", start, end) splitpoints = [start] previous = start for _ in range(n-1): point = previous + stride if point == previous or point > end: continue previous = point splitpoints.append(point) if end not in splitpoints: splitpoints.append(end) return splitpoints def _split_float_property(start, end, n): delta = float(end - start) stride = delta / n if stride <= 0: raise ValueError("Range too small to split: start %r end %r", start, end) splitpoints = [] for i in range(1, n): splitpoints.append(start + i * stride) return splitpoints def _split_integer_property(start, end, n, include_start, include_end): if not include_start: start += 1 if include_end: end += 1 delta = float(end - start) stride = delta / n if stride <= 0: raise ValueError("Range too small to split: start %r end %r", start, end) splitpoints = [start] previous = start for i in range(1, n): point = start + int(round(i * stride)) if point == previous or point > end: continue previous = point splitpoints.append(point) if end not in splitpoints: splitpoints.append(end) return splitpoints def _split_string_property(start, end, n, include_start, include_end): try: start = start.encode("ascii") end = end.encode("ascii") except UnicodeEncodeError, e: raise ValueError("Only ascii str is supported.", e) return _split_byte_string_property(start, end, n, include_start, include_end) # The alphabet splitting supports. _ALPHABET = "".join(chr(i) for i in range(128)) # String length determines how many unique strings we can choose from. # We can't split into more shards than this: len(_ALPHABET)^_STRING_LENGTH _STRING_LENGTH = 4 def _split_byte_string_property(start, end, n, include_start, include_end): # Get prefix, suffix, and the real start/end to split on. i = 0 for i, (s, e) in enumerate(zip(start, end)): if s != e: break common_prefix = start[:i] start_suffix = start[i+_STRING_LENGTH:] end_suffix = end[i+_STRING_LENGTH:] start = start[i:i+_STRING_LENGTH] end = end[i:i+_STRING_LENGTH] # Convert str to ord. weights = _get_weights(_STRING_LENGTH) start_ord = _str_to_ord(start, weights) if not include_start: start_ord += 1 end_ord = _str_to_ord(end, weights) if include_end: end_ord += 1 # Do split. stride = (end_ord - start_ord) / float(n) if stride <= 0: raise ValueError("Range too small to split: start %s end %s", start, end) splitpoints = [_ord_to_str(start_ord, weights)] previous = start_ord for i in range(1, n): point = start_ord + int(round(stride * i)) if point == previous or point > end_ord: continue previous = point splitpoints.append(_ord_to_str(point, weights)) end_str = _ord_to_str(end_ord, weights) if end_str not in splitpoints: splitpoints.append(end_str) # Append suffix. splitpoints[0] += start_suffix splitpoints[-1] += end_suffix return [common_prefix + point for point in splitpoints] def _get_weights(max_length): """Get weights for each offset in str of certain max length. Args: max_length: max length of the strings. Returns: A list of ints as weights. Example: If max_length is 2 and alphabet is "ab", then we have order "", "a", "aa", "ab", "b", "ba", "bb". So the weight for the first char is 3. """ weights = [1] for i in range(1, max_length): weights.append(weights[i-1] * len(_ALPHABET) + 1) weights.reverse() return weights def _str_to_ord(content, weights): """Converts a string to its lexicographical order. Args: content: the string to convert. Of type str. weights: weights from _get_weights. Returns: an int or long that represents the order of this string. "" has order 0. """ ordinal = 0 for i, c in enumerate(content): ordinal += weights[i] * _ALPHABET.index(c) + 1 return ordinal def _ord_to_str(ordinal, weights): """Reverse function of _str_to_ord.""" chars = [] for weight in weights: if ordinal == 0: return "".join(chars) ordinal -= 1 index, ordinal = divmod(ordinal, weight) chars.append(_ALPHABET[index]) return "".join(chars) # discrete property split functions all have the same interface. # They take start, end, shard_number n, include_start, include_end. # They return at most n+1 points, forming n ranges. # Each range should be include_start, exclude_end. _DISCRETE_PROPERTY_SPLIT_FUNCTIONS = { db.DateTimeProperty: _split_datetime_property, db.IntegerProperty: _split_integer_property, db.StringProperty: _split_string_property, db.ByteStringProperty: _split_byte_string_property, # ndb. ndb.DateTimeProperty: _split_datetime_property, ndb.IntegerProperty: _split_integer_property, ndb.StringProperty: _split_string_property, ndb.BlobProperty: _split_byte_string_property } _CONTINUOUS_PROPERTY_SPLIT_FUNCTIONS = { db.FloatProperty: _split_float_property, # ndb. ndb.FloatProperty: _split_float_property, }
# Copyright 2013 Mirantis Inc. # Copyright 2014 Cloudbase Solutions Srl # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import mock import unittest from cloudbaseinit.openstack.common import cfg from cloudbaseinit.plugins.windows.userdataplugins import urldownload CONF = cfg.CONF class UrlDownloadHandlerTests(unittest.TestCase): def setUp(self): self._urldownload = urldownload.URLDownloadPlugin() @mock.patch('cloudbaseinit.plugins.windows.userdatautils' '.execute_user_data_script') def _test_process(self, mock_execute_user_data_script, filename): mock_part = mock.MagicMock() mock_part.get_filename.return_value = filename response = self._urldownload.process(mock_part) mock_part.get_filename.assert_called_with() if filename: mock_execute_user_data_script.assert_called_with( mock_part.get_payload()) self.assertEqual(response, mock_execute_user_data_script()) else: self.assertTrue(response is None) response = self._urldownload.process(mock_part) mock_part.get_filename.assert_called_with() if filename: mock_execute_user_data_script.assert_called_with( mock_part.get_payload()) self.assertEqual(response, mock_execute_user_data_script()) else: self.assertTrue(response is None) def test_process(self): self._test_process(filename='cfn-userdata') def test_process_content_not_supported(self): self._test_process(filename=None)
# -*- coding: utf-8 -*- """ werkzeug.contrib.profiler ~~~~~~~~~~~~~~~~~~~~~~~~~ This module provides a simple WSGI profiler middleware for finding bottlenecks in web application. It uses the :mod:`profile` or :mod:`cProfile` module to do the profiling and writes the stats to the stream provided (defaults to stderr). Example usage:: from werkzeug.contrib.profiler import ProfilerMiddleware app = ProfilerMiddleware(app) :copyright: (c) 2014 by the Werkzeug Team, see AUTHORS for more details. :license: BSD, see LICENSE for more details. """ import sys import time import os.path try: try: from cProfile import Profile except ImportError: from profile import Profile from pstats import Stats available = True except ImportError: available = False class MergeStream(object): """An object that redirects `write` calls to multiple streams. Use this to log to both `sys.stdout` and a file:: f = open('profiler.log', 'w') stream = MergeStream(sys.stdout, f) profiler = ProfilerMiddleware(app, stream) """ def __init__(self, *streams): if not streams: raise TypeError('at least one stream must be given') self.streams = streams def write(self, data): for stream in self.streams: stream.write(data) class ProfilerMiddleware(object): """Simple profiler middleware. Wraps a WSGI application and profiles a request. This intentionally buffers the response so that timings are more exact. By giving the `profile_dir` argument, pstat.Stats files are saved to that directory, one file per request. Without it, a summary is printed to `stream` instead. For the exact meaning of `sort_by` and `restrictions` consult the :mod:`profile` documentation. .. versionadded:: 0.9 Added support for `restrictions` and `profile_dir`. :param app: the WSGI application to profile. :param stream: the stream for the profiled stats. defaults to stderr. :param sort_by: a tuple of columns to sort the result by. :param restrictions: a tuple of profiling strictions, not used if dumping to `profile_dir`. :param profile_dir: directory name to save pstat files """ def __init__(self, app, stream=None, sort_by=('time', 'calls'), restrictions=(), profile_dir=None): if not available: raise RuntimeError('the profiler is not available because ' 'profile or pstat is not installed.') self._app = app self._stream = stream or sys.stdout self._sort_by = sort_by self._restrictions = restrictions self._profile_dir = profile_dir def __call__(self, environ, start_response): response_body = [] def catching_start_response(status, headers, exc_info=None): start_response(status, headers, exc_info) return response_body.append def runapp(): appiter = self._app(environ, catching_start_response) response_body.extend(appiter) if hasattr(appiter, 'close'): appiter.close() p = Profile() start = time.time() p.runcall(runapp) body = b''.join(response_body) elapsed = time.time() - start if self._profile_dir is not None: prof_filename = os.path.join(self._profile_dir, '%s.%s.%06dms.%d.prof' % ( environ['REQUEST_METHOD'], environ.get('PATH_INFO').strip( '/').replace('/', '.') or 'root', elapsed * 1000.0, time.time() )) p.dump_stats(prof_filename) else: stats = Stats(p, stream=self._stream) stats.sort_stats(*self._sort_by) self._stream.write('-' * 80) self._stream.write('\nPATH: %r\n' % environ.get('PATH_INFO')) stats.print_stats(*self._restrictions) self._stream.write('-' * 80 + '\n\n') return [body] def make_action(app_factory, hostname='localhost', port=5000, threaded=False, processes=1, stream=None, sort_by=('time', 'calls'), restrictions=()): """Return a new callback for :mod:`werkzeug.script` that starts a local server with the profiler enabled. :: from werkzeug.contrib import profiler action_profile = profiler.make_action(make_app) """ def action(hostname=('h', hostname), port=('p', port), threaded=threaded, processes=processes): """Start a new development server.""" from werkzeug.serving import run_simple app = ProfilerMiddleware(app_factory(), stream, sort_by, restrictions) run_simple(hostname, port, app, False, None, threaded, processes) return action
# vim: ft=python fileencoding=utf-8 sts=4 sw=4 et: # Copyright 2014-2016 Florian Bruhin (The Compiler) <mail@qutebrowser.org> # # This file is part of qutebrowser. # # qutebrowser is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # qutebrowser is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with qutebrowser. If not, see <http://www.gnu.org/licenses/>. """Tests for qutebrowser.misc.split.""" import collections import pytest from qutebrowser.misc import split # Most tests copied from Python's shlex. # The original test data set was from shellwords, by Hartmut Goebel. # Format: input/split|output|without|keep/split|output|with|keep/ test_data_str = r""" one two/one|two/one| two/ one "two three" four/one|two three|four/one| "two three"| four/ one 'two three' four/one|two three|four/one| 'two three'| four/ one "two\" three" four/one|two" three|four/one| "two\" three"| four/ one 'two'\'' three' four/one|two' three|four/one| 'two'\'' three'| four/ one "two three/one|two three/one| "two three/ one 'two three/one|two three/one| 'two three/ one\/one\/one\/ one "two\/one|two\/one| "two\/ one /one/one| / open -t i/open|-t|i/open| -t| i/ foo bar/foo|bar/foo| bar/ foo bar/foo|bar/ foo| bar/ foo bar /foo|bar/ foo| bar| / foo bar bla fasel/foo|bar|bla|fasel/foo| bar| bla| fasel/ x y z xxxx/x|y|z|xxxx/x| y| z| xxxx/ \x bar/x|bar/\x| bar/ \ x bar/ x|bar/\ x| bar/ \ bar/ bar/\ bar/ foo \x bar/foo|x|bar/foo| \x| bar/ foo \ x bar/foo| x|bar/foo| \ x| bar/ foo \ bar/foo| bar/foo| \ bar/ foo "bar" bla/foo|bar|bla/foo| "bar"| bla/ "foo" "bar" "bla"/foo|bar|bla/"foo"| "bar"| "bla"/ "foo" bar "bla"/foo|bar|bla/"foo"| bar| "bla"/ "foo" bar bla/foo|bar|bla/"foo"| bar| bla/ foo 'bar' bla/foo|bar|bla/foo| 'bar'| bla/ 'foo' 'bar' 'bla'/foo|bar|bla/'foo'| 'bar'| 'bla'/ 'foo' bar 'bla'/foo|bar|bla/'foo'| bar| 'bla'/ 'foo' bar bla/foo|bar|bla/'foo'| bar| bla/ blurb foo"bar"bar"fasel" baz/blurb|foobarbarfasel|baz/blurb| foo"bar"bar"fasel"| baz/ blurb foo'bar'bar'fasel' baz/blurb|foobarbarfasel|baz/blurb| foo'bar'bar'fasel'| baz/ ""//""/ ''//''/ foo "" bar/foo||bar/foo| ""| bar/ foo '' bar/foo||bar/foo| ''| bar/ foo "" "" "" bar/foo||||bar/foo| ""| ""| ""| bar/ foo '' '' '' bar/foo||||bar/foo| ''| ''| ''| bar/ \"/"/\"/ "\""/"/"\""/ "foo\ bar"/foo\ bar/"foo\ bar"/ "foo\\ bar"/foo\ bar/"foo\\ bar"/ "foo\\ bar\""/foo\ bar"/"foo\\ bar\""/ "foo\\" bar\"/foo\|bar"/"foo\\"| bar\"/ "foo\\ bar\" dfadf"/foo\ bar" dfadf/"foo\\ bar\" dfadf"/ "foo\\\ bar\" dfadf"/foo\\ bar" dfadf/"foo\\\ bar\" dfadf"/ "foo\\\x bar\" dfadf"/foo\\x bar" dfadf/"foo\\\x bar\" dfadf"/ "foo\x bar\" dfadf"/foo\x bar" dfadf/"foo\x bar\" dfadf"/ \'/'/\'/ 'foo\ bar'/foo\ bar/'foo\ bar'/ 'foo\\ bar'/foo\\ bar/'foo\\ bar'/ "foo\\\x bar\" df'a\ 'df"/foo\\x bar" df'a\ 'df/"foo\\\x bar\" df'a\ 'df"/ \"foo/"foo/\"foo/ \"foo\x/"foox/\"foo\x/ "foo\x"/foo\x/"foo\x"/ "foo\ "/foo\ /"foo\ "/ foo\ xx/foo xx/foo\ xx/ foo\ x\x/foo xx/foo\ x\x/ foo\ x\x\"/foo xx"/foo\ x\x\"/ "foo\ x\x"/foo\ x\x/"foo\ x\x"/ "foo\ x\x\\"/foo\ x\x\/"foo\ x\x\\"/ "foo\ x\x\\""foobar"/foo\ x\x\foobar/"foo\ x\x\\""foobar"/ "foo\ x\x\\"\'"foobar"/foo\ x\x\'foobar/"foo\ x\x\\"\'"foobar"/ "foo\ x\x\\"\'"fo'obar"/foo\ x\x\'fo'obar/"foo\ x\x\\"\'"fo'obar"/ "foo\ x\x\\"\'"fo'obar" 'don'\''t'/foo\ x\x\'fo'obar|don't/"foo\ x\x\\"\'"fo'obar"| 'don'\''t'/ "foo\ x\x\\"\'"fo'obar" 'don'\''t' \\/foo\ x\x\'fo'obar|don't|\/"foo\ x\x\\"\'"fo'obar"| 'don'\''t'| \\/ foo\ bar/foo bar/foo\ bar/ :-) ;-)/:-)|;-)/:-)| ;-)/ /// """ def _parse_split_test_data_str(): """ Parse the test data set into a namedtuple to use in tests. Returns: A list of namedtuples with str attributes: input, keep, no_keep """ tuple_class = collections.namedtuple('TestCase', 'input, keep, no_keep') for line in test_data_str.splitlines(): if not line: continue data = line.split('/') item = tuple_class(input=data[0], keep=data[1].split('|'), no_keep=data[2].split('|')) yield item yield tuple_class(input='', keep=[], no_keep=[]) class TestSplit: """Test split.""" @pytest.fixture(params=_parse_split_test_data_str(), ids=lambda e: e.input) def split_test_case(self, request): """Fixture to automatically parametrize all depending tests. It will use the test data from test_data_str, parsed using _parse_split_test_data_str(). """ return request.param def test_split(self, split_test_case): """Test splitting.""" items = split.split(split_test_case.input) assert items == split_test_case.keep def test_split_keep_original(self, split_test_case): """Test if splitting with keep=True yields the original string.""" items = split.split(split_test_case.input, keep=True) assert ''.join(items) == split_test_case.input def test_split_keep(self, split_test_case): """Test splitting with keep=True.""" items = split.split(split_test_case.input, keep=True) assert items == split_test_case.no_keep class TestSimpleSplit: """Test simple_split.""" TESTS = { ' foo bar': [' foo', ' bar'], 'foobar': ['foobar'], ' foo bar baz ': [' foo', ' bar', ' baz', ' '], 'f\ti\ts\th': ['f', '\ti', '\ts', '\th'], 'foo\nbar': ['foo', '\nbar'], } @pytest.mark.parametrize('test', TESTS, ids=repr) def test_str_split(self, test): """Test if the behavior matches str.split.""" assert split.simple_split(test) == test.rstrip().split() @pytest.mark.parametrize('s, maxsplit', [("foo bar baz", 1), (" foo bar baz ", 0)], ids=repr) def test_str_split_maxsplit(self, s, maxsplit): """Test if the behavior matches str.split with given maxsplit.""" actual = split.simple_split(s, maxsplit=maxsplit) expected = s.rstrip().split(maxsplit=maxsplit) assert actual == expected @pytest.mark.parametrize('test, expected', TESTS.items(), ids=repr) def test_split_keep(self, test, expected): """Test splitting with keep=True.""" assert split.simple_split(test, keep=True) == expected def test_maxsplit_0_keep(self): """Test special case with maxsplit=0 and keep=True.""" s = "foo bar" assert split.simple_split(s, keep=True, maxsplit=0) == [s]
""" Plugin for ResolveUrl Copyright (C) 2020 gujal This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see <http://www.gnu.org/licenses/>. """ from __resolve_generic__ import ResolveGeneric from lib import helpers class ProStreamResolver(ResolveGeneric): name = "prostream.to" domains = ['prostream.to'] pattern = r'(?://|\.)(prostream\.to)/(?:embed-)?([0-9a-zA-Z]+)' def get_media_url(self, host, media_id): return helpers.get_media_url(self.get_url(host, media_id), patterns=[r'''sources:\s*\["\s*(?P<url>[^"]+)'''], generic_patterns=False) def get_url(self, host, media_id): return self._default_get_url(host, media_id, template='https://{host}/embed-{media_id}.html')
# Copyright 2019 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== """Test configs for unpack.""" from __future__ import absolute_import from __future__ import division from __future__ import print_function import numpy as np import tensorflow as tf from tensorflow.lite.testing.zip_test_utils import create_tensor_data from tensorflow.lite.testing.zip_test_utils import make_zip_of_tests from tensorflow.lite.testing.zip_test_utils import register_make_test_function @register_make_test_function() def make_unpack_tests(options): """Make a set of tests to do unpack.""" test_parameters = [{ "base_shape": [[3, 4, 3], [3, 4], [5, 6, 7, 8]], "axis": [0, 1, 2, 3], }] def get_valid_axis(parameters): """Return a tweaked version of 'axis'.""" axis = parameters["axis"] shape = parameters["base_shape"][:] while axis > len(shape) - 1: axis -= 1 return axis def build_graph(parameters): input_tensor = tf.compat.v1.placeholder( dtype=tf.float32, name=("input"), shape=parameters["base_shape"]) outs = tf.unstack(input_tensor, axis=get_valid_axis(parameters)) return [input_tensor], [outs[0]] def build_inputs(parameters, sess, inputs, outputs): input_value = create_tensor_data(np.float32, shape=parameters["base_shape"]) return [input_value], sess.run( outputs, feed_dict=dict(zip(inputs, [input_value]))) make_zip_of_tests(options, test_parameters, build_graph, build_inputs)
"""<MyProject>, a CherryPy application. Use this as a base for creating new CherryPy applications. When you want to make a new app, copy and paste this folder to some other location (maybe site-packages) and rename it to the name of your project, then tweak as desired. Even before any tweaking, this should serve a few demonstration pages. Change to this directory and run: ../cherryd -c site.conf """ import cherrypy from cherrypy import tools, url import os local_dir = os.path.join(os.getcwd(), os.path.dirname(__file__)) class Root: _cp_config = {'tools.log_tracebacks.on': True, } def index(self): return """<html> <body>Try some <a href='%s?a=7'>other</a> path, or a <a href='%s?n=14'>default</a> path.<br /> Or, just look at the pretty picture:<br /> <img src='%s' /> </body></html>""" % (url("other"), url("else"), url("files/made_with_cherrypy_small.png")) index.exposed = True def default(self, *args, **kwargs): return "args: %s kwargs: %s" % (args, kwargs) default.exposed = True def other(self, a=2, b='bananas', c=None): cherrypy.response.headers['Content-Type'] = 'text/plain' if c is None: return "Have %d %s." % (int(a), b) else: return "Have %d %s, %s." % (int(a), b, c) other.exposed = True files = cherrypy.tools.staticdir.handler( section="/files", dir=os.path.join(local_dir, "static"), # Ignore .php files, etc. match=r'\.(css|gif|html?|ico|jpe?g|js|png|swf|xml)$', ) root = Root() # Uncomment the following to use your own favicon instead of CP's default. #favicon_path = os.path.join(local_dir, "favicon.ico") #root.favicon_ico = tools.staticfile.handler(filename=favicon_path)
"""Support for switch sensor using I2C MCP23017 chip.""" from adafruit_mcp230xx.mcp23017 import MCP23017 # pylint: disable=import-error import board # pylint: disable=import-error import busio # pylint: disable=import-error import digitalio # pylint: disable=import-error import voluptuous as vol from homeassistant.components.switch import PLATFORM_SCHEMA from homeassistant.const import DEVICE_DEFAULT_NAME import homeassistant.helpers.config_validation as cv from homeassistant.helpers.entity import ToggleEntity CONF_INVERT_LOGIC = "invert_logic" CONF_I2C_ADDRESS = "i2c_address" CONF_PINS = "pins" CONF_PULL_MODE = "pull_mode" DEFAULT_INVERT_LOGIC = False DEFAULT_I2C_ADDRESS = 0x20 _SWITCHES_SCHEMA = vol.Schema({cv.positive_int: cv.string}) PLATFORM_SCHEMA = PLATFORM_SCHEMA.extend( { vol.Required(CONF_PINS): _SWITCHES_SCHEMA, vol.Optional(CONF_INVERT_LOGIC, default=DEFAULT_INVERT_LOGIC): cv.boolean, vol.Optional(CONF_I2C_ADDRESS, default=DEFAULT_I2C_ADDRESS): vol.Coerce(int), } ) def setup_platform(hass, config, add_entities, discovery_info=None): """Set up the MCP23017 devices.""" invert_logic = config.get(CONF_INVERT_LOGIC) i2c_address = config.get(CONF_I2C_ADDRESS) i2c = busio.I2C(board.SCL, board.SDA) mcp = MCP23017(i2c, address=i2c_address) switches = [] pins = config.get(CONF_PINS) for pin_num, pin_name in pins.items(): pin = mcp.get_pin(pin_num) switches.append(MCP23017Switch(pin_name, pin, invert_logic)) add_entities(switches) class MCP23017Switch(ToggleEntity): """Representation of a MCP23017 output pin.""" def __init__(self, name, pin, invert_logic): """Initialize the pin.""" self._name = name or DEVICE_DEFAULT_NAME self._pin = pin self._invert_logic = invert_logic self._state = False self._pin.direction = digitalio.Direction.OUTPUT self._pin.value = self._invert_logic @property def name(self): """Return the name of the switch.""" return self._name @property def should_poll(self): """No polling needed.""" return False @property def is_on(self): """Return true if device is on.""" return self._state @property def assumed_state(self): """Return true if optimistic updates are used.""" return True def turn_on(self, **kwargs): """Turn the device on.""" self._pin.value = not self._invert_logic self._state = True self.schedule_update_ha_state() def turn_off(self, **kwargs): """Turn the device off.""" self._pin.value = self._invert_logic self._state = False self.schedule_update_ha_state()
from django.db.models.manager import Manager from django.contrib.gis.db.models.query import GeoQuerySet class GeoManager(Manager): "Overrides Manager to return Geographic QuerySets." # This manager should be used for queries on related fields # so that geometry columns on Oracle and MySQL are selected # properly. use_for_related_fields = True def get_query_set(self): return GeoQuerySet(self.model, using=self._db) def area(self, *args, **kwargs): return self.get_query_set().area(*args, **kwargs) def centroid(self, *args, **kwargs): return self.get_query_set().centroid(*args, **kwargs) def collect(self, *args, **kwargs): return self.get_query_set().collect(*args, **kwargs) def difference(self, *args, **kwargs): return self.get_query_set().difference(*args, **kwargs) def distance(self, *args, **kwargs): return self.get_query_set().distance(*args, **kwargs) def envelope(self, *args, **kwargs): return self.get_query_set().envelope(*args, **kwargs) def extent(self, *args, **kwargs): return self.get_query_set().extent(*args, **kwargs) def extent3d(self, *args, **kwargs): return self.get_query_set().extent3d(*args, **kwargs) def force_rhr(self, *args, **kwargs): return self.get_query_set().force_rhr(*args, **kwargs) def geohash(self, *args, **kwargs): return self.get_query_set().geohash(*args, **kwargs) def geojson(self, *args, **kwargs): return self.get_query_set().geojson(*args, **kwargs) def gml(self, *args, **kwargs): return self.get_query_set().gml(*args, **kwargs) def intersection(self, *args, **kwargs): return self.get_query_set().intersection(*args, **kwargs) def kml(self, *args, **kwargs): return self.get_query_set().kml(*args, **kwargs) def length(self, *args, **kwargs): return self.get_query_set().length(*args, **kwargs) def make_line(self, *args, **kwargs): return self.get_query_set().make_line(*args, **kwargs) def mem_size(self, *args, **kwargs): return self.get_query_set().mem_size(*args, **kwargs) def num_geom(self, *args, **kwargs): return self.get_query_set().num_geom(*args, **kwargs) def num_points(self, *args, **kwargs): return self.get_query_set().num_points(*args, **kwargs) def perimeter(self, *args, **kwargs): return self.get_query_set().perimeter(*args, **kwargs) def point_on_surface(self, *args, **kwargs): return self.get_query_set().point_on_surface(*args, **kwargs) def reverse_geom(self, *args, **kwargs): return self.get_query_set().reverse_geom(*args, **kwargs) def scale(self, *args, **kwargs): return self.get_query_set().scale(*args, **kwargs) def snap_to_grid(self, *args, **kwargs): return self.get_query_set().snap_to_grid(*args, **kwargs) def svg(self, *args, **kwargs): return self.get_query_set().svg(*args, **kwargs) def sym_difference(self, *args, **kwargs): return self.get_query_set().sym_difference(*args, **kwargs) def transform(self, *args, **kwargs): return self.get_query_set().transform(*args, **kwargs) def translate(self, *args, **kwargs): return self.get_query_set().translate(*args, **kwargs) def union(self, *args, **kwargs): return self.get_query_set().union(*args, **kwargs) def unionagg(self, *args, **kwargs): return self.get_query_set().unionagg(*args, **kwargs)
# Copyright 2012 NEC Corporation # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. """ Views for managing Neutron Networks. """ from django.core.urlresolvers import reverse from django.core.urlresolvers import reverse_lazy from django.utils.translation import ugettext_lazy as _ from horizon import exceptions from horizon import forms from horizon import tables from horizon.utils import memoized from horizon import workflows from openstack_dashboard import api from openstack_dashboard.dashboards.project.networks \ import forms as project_forms from openstack_dashboard.dashboards.project.networks.ports \ import tables as port_tables from openstack_dashboard.dashboards.project.networks.subnets \ import tables as subnet_tables from openstack_dashboard.dashboards.project.networks \ import tables as project_tables from openstack_dashboard.dashboards.project.networks \ import workflows as project_workflows class IndexView(tables.DataTableView): table_class = project_tables.NetworksTable template_name = 'project/networks/index.html' page_title = _("Networks") def get_data(self): try: tenant_id = self.request.user.tenant_id networks = api.neutron.network_list_for_tenant(self.request, tenant_id) except Exception: networks = [] msg = _('Network list can not be retrieved.') exceptions.handle(self.request, msg) return networks class CreateView(workflows.WorkflowView): workflow_class = project_workflows.CreateNetwork ajax_template_name = 'project/networks/create.html' class UpdateView(forms.ModalFormView): context_object_name = 'network' form_class = project_forms.UpdateNetwork form_id = "update_network_form" modal_header = _("Edit Network") submit_label = _("Save Changes") submit_url = "horizon:project:networks:update" success_url = reverse_lazy("horizon:project:networks:index") template_name = 'project/networks/update.html' page_title = _("Update Network") def get_context_data(self, **kwargs): context = super(UpdateView, self).get_context_data(**kwargs) args = (self.kwargs['network_id'],) context["network_id"] = self.kwargs['network_id'] context["submit_url"] = reverse(self.submit_url, args=args) return context @memoized.memoized_method def _get_object(self, *args, **kwargs): network_id = self.kwargs['network_id'] try: return api.neutron.network_get(self.request, network_id) except Exception: redirect = self.success_url msg = _('Unable to retrieve network details.') exceptions.handle(self.request, msg, redirect=redirect) def get_initial(self): network = self._get_object() return {'network_id': network['id'], 'tenant_id': network['tenant_id'], 'name': network['name'], 'admin_state': network['admin_state_up']} class DetailView(tables.MultiTableView): table_classes = (subnet_tables.SubnetsTable, port_tables.PortsTable) template_name = 'project/networks/detail.html' page_title = _("Network Details: {{ network.name }}") def get_subnets_data(self): try: network = self._get_data() subnets = api.neutron.subnet_list(self.request, network_id=network.id) except Exception: subnets = [] msg = _('Subnet list can not be retrieved.') exceptions.handle(self.request, msg) return subnets def get_ports_data(self): try: network_id = self.kwargs['network_id'] ports = api.neutron.port_list(self.request, network_id=network_id) except Exception: ports = [] msg = _('Port list can not be retrieved.') exceptions.handle(self.request, msg) return ports @memoized.memoized_method def _get_data(self): try: network_id = self.kwargs['network_id'] network = api.neutron.network_get(self.request, network_id) network.set_id_as_name_if_empty(length=0) except Exception: msg = _('Unable to retrieve details for network "%s".') \ % (network_id) exceptions.handle(self.request, msg, redirect=self.get_redirect_url()) return network def get_context_data(self, **kwargs): context = super(DetailView, self).get_context_data(**kwargs) network = self._get_data() context["network"] = network table = project_tables.NetworksTable(self.request) context["url"] = self.get_redirect_url() context["actions"] = table.render_row_actions(network) return context @staticmethod def get_redirect_url(): return reverse_lazy('horizon:project:networks:index')
import re import os import kirk import numpy as n import pickle def trimmean(arr, percent): n = len(arr) k = int(round(n*(float(percent)/100)/2)) return n.mean(arr[k+1:n-k]) File = kirk.File width = kirk.width height = kirk.height box_size = kirk.box_size #Dictionary data structure to hold out parsed data #For each MAC address there is a multidimensional array of size [x][y] #In each of those arrays is a list of RSSI values found at that location rssi = {} #Loop through every file in our data directory and extract data into rssi for filename in os.listdir('./fingerprint'): data = re.split('_',filename) x = int(data[0]) y = int(data[1]) f = open('./fingerprint/'+filename) for line in f: read = line.split() if len(read)==3 and read[0] == read[1]: mac = read[0] if read[2] != '': strength = int(read[2].strip()) if mac in rssi: rssi[mac][x][y].append(strength) else: if mac != "48:5a:3f:45:21:0f": #Filter out my cellphone arr = [[[] for _ in range(kirk.x)] for _ in range(kirk.y)] rssi.update({mac:arr}) rssi[mac][x][y].append(strength) #Now that we have the data, calculate averages for each location fingerprint = {} for mac in rssi: avg = [[None for _ in range(kirk.x)] for _ in range(kirk.y)] for x in range(len(rssi[mac])): for y in range(len(rssi[mac][x])): l = rssi[mac][x][y] if len(l) > 0: avg[x][y] = n.mean(l) #avg[x][y] = trimmean(l, 80) fingerprint.update({mac:avg}) finger_file = open(r'fingerprint.pkl', 'wb') pickle.dump(fingerprint, finger_file) finger_file.close()
"""Timeseries plotting functions.""" from __future__ import division import numpy as np import pandas as pd from scipy import stats, interpolate import matplotlib as mpl import matplotlib.pyplot as plt from .external.six import string_types from . import utils from . import algorithms as algo from .palettes import color_palette def tsplot(data, time=None, unit=None, condition=None, value=None, err_style="ci_band", ci=68, interpolate=True, color=None, estimator=np.mean, n_boot=5000, err_palette=None, err_kws=None, legend=True, ax=None, **kwargs): """Plot one or more timeseries with flexible representation of uncertainty. This function can take data specified either as a long-form (tidy) DataFrame or as an ndarray with dimensions for sampling unit, time, and (optionally) condition. The interpretation of some of the other parameters changes depending on the type of object passed as data. Parameters ---------- data : DataFrame or ndarray Data for the plot. Should either be a "long form" dataframe or an array with dimensions (unit, time, condition). In both cases, the condition field/dimension is optional. The type of this argument determines the interpretation of the next few parameters. time : string or series-like Either the name of the field corresponding to time in the data DataFrame or x values for a plot when data is an array. If a Series, the name will be used to label the x axis. unit : string Field in the data DataFrame identifying the sampling unit (e.g. subject, neuron, etc.). The error representation will collapse over units at each time/condition observation. This has no role when data is an array. value : string Either the name of the field corresponding to the data values in the data DataFrame (i.e. the y coordinate) or a string that forms the y axis label when data is an array. condition : string or Series-like Either the name of the field identifying the condition an observation falls under in the data DataFrame, or a sequence of names with a length equal to the size of the third dimension of data. There will be a separate trace plotted for each condition. If condition is a Series with a name attribute, the name will form the title for the plot legend (unless legend is set to False). err_style : string or list of strings or None Names of ways to plot uncertainty across units from set of {ci_band, ci_bars, boot_traces, boot_kde, unit_traces, unit_points}. Can use one or more than one method. ci : float or list of floats in [0, 100] Confidence interaval size(s). If a list, it will stack the error plots for each confidence interval. Only relevant for error styles with "ci" in the name. interpolate : boolean Whether to do a linear interpolation between each timepoint when plotting. The value of this parameter also determines the marker used for the main plot traces, unless marker is specified as a keyword argument. color : seaborn palette or matplotlib color name or dictionary Palette or color for the main plots and error representation (unless plotting by unit, which can be separately controlled with err_palette). If a dictionary, should map condition name to color spec. estimator : callable Function to determine central tendency and to pass to bootstrap must take an ``axis`` argument. n_boot : int Number of bootstrap iterations. err_palette: seaborn palette Palette name or list of colors used when plotting data for each unit. err_kws : dict, optional Keyword argument dictionary passed through to matplotlib function generating the error plot, ax : axis object, optional Plot in given axis; if None creates a new figure kwargs : Other keyword arguments are passed to main plot() call Returns ------- ax : matplotlib axis axis with plot data """ # Sort out default values for the parameters if ax is None: ax = plt.gca() if err_kws is None: err_kws = {} # Handle different types of input data if isinstance(data, pd.DataFrame): xlabel = time ylabel = value # Condition is optional if condition is None: condition = pd.Series(np.ones(len(data))) legend = False legend_name = None n_cond = 1 else: legend = True and legend legend_name = condition n_cond = len(data[condition].unique()) else: data = np.asarray(data) # Data can be a timecourse from a single unit or # several observations in one condition if data.ndim == 1: data = data[np.newaxis, :, np.newaxis] elif data.ndim == 2: data = data[:, :, np.newaxis] n_unit, n_time, n_cond = data.shape # Units are experimental observations. Maybe subjects, or neurons if unit is None: units = np.arange(n_unit) unit = "unit" units = np.repeat(units, n_time * n_cond) ylabel = None # Time forms the xaxis of the plot if time is None: times = np.arange(n_time) else: times = np.asarray(time) xlabel = None if hasattr(time, "name"): xlabel = time.name time = "time" times = np.tile(np.repeat(times, n_cond), n_unit) # Conditions split the timeseries plots if condition is None: conds = range(n_cond) legend = False if isinstance(color, dict): err = "Must have condition names if using color dict." raise ValueError(err) else: conds = np.asarray(condition) legend = True and legend if hasattr(condition, "name"): legend_name = condition.name else: legend_name = None condition = "cond" conds = np.tile(conds, n_unit * n_time) # Value forms the y value in the plot if value is None: ylabel = None else: ylabel = value value = "value" # Convert to long-form DataFrame data = pd.DataFrame(dict(value=data.ravel(), time=times, unit=units, cond=conds)) # Set up the err_style and ci arguments for the loop below if isinstance(err_style, string_types): err_style = [err_style] elif err_style is None: err_style = [] if not hasattr(ci, "__iter__"): ci = [ci] # Set up the color palette if color is None: current_palette = mpl.rcParams["axes.color_cycle"] if len(current_palette) < n_cond: colors = color_palette("husl", n_cond) else: colors = color_palette(n_colors=n_cond) elif isinstance(color, dict): colors = [color[c] for c in data[condition].unique()] else: try: colors = color_palette(color, n_cond) except ValueError: color = mpl.colors.colorConverter.to_rgb(color) colors = [color] * n_cond # Do a groupby with condition and plot each trace for c, (cond, df_c) in enumerate(data.groupby(condition, sort=False)): df_c = df_c.pivot(unit, time, value) x = df_c.columns.values.astype(np.float) # Bootstrap the data for confidence intervals boot_data = algo.bootstrap(df_c.values, n_boot=n_boot, axis=0, func=estimator) cis = [utils.ci(boot_data, v, axis=0) for v in ci] central_data = estimator(df_c.values, axis=0) # Get the color for this condition color = colors[c] # Use subroutines to plot the uncertainty for style in err_style: # Allow for null style (only plot central tendency) if style is None: continue # Grab the function from the global environment try: plot_func = globals()["_plot_%s" % style] except KeyError: raise ValueError("%s is not a valid err_style" % style) # Possibly set up to plot each observation in a different color if err_palette is not None and "unit" in style: orig_color = color color = color_palette(err_palette, len(df_c.values)) # Pass all parameters to the error plotter as keyword args plot_kwargs = dict(ax=ax, x=x, data=df_c.values, boot_data=boot_data, central_data=central_data, color=color, err_kws=err_kws) # Plot the error representation, possibly for multiple cis for ci_i in cis: plot_kwargs["ci"] = ci_i plot_func(**plot_kwargs) if err_palette is not None and "unit" in style: color = orig_color # Plot the central trace kwargs.setdefault("marker", "" if interpolate else "o") ls = kwargs.pop("ls", "-" if interpolate else "") kwargs.setdefault("linestyle", ls) label = cond if legend else "_nolegend_" ax.plot(x, central_data, color=color, label=label, **kwargs) # Pad the sides of the plot only when not interpolating ax.set_xlim(x.min(), x.max()) x_diff = x[1] - x[0] if not interpolate: ax.set_xlim(x.min() - x_diff, x.max() + x_diff) # Add the plot labels if xlabel is not None: ax.set_xlabel(xlabel) if ylabel is not None: ax.set_ylabel(ylabel) if legend: ax.legend(loc=0, title=legend_name) return ax # Subroutines for tsplot errorbar plotting # ---------------------------------------- def _plot_ci_band(ax, x, ci, color, err_kws, **kwargs): """Plot translucent error bands around the central tendancy.""" low, high = ci if "alpha" not in err_kws: err_kws["alpha"] = 0.2 ax.fill_between(x, low, high, color=color, **err_kws) def _plot_ci_bars(ax, x, central_data, ci, color, err_kws, **kwargs): """Plot error bars at each data point.""" for x_i, y_i, (low, high) in zip(x, central_data, ci.T): ax.plot([x_i, x_i], [low, high], color=color, solid_capstyle="round", **err_kws) def _plot_boot_traces(ax, x, boot_data, color, err_kws, **kwargs): """Plot 250 traces from bootstrap.""" err_kws.setdefault("alpha", 0.25) err_kws.setdefault("linewidth", 0.25) if "lw" in err_kws: err_kws["linewidth"] = err_kws.pop("lw") ax.plot(x, boot_data.T, color=color, label="_nolegend_", **err_kws) def _plot_unit_traces(ax, x, data, ci, color, err_kws, **kwargs): """Plot a trace for each observation in the original data.""" if isinstance(color, list): if "alpha" not in err_kws: err_kws["alpha"] = .5 for i, obs in enumerate(data): ax.plot(x, obs, color=color[i], label="_nolegend_", **err_kws) else: if "alpha" not in err_kws: err_kws["alpha"] = .2 ax.plot(x, data.T, color=color, label="_nolegend_", **err_kws) def _plot_unit_points(ax, x, data, color, err_kws, **kwargs): """Plot each original data point discretely.""" if isinstance(color, list): for i, obs in enumerate(data): ax.plot(x, obs, "o", color=color[i], alpha=0.8, markersize=4, label="_nolegend_", **err_kws) else: ax.plot(x, data.T, "o", color=color, alpha=0.5, markersize=4, label="_nolegend_", **err_kws) def _plot_boot_kde(ax, x, boot_data, color, **kwargs): """Plot the kernal density estimate of the bootstrap distribution.""" kwargs.pop("data") _ts_kde(ax, x, boot_data, color, **kwargs) def _plot_unit_kde(ax, x, data, color, **kwargs): """Plot the kernal density estimate over the sample.""" _ts_kde(ax, x, data, color, **kwargs) def _ts_kde(ax, x, data, color, **kwargs): """Upsample over time and plot a KDE of the bootstrap distribution.""" kde_data = [] y_min, y_max = data.min(), data.max() y_vals = np.linspace(y_min, y_max, 100) upsampler = interpolate.interp1d(x, data) data_upsample = upsampler(np.linspace(x.min(), x.max(), 100)) for pt_data in data_upsample.T: pt_kde = stats.kde.gaussian_kde(pt_data) kde_data.append(pt_kde(y_vals)) kde_data = np.transpose(kde_data) rgb = mpl.colors.ColorConverter().to_rgb(color) img = np.zeros((kde_data.shape[0], kde_data.shape[1], 4)) img[:, :, :3] = rgb kde_data /= kde_data.max(axis=0) kde_data[kde_data > 1] = 1 img[:, :, 3] = kde_data ax.imshow(img, interpolation="spline16", zorder=2, extent=(x.min(), x.max(), y_min, y_max), aspect="auto", origin="lower")
# -*- encoding: utf-8 -*- ############################################################################## # # OpenERP, Open Source Management Solution # # Copyright (c) 2011 Noviat nv/sa (www.noviat.be). All rights reserved. # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## import re, time, random from openerp import api from openerp.osv import fields, osv from openerp.tools.translate import _ import logging _logger = logging.getLogger(__name__) """ account.invoice object: - Add support for Belgian structured communication - Rename 'reference' field labels to 'Communication' """ class account_invoice(osv.osv): _inherit = 'account.invoice' @api.cr_uid_context def _get_reference_type(self, cursor, user, context=None): """Add BBA Structured Communication Type and change labels from 'reference' into 'communication' """ res = super(account_invoice, self)._get_reference_type(cursor, user, context=context) res[[i for i,x in enumerate(res) if x[0] == 'none'][0]] = ('none', 'Free Communication') res.append(('bba', 'BBA Structured Communication')) #l_logger.warning('reference_type = %s' %res ) return res def check_bbacomm(self, val): supported_chars = '0-9+*/ ' pattern = re.compile('[^' + supported_chars + ']') if pattern.findall(val or ''): return False bbacomm = re.sub('\D', '', val or '') if len(bbacomm) == 12: base = int(bbacomm[:10]) mod = base % 97 or 97 if mod == int(bbacomm[-2:]): return True return False def _check_communication(self, cr, uid, ids): for inv in self.browse(cr, uid, ids): if inv.reference_type == 'bba': return self.check_bbacomm(inv.reference) return True def onchange_partner_id(self, cr, uid, ids, type, partner_id, date_invoice=False, payment_term=False, partner_bank_id=False, company_id=False, context=None): result = super(account_invoice, self).onchange_partner_id(cr, uid, ids, type, partner_id, date_invoice, payment_term, partner_bank_id, company_id, context) # reference_type = self.default_get(cr, uid, ['reference_type'])['reference_type'] # _logger.warning('partner_id %s' % partner_id) reference = False reference_type = 'none' if partner_id: if (type == 'out_invoice'): reference_type = self.pool.get('res.partner').browse(cr, uid, partner_id, context=context).out_inv_comm_type if reference_type: reference = self.generate_bbacomm(cr, uid, ids, type, reference_type, partner_id, '', context=context)['value']['reference'] res_update = { 'reference_type': reference_type or 'none', 'reference': reference, } result['value'].update(res_update) return result def generate_bbacomm(self, cr, uid, ids, type, reference_type, partner_id, reference, context=None): partner_obj = self.pool.get('res.partner') reference = reference or '' algorithm = False if partner_id: algorithm = partner_obj.browse(cr, uid, partner_id, context=context).out_inv_comm_algorithm algorithm = algorithm or 'random' if (type == 'out_invoice'): if reference_type == 'bba': if algorithm == 'date': if not self.check_bbacomm(reference): doy = time.strftime('%j') year = time.strftime('%Y') seq = '001' seq_ids = self.search(cr, uid, [('type', '=', 'out_invoice'), ('reference_type', '=', 'bba'), ('reference', 'like', '+++%s/%s/%%' % (doy, year))], order='reference') if seq_ids: prev_seq = int(self.browse(cr, uid, seq_ids[-1]).reference[12:15]) if prev_seq < 999: seq = '%03d' % (prev_seq + 1) else: raise osv.except_osv(_('Warning!'), _('The daily maximum of outgoing invoices with an automatically generated BBA Structured Communications has been exceeded!' \ '\nPlease create manually a unique BBA Structured Communication.')) bbacomm = doy + year + seq base = int(bbacomm) mod = base % 97 or 97 reference = '+++%s/%s/%s%02d+++' % (doy, year, seq, mod) elif algorithm == 'partner_ref': if not self.check_bbacomm(reference): partner_ref = self.pool.get('res.partner').browse(cr, uid, partner_id).ref partner_ref_nr = re.sub('\D', '', partner_ref or '') if (len(partner_ref_nr) < 3) or (len(partner_ref_nr) > 7): raise osv.except_osv(_('Warning!'), _('The Partner should have a 3-7 digit Reference Number for the generation of BBA Structured Communications!' \ '\nPlease correct the Partner record.')) else: partner_ref_nr = partner_ref_nr.ljust(7, '0') seq = '001' seq_ids = self.search(cr, uid, [('type', '=', 'out_invoice'), ('reference_type', '=', 'bba'), ('reference', 'like', '+++%s/%s/%%' % (partner_ref_nr[:3], partner_ref_nr[3:]))], order='reference') if seq_ids: prev_seq = int(self.browse(cr, uid, seq_ids[-1]).reference[12:15]) if prev_seq < 999: seq = '%03d' % (prev_seq + 1) else: raise osv.except_osv(_('Warning!'), _('The daily maximum of outgoing invoices with an automatically generated BBA Structured Communications has been exceeded!' \ '\nPlease create manually a unique BBA Structured Communication.')) bbacomm = partner_ref_nr + seq base = int(bbacomm) mod = base % 97 or 97 reference = '+++%s/%s/%s%02d+++' % (partner_ref_nr[:3], partner_ref_nr[3:], seq, mod) elif algorithm == 'random': if not self.check_bbacomm(reference): base = random.randint(1, 9999999999) bbacomm = str(base).rjust(10, '0') base = int(bbacomm) mod = base % 97 or 97 mod = str(mod).rjust(2, '0') reference = '+++%s/%s/%s%s+++' % (bbacomm[:3], bbacomm[3:7], bbacomm[7:], mod) else: raise osv.except_osv(_('Error!'), _("Unsupported Structured Communication Type Algorithm '%s' !" \ "\nPlease contact your OpenERP support channel.") % algorithm) return {'value': {'reference': reference}} def create(self, cr, uid, vals, context=None): reference = vals.get('reference', False) reference_type = vals.get('reference_type', False) if vals.get('type') == 'out_invoice' and not reference_type: # fallback on default communication type for partner reference_type = self.pool.get('res.partner').browse(cr, uid, vals['partner_id']).out_inv_comm_type if reference_type == 'bba': reference = self.generate_bbacomm(cr, uid, [], vals['type'], reference_type, vals['partner_id'], '', context={})['value']['reference'] vals.update({ 'reference_type': reference_type or 'none', 'reference': reference, }) if reference_type == 'bba': if not reference: raise osv.except_osv(_('Warning!'), _('Empty BBA Structured Communication!' \ '\nPlease fill in a unique BBA Structured Communication.')) if self.check_bbacomm(reference): reference = re.sub('\D', '', reference) vals['reference'] = '+++' + reference[0:3] + '/' + reference[3:7] + '/' + reference[7:] + '+++' same_ids = self.search(cr, uid, [('type', '=', 'out_invoice'), ('reference_type', '=', 'bba'), ('reference', '=', vals['reference'])]) if same_ids: raise osv.except_osv(_('Warning!'), _('The BBA Structured Communication has already been used!' \ '\nPlease create manually a unique BBA Structured Communication.')) return super(account_invoice, self).create(cr, uid, vals, context=context) def write(self, cr, uid, ids, vals, context=None): if isinstance(ids, (int, long)): ids = [ids] for inv in self.browse(cr, uid, ids, context): if vals.has_key('reference_type'): reference_type = vals['reference_type'] else: reference_type = inv.reference_type or '' if reference_type == 'bba': if vals.has_key('reference'): bbacomm = vals['reference'] else: bbacomm = inv.reference or '' if self.check_bbacomm(bbacomm): reference = re.sub('\D', '', bbacomm) vals['reference'] = '+++' + reference[0:3] + '/' + reference[3:7] + '/' + reference[7:] + '+++' same_ids = self.search(cr, uid, [('id', '!=', inv.id), ('type', '=', 'out_invoice'), ('reference_type', '=', 'bba'), ('reference', '=', vals['reference'])]) if same_ids: raise osv.except_osv(_('Warning!'), _('The BBA Structured Communication has already been used!' \ '\nPlease create manually a unique BBA Structured Communication.')) return super(account_invoice, self).write(cr, uid, ids, vals, context) def copy(self, cr, uid, id, default=None, context=None): default = default or {} invoice = self.browse(cr, uid, id, context=context) if invoice.type in ['out_invoice']: reference_type = invoice.reference_type or 'none' default['reference_type'] = reference_type if reference_type == 'bba': partner = invoice.partner_id default['reference'] = self.generate_bbacomm(cr, uid, id, invoice.type, reference_type, partner.id, '', context=context)['value']['reference'] return super(account_invoice, self).copy(cr, uid, id, default, context=context) _columns = { 'reference': fields.char('Communication', help="The partner reference of this invoice."), 'reference_type': fields.selection(_get_reference_type, 'Communication Type', required=True), } _constraints = [ (_check_communication, 'Invalid BBA Structured Communication !', ['Communication']), ] account_invoice() # vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
""" Test functions for linalg module """ from __future__ import division, absolute_import, print_function import numpy as np from numpy import linalg, arange, float64, array, dot, transpose from numpy.testing import ( TestCase, run_module_suite, assert_equal, assert_array_equal, assert_array_almost_equal, assert_array_less ) rlevel = 1 class TestRegression(TestCase): def test_eig_build(self, level=rlevel): # Ticket #652 rva = array([1.03221168e+02 + 0.j, -1.91843603e+01 + 0.j, -6.04004526e-01 + 15.84422474j, -6.04004526e-01 - 15.84422474j, -1.13692929e+01 + 0.j, -6.57612485e-01 + 10.41755503j, -6.57612485e-01 - 10.41755503j, 1.82126812e+01 + 0.j, 1.06011014e+01 + 0.j, 7.80732773e+00 + 0.j, -7.65390898e-01 + 0.j, 1.51971555e-15 + 0.j, -1.51308713e-15 + 0.j]) a = arange(13 * 13, dtype=float64) a.shape = (13, 13) a = a % 17 va, ve = linalg.eig(a) va.sort() rva.sort() assert_array_almost_equal(va, rva) def test_eigh_build(self, level=rlevel): # Ticket 662. rvals = [68.60568999, 89.57756725, 106.67185574] cov = array([[77.70273908, 3.51489954, 15.64602427], [3.51489954, 88.97013878, -1.07431931], [15.64602427, -1.07431931, 98.18223512]]) vals, vecs = linalg.eigh(cov) assert_array_almost_equal(vals, rvals) def test_svd_build(self, level=rlevel): # Ticket 627. a = array([[0., 1.], [1., 1.], [2., 1.], [3., 1.]]) m, n = a.shape u, s, vh = linalg.svd(a) b = dot(transpose(u[:, n:]), a) assert_array_almost_equal(b, np.zeros((2, 2))) def test_norm_vector_badarg(self): # Regression for #786: Froebenius norm for vectors raises # TypeError. self.assertRaises(ValueError, linalg.norm, array([1., 2., 3.]), 'fro') def test_lapack_endian(self): # For bug #1482 a = array([[5.7998084, -2.1825367], [-2.1825367, 9.85910595]], dtype='>f8') b = array(a, dtype='<f8') ap = linalg.cholesky(a) bp = linalg.cholesky(b) assert_array_equal(ap, bp) def test_large_svd_32bit(self): # See gh-4442, 64bit would require very large/slow matrices. x = np.eye(1000, 66) np.linalg.svd(x) def test_svd_no_uv(self): # gh-4733 for shape in (3, 4), (4, 4), (4, 3): for t in float, complex: a = np.ones(shape, dtype=t) w = linalg.svd(a, compute_uv=False) c = np.count_nonzero(np.absolute(w) > 0.5) assert_equal(c, 1) assert_equal(np.linalg.matrix_rank(a), 1) assert_array_less(1, np.linalg.norm(a, ord=2)) if __name__ == '__main__': run_module_suite()
#!/usr/bin/env python # # Copyright 2015 Google Inc. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. """Exceptions for generated client libraries.""" class Error(Exception): """Base class for all exceptions.""" class TypecheckError(Error, TypeError): """An object of an incorrect type is provided.""" class NotFoundError(Error): """A specified resource could not be found.""" class UserError(Error): """Base class for errors related to user input.""" class InvalidDataError(Error): """Base class for any invalid data error.""" class CommunicationError(Error): """Any communication error talking to an API server.""" class HttpError(CommunicationError): """Error making a request. Soon to be HttpError.""" def __init__(self, response, content, url, method_config=None, request=None): super(HttpError, self).__init__() self.response = response self.content = content self.url = url self.method_config = method_config self.request = request def __str__(self): content = self.content if isinstance(content, bytes): content = self.content.decode('ascii', 'replace') return 'HttpError accessing <%s>: response: <%s>, content <%s>' % ( self.url, self.response, content) @property def status_code(self): # TODO(craigcitro): Turn this into something better than a # KeyError if there is no status. return int(self.response['status']) @classmethod def FromResponse(cls, http_response): return cls(http_response.info, http_response.content, http_response.request_url) class InvalidUserInputError(InvalidDataError): """User-provided input is invalid.""" class InvalidDataFromServerError(InvalidDataError, CommunicationError): """Data received from the server is malformed.""" class BatchError(Error): """Error generated while constructing a batch request.""" class ConfigurationError(Error): """Base class for configuration errors.""" class GeneratedClientError(Error): """The generated client configuration is invalid.""" class ConfigurationValueError(UserError): """Some part of the user-specified client configuration is invalid.""" class ResourceUnavailableError(Error): """User requested an unavailable resource.""" class CredentialsError(Error): """Errors related to invalid credentials.""" class TransferError(CommunicationError): """Errors related to transfers.""" class TransferRetryError(TransferError): """Retryable errors related to transfers.""" class TransferInvalidError(TransferError): """The given transfer is invalid.""" class RequestError(CommunicationError): """The request was not successful.""" class RetryAfterError(HttpError): """The response contained a retry-after header.""" def __init__(self, response, content, url, retry_after): super(RetryAfterError, self).__init__(response, content, url) self.retry_after = int(retry_after) @classmethod def FromResponse(cls, http_response): return cls(http_response.info, http_response.content, http_response.request_url, http_response.retry_after) class BadStatusCodeError(HttpError): """The request completed but returned a bad status code.""" class NotYetImplementedError(GeneratedClientError): """This functionality is not yet implemented.""" class StreamExhausted(Error): """Attempted to read more bytes from a stream than were available."""
# -*- coding: utf-8 -*- from odoo.tests import common class TestStockCommon(common.TransactionCase): def setUp(self): super(TestStockCommon, self).setUp() self.ProductObj = self.env['product.product'] self.UomObj = self.env['uom.uom'] self.PartnerObj = self.env['res.partner'] self.ModelDataObj = self.env['ir.model.data'] self.StockPackObj = self.env['stock.move.line'] self.StockQuantObj = self.env['stock.quant'] self.PickingObj = self.env['stock.picking'] self.MoveObj = self.env['stock.move'] self.InvObj = self.env['stock.inventory'] self.InvLineObj = self.env['stock.inventory.line'] self.LotObj = self.env['stock.production.lot'] # Model Data self.partner_agrolite_id = self.ModelDataObj.xmlid_to_res_id('base.res_partner_2') self.partner_delta_id = self.ModelDataObj.xmlid_to_res_id('base.res_partner_4') self.picking_type_in = self.ModelDataObj.xmlid_to_res_id('stock.picking_type_in') self.picking_type_out = self.ModelDataObj.xmlid_to_res_id('stock.picking_type_out') self.supplier_location = self.ModelDataObj.xmlid_to_res_id('stock.stock_location_suppliers') self.stock_location = self.ModelDataObj.xmlid_to_res_id('stock.stock_location_stock') pack_location = self.env.ref('stock.location_pack_zone') pack_location.active = True self.pack_location = pack_location.id output_location = self.env.ref('stock.stock_location_output') output_location.active = True self.output_location = output_location.id self.customer_location = self.ModelDataObj.xmlid_to_res_id('stock.stock_location_customers') self.categ_unit = self.ModelDataObj.xmlid_to_res_id('uom.product_uom_categ_unit') self.categ_kgm = self.ModelDataObj.xmlid_to_res_id('uom.product_uom_categ_kgm') # Product Created A, B, C, D self.productA = self.ProductObj.create({'name': 'Product A', 'type': 'product'}) self.productB = self.ProductObj.create({'name': 'Product B', 'type': 'product'}) self.productC = self.ProductObj.create({'name': 'Product C', 'type': 'product'}) self.productD = self.ProductObj.create({'name': 'Product D', 'type': 'product'}) self.productE = self.ProductObj.create({'name': 'Product E', 'type': 'product'}) # Configure unit of measure. self.uom_kg = self.env['uom.uom'].search([('category_id', '=', self.categ_kgm), ('uom_type', '=', 'reference')], limit=1) self.uom_kg.write({ 'name': 'Test-KG', 'rounding': 0.000001}) self.uom_tone = self.UomObj.create({ 'name': 'Test-Tone', 'category_id': self.categ_kgm, 'uom_type': 'bigger', 'factor_inv': 1000.0, 'rounding': 0.001}) self.uom_gm = self.UomObj.create({ 'name': 'Test-G', 'category_id': self.categ_kgm, 'uom_type': 'smaller', 'factor': 1000.0, 'rounding': 0.001}) self.uom_mg = self.UomObj.create({ 'name': 'Test-MG', 'category_id': self.categ_kgm, 'uom_type': 'smaller', 'factor': 100000.0, 'rounding': 0.001}) # Check Unit self.uom_unit = self.env['uom.uom'].search([('category_id', '=', self.categ_unit), ('uom_type', '=', 'reference')], limit=1) self.uom_unit.write({ 'name': 'Test-Unit', 'rounding': 1.0}) self.uom_dozen = self.UomObj.create({ 'name': 'Test-DozenA', 'category_id': self.categ_unit, 'factor_inv': 12, 'uom_type': 'bigger', 'rounding': 0.001}) self.uom_sdozen = self.UomObj.create({ 'name': 'Test-SDozenA', 'category_id': self.categ_unit, 'factor_inv': 144, 'uom_type': 'bigger', 'rounding': 0.001}) self.uom_sdozen_round = self.UomObj.create({ 'name': 'Test-SDozenA Round', 'category_id': self.categ_unit, 'factor_inv': 144, 'uom_type': 'bigger', 'rounding': 1.0}) # Product for different unit of measure. self.DozA = self.ProductObj.create({'name': 'Dozon-A', 'type': 'product', 'uom_id': self.uom_dozen.id, 'uom_po_id': self.uom_dozen.id}) self.SDozA = self.ProductObj.create({'name': 'SuperDozon-A', 'type': 'product', 'uom_id': self.uom_sdozen.id, 'uom_po_id': self.uom_sdozen.id}) self.SDozARound = self.ProductObj.create({'name': 'SuperDozenRound-A', 'type': 'product', 'uom_id': self.uom_sdozen_round.id, 'uom_po_id': self.uom_sdozen_round.id}) self.UnitA = self.ProductObj.create({'name': 'Unit-A', 'type': 'product'}) self.kgB = self.ProductObj.create({'name': 'kg-B', 'type': 'product', 'uom_id': self.uom_kg.id, 'uom_po_id': self.uom_kg.id}) self.gB = self.ProductObj.create({'name': 'g-B', 'type': 'product', 'uom_id': self.uom_gm.id, 'uom_po_id': self.uom_gm.id})
from __future__ import (absolute_import, division, print_function, unicode_literals) import logging import datetime import time from . import boundary_plugin from . import status_store """ If getting statistics from CloudWatch fails, we will retry up to this number of times before giving up and aborting the plugin. Use 0 for unlimited retries. """ PLUGIN_RETRY_COUNT = 0 """ If getting statistics from CloudWatch fails, we will wait this long (in seconds) before retrying. This value must not be greater than 30 seconds, because the Boundary Relay will think we've timed out and terminate us after 30 seconds of inactivity. """ PLUGIN_RETRY_DELAY = 5 class CloudwatchPlugin(object): def __init__(self, cloudwatch_metrics_type, boundary_metric_prefix, status_store_filename): self.cloudwatch_metrics_type = cloudwatch_metrics_type self.boundary_metric_prefix = boundary_metric_prefix self.status_store_filename = status_store_filename def get_metric_data_with_retries(self, *args, **kwargs): """ Calls the get_metric_data function, taking into account retry configuration. """ retry_range = xrange(PLUGIN_RETRY_COUNT) if PLUGIN_RETRY_COUNT > 0 else iter(int, 1) for _ in retry_range: try: return self.cloudwatch_metrics.get_metric_data(*args, **kwargs) except Exception as e: logging.error("Error retrieving CloudWatch data: %s" % e) boundary_plugin.report_alive() time.sleep(PLUGIN_RETRY_DELAY) boundary_plugin.report_alive() logging.fatal("Max retries exceeded retrieving CloudWatch data") raise Exception("Max retries exceeded retrieving CloudWatch data") def handle_metrics(self, data, reported_metrics): # Data format: # (RegionId, EntityName, MetricName) -> [(Timestamp, Value, Statistic), (Timestamp, Value, Statistic), ...] for metric_key, metric_list in data.items(): region_id, entity_name, metric_name = metric_key for metric_list_item in metric_list: # Do not report duplicate or past samples (note: we are comparing tuples here, which # amounts to comparing their timestamps). if reported_metrics.get(metric_key, (datetime.datetime.min,)) >= metric_list_item: continue metric_timestamp, metric_value, metric_statistic = metric_list_item boundary_plugin.boundary_report_metric(self.boundary_metric_prefix + metric_name, metric_value, entity_name, metric_timestamp) reported_metrics[metric_key] = metric_list_item status_store.save_status_store(self.status_store_filename, reported_metrics) def main(self): settings = boundary_plugin.parse_params() reported_metrics = status_store.load_status_store(self.status_store_filename) or dict() logging.basicConfig(level=logging.ERROR, filename=settings.get('log_file', None)) reports_log = settings.get('report_log_file', None) if reports_log: boundary_plugin.log_metrics_to_file(reports_log) boundary_plugin.start_keepalive_subprocess() self.cloudwatch_metrics = self.cloudwatch_metrics_type(settings['access_key_id'], settings['secret_key']) # Bring us up to date! Get all data since the last time we know we reported valid data # (minus 20 minutes as a buffer), and report it now, so that we report data on any time # this plugin was down for any reason. try: earliest_timestamp = max(reported_metrics.values(), key=lambda v: v[0])[0] - datetime.timedelta(minutes=20) except ValueError: # Probably first run or someone deleted our status store file - just start from now logging.error("No status store data; starting data collection from now") pass else: logging.error("Starting historical data collection from %s" % earliest_timestamp) data = self.get_metric_data_with_retries(only_latest=False, start_time=earliest_timestamp, end_time=datetime.datetime.utcnow()) self.handle_metrics(data, reported_metrics) logging.error("Historical data collection complete") while True: data = self.get_metric_data_with_retries() self.handle_metrics(data, reported_metrics) boundary_plugin.sleep_interval()
# Copyright 2016 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== """Keras scikit-learn API wrapper.""" from __future__ import absolute_import from __future__ import division from __future__ import print_function from tensorflow.python.keras._impl.keras.wrappers.scikit_learn import KerasClassifier from tensorflow.python.keras._impl.keras.wrappers.scikit_learn import KerasRegressor del absolute_import del division del print_function
#!/usr/bin/env python import re, os, sys, fnmatch # Regex pattern to extract the directory path in a #define FILE_DIR filedir_pattern = re.compile(r'^#define\s*FILE_DIR\s*"(.*?)"') # Regex pattern to extract any single quoted piece of text. This can also # match single quoted strings inside of double quotes, which is part of a # regular text string and should not be replaced. The replacement function # however will any match that doesn't appear to be a filename so these # extra matches should not be a problem. rename_pattern = re.compile(r"'(.+?)'") # Only filenames matching this pattern will have their resources renamed source_pattern = re.compile(r"^.*?\.(dm|dmm)$") # Open the .dme file and return a list of all FILE_DIR paths in it def read_filedirs(filename): result = [] dme_file = file(filename, "rt") # Read each line from the file and check for regex pattern match for row in dme_file: match = filedir_pattern.match(row) if match: result.append(match.group(1)) dme_file.close() return result # Search through a list of directories, and build a dictionary which # maps every file to its full pathname (relative to the .dme file) # If the same filename appears in more than one directory, the earlier # directory in the list takes preference. def index_files(file_dirs): result = {} # Reverse the directory list so the earlier directories take precedence # by replacing the previously indexed file of the same name for directory in reversed(file_dirs): for name in os.listdir(directory): # Replace backslash path separators on Windows with forward slash # Force "name" to lowercase when used as a key since BYOND resource # names are case insensitive, even on Linux. if name.find(".") == -1: continue result[name.lower()] = directory.replace('\\', '/') + '/' + name return result # Recursively search for every .dm/.dmm file in the .dme file directory. For # each file, search it for any resource names in single quotes, and replace # them with the full path previously found by index_files() def rewrite_sources(resources): # Create a closure for the regex replacement function to capture the # resources dictionary which can't be passed directly to this function def replace_func(name): key = name.group(1).lower() if key in resources: replacement = resources[key] else: replacement = name.group(1) return "'" + replacement + "'" # Search recursively for all .dm and .dmm files for (dirpath, dirs, files) in os.walk("."): for name in files: if source_pattern.match(name): path = dirpath + '/' + name source_file = file(path, "rt") output_file = file(path + ".tmp", "wt") # Read file one line at a time and perform replacement of all # single quoted resource names with the fullpath to that resource # file. Write the updated text back out to a temporary file. for row in source_file: row = rename_pattern.sub(replace_func, row) output_file.write(row) output_file.close() source_file.close() # Delete original source file and replace with the temporary # output. On Windows, an atomic rename() operation is not # possible like it is under POSIX. os.remove(path) os.rename(path + ".tmp", path) dirs = read_filedirs("tgstation.dme"); resources = index_files(dirs) rewrite_sources(resources)
import os import urllib2 import re import logging from subprocess import Popen from subprocess import PIPE class Downloader(object): def __init__(self, config): self._ctx = config self._log = logging.getLogger('downloads') self._init_proxy() def _init_proxy(self): handlers = {} for key in self._ctx.keys(): if key.lower().endswith('_proxy'): handlers[key.split('_')[0]] = self._ctx[key] self._log.debug('Loaded proxy handlers [%s]', handlers) openers = [] if handlers: openers.append(urllib2.ProxyHandler(handlers)) for handler in handlers.values(): if '@' in handler: openers.append(urllib2.ProxyBasicAuthHandler()) opener = urllib2.build_opener(*openers) urllib2.install_opener(opener) def download(self, url, toFile): path_to_download_executable = os.path.join( self._ctx['BP_DIR'], 'compile-extensions', 'bin', 'download_dependency') command_arguments = [ path_to_download_executable, url, toFile] process = Popen(command_arguments, stdout=PIPE) exit_code = process.wait() translated_uri = process.stdout.read().rstrip() if exit_code == 0: print "Downloaded [%s] to [%s]" % (translated_uri, toFile) elif exit_code == 1: raise RuntimeError("Could not download dependency: %s" % url) elif exit_code == 3: raise RuntimeError("MD5 of downloaded dependency does not match expected value") def custom_extension_download(self, url, toFile): res = urllib2.urlopen(url) with open(toFile, 'w') as f: f.write(res.read()) print 'Downloaded [%s] to [%s]' % (url, toFile) self._log.info('Downloaded [%s] to [%s]', url, toFile) def download_direct(self, url): buf = urllib2.urlopen(url).read() self._log.info('Downloaded [%s] to memory', url) self._log.debug("Downloaded [%s] [%s]", url, buf) return buf class CurlDownloader(object): def __init__(self, config): self._ctx = config self._status_pattern = re.compile(r'^(.*)<!-- Status: (\d+) -->$', re.DOTALL) self._log = logging.getLogger('downloads') def download(self, url, toFile): cmd = ["curl", "-s", "-o", toFile, "-w", '%{http_code}'] for key in self._ctx.keys(): if key.lower().endswith('_proxy'): cmd.extend(['-x', self._ctx[key]]) cmd.append(url) self._log.debug("Running [%s]", cmd) proc = Popen(cmd, stdout=PIPE) output, unused_err = proc.communicate() proc.poll() self._log.debug("Curl returned [%s]", output) if output and \ (output.startswith('4') or output.startswith('5')): raise RuntimeError("curl says [%s]" % output) print 'Downloaded [%s] to [%s]' % (url, toFile) self._log.info('Downloaded [%s] to [%s]', url, toFile) def download_direct(self, url): cmd = ["curl", "-s", "-w", '<!-- Status: %{http_code} -->'] for key in self._ctx.keys(): if key.lower().endswith('_proxy'): cmd.extend(['-x', self._ctx[key]]) cmd.append(url) self._log.debug("Running [%s]", cmd) proc = Popen(cmd, stdout=PIPE) output, unused_err = proc.communicate() proc.poll() m = self._status_pattern.match(output) if m: resp = m.group(1) code = m.group(2) self._log.debug("Curl returned [%s]", code) if (code.startswith('4') or code.startswith('5')): raise RuntimeError("curl says [%s]" % output) self._log.info('Downloaded [%s] to memory', url) self._log.debug('Downloaded [%s] [%s]', url, resp) return resp
# !/usr/bin/env python3 import threading import time import urllib.request import json from .. import modulebase weather_check_interval = 60 # check every minute city = 'Kanata,ON' cur_weather_url = ('http://api.openweathermap.org/data/2.5/weather?q=%s&units=metric') % (city) forecast_url = ('http://api.openweathermap.org/data/2.5/forecast?q=%s&units=metric') % (city) class weather(modulebase.ModuleBase): data = None encode = lambda x : json.dumps(x).encode('utf-8') def __init__(self) : weather.data = WeatherData() def deinit(self) : pass def GET_temperature(self): data = { 'temp' : weather.data.cur_temp() } return weather.encode(data) def GET_current(self) : wd = weather.data data = { 'city' : city, 'temp' : wd.cur_temp(), 'weather' : wd.cur_weather(), 'humidity' : wd.cur_humidity(), 'clouds' : wd.cur_clouds(), 'timestamp' : wd.timestamp() } return weather.encode(data) def GET_forecast(self) : data = weather.data.forecast() return weather.encode(data) def POST_test(self) : return "Good!" class WeatherData : def __init__(self) : self.__cur_temp = -1 self.__humidity = -1 self.__clouds = -1 self.__cur_weather = {} self.__forecast = [] self.__timestamp = 0 self.__lock = threading.Lock() self.__start_checker() ''' Public getters ''' def cur_temp(self) : with self.__lock : return self.__cur_temp def cur_weather(self) : with self.__lock : return self.__cur_weather def cur_humidity(self) : with self.__lock : return self.__humidity def cur_clouds(self) : with self.__lock : return self.__clouds def forecast(self) : with self.__lock : return self.__forecast def timestamp(self) : with self.__lock : return self.__timestamp ''' Private setters ''' def __set_cur_temp(self, temp) : with self.__lock : self.__cur_temp = temp def __set_cur_weather(self, weather_id, weather_descr) : with self.__lock : self.__cur_weather['id'] = weather_id self.__cur_weather['descr'] = weather_descr def __set_cur_humidity(self, hum) : with self.__lock : self.__humidity = hum def __set_cur_clouds(self, clouds) : with self.__lock : self.__clouds = clouds def __set_forecast(self, forecast) : with self.__lock : self.__forecast = forecast def __set_timestamp(self, timestamp) : with self.__lock : self.__timestamp = timestamp ''' Threading ''' def __start_checker(self) : print('Starting weather checker...') self.__checker = threading.Thread(target=self.__check_weather) self.__checker.daemon = True self.__checker.start() def __check_weather(self) : while True : print('Checking weather...') response = urllib.request.urlopen( urllib.request.Request(url=cur_weather_url) ) json_obj = json.loads(response.read().decode('utf-8')) print (str(json_obj)) self.__set_timestamp(int(time.time())) main = json_obj.get('main', {}) temp = main.get('temp', -1) hum = main.get('humidity', -1) self.__set_cur_temp(temp) self.__set_cur_humidity(hum) weather = json_obj.get('weather', []) if len(weather) > 0 : wthr_id = weather[0].get('id', 0) wthr_descr = weather[0].get('main', '') self.__set_cur_weather(wthr_id, wthr_descr) clouds = json_obj.get('clouds', {}).get('all', -1) self.__set_cur_clouds(clouds) # get forecast response = urllib.request.urlopen( urllib.request.Request(url=forecast_url) ) json_obj = json.loads(response.read().decode('utf-8')) # extract data data_list = json_obj.get('list', []) fc_data = [] for list_item in data_list[:8] : fc_item = {} fc_item['timestamp'] = list_item.get('dt', 0) fc_main = list_item.get('main', {}) fc_item['temp'] = fc_main.get('temp', -1) fc_item['humidity'] = fc_main.get('humidity', -1) fc_weather = list_item.get('weather', []) fc_item['weather'] = { 'id' : fc_weather[0].get('id', 0), 'descr' : fc_weather[0].get('main', '') } if len(fc_weather) > 0 else { 'id' : 0, 'descr': '' } fc_data.append(fc_item) self.__set_forecast(fc_data) time.sleep(weather_check_interval)
#!/usr/bin/env python ''' Run this script to update all the copyright headers of files that were changed this year. For example: // Copyright (c) 2009-2012 The Bitcoin Core developers it will change it to // Copyright (c) 2009-2015 The Bitcoin Core developers ''' import os import time import re year = time.gmtime()[0] CMD_GIT_DATE = 'git log --format=@%%at -1 %s | date +"%%Y" -u -f -' CMD_REGEX= "perl -pi -e 's/(20\d\d)(?:-20\d\d)? The Bitcoin/$1-%s The Bitcoin/' %s" REGEX_CURRENT= re.compile("%s The Bitcoin" % year) CMD_LIST_FILES= "find %s | grep %s" FOLDERS = ["./qa", "./src"] EXTENSIONS = [".cpp",".h", ".py"] def get_git_date(file_path): r = os.popen(CMD_GIT_DATE % file_path) for l in r: # Result is one line, so just return return l.replace("\n","") return "" n=1 for folder in FOLDERS: for extension in EXTENSIONS: for file_path in os.popen(CMD_LIST_FILES % (folder, extension)): file_path = os.getcwd() + file_path[1:-1] if file_path.endswith(extension): git_date = get_git_date(file_path) if str(year) == git_date: # Only update if current year is not found if REGEX_CURRENT.search(open(file_path, "r").read()) is None: print n,"Last git edit", git_date, "-", file_path os.popen(CMD_REGEX % (year,file_path)) n = n + 1
# Generated by the protocol buffer compiler. DO NOT EDIT! # source: helloworld.proto import sys _b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode('latin1')) from google.protobuf import descriptor as _descriptor from google.protobuf import message as _message from google.protobuf import reflection as _reflection from google.protobuf import symbol_database as _symbol_database from google.protobuf import descriptor_pb2 # @@protoc_insertion_point(imports) _sym_db = _symbol_database.Default() DESCRIPTOR = _descriptor.FileDescriptor( name='helloworld.proto', package='helloworld', syntax='proto3', serialized_pb=_b('\n\x10helloworld.proto\x12\nhelloworld\"\x1c\n\x0cHelloRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\"\x1d\n\nHelloReply\x12\x0f\n\x07message\x18\x01 \x01(\t2I\n\x07Greeter\x12>\n\x08SayHello\x12\x18.helloworld.HelloRequest\x1a\x16.helloworld.HelloReply\"\x00\x42\x36\n\x1bio.grpc.examples.helloworldB\x0fHelloWorldProtoP\x01\xa2\x02\x03HLWb\x06proto3') ) _sym_db.RegisterFileDescriptor(DESCRIPTOR) _HELLOREQUEST = _descriptor.Descriptor( name='HelloRequest', full_name='helloworld.HelloRequest', filename=None, file=DESCRIPTOR, containing_type=None, fields=[ _descriptor.FieldDescriptor( name='name', full_name='helloworld.HelloRequest.name', index=0, number=1, type=9, cpp_type=9, label=1, has_default_value=False, default_value=_b("").decode('utf-8'), message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), ], extensions=[ ], nested_types=[], enum_types=[ ], options=None, is_extendable=False, syntax='proto3', extension_ranges=[], oneofs=[ ], serialized_start=32, serialized_end=60, ) _HELLOREPLY = _descriptor.Descriptor( name='HelloReply', full_name='helloworld.HelloReply', filename=None, file=DESCRIPTOR, containing_type=None, fields=[ _descriptor.FieldDescriptor( name='message', full_name='helloworld.HelloReply.message', index=0, number=1, type=9, cpp_type=9, label=1, has_default_value=False, default_value=_b("").decode('utf-8'), message_type=None, enum_type=None, containing_type=None, is_extension=False, extension_scope=None, options=None), ], extensions=[ ], nested_types=[], enum_types=[ ], options=None, is_extendable=False, syntax='proto3', extension_ranges=[], oneofs=[ ], serialized_start=62, serialized_end=91, ) DESCRIPTOR.message_types_by_name['HelloRequest'] = _HELLOREQUEST DESCRIPTOR.message_types_by_name['HelloReply'] = _HELLOREPLY HelloRequest = _reflection.GeneratedProtocolMessageType('HelloRequest', (_message.Message,), dict( DESCRIPTOR = _HELLOREQUEST, __module__ = 'helloworld_pb2' # @@protoc_insertion_point(class_scope:helloworld.HelloRequest) )) _sym_db.RegisterMessage(HelloRequest) HelloReply = _reflection.GeneratedProtocolMessageType('HelloReply', (_message.Message,), dict( DESCRIPTOR = _HELLOREPLY, __module__ = 'helloworld_pb2' # @@protoc_insertion_point(class_scope:helloworld.HelloReply) )) _sym_db.RegisterMessage(HelloReply) DESCRIPTOR.has_options = True DESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b('\n\033io.grpc.examples.helloworldB\017HelloWorldProtoP\001\242\002\003HLW')) import grpc from grpc.beta import implementations as beta_implementations from grpc.beta import interfaces as beta_interfaces from grpc.framework.common import cardinality from grpc.framework.interfaces.face import utilities as face_utilities class GreeterStub(object): """The greeting service definition. """ def __init__(self, channel): """Constructor. Args: channel: A grpc.Channel. """ self.SayHello = channel.unary_unary( '/helloworld.Greeter/SayHello', request_serializer=HelloRequest.SerializeToString, response_deserializer=HelloReply.FromString, ) class GreeterServicer(object): """The greeting service definition. """ def SayHello(self, request, context): """Sends a greeting """ context.set_code(grpc.StatusCode.UNIMPLEMENTED) context.set_details('Method not implemented!') raise NotImplementedError('Method not implemented!') def add_GreeterServicer_to_server(servicer, server): rpc_method_handlers = { 'SayHello': grpc.unary_unary_rpc_method_handler( servicer.SayHello, request_deserializer=HelloRequest.FromString, response_serializer=HelloReply.SerializeToString, ), } generic_handler = grpc.method_handlers_generic_handler( 'helloworld.Greeter', rpc_method_handlers) server.add_generic_rpc_handlers((generic_handler,)) class BetaGreeterServicer(object): """The greeting service definition. """ def SayHello(self, request, context): """Sends a greeting """ context.code(beta_interfaces.StatusCode.UNIMPLEMENTED) class BetaGreeterStub(object): """The greeting service definition. """ def SayHello(self, request, timeout, metadata=None, with_call=False, protocol_options=None): """Sends a greeting """ raise NotImplementedError() SayHello.future = None def beta_create_Greeter_server(servicer, pool=None, pool_size=None, default_timeout=None, maximum_timeout=None): request_deserializers = { ('helloworld.Greeter', 'SayHello'): HelloRequest.FromString, } response_serializers = { ('helloworld.Greeter', 'SayHello'): HelloReply.SerializeToString, } method_implementations = { ('helloworld.Greeter', 'SayHello'): face_utilities.unary_unary_inline(servicer.SayHello), } server_options = beta_implementations.server_options(request_deserializers=request_deserializers, response_serializers=response_serializers, thread_pool=pool, thread_pool_size=pool_size, default_timeout=default_timeout, maximum_timeout=maximum_timeout) return beta_implementations.server(method_implementations, options=server_options) def beta_create_Greeter_stub(channel, host=None, metadata_transformer=None, pool=None, pool_size=None): request_serializers = { ('helloworld.Greeter', 'SayHello'): HelloRequest.SerializeToString, } response_deserializers = { ('helloworld.Greeter', 'SayHello'): HelloReply.FromString, } cardinalities = { 'SayHello': cardinality.Cardinality.UNARY_UNARY, } stub_options = beta_implementations.stub_options(host=host, metadata_transformer=metadata_transformer, request_serializers=request_serializers, response_deserializers=response_deserializers, thread_pool=pool, thread_pool_size=pool_size) return beta_implementations.dynamic_stub(channel, 'helloworld.Greeter', cardinalities, options=stub_options) # @@protoc_insertion_point(module_scope)
# -*- coding: utf-8 -*- import random from openerp import SUPERUSER_ID from openerp.osv import osv, orm, fields from openerp.addons.web.http import request class payment_transaction(orm.Model): _inherit = 'payment.transaction' _columns = { # link with the sale order 'sale_order_id': fields.many2one('sale.order', 'Sale Order'), } class sale_order(osv.Model): _inherit = "sale.order" def _cart_qty(self, cr, uid, ids, field_name, arg, context=None): res = dict() for order in self.browse(cr, uid, ids, context=context): res[order.id] = int(sum(l.product_uom_qty for l in (order.website_order_line or []))) return res _columns = { 'website_order_line': fields.one2many( 'sale.order.line', 'order_id', string='Order Lines displayed on Website', readonly=True, help='Order Lines to be displayed on the website. They should not be used for computation purpose.', ), 'cart_quantity': fields.function(_cart_qty, type='integer', string='Cart Quantity'), 'payment_acquirer_id': fields.many2one('payment.acquirer', 'Payment Acquirer', on_delete='set null'), 'payment_tx_id': fields.many2one('payment.transaction', 'Transaction', on_delete='set null'), } def _get_errors(self, cr, uid, order, context=None): return [] def _get_website_data(self, cr, uid, order, context): return { 'partner': order.partner_id.id, 'order': order } def _cart_find_product_line(self, cr, uid, ids, product_id=None, line_id=None, context=None, **kwargs): for so in self.browse(cr, uid, ids, context=context): domain = [('order_id', '=', so.id), ('product_id', '=', product_id)] if line_id: domain += [('id', '=', line_id)] return self.pool.get('sale.order.line').search(cr, SUPERUSER_ID, domain, context=context) def _website_product_id_change(self, cr, uid, ids, order_id, product_id, line_id=None, context=None): so = self.pool.get('sale.order').browse(cr, uid, order_id, context=context) values = self.pool.get('sale.order.line').product_id_change(cr, SUPERUSER_ID, [], pricelist=so.pricelist_id.id, product=product_id, partner_id=so.partner_id.id, context=context )['value'] if line_id: line = self.pool.get('sale.order.line').browse(cr, SUPERUSER_ID, line_id, context=context) values['name'] = line.name else: product = self.pool.get('product.product').browse(cr, uid, product_id, context=context) values['name'] = product.description_sale or product.name values['product_id'] = product_id values['order_id'] = order_id if values.get('tax_id') != None: values['tax_id'] = [(6, 0, values['tax_id'])] return values def _cart_update(self, cr, uid, ids, product_id=None, line_id=None, add_qty=0, set_qty=0, context=None, **kwargs): """ Add or set product quantity, add_qty can be negative """ sol = self.pool.get('sale.order.line') quantity = 0 for so in self.browse(cr, uid, ids, context=context): if line_id != False: line_ids = so._cart_find_product_line(product_id, line_id, context=context, **kwargs) if line_ids: line_id = line_ids[0] # Create line if no line with product_id can be located if not line_id: values = self._website_product_id_change(cr, uid, ids, so.id, product_id, context=context) line_id = sol.create(cr, SUPERUSER_ID, values, context=context) if add_qty: add_qty -= 1 # compute new quantity if set_qty: quantity = set_qty elif add_qty != None: quantity = sol.browse(cr, SUPERUSER_ID, line_id, context=context).product_uom_qty + (add_qty or 0) # Remove zero of negative lines if quantity <= 0: sol.unlink(cr, SUPERUSER_ID, [line_id], context=context) else: # update line values = self._website_product_id_change(cr, uid, ids, so.id, product_id, line_id, context=context) values['product_uom_qty'] = quantity sol.write(cr, SUPERUSER_ID, [line_id], values, context=context) return {'line_id': line_id, 'quantity': quantity} def _cart_accessories(self, cr, uid, ids, context=None): for order in self.browse(cr, uid, ids, context=context): s = set(j.id for l in (order.website_order_line or []) for j in (l.product_id.accessory_product_ids or [])) s -= set(l.product_id.id for l in order.order_line) product_ids = random.sample(s, min(len(s),3)) return self.pool['product.product'].browse(cr, uid, product_ids, context=context) class website(orm.Model): _inherit = 'website' _columns = { 'pricelist_id': fields.related('user_id','partner_id','property_product_pricelist', type='many2one', relation='product.pricelist', string='Default Pricelist'), 'currency_id': fields.related('pricelist_id','currency_id', type='many2one', relation='res.currency', string='Default Currency'), } def sale_product_domain(self, cr, uid, ids, context=None): return [("sale_ok", "=", True)] def sale_get_order(self, cr, uid, ids, force_create=False, code=None, update_pricelist=None, context=None): sale_order_obj = self.pool['sale.order'] sale_order_id = request.session.get('sale_order_id') sale_order = None # create so if needed if not sale_order_id and (force_create or code): # TODO cache partner_id session partner = self.pool['res.users'].browse(cr, SUPERUSER_ID, uid, context=context).partner_id for w in self.browse(cr, uid, ids): values = { 'user_id': w.user_id.id, 'partner_id': partner.id, 'pricelist_id': partner.property_product_pricelist.id, 'section_id': self.pool.get('ir.model.data').get_object_reference(cr, uid, 'website', 'salesteam_website_sales')[1], } sale_order_id = sale_order_obj.create(cr, SUPERUSER_ID, values, context=context) values = sale_order_obj.onchange_partner_id(cr, SUPERUSER_ID, [], partner.id, context=context)['value'] sale_order_obj.write(cr, SUPERUSER_ID, [sale_order_id], values, context=context) request.session['sale_order_id'] = sale_order_id if sale_order_id: # TODO cache partner_id session partner = self.pool['res.users'].browse(cr, SUPERUSER_ID, uid, context=context).partner_id sale_order = sale_order_obj.browse(cr, SUPERUSER_ID, sale_order_id, context=context) if not sale_order.exists(): request.session['sale_order_id'] = None return None # check for change of pricelist with a coupon if code and code != sale_order.pricelist_id.code: pricelist_ids = self.pool['product.pricelist'].search(cr, SUPERUSER_ID, [('code', '=', code)], context=context) if pricelist_ids: pricelist_id = pricelist_ids[0] request.session['sale_order_code_pricelist_id'] = pricelist_id update_pricelist = True request.session['sale_order_code_pricelist_id'] = False pricelist_id = request.session.get('sale_order_code_pricelist_id') or partner.property_product_pricelist.id # check for change of partner_id ie after signup if sale_order.partner_id.id != partner.id and request.website.partner_id.id != partner.id: flag_pricelist = False if pricelist_id != sale_order.pricelist_id.id: flag_pricelist = True fiscal_position = sale_order.fiscal_position and sale_order.fiscal_position.id or False values = sale_order_obj.onchange_partner_id(cr, SUPERUSER_ID, [sale_order_id], partner.id, context=context)['value'] if values.get('fiscal_position'): order_lines = map(int,sale_order.order_line) values.update(sale_order_obj.onchange_fiscal_position(cr, SUPERUSER_ID, [], values['fiscal_position'], [[6, 0, order_lines]], context=context)['value']) values['partner_id'] = partner.id sale_order_obj.write(cr, SUPERUSER_ID, [sale_order_id], values, context=context) if flag_pricelist or values.get('fiscal_position') != fiscal_position: update_pricelist = True # update the pricelist if update_pricelist: values = {'pricelist_id': pricelist_id} values.update(sale_order.onchange_pricelist_id(pricelist_id, None)['value']) sale_order.write(values) for line in sale_order.order_line: sale_order._cart_update(product_id=line.product_id.id, add_qty=0) # update browse record if (code and code != sale_order.pricelist_id.code) or sale_order.partner_id.id != partner.id: sale_order = sale_order_obj.browse(cr, SUPERUSER_ID, sale_order.id, context=context) return sale_order def sale_get_transaction(self, cr, uid, ids, context=None): transaction_obj = self.pool.get('payment.transaction') tx_id = request.session.get('sale_transaction_id') if tx_id: tx_ids = transaction_obj.search(cr, uid, [('id', '=', tx_id), ('state', 'not in', ['cancel'])], context=context) if tx_ids: return transaction_obj.browse(cr, uid, tx_ids[0], context=context) else: request.session['sale_transaction_id'] = False return False def sale_reset(self, cr, uid, ids, context=None): request.session.update({ 'sale_order_id': False, 'sale_transaction_id': False, 'sale_order_code_pricelist_id': False, })
# Copyright (c) 2013, Web Notes Technologies Pvt. Ltd. and Contributors # MIT License. See license.txt from __future__ import unicode_literals import webnotes from webnotes.utils import cstr class DocType: def __init__(self, d, dl): self.doc, self.doclist = d, dl def autoname(self): self.doc.name = self.doc.dt + "-" + self.doc.script_type def on_update(self): webnotes.clear_cache(doctype=self.doc.dt) def on_trash(self): webnotes.clear_cache(doctype=self.doc.dt) def make_custom_server_script_file(doctype, script=None): import os from webnotes.plugins import get_path file_path = get_path(None, "DocType", doctype) if os.path.exists(file_path): raise IOError(file_path + " already exists") # create folder if not exists webnotes.create_folder(os.path.dirname(file_path)) # create file custom_script = """from __future__ import unicode_literals import webnotes from webnotes.utils import cint, cstr, flt from webnotes.model.doc import Document from webnotes.model.code import get_obj from webnotes import msgprint, _ class CustomDocType(DocType): {script}""".format(script=script or "\tpass") with open(file_path, "w") as f: f.write(custom_script.encode("utf-8"))
# -*- coding: utf-8 -*- import json from .base import BikeShareSystem, BikeShareStation from . import utils class BySykkel(BikeShareSystem): authed = True meta = { 'system': 'BySykkel', 'company': ['Urban Infrastructure Partner'] } def __init__(self, tag, meta, feed_url, feed_details_url, key): super(BySykkel, self).__init__(tag, meta) self.feed_url = feed_url self.feed_details_url = feed_details_url self.key = key def update(self, scraper=None): if scraper is None: scraper = utils.PyBikesScraper() scraper.headers['Client-Identifier'] = self.key self.stations = [] stations_data = json.loads(scraper.request(self.feed_url)) details_data = json.loads(scraper.request(self.feed_details_url)) # Aggregate status and information by uid stations_data = {s['id']: s for s in stations_data['stations']} details_data = {s['id']: s for s in details_data['stations']} # Join stationsdata in stations stations = [ (stations_data[id], details_data[id]) for id in stations_data.keys() ] # append all data to info part of stations and create objects of this for info, status in stations: info.update(status) station = BySykkelStation(info) self.stations.append(station) class BySykkelStation(BikeShareStation): def __init__(self, info): super(BySykkelStation, self).__init__() self.name = info['title'] self.longitude = float(info['center']['longitude']) self.latitude = float(info['center']['latitude']) self.bikes = info['availability']['bikes'] self.free = info['availability']['locks'] self.extra = { 'uid': info['id'], 'placement': info['subtitle'], }
# Copyright 2013 craigslist # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. '''craigslist blob event module. This should only be used internally by the client module.''' import hashlib import clblob import clcommon.anybase import clcommon.profile class Event(object): '''Base class for various events used in the client.''' params = [] def __init__(self, client, method, name, timeout, http_method=None): self._client = client if method not in self._client.events: self._client.events[method] = dict(current=0, max=0, total=0) self._client.events[method]['total'] += 1 self._client.events[method]['current'] += 1 current = self._client.events[method]['current'] if current > self._client.events[method]['max']: self._client.events[method]['max'] = current self.method = method self.name = name self.timeout = timeout self.http_method = http_method or method.upper() self.parse_response = True self.profile = clcommon.profile.Profile() self.data = None self.modified = None self.deleted = None self.modified_deleted = None self.index_id = None self.store_id = None self.encoded = None self._buckets = None self._replicas = None self._is_local = None def __del__(self): if hasattr(self, '_client'): self._client.events[self.method]['current'] -= 1 if hasattr(self, 'profile') and len(self.profile.marks) > 0: self._client.log.info('profile %s', self.profile) @property def url(self): '''Make a URL for this event.''' url = '/%s' % self.name separator = '?' for param in self.params: value = getattr(self, param) if value is not None: url = '%s%s%s=%s' % (url, separator, param, value) separator = '&' return url def buckets(self, buckets=None): '''Get or set the buckets for this event.''' if buckets is not None: self._buckets = buckets return if self._buckets is not None: return self._buckets self._buckets = {} if self.encoded is not False and self._client.config['encode_name']: self._get_encoded_buckets() else: self._get_buckets() return self._buckets def _get_buckets(self): '''Get buckets for a name.''' name_hash = hashlib.md5(self.name).hexdigest() # pylint: disable=E1101 name_hash = int(name_hash[:8], 16) for cluster in xrange(len(self._client.weighted_clusters)): weighted_cluster = self._client.weighted_clusters[cluster] bucket = weighted_cluster[name_hash % len(weighted_cluster)] self._buckets[cluster] = bucket def _get_encoded_buckets(self): '''Get buckets for an encoded name.''' if clcommon.anybase.decode(self.name[0], 62) != 0: raise clblob.InvalidRequest(_('Name version not valid: %s') % self.name) buckets = self.name[1:].split('_', 1)[0] if len(buckets) % 2 != 0: raise clblob.InvalidRequest(_('Name bucket list corrupt: %s') % self.name) buckets = [buckets[offset:offset + 2] for offset in xrange(0, len(buckets), 2)] for cluster, bucket in enumerate(buckets): self._buckets[cluster] = clcommon.anybase.decode(bucket, 62) def replicas(self, replicas=None): '''Get or set the replicas for this event.''' if replicas is not None: self._replicas = replicas return if self._replicas is None: self._get_replicas() return self._replicas def _get_replicas(self): '''Get a preferred list of replicas for the given buckets. This will ignore replicas in other clusters if a cluster is configured, as well as the local replica if the client is a replica.''' self._replicas = {} self._is_local = False for cluster, bucket in self.buckets().iteritems(): if self._client.cluster is None or self._client.cluster == cluster: if self._client.bucket == bucket: self._is_local = True bucket = self._client.config['clusters'][cluster][bucket] for replica in bucket['replicas']: if self._client.replica != replica: self._replicas[replica] = True @property def is_local(self): '''Check to see if the local replica can handle this event.''' if self._is_local is None: self._get_replicas() return self._is_local @property def info(self): '''Make an info dictionary for responses.''' return dict(name=self.name, modified=self.modified, deleted=self.deleted, modified_deleted=self.modified_deleted, buckets=self.buckets()) class Get(Event): '''Event for tracking getting a blob.''' params = ['response'] def __init__(self, client, name, response): super(Get, self).__init__(client, 'get', name, client.config['request_timeout']) self.response = response if response == 'data': self.parse_response = False class Delete(Event): '''Event for tracking deleting a blob.''' params = ['deleted', 'modified_deleted', 'replicate'] def __init__(self, client, name, replicate): super(Delete, self).__init__(client, 'delete', name, client.config['request_timeout']) self.replicate = replicate class Put(Event): '''Event for tracking putting a blob.''' params = ['modified', 'deleted', 'modified_deleted', 'replicate', 'encoded'] def __init__(self, client, name, replicate, encoded): super(Put, self).__init__(client, 'put', name, client.config['request_timeout']) self.replicate = replicate self.encoded = encoded if encoded is False and client.config['encode_name']: self._encode_name() def _encode_name(self, version=0): '''Make a name encoded with clusters and buckets.''' encoded = [clcommon.anybase.encode(version, 62)] for _cluster, bucket in sorted(self.buckets().iteritems()): encoded.append(clcommon.anybase.encode(bucket, 62).zfill(2)) self.name = '%s_%s' % (''.join(encoded), self.name) self.encoded = True class Admin(Event): '''Event for tracking various admin tasks.''' def __init__(self, client, method, replica=None): replica = replica or client.replica if replica is None: raise clblob.RequestError(_('Must give replica')) elif replica not in client.config['replicas']: raise clblob.RequestError(_('Unknown replica: %s') % replica) super(Admin, self).__init__(client, method, '_%s/%s' % (method, replica), client.config['admin_timeout'], 'GET') self.replica = replica class ConfigCheck(Event): '''Event for tracking configcheck requests.''' params = ['brief', 'tolerance'] def __init__(self, client, replica=None): replica = replica or client.replica if replica is not None and replica not in client.config['replicas']: raise clblob.RequestError(_('Unknown replica: %s') % replica) super(ConfigCheck, self).__init__(client, 'configcheck', '_configcheck/%s' % replica, client.config['request_timeout'], 'PUT') self.replica = replica self.brief = None self.tolerance = None class List(Admin): '''Event for tracking list requests.''' params = ['modified_start', 'modified_stop', 'checksum', 'checksum_modulo'] def __init__(self, client, replica=None): super(List, self).__init__(client, 'list', replica) self.modified_start = None self.modified_stop = None self.checksum = None self.checksum_modulo = None class Sync(Admin): '''Event for tracking list requests.''' params = ['source', 'modified_start', 'modified_stop'] def __init__(self, client, replica=None): super(Sync, self).__init__(client, 'sync', replica) self.source = None self.modified_start = None self.modified_stop = None
#! /usr/bin/python # # Protocol Buffers - Google's data interchange format # Copyright 2008 Google Inc. All rights reserved. # http://code.google.com/p/protobuf/ # # Redistribution and use in source and binary forms, with or without # modification, are permitted provided that the following conditions are # met: # # * Redistributions of source code must retain the above copyright # notice, this list of conditions and the following disclaimer. # * Redistributions in binary form must reproduce the above # copyright notice, this list of conditions and the following disclaimer # in the documentation and/or other materials provided with the # distribution. # * Neither the name of Google Inc. nor the names of its # contributors may be used to endorse or promote products derived from # this software without specific prior written permission. # # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS # "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT # LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR # A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT # OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, # SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT # LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, # DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY # THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT # (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE # OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. """Test for google.protobuf.internal.wire_format.""" __author__ = 'robinson@google.com (Will Robinson)' import unittest from google.protobuf import message from google.protobuf.internal import wire_format class WireFormatTest(unittest.TestCase): def testPackTag(self): field_number = 0xabc tag_type = 2 self.assertEqual((field_number << 3) | tag_type, wire_format.PackTag(field_number, tag_type)) PackTag = wire_format.PackTag # Number too high. self.assertRaises(message.EncodeError, PackTag, field_number, 6) # Number too low. self.assertRaises(message.EncodeError, PackTag, field_number, -1) def testUnpackTag(self): # Test field numbers that will require various varint sizes. for expected_field_number in (1, 15, 16, 2047, 2048): for expected_wire_type in range(6): # Highest-numbered wiretype is 5. field_number, wire_type = wire_format.UnpackTag( wire_format.PackTag(expected_field_number, expected_wire_type)) self.assertEqual(expected_field_number, field_number) self.assertEqual(expected_wire_type, wire_type) self.assertRaises(TypeError, wire_format.UnpackTag, None) self.assertRaises(TypeError, wire_format.UnpackTag, 'abc') self.assertRaises(TypeError, wire_format.UnpackTag, 0.0) self.assertRaises(TypeError, wire_format.UnpackTag, object()) def testZigZagEncode(self): Z = wire_format.ZigZagEncode self.assertEqual(0, Z(0)) self.assertEqual(1, Z(-1)) self.assertEqual(2, Z(1)) self.assertEqual(3, Z(-2)) self.assertEqual(4, Z(2)) self.assertEqual(0xfffffffe, Z(0x7fffffff)) self.assertEqual(0xffffffff, Z(-0x80000000)) self.assertEqual(0xfffffffffffffffe, Z(0x7fffffffffffffff)) self.assertEqual(0xffffffffffffffff, Z(-0x8000000000000000)) self.assertRaises(TypeError, Z, None) self.assertRaises(TypeError, Z, 'abcd') self.assertRaises(TypeError, Z, 0.0) self.assertRaises(TypeError, Z, object()) def testZigZagDecode(self): Z = wire_format.ZigZagDecode self.assertEqual(0, Z(0)) self.assertEqual(-1, Z(1)) self.assertEqual(1, Z(2)) self.assertEqual(-2, Z(3)) self.assertEqual(2, Z(4)) self.assertEqual(0x7fffffff, Z(0xfffffffe)) self.assertEqual(-0x80000000, Z(0xffffffff)) self.assertEqual(0x7fffffffffffffff, Z(0xfffffffffffffffe)) self.assertEqual(-0x8000000000000000, Z(0xffffffffffffffff)) self.assertRaises(TypeError, Z, None) self.assertRaises(TypeError, Z, 'abcd') self.assertRaises(TypeError, Z, 0.0) self.assertRaises(TypeError, Z, object()) def NumericByteSizeTestHelper(self, byte_size_fn, value, expected_value_size): # Use field numbers that cause various byte sizes for the tag information. for field_number, tag_bytes in ((15, 1), (16, 2), (2047, 2), (2048, 3)): expected_size = expected_value_size + tag_bytes actual_size = byte_size_fn(field_number, value) self.assertEqual(expected_size, actual_size, 'byte_size_fn: %s, field_number: %d, value: %r\n' 'Expected: %d, Actual: %d'% ( byte_size_fn, field_number, value, expected_size, actual_size)) def testByteSizeFunctions(self): # Test all numeric *ByteSize() functions. NUMERIC_ARGS = [ # Int32ByteSize(). [wire_format.Int32ByteSize, 0, 1], [wire_format.Int32ByteSize, 127, 1], [wire_format.Int32ByteSize, 128, 2], [wire_format.Int32ByteSize, -1, 10], # Int64ByteSize(). [wire_format.Int64ByteSize, 0, 1], [wire_format.Int64ByteSize, 127, 1], [wire_format.Int64ByteSize, 128, 2], [wire_format.Int64ByteSize, -1, 10], # UInt32ByteSize(). [wire_format.UInt32ByteSize, 0, 1], [wire_format.UInt32ByteSize, 127, 1], [wire_format.UInt32ByteSize, 128, 2], [wire_format.UInt32ByteSize, wire_format.UINT32_MAX, 5], # UInt64ByteSize(). [wire_format.UInt64ByteSize, 0, 1], [wire_format.UInt64ByteSize, 127, 1], [wire_format.UInt64ByteSize, 128, 2], [wire_format.UInt64ByteSize, wire_format.UINT64_MAX, 10], # SInt32ByteSize(). [wire_format.SInt32ByteSize, 0, 1], [wire_format.SInt32ByteSize, -1, 1], [wire_format.SInt32ByteSize, 1, 1], [wire_format.SInt32ByteSize, -63, 1], [wire_format.SInt32ByteSize, 63, 1], [wire_format.SInt32ByteSize, -64, 1], [wire_format.SInt32ByteSize, 64, 2], # SInt64ByteSize(). [wire_format.SInt64ByteSize, 0, 1], [wire_format.SInt64ByteSize, -1, 1], [wire_format.SInt64ByteSize, 1, 1], [wire_format.SInt64ByteSize, -63, 1], [wire_format.SInt64ByteSize, 63, 1], [wire_format.SInt64ByteSize, -64, 1], [wire_format.SInt64ByteSize, 64, 2], # Fixed32ByteSize(). [wire_format.Fixed32ByteSize, 0, 4], [wire_format.Fixed32ByteSize, wire_format.UINT32_MAX, 4], # Fixed64ByteSize(). [wire_format.Fixed64ByteSize, 0, 8], [wire_format.Fixed64ByteSize, wire_format.UINT64_MAX, 8], # SFixed32ByteSize(). [wire_format.SFixed32ByteSize, 0, 4], [wire_format.SFixed32ByteSize, wire_format.INT32_MIN, 4], [wire_format.SFixed32ByteSize, wire_format.INT32_MAX, 4], # SFixed64ByteSize(). [wire_format.SFixed64ByteSize, 0, 8], [wire_format.SFixed64ByteSize, wire_format.INT64_MIN, 8], [wire_format.SFixed64ByteSize, wire_format.INT64_MAX, 8], # FloatByteSize(). [wire_format.FloatByteSize, 0.0, 4], [wire_format.FloatByteSize, 1000000000.0, 4], [wire_format.FloatByteSize, -1000000000.0, 4], # DoubleByteSize(). [wire_format.DoubleByteSize, 0.0, 8], [wire_format.DoubleByteSize, 1000000000.0, 8], [wire_format.DoubleByteSize, -1000000000.0, 8], # BoolByteSize(). [wire_format.BoolByteSize, False, 1], [wire_format.BoolByteSize, True, 1], # EnumByteSize(). [wire_format.EnumByteSize, 0, 1], [wire_format.EnumByteSize, 127, 1], [wire_format.EnumByteSize, 128, 2], [wire_format.EnumByteSize, wire_format.UINT32_MAX, 5], ] for args in NUMERIC_ARGS: self.NumericByteSizeTestHelper(*args) # Test strings and bytes. for byte_size_fn in (wire_format.StringByteSize, wire_format.BytesByteSize): # 1 byte for tag, 1 byte for length, 3 bytes for contents. self.assertEqual(5, byte_size_fn(10, 'abc')) # 2 bytes for tag, 1 byte for length, 3 bytes for contents. self.assertEqual(6, byte_size_fn(16, 'abc')) # 2 bytes for tag, 2 bytes for length, 128 bytes for contents. self.assertEqual(132, byte_size_fn(16, 'a' * 128)) # Test UTF-8 string byte size calculation. # 1 byte for tag, 1 byte for length, 8 bytes for content. self.assertEqual(10, wire_format.StringByteSize( 5, unicode('\xd0\xa2\xd0\xb5\xd1\x81\xd1\x82', 'utf-8'))) class MockMessage(object): def __init__(self, byte_size): self.byte_size = byte_size def ByteSize(self): return self.byte_size message_byte_size = 10 mock_message = MockMessage(byte_size=message_byte_size) # Test groups. # (2 * 1) bytes for begin and end tags, plus message_byte_size. self.assertEqual(2 + message_byte_size, wire_format.GroupByteSize(1, mock_message)) # (2 * 2) bytes for begin and end tags, plus message_byte_size. self.assertEqual(4 + message_byte_size, wire_format.GroupByteSize(16, mock_message)) # Test messages. # 1 byte for tag, plus 1 byte for length, plus contents. self.assertEqual(2 + mock_message.byte_size, wire_format.MessageByteSize(1, mock_message)) # 2 bytes for tag, plus 1 byte for length, plus contents. self.assertEqual(3 + mock_message.byte_size, wire_format.MessageByteSize(16, mock_message)) # 2 bytes for tag, plus 2 bytes for length, plus contents. mock_message.byte_size = 128 self.assertEqual(4 + mock_message.byte_size, wire_format.MessageByteSize(16, mock_message)) # Test message set item byte size. # 4 bytes for tags, plus 1 byte for length, plus 1 byte for type_id, # plus contents. mock_message.byte_size = 10 self.assertEqual(mock_message.byte_size + 6, wire_format.MessageSetItemByteSize(1, mock_message)) # 4 bytes for tags, plus 2 bytes for length, plus 1 byte for type_id, # plus contents. mock_message.byte_size = 128 self.assertEqual(mock_message.byte_size + 7, wire_format.MessageSetItemByteSize(1, mock_message)) # 4 bytes for tags, plus 2 bytes for length, plus 2 byte for type_id, # plus contents. self.assertEqual(mock_message.byte_size + 8, wire_format.MessageSetItemByteSize(128, mock_message)) # Too-long varint. self.assertRaises(message.EncodeError, wire_format.UInt64ByteSize, 1, 1 << 128) if __name__ == '__main__': unittest.main()
import unittest from test import support from test.test_urllib2 import sanepathname2url import os import socket import urllib.error import urllib.request import sys try: import ssl except ImportError: ssl = None support.requires("network") TIMEOUT = 60 # seconds def _retry_thrice(func, exc, *args, **kwargs): for i in range(3): try: return func(*args, **kwargs) except exc as e: last_exc = e continue except: raise raise last_exc def _wrap_with_retry_thrice(func, exc): def wrapped(*args, **kwargs): return _retry_thrice(func, exc, *args, **kwargs) return wrapped # Connecting to remote hosts is flaky. Make it more robust by retrying # the connection several times. _urlopen_with_retry = _wrap_with_retry_thrice(urllib.request.urlopen, urllib.error.URLError) class AuthTests(unittest.TestCase): """Tests urllib2 authentication features.""" ## Disabled at the moment since there is no page under python.org which ## could be used to HTTP authentication. # # def test_basic_auth(self): # import http.client # # test_url = "http://www.python.org/test/test_urllib2/basic_auth" # test_hostport = "www.python.org" # test_realm = 'Test Realm' # test_user = 'test.test_urllib2net' # test_password = 'blah' # # # failure # try: # _urlopen_with_retry(test_url) # except urllib2.HTTPError, exc: # self.assertEqual(exc.code, 401) # else: # self.fail("urlopen() should have failed with 401") # # # success # auth_handler = urllib2.HTTPBasicAuthHandler() # auth_handler.add_password(test_realm, test_hostport, # test_user, test_password) # opener = urllib2.build_opener(auth_handler) # f = opener.open('http://localhost/') # response = _urlopen_with_retry("http://www.python.org/") # # # The 'userinfo' URL component is deprecated by RFC 3986 for security # # reasons, let's not implement it! (it's already implemented for proxy # # specification strings (that is, URLs or authorities specifying a # # proxy), so we must keep that) # self.assertRaises(http.client.InvalidURL, # urllib2.urlopen, "http://evil:thing@example.com") class CloseSocketTest(unittest.TestCase): def test_close(self): # calling .close() on urllib2's response objects should close the # underlying socket url = "http://www.example.com/" with support.transient_internet(url): response = _urlopen_with_retry(url) sock = response.fp self.assertFalse(sock.closed) response.close() self.assertTrue(sock.closed) class OtherNetworkTests(unittest.TestCase): def setUp(self): if 0: # for debugging import logging logger = logging.getLogger("test_urllib2net") logger.addHandler(logging.StreamHandler()) # XXX The rest of these tests aren't very good -- they don't check much. # They do sometimes catch some major disasters, though. def test_ftp(self): urls = [ 'ftp://ftp.debian.org/debian/README', ('ftp://ftp.debian.org/debian/non-existent-file', None, urllib.error.URLError), 'ftp://gatekeeper.research.compaq.com/pub/DEC/SRC' '/research-reports/00README-Legal-Rules-Regs', ] self._test_urls(urls, self._extra_handlers()) def test_file(self): TESTFN = support.TESTFN f = open(TESTFN, 'w') try: f.write('hi there\n') f.close() urls = [ 'file:' + sanepathname2url(os.path.abspath(TESTFN)), ('file:///nonsensename/etc/passwd', None, urllib.error.URLError), ] self._test_urls(urls, self._extra_handlers(), retry=True) finally: os.remove(TESTFN) self.assertRaises(ValueError, urllib.request.urlopen,'./relative_path/to/file') # XXX Following test depends on machine configurations that are internal # to CNRI. Need to set up a public server with the right authentication # configuration for test purposes. ## def test_cnri(self): ## if socket.gethostname() == 'bitdiddle': ## localhost = 'bitdiddle.cnri.reston.va.us' ## elif socket.gethostname() == 'bitdiddle.concentric.net': ## localhost = 'localhost' ## else: ## localhost = None ## if localhost is not None: ## urls = [ ## 'file://%s/etc/passwd' % localhost, ## 'http://%s/simple/' % localhost, ## 'http://%s/digest/' % localhost, ## 'http://%s/not/found.h' % localhost, ## ] ## bauth = HTTPBasicAuthHandler() ## bauth.add_password('basic_test_realm', localhost, 'jhylton', ## 'password') ## dauth = HTTPDigestAuthHandler() ## dauth.add_password('digest_test_realm', localhost, 'jhylton', ## 'password') ## self._test_urls(urls, self._extra_handlers()+[bauth, dauth]) def test_urlwithfrag(self): urlwith_frag = "https://docs.python.org/2/glossary.html#glossary" with support.transient_internet(urlwith_frag): req = urllib.request.Request(urlwith_frag) res = urllib.request.urlopen(req) self.assertEqual(res.geturl(), "https://docs.python.org/2/glossary.html#glossary") def test_redirect_url_withfrag(self): redirect_url_with_frag = "http://bit.ly/1iSHToT" with support.transient_internet(redirect_url_with_frag): req = urllib.request.Request(redirect_url_with_frag) res = urllib.request.urlopen(req) self.assertEqual(res.geturl(), "https://docs.python.org/3.4/glossary.html#term-global-interpreter-lock") def test_custom_headers(self): url = "http://www.example.com" with support.transient_internet(url): opener = urllib.request.build_opener() request = urllib.request.Request(url) self.assertFalse(request.header_items()) opener.open(request) self.assertTrue(request.header_items()) self.assertTrue(request.has_header('User-agent')) request.add_header('User-Agent','Test-Agent') opener.open(request) self.assertEqual(request.get_header('User-agent'),'Test-Agent') def test_sites_no_connection_close(self): # Some sites do not send Connection: close header. # Verify that those work properly. (#issue12576) URL = 'http://www.imdb.com' # mangles Connection:close with support.transient_internet(URL): try: with urllib.request.urlopen(URL) as res: pass except ValueError as e: self.fail("urlopen failed for site not sending \ Connection:close") else: self.assertTrue(res) req = urllib.request.urlopen(URL) res = req.read() self.assertTrue(res) def _test_urls(self, urls, handlers, retry=True): import time import logging debug = logging.getLogger("test_urllib2").debug urlopen = urllib.request.build_opener(*handlers).open if retry: urlopen = _wrap_with_retry_thrice(urlopen, urllib.error.URLError) for url in urls: with self.subTest(url=url): if isinstance(url, tuple): url, req, expected_err = url else: req = expected_err = None with support.transient_internet(url): try: f = urlopen(url, req, TIMEOUT) except OSError as err: if expected_err: msg = ("Didn't get expected error(s) %s for %s %s, got %s: %s" % (expected_err, url, req, type(err), err)) self.assertIsInstance(err, expected_err, msg) else: raise except urllib.error.URLError as err: if isinstance(err[0], socket.timeout): print("<timeout: %s>" % url, file=sys.stderr) continue else: raise else: try: with support.time_out, \ support.socket_peer_reset, \ support.ioerror_peer_reset: buf = f.read() debug("read %d bytes" % len(buf)) except socket.timeout: print("<timeout: %s>" % url, file=sys.stderr) f.close() time.sleep(0.1) def _extra_handlers(self): handlers = [] cfh = urllib.request.CacheFTPHandler() self.addCleanup(cfh.clear_cache) cfh.setTimeout(1) handlers.append(cfh) return handlers class TimeoutTest(unittest.TestCase): def test_http_basic(self): self.assertIsNone(socket.getdefaulttimeout()) url = "http://www.example.com" with support.transient_internet(url, timeout=None): u = _urlopen_with_retry(url) self.addCleanup(u.close) self.assertIsNone(u.fp.raw._sock.gettimeout()) def test_http_default_timeout(self): self.assertIsNone(socket.getdefaulttimeout()) url = "http://www.example.com" with support.transient_internet(url): socket.setdefaulttimeout(60) try: u = _urlopen_with_retry(url) self.addCleanup(u.close) finally: socket.setdefaulttimeout(None) self.assertEqual(u.fp.raw._sock.gettimeout(), 60) def test_http_no_timeout(self): self.assertIsNone(socket.getdefaulttimeout()) url = "http://www.example.com" with support.transient_internet(url): socket.setdefaulttimeout(60) try: u = _urlopen_with_retry(url, timeout=None) self.addCleanup(u.close) finally: socket.setdefaulttimeout(None) self.assertIsNone(u.fp.raw._sock.gettimeout()) def test_http_timeout(self): url = "http://www.example.com" with support.transient_internet(url): u = _urlopen_with_retry(url, timeout=120) self.addCleanup(u.close) self.assertEqual(u.fp.raw._sock.gettimeout(), 120) FTP_HOST = "ftp://ftp.mirror.nl/pub/gnu/" def test_ftp_basic(self): self.assertIsNone(socket.getdefaulttimeout()) with support.transient_internet(self.FTP_HOST, timeout=None): u = _urlopen_with_retry(self.FTP_HOST) self.addCleanup(u.close) self.assertIsNone(u.fp.fp.raw._sock.gettimeout()) def test_ftp_default_timeout(self): self.assertIsNone(socket.getdefaulttimeout()) with support.transient_internet(self.FTP_HOST): socket.setdefaulttimeout(60) try: u = _urlopen_with_retry(self.FTP_HOST) self.addCleanup(u.close) finally: socket.setdefaulttimeout(None) self.assertEqual(u.fp.fp.raw._sock.gettimeout(), 60) def test_ftp_no_timeout(self): self.assertIsNone(socket.getdefaulttimeout()) with support.transient_internet(self.FTP_HOST): socket.setdefaulttimeout(60) try: u = _urlopen_with_retry(self.FTP_HOST, timeout=None) self.addCleanup(u.close) finally: socket.setdefaulttimeout(None) self.assertIsNone(u.fp.fp.raw._sock.gettimeout()) def test_ftp_timeout(self): with support.transient_internet(self.FTP_HOST): u = _urlopen_with_retry(self.FTP_HOST, timeout=60) self.addCleanup(u.close) self.assertEqual(u.fp.fp.raw._sock.gettimeout(), 60) if __name__ == "__main__": unittest.main()
#!/usr/bin/python # Copyright (c) 2009 The Regents of the University of California. All rights reserved. # # Permission is hereby granted, without written agreement and without # license or royalty fees, to use, copy, modify, and distribute this # software and its documentation for any purpose, provided that the # above copyright notice and the following two paragraphs appear in # all copies of this software. # # IN NO EVENT SHALL THE UNIVERSITY OF CALIFORNIA BE LIABLE TO ANY PARTY # FOR DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES # ARISING OUT OF THE USE OF THIS SOFTWARE AND ITS DOCUMENTATION, EVEN # IF THE UNIVERSITY OF CALIFORNIA HAS BEEN ADVISED OF THE POSSIBILITY # OF SUCH DAMAGE. # # THE UNIVERSITY OF CALIFORNIA SPECIFICALLY DISCLAIMS ANY WARRANTIES, # INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY # AND FITNESS FOR A PARTICULAR PURPOSE. THE SOFTWARE PROVIDED HEREUNDER IS # ON AN "AS IS" BASIS, AND THE UNIVERSITY OF CALIFORNIA HAS NO OBLIGATION # TO PROVIDE MAINTENANCE, SUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS. import time, subprocess, optparse, sys, socket, os sys.path.append("../") import rtest as rtest solve = "liquid ".split() null = open("/dev/null", "w") now = (time.asctime(time.localtime(time.time()))).replace(" ","_") logfile = "../tests/logs/regrtest_results_%s_%s" % (socket.gethostname (), now) argcomment = "--! run with " liquidcomment = "{--! run liquid with " endcomment = "-}" def logged_sys_call(args, out=None, err=None, dir=None): print "exec: " + " ".join(args) return subprocess.call(args, stdout=out, stderr=err, cwd=dir) def solve_quals(dir,file,bare,time,quiet,flags,lflags): if quiet: out = null else: out = None if time: time = ["time"] else: time = [] if lflags: lflags = ["--" + f for f in lflags] hygiene_flags = [] (dn, bn) = os.path.split(file) try: os.makedirs(os.path.join(dir,dn,".liquid")) except OSError: pass out = open(os.path.join(dir,dn,".liquid",bn) + ".log", "w") rv = logged_sys_call(time + solve + flags + lflags + hygiene_flags + [file], out=None, err=subprocess.STDOUT, dir=dir) out.close() return rv def run_script(file,quiet): if quiet: out = null else: out = None return logged_sys_call(file, out) def getfileargs(file): f = open(file) l = f.readline() f.close() if l.startswith(argcomment): return l[len(argcomment):].strip().split(" ") else: return [] def getliquidargs(file): f = open(file) l = f.readline() f.close() if l.startswith(liquidcomment): return [arg for arg in l[len(liquidcomment):].strip().split(" ") if arg!=endcomment] else: return [] class Config (rtest.TestConfig): def __init__ (self, dargs, testdirs, logfile, threadcount): rtest.TestConfig.__init__ (self, testdirs, logfile, threadcount) self.dargs = dargs def run_test (self, dir, file): path = os.path.join(dir,file) if self.is_test(file): lflags = getliquidargs(path) fargs = getfileargs(path) fargs = self.dargs + fargs return solve_quals(dir, file, True, False, True, fargs, lflags) elif file.endswith(".sh"): return run_script(path, True) def is_test (self, file): return file.endswith(".hs") # or file.endswith(".lhs") ##################################################################################### #DEFAULT textIgnored = { "Data/Text/Axioms.hs" , "Data/Text/Encoding/Error.hs" , "Data/Text/Encoding/Fusion.hs" , "Data/Text/Encoding/Fusion/Common.hs" , "Data/Text/Encoding/Utf16.hs" , "Data/Text/Encoding/Utf32.hs" , "Data/Text/Encoding/Utf8.hs" , "Data/Text/Fusion/CaseMapping.hs" , "Data/Text/Fusion/Common.hs" , "Data/Text/Fusion/Internal.hs" , "Data/Text/IO.hs" , "Data/Text/IO/Internal.hs" , "Data/Text/Lazy/Builder/Functions.hs" , "Data/Text/Lazy/Builder/Int.hs" , "Data/Text/Lazy/Builder/Int/Digits.hs" , "Data/Text/Lazy/Builder/Internal.hs" , "Data/Text/Lazy/Builder/RealFloat.hs" , "Data/Text/Lazy/Builder/RealFloat/Functions.hs" , "Data/Text/Lazy/Encoding/Fusion.hs" , "Data/Text/Lazy/IO.hs" , "Data/Text/Lazy/Read.hs" , "Data/Text/Read.hs" , "Data/Text/Unsafe/Base.hs" , "Data/Text/UnsafeShift.hs" , "Data/Text/Util.hs" } demosIgnored = { "Composition.hs" , "Eval.hs" , "Inductive.hs" , "Loop.hs" , "TalkingAboutSets.hs" , "refinements101reax.hs" } regtestdirs = [ ("pos", {}, 0) , ("neg", {}, 1) , ("crash", {}, 2) , ("parser/pos", {}, 0) , ("error_messages/pos", {}, 0) , ("error_messages/crash", {}, 2) ] benchtestdirs = [ ("../web/demos", demosIgnored, 0) , ("../benchmarks/esop2013-submission", {"Base0.hs"}, 0) , ("../benchmarks/bytestring-0.9.2.1", {}, 0) , ("../benchmarks/text-0.11.2.3", textIgnored, 0) , ("../benchmarks/vector-algorithms-0.5.4.2", {}, 0) , ("../benchmarks/hscolour-1.20.0.0", {}, 0) ] parser = optparse.OptionParser() parser.add_option("-a", "--all", action="store_true", dest="alltests", help="run all tests") parser.add_option("-t", "--threads", dest="threadcount", default=1, type=int, help="spawn n threads") parser.add_option("-o", "--opts", dest="opts", default=[], action='append', type=str, help="additional arguments to liquid") parser.disable_interspersed_args() options, args = parser.parse_args() print "options =", options print "args =", args def testdirs(): global testdirs if options.alltests: return regtestdirs + benchtestdirs else: return regtestdirs testdirs = testdirs() clean = os.path.abspath("../cleanup") [os.system(("cd %s; %s; cd ../" % (d,clean))) for (d,_,_) in testdirs] runner = rtest.TestRunner (Config (options.opts, testdirs, logfile, options.threadcount)) sys.exit(runner.run())
# Copyright (c) 2006-2012 Mitch Garnaat http://garnaat.org/ # Copyright (c) 2010, Eucalyptus Systems, Inc. # Copyright (c) 2014, Steven Richards <sbrichards@mit.edu> # All rights reserved. # # Permission is hereby granted, free of charge, to any person obtaining a # copy of this software and associated documentation files (the # "Software"), to deal in the Software without restriction, including # without limitation the rights to use, copy, modify, merge, publish, dis- # tribute, sublicense, and/or sell copies of the Software, and to permit # persons to whom the Software is furnished to do so, subject to the fol- # lowing conditions: # # The above copyright notice and this permission notice shall be included # in all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS # OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL- # ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT # SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, # WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS # IN THE SOFTWARE. # from boto.regioninfo import RegionInfo, get_regions class S3RegionInfo(RegionInfo): def connect(self, **kw_params): """ Connect to this Region's endpoint. Returns an connection object pointing to the endpoint associated with this region. You may pass any of the arguments accepted by the connection class's constructor as keyword arguments and they will be passed along to the connection object. :rtype: Connection object :return: The connection to this regions endpoint """ if self.connection_cls: return self.connection_cls(host=self.endpoint, **kw_params) def regions(): """ Get all available regions for the Amazon S3 service. :rtype: list :return: A list of :class:`boto.regioninfo.RegionInfo` """ from boto.s3.connection import S3Connection return get_regions( 's3', region_cls=S3RegionInfo, connection_cls=S3Connection ) def connect_to_region(region_name, **kw_params): for region in regions(): if 'host' in kw_params.keys(): # Make sure the host specified is not nothing if kw_params['host'] not in ['', None]: region.endpoint = kw_params['host'] del kw_params['host'] return region.connect(**kw_params) # If it is nothing then remove it from kw_params and proceed with default else: del kw_params['host'] if region.name == region_name: return region.connect(**kw_params) return None
# Copyright 2015 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== """Utility functions for reading/writing graphs.""" from __future__ import absolute_import from __future__ import division from __future__ import print_function import os import os.path from google.protobuf import text_format from tensorflow.python.framework import ops from tensorflow.python.lib.io import file_io from tensorflow.python.util.tf_export import tf_export @tf_export('io.write_graph', 'train.write_graph') def write_graph(graph_or_graph_def, logdir, name, as_text=True): """Writes a graph proto to a file. The graph is written as a text proto unless `as_text` is `False`. ```python v = tf.Variable(0, name='my_variable') sess = tf.Session() tf.train.write_graph(sess.graph_def, '/tmp/my-model', 'train.pbtxt') ``` or ```python v = tf.Variable(0, name='my_variable') sess = tf.Session() tf.train.write_graph(sess.graph, '/tmp/my-model', 'train.pbtxt') ``` Args: graph_or_graph_def: A `Graph` or a `GraphDef` protocol buffer. logdir: Directory where to write the graph. This can refer to remote filesystems, such as Google Cloud Storage (GCS). name: Filename for the graph. as_text: If `True`, writes the graph as an ASCII proto. Returns: The path of the output proto file. """ if isinstance(graph_or_graph_def, ops.Graph): graph_def = graph_or_graph_def.as_graph_def() else: graph_def = graph_or_graph_def # gcs does not have the concept of directory at the moment. if not file_io.file_exists(logdir) and not logdir.startswith('gs:'): file_io.recursive_create_dir(logdir) path = os.path.join(logdir, name) if as_text: file_io.atomic_write_string_to_file(path, text_format.MessageToString(graph_def)) else: file_io.atomic_write_string_to_file(path, graph_def.SerializeToString()) return path
#!/usr/bin/env python # (C) British Crown Copyright 2010 - 2014, Met Office # # This file was heavily influenced by a similar file in the iris package. """ Provides "diff-like" comparison of images. Currently relies on matplotlib for image processing so limited to PNG format. """ from __future__ import (absolute_import, division, print_function) import os.path import shutil import matplotlib.pyplot as plt import matplotlib.image as mimg import matplotlib.widgets as mwidget def diff_viewer(expected_fname, result_fname, diff_fname): plt.figure(figsize=(16, 16)) plt.suptitle(os.path.basename(expected_fname)) ax = plt.subplot(221) ax.imshow(mimg.imread(expected_fname)) ax = plt.subplot(222, sharex=ax, sharey=ax) ax.imshow(mimg.imread(result_fname)) ax = plt.subplot(223, sharex=ax, sharey=ax) ax.imshow(mimg.imread(diff_fname)) def accept(event): # removes the expected result, and move the most recent result in print('ACCEPTED NEW FILE: %s' % (os.path.basename(expected_fname), )) os.remove(expected_fname) shutil.copy2(result_fname, expected_fname) os.remove(diff_fname) plt.close() def reject(event): print('REJECTED: %s' % (os.path.basename(expected_fname), )) plt.close() ax_accept = plt.axes([0.6, 0.35, 0.1, 0.075]) ax_reject = plt.axes([0.71, 0.35, 0.1, 0.075]) bnext = mwidget.Button(ax_accept, 'Accept change') bnext.on_clicked(accept) bprev = mwidget.Button(ax_reject, 'Reject') bprev.on_clicked(reject) plt.show() def step_over_diffs(): import cis.test.plot_tests image_dir = os.path.join(os.path.dirname(cis.test.plot_tests.__file__), 'reference', 'visual_tests') diff_dir = os.path.join(os.path.dirname(cis.test.plot_tests.__file__), 'result_image_comparison') for expected_fname in sorted(os.listdir(image_dir)): result_path = os.path.join(diff_dir, expected_fname) diff_path = result_path[:-4] + '-failed-diff.png' # if the test failed, there will be a diff file if os.path.exists(diff_path): expected_path = os.path.join(image_dir, expected_fname) diff_viewer(expected_path, result_path, diff_path) if __name__ == '__main__': step_over_diffs()
"""Steps and utility functions for taking screenshots.""" import uuid from lettuce import ( after, step, world, ) import os.path import json def set_save_directory(base, source): """Sets the root save directory for saving screenshots. Screenshots will be saved in subdirectories under this directory by browser window size. """ root = os.path.join(base, source) if not os.path.isdir(root): os.makedirs(root) world.screenshot_root = root def resolution_path(world): window_size = world.browser.get_window_size() return os.path.join( world.screenshot_root, '{}x{}'.format(window_size['width'], window_size['height']), ) @step(r'I capture a screenshot$') def capture_screenshot(step): feature = step.scenario.feature step.shot_name = '{}.png'.format(uuid.uuid4()) if getattr(feature, 'dir_path', None) is None: feature.dir_path = resolution_path(world) if not os.path.isdir(feature.dir_path): os.makedirs(feature.dir_path) filename = os.path.join( feature.dir_path, step.shot_name, ) world.browser.get_screenshot_as_file(filename) @step(r'I capture a screenshot after (\d+) seconds?$') def capture_screenshot_delay(step, delay): time.sleep(delay) capture_screenshot() @after.each_feature def record_run_feature_report(feature): if getattr(feature, 'dir_path', None) is None: return feature_name_json = '{}.json'.format(os.path.splitext( os.path.basename(feature.described_at.file) )[0]) report = {} for scenario in feature.scenarios: scenario_report = [] for step in scenario.steps: shot_name = getattr(step, 'shot_name', None) if shot_name is not None: scenario_report.append(shot_name) if scenario_report: report[scenario.name] = scenario_report if report: with open(os.path.join(feature.dir_path, feature_name_json), 'w') as f: json.dump(report, f)
""" byceps.services.ticketing.models.ticket_event ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ :Copyright: 2006-2019 Jochen Kupperschmidt :License: Modified BSD, see LICENSE for details. """ from datetime import datetime from typing import Any, Dict from ....database import db, generate_uuid from ....util.instances import ReprBuilder from ..transfer.models import TicketID TicketEventData = Dict[str, Any] class TicketEvent(db.Model): """An event that refers to a ticket.""" __tablename__ = 'ticket_events' id = db.Column(db.Uuid, default=generate_uuid, primary_key=True) occurred_at = db.Column(db.DateTime, nullable=False) event_type = db.Column(db.UnicodeText, index=True, nullable=False) ticket_id = db.Column(db.Uuid, db.ForeignKey('tickets.id'), index=True, nullable=False) data = db.Column(db.JSONB) def __init__(self, occurred_at: datetime, event_type: str, ticket_id: TicketID, data: TicketEventData) -> None: self.occurred_at = occurred_at self.event_type = event_type self.ticket_id = ticket_id self.data = data def __repr__(self) -> str: return ReprBuilder(self) \ .add_custom(repr(self.event_type)) \ .add_with_lookup('ticket_id') \ .add_with_lookup('data') \ .build()
"""Tests for distutils.command.install_data.""" import sys import os import unittest from distutils.command.install_lib import install_lib from distutils.extension import Extension from distutils.tests import support from distutils.errors import DistutilsOptionError from test.support import run_unittest class InstallLibTestCase(support.TempdirManager, support.LoggingSilencer, support.EnvironGuard, unittest.TestCase): def test_finalize_options(self): pkg_dir, dist = self.create_dist() cmd = install_lib(dist) cmd.finalize_options() self.assertEqual(cmd.compile, 1) self.assertEqual(cmd.optimize, 0) # optimize must be 0, 1, or 2 cmd.optimize = 'foo' self.assertRaises(DistutilsOptionError, cmd.finalize_options) cmd.optimize = '4' self.assertRaises(DistutilsOptionError, cmd.finalize_options) cmd.optimize = '2' cmd.finalize_options() self.assertEqual(cmd.optimize, 2) @unittest.skipUnless(not sys.dont_write_bytecode, 'byte-compile not supported') def test_byte_compile(self): pkg_dir, dist = self.create_dist() cmd = install_lib(dist) cmd.compile = cmd.optimize = 1 f = os.path.join(pkg_dir, 'foo.py') self.write_file(f, '# python file') cmd.byte_compile([f]) self.assertTrue(os.path.exists(os.path.join(pkg_dir, 'foo.pyc'))) self.assertTrue(os.path.exists(os.path.join(pkg_dir, 'foo.pyo'))) def test_get_outputs(self): pkg_dir, dist = self.create_dist() cmd = install_lib(dist) # setting up a dist environment cmd.compile = cmd.optimize = 1 cmd.install_dir = pkg_dir f = os.path.join(pkg_dir, 'foo.py') self.write_file(f, '# python file') cmd.distribution.py_modules = [pkg_dir] cmd.distribution.ext_modules = [Extension('foo', ['xxx'])] cmd.distribution.packages = [pkg_dir] cmd.distribution.script_name = 'setup.py' # get_output should return 4 elements self.assertTrue(len(cmd.get_outputs()) >= 2) def test_get_inputs(self): pkg_dir, dist = self.create_dist() cmd = install_lib(dist) # setting up a dist environment cmd.compile = cmd.optimize = 1 cmd.install_dir = pkg_dir f = os.path.join(pkg_dir, 'foo.py') self.write_file(f, '# python file') cmd.distribution.py_modules = [pkg_dir] cmd.distribution.ext_modules = [Extension('foo', ['xxx'])] cmd.distribution.packages = [pkg_dir] cmd.distribution.script_name = 'setup.py' # get_input should return 2 elements self.assertEqual(len(cmd.get_inputs()), 2) def test_dont_write_bytecode(self): # makes sure byte_compile is not used pkg_dir, dist = self.create_dist() cmd = install_lib(dist) cmd.compile = 1 cmd.optimize = 1 old_dont_write_bytecode = sys.dont_write_bytecode sys.dont_write_bytecode = True try: cmd.byte_compile([]) finally: sys.dont_write_bytecode = old_dont_write_bytecode self.assertTrue('byte-compiling is disabled' in self.logs[0][1]) def test_suite(): return unittest.makeSuite(InstallLibTestCase) if __name__ == "__main__": run_unittest(test_suite())
# -*- coding: utf-8 -*- # Copyright 2007-2016 The HyperSpy developers # # This file is part of HyperSpy. # # HyperSpy is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # HyperSpy is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with HyperSpy. If not, see <http://www.gnu.org/licenses/>. import os import warnings from hyperspy.defaults_parser import preferences preferences.General.show_progressbar = False # Check if we should fail on external deprecation messages fail_on_external = os.environ.pop('FAIL_ON_EXTERNAL_DEPRECATION', False) if isinstance(fail_on_external, str): fail_on_external = (fail_on_external.lower() in ['true', 't', '1', 'yes', 'y', 'set']) if fail_on_external: warnings.filterwarnings( 'error', category=DeprecationWarning) # Travis setup has these warnings, so ignore: warnings.filterwarnings( 'ignore', r"BaseException\.message has been deprecated as of Python 2\.6", DeprecationWarning) # Don't care about warnings in hyperspy in this mode! warnings.filterwarnings('default', module="hyperspy") else: # Fall-back filter: Error warnings.simplefilter('error') warnings.filterwarnings( 'ignore', "Failed to import the optional scikit image package", UserWarning) # We allow extrernal warnings: warnings.filterwarnings('default', module="(?!hyperspy)")
# Copyright (c) 2006,2007 Mitch Garnaat http://garnaat.org/ # # Permission is hereby granted, free of charge, to any person obtaining a # copy of this software and associated documentation files (the # "Software"), to deal in the Software without restriction, including # without limitation the rights to use, copy, modify, merge, publish, dis- # tribute, sublicense, and/or sell copies of the Software, and to permit # persons to whom the Software is furnished to do so, subject to the fol- # lowing conditions: # # The above copyright notice and this permission notice shall be included # in all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS # OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL- # ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT # SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, # WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS # IN THE SOFTWARE. from boto.s3.user import User class ResultSet(list): """ The ResultSet is used to pass results back from the Amazon services to the client. It is light wrapper around Python's :py:class:`list` class, with some additional methods for parsing XML results from AWS. Because I don't really want any dependencies on external libraries, I'm using the standard SAX parser that comes with Python. The good news is that it's quite fast and efficient but it makes some things rather difficult. You can pass in, as the marker_elem parameter, a list of tuples. Each tuple contains a string as the first element which represents the XML element that the resultset needs to be on the lookout for and a Python class as the second element of the tuple. Each time the specified element is found in the XML, a new instance of the class will be created and popped onto the stack. :ivar str next_token: A hash used to assist in paging through very long result sets. In most cases, passing this value to certain methods will give you another 'page' of results. """ def __init__(self, marker_elem=None): list.__init__(self) if isinstance(marker_elem, list): self.markers = marker_elem else: self.markers = [] self.marker = None self.key_marker = None self.next_marker = None # avail when delimiter used self.next_key_marker = None self.next_upload_id_marker = None self.next_version_id_marker = None self.next_generation_marker= None self.version_id_marker = None self.is_truncated = False self.next_token = None self.status = True def startElement(self, name, attrs, connection): for t in self.markers: if name == t[0]: obj = t[1](connection) self.append(obj) return obj if name == 'Owner': # Makes owner available for get_service and # perhaps other lists where not handled by # another element. self.owner = User() return self.owner return None def to_boolean(self, value, true_value='true'): if value == true_value: return True else: return False def endElement(self, name, value, connection): if name == 'IsTruncated': self.is_truncated = self.to_boolean(value) elif name == 'Marker': self.marker = value elif name == 'KeyMarker': self.key_marker = value elif name == 'NextMarker': self.next_marker = value elif name == 'NextKeyMarker': self.next_key_marker = value elif name == 'VersionIdMarker': self.version_id_marker = value elif name == 'NextVersionIdMarker': self.next_version_id_marker = value elif name == 'NextGenerationMarker': self.next_generation_marker = value elif name == 'UploadIdMarker': self.upload_id_marker = value elif name == 'NextUploadIdMarker': self.next_upload_id_marker = value elif name == 'Bucket': self.bucket = value elif name == 'MaxUploads': self.max_uploads = int(value) elif name == 'MaxItems': self.max_items = int(value) elif name == 'Prefix': self.prefix = value elif name == 'return': self.status = self.to_boolean(value) elif name == 'StatusCode': self.status = self.to_boolean(value, 'Success') elif name == 'ItemName': self.append(value) elif name == 'NextToken': self.next_token = value elif name == 'nextToken': self.next_token = value # Code exists which expects nextToken to be available, so we # set it here to remain backwards-compatibile. self.nextToken = value elif name == 'BoxUsage': try: connection.box_usage += float(value) except: pass elif name == 'IsValid': self.status = self.to_boolean(value, 'True') else: setattr(self, name, value) class BooleanResult(object): def __init__(self, marker_elem=None): self.status = True self.request_id = None self.box_usage = None def __repr__(self): if self.status: return 'True' else: return 'False' def __nonzero__(self): return self.status def startElement(self, name, attrs, connection): return None def to_boolean(self, value, true_value='true'): if value == true_value: return True else: return False def endElement(self, name, value, connection): if name == 'return': self.status = self.to_boolean(value) elif name == 'StatusCode': self.status = self.to_boolean(value, 'Success') elif name == 'IsValid': self.status = self.to_boolean(value, 'True') elif name == 'RequestId': self.request_id = value elif name == 'requestId': self.request_id = value elif name == 'BoxUsage': self.request_id = value else: setattr(self, name, value)
#! /usr/bin/env python # encoding: utf-8 # WARNING! Do not edit! http://waf.googlecode.com/git/docs/wafbook/single.html#_obtaining_the_waf_file import os,re,traceback,sys _nocolor=os.environ.get('NOCOLOR','no')not in('no','0','false') try: if not _nocolor: import waflib.ansiterm except ImportError: pass try: import threading except ImportError: if not'JOBS'in os.environ: os.environ['JOBS']='1' else: wlock=threading.Lock() class sync_stream(object): def __init__(self,stream): self.stream=stream self.encoding=self.stream.encoding def write(self,txt): try: wlock.acquire() self.stream.write(txt) self.stream.flush() finally: wlock.release() def fileno(self): return self.stream.fileno() def flush(self): self.stream.flush() def isatty(self): return self.stream.isatty() if not os.environ.get('NOSYNC',False): if id(sys.stdout)==id(sys.__stdout__): sys.stdout=sync_stream(sys.stdout) sys.stderr=sync_stream(sys.stderr) import logging LOG_FORMAT="%(asctime)s %(c1)s%(zone)s%(c2)s %(message)s" HOUR_FORMAT="%H:%M:%S" zones='' verbose=0 colors_lst={'USE':True,'BOLD':'\x1b[01;1m','RED':'\x1b[01;31m','GREEN':'\x1b[32m','YELLOW':'\x1b[33m','PINK':'\x1b[35m','BLUE':'\x1b[01;34m','CYAN':'\x1b[36m','NORMAL':'\x1b[0m','cursor_on':'\x1b[?25h','cursor_off':'\x1b[?25l',} got_tty=not os.environ.get('TERM','dumb')in['dumb','emacs'] if got_tty: try: got_tty=sys.stderr.isatty()and sys.stdout.isatty() except AttributeError: got_tty=False if(not got_tty and os.environ.get('TERM','dumb')!='msys')or _nocolor: colors_lst['USE']=False def get_term_cols(): return 80 try: import struct,fcntl,termios except ImportError: pass else: if got_tty: def get_term_cols_real(): dummy_lines,cols=struct.unpack("HHHH",fcntl.ioctl(sys.stderr.fileno(),termios.TIOCGWINSZ,struct.pack("HHHH",0,0,0,0)))[:2] return cols try: get_term_cols_real() except Exception: pass else: get_term_cols=get_term_cols_real get_term_cols.__doc__=""" Get the console width in characters. :return: the number of characters per line :rtype: int """ def get_color(cl): if not colors_lst['USE']:return'' return colors_lst.get(cl,'') class color_dict(object): def __getattr__(self,a): return get_color(a) def __call__(self,a): return get_color(a) colors=color_dict() re_log=re.compile(r'(\w+): (.*)',re.M) class log_filter(logging.Filter): def __init__(self,name=None): pass def filter(self,rec): rec.c1=colors.PINK rec.c2=colors.NORMAL rec.zone=rec.module if rec.levelno>=logging.INFO: if rec.levelno>=logging.ERROR: rec.c1=colors.RED elif rec.levelno>=logging.WARNING: rec.c1=colors.YELLOW else: rec.c1=colors.GREEN return True m=re_log.match(rec.msg) if m: rec.zone=m.group(1) rec.msg=m.group(2) if zones: return getattr(rec,'zone','')in zones or'*'in zones elif not verbose>2: return False return True class formatter(logging.Formatter): def __init__(self): logging.Formatter.__init__(self,LOG_FORMAT,HOUR_FORMAT) def format(self,rec): if rec.levelno>=logging.WARNING or rec.levelno==logging.INFO: try: msg=rec.msg.decode('utf-8') except Exception: msg=rec.msg return'%s%s%s'%(rec.c1,msg,rec.c2) return logging.Formatter.format(self,rec) log=None def debug(*k,**kw): if verbose: k=list(k) k[0]=k[0].replace('\n',' ') global log log.debug(*k,**kw) def error(*k,**kw): global log log.error(*k,**kw) if verbose>2: st=traceback.extract_stack() if st: st=st[:-1] buf=[] for filename,lineno,name,line in st: buf.append(' File "%s", line %d, in %s'%(filename,lineno,name)) if line: buf.append(' %s'%line.strip()) if buf:log.error("\n".join(buf)) def warn(*k,**kw): global log log.warn(*k,**kw) def info(*k,**kw): global log log.info(*k,**kw) def init_log(): global log log=logging.getLogger('waflib') log.handlers=[] log.filters=[] hdlr=logging.StreamHandler() hdlr.setFormatter(formatter()) log.addHandler(hdlr) log.addFilter(log_filter()) log.setLevel(logging.DEBUG) def make_logger(path,name): logger=logging.getLogger(name) hdlr=logging.FileHandler(path,'w') formatter=logging.Formatter('%(message)s') hdlr.setFormatter(formatter) logger.addHandler(hdlr) logger.setLevel(logging.DEBUG) return logger def make_mem_logger(name,to_log,size=10000): from logging.handlers import MemoryHandler logger=logging.getLogger(name) hdlr=MemoryHandler(size,target=to_log) formatter=logging.Formatter('%(message)s') hdlr.setFormatter(formatter) logger.addHandler(hdlr) logger.memhandler=hdlr logger.setLevel(logging.DEBUG) return logger def pprint(col,str,label='',sep='\n'): sys.stderr.write("%s%s%s %s%s"%(colors(col),str,colors.NORMAL,label,sep))
"""Redo the builtin repr() (representation) but with limits on most sizes.""" __all__ = ["Repr","repr"] import __builtin__ from itertools import islice class Repr: def __init__(self): self.maxlevel = 6 self.maxtuple = 6 self.maxlist = 6 self.maxarray = 5 self.maxdict = 4 self.maxset = 6 self.maxfrozenset = 6 self.maxdeque = 6 self.maxstring = 30 self.maxlong = 40 self.maxother = 20 def repr(self, x): return self.repr1(x, self.maxlevel) def repr1(self, x, level): typename = type(x).__name__ if ' ' in typename: parts = typename.split() typename = '_'.join(parts) if hasattr(self, 'repr_' + typename): return getattr(self, 'repr_' + typename)(x, level) else: s = __builtin__.repr(x) if len(s) > self.maxother: i = max(0, (self.maxother-3)//2) j = max(0, self.maxother-3-i) s = s[:i] + '...' + s[len(s)-j:] return s def _repr_iterable(self, x, level, left, right, maxiter, trail=''): n = len(x) if level <= 0 and n: s = '...' else: newlevel = level - 1 repr1 = self.repr1 pieces = [repr1(elem, newlevel) for elem in islice(x, maxiter)] if n > maxiter: pieces.append('...') s = ', '.join(pieces) if n == 1 and trail: right = trail + right return '%s%s%s' % (left, s, right) def repr_tuple(self, x, level): return self._repr_iterable(x, level, '(', ')', self.maxtuple, ',') def repr_list(self, x, level): return self._repr_iterable(x, level, '[', ']', self.maxlist) def repr_array(self, x, level): header = "array('%s', [" % x.typecode return self._repr_iterable(x, level, header, '])', self.maxarray) def repr_set(self, x, level): x = _possibly_sorted(x) return self._repr_iterable(x, level, 'set([', '])', self.maxset) def repr_frozenset(self, x, level): x = _possibly_sorted(x) return self._repr_iterable(x, level, 'frozenset([', '])', self.maxfrozenset) def repr_deque(self, x, level): return self._repr_iterable(x, level, 'deque([', '])', self.maxdeque) def repr_dict(self, x, level): n = len(x) if n == 0: return '{}' if level <= 0: return '{...}' newlevel = level - 1 repr1 = self.repr1 pieces = [] for key in islice(_possibly_sorted(x), self.maxdict): keyrepr = repr1(key, newlevel) valrepr = repr1(x[key], newlevel) pieces.append('%s: %s' % (keyrepr, valrepr)) if n > self.maxdict: pieces.append('...') s = ', '.join(pieces) return '{%s}' % (s,) def repr_str(self, x, level): s = __builtin__.repr(x[:self.maxstring]) if len(s) > self.maxstring: i = max(0, (self.maxstring-3)//2) j = max(0, self.maxstring-3-i) s = __builtin__.repr(x[:i] + x[len(x)-j:]) s = s[:i] + '...' + s[len(s)-j:] return s def repr_long(self, x, level): s = __builtin__.repr(x) # XXX Hope this isn't too slow... if len(s) > self.maxlong: i = max(0, (self.maxlong-3)//2) j = max(0, self.maxlong-3-i) s = s[:i] + '...' + s[len(s)-j:] return s def repr_instance(self, x, level): try: s = __builtin__.repr(x) # Bugs in x.__repr__() can cause arbitrary # exceptions -- then make up something except Exception: return '<%s instance at %x>' % (x.__class__.__name__, id(x)) if len(s) > self.maxstring: i = max(0, (self.maxstring-3)//2) j = max(0, self.maxstring-3-i) s = s[:i] + '...' + s[len(s)-j:] return s def _possibly_sorted(x): # Since not all sequences of items can be sorted and comparison # functions may raise arbitrary exceptions, return an unsorted # sequence in that case. try: return sorted(x) except Exception: return list(x) aRepr = Repr() repr = aRepr.repr
# -*- coding: utf-8 -*- from trapp.checker import Checker from trapp.competition import Competition class CheckerGames(Checker): def reviewCompetition(self, competition, year): self.log.message('Reviewing competition ' + str(competition)) # Get years this competition was held sql = ("SELECT DISTINCT(YEAR(MatchTime)) AS MatchYear, " " COUNT(ID) AS Games " "FROM tbl_games " "WHERE MatchTypeID = %s AND YEAR(MatchTime) >= %s " "GROUP BY YEAR(MatchTime) " "ORDER BY MatchYear ASC") rs = self.db.query(sql, (competition, year, )) if (rs.with_rows): records = rs.fetchall() for index, item in enumerate(records): self.output.message(str(competition) + ',' + str(item[0]) + ',' + str(item[1])) def checkGames(self): # What year are we starting our checks startYear = 1990 # Label Columns self.output.message('Competition,Year,Games') # Get Competitions list c = Competition() c.connectDB() competitions = c.loadAll() # Do work [self.reviewCompetition(record['CompetitionID'], startYear) for record in competitions]
# -*- coding: utf-8 -*- # # Pysllo documentation build configuration file, created by # sphinx-quickstart on Tue May 31 19:45:48 2016. # # This file is execfile()d with the current directory set to its # containing dir. # # Note that not all possible configuration values are present in this # autogenerated file. # # All configuration values have a default; values that are commented out # serve to show the default. # If extensions (or modules to document with autodoc) are in another directory, # add these directories to sys.path here. If the directory is relative to the # documentation root, use os.path.abspath to make it absolute, like shown here. # import os import sys sys.path.insert(0, os.path.abspath('..')) # -- General configuration ------------------------------------------------ # If your documentation needs a minimal Sphinx version, state it here. # # needs_sphinx = '1.0' # Add any Sphinx extension module names here, as strings. They can be # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom # ones. extensions = ['sphinx.ext.autodoc'] autodoc_member_order = 'bysource' # Add any paths that contain templates here, relative to this directory. templates_path = ['_templates'] # The suffix(es) of source filenames. # You can specify multiple suffix as a list of string: # # source_suffix = ['.rst', '.md'] source_suffix = '.rst' # The encoding of source files. # # source_encoding = 'utf-8-sig' # The master toctree document. master_doc = 'index' # General information about the project. project = u'Pysllo' copyright = u'2016, Marcin Karkocha' author = u'Marcin Karkocha' # The version info for the project you're documenting, acts as replacement for # |version| and |release|, also used in various other places throughout the # built documents. # # The short X.Y version. version = u'0.1' # The full version, including alpha/beta/rc tags. release = u'0.1' # The language for content autogenerated by Sphinx. Refer to documentation # for a list of supported languages. # # This is also used if you do content translation via gettext catalogs. # Usually you set "language" from the command line for these cases. language = None # There are two options for replacing |today|: either, you set today to some # non-false value, then it is used: # # today = '' # # Else, today_fmt is used as the format for a strftime call. # # today_fmt = '%B %d, %Y' # List of patterns, relative to source directory, that match files and # directories to ignore when looking for source files. # This patterns also effect to html_static_path and html_extra_path exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store'] # The reST default role (used for this markup: `text`) to use for all # documents. # # default_role = None # If true, '()' will be appended to :func: etc. cross-reference text. # # add_function_parentheses = True # If true, the current module name will be prepended to all description # unit titles (such as .. function::). # # add_module_names = True # If true, sectionauthor and moduleauthor directives will be shown in the # output. They are ignored by default. # # show_authors = False # The name of the Pygments (syntax highlighting) style to use. pygments_style = 'manni' # A list of ignored prefixes for module index sorting. # modindex_common_prefix = [] # If true, keep warnings as "system message" paragraphs in the built documents. # keep_warnings = False # If true, `todo` and `todoList` produce output, else they produce nothing. todo_include_todos = False # -- Options for HTML output ---------------------------------------------- # The theme to use for HTML and HTML Help pages. See the documentation for # a list of builtin themes. # html_theme = 'alabaster' # Theme options are theme-specific and customize the look and feel of a theme # further. For a list of options available for each theme, see the # documentation. # html_theme_options = { 'fixed_sidebar': True, 'analytics_id': 'UA-79713650-1', 'github_user': 'kivio', 'github_repo': 'pysllo', 'github_banner': True } # Add any paths that contain custom themes here, relative to this directory. # html_theme_path = [] # The name for this set of Sphinx documents. # "<project> v<release> documentation" by default. # # html_title = u'Pysllo v0.1' # A shorter title for the navigation bar. Default is the same as html_title. # # html_short_title = None # The name of an image file (relative to this directory) to place at the top # of the sidebar. # html_logo = "pysllo2.png" # The name of an image file (relative to this directory) to use as a favicon of # the docs. This file should be a Windows icon file (.ico) # being 16x16 or 32x32 # pixels large. # # html_favicon = None # Add any paths that contain custom static files (such as style sheets) here, # relative to this directory. They are copied after the builtin static files, # so a file named "default.css" will overwrite the builtin "default.css". html_static_path = ['_static'] # Add any extra paths that contain custom files (such as robots.txt or # .htaccess) here, relative to this directory. These files are copied # directly to the root of the documentation. # # html_extra_path = [] # If not None, a 'Last updated on:' timestamp is inserted at every page # bottom, using the given strftime format. # The empty string is equivalent to '%b %d, %Y'. # # html_last_updated_fmt = None # If true, SmartyPants will be used to convert quotes and dashes to # typographically correct entities. # # html_use_smartypants = True # Custom sidebar templates, maps document names to template names. # # html_sidebars = {} # Additional templates that should be rendered to pages, maps page names to # template names. # # html_additional_pages = {} # If false, no module index is generated. # # html_domain_indices = True # If false, no index is generated. # # html_use_index = True # If true, the index is split into individual pages for each letter. # # html_split_index = False # If true, links to the reST sources are added to the pages. # # html_show_sourcelink = True # If true, "Created using Sphinx" is shown in the HTML footer. Default is True. # # html_show_sphinx = True # If true, "(C) Copyright ..." is shown in the HTML footer. Default is True. # # html_show_copyright = True # If true, an OpenSearch description file will be output, and all pages will # contain a <link> tag referring to it. The value of this option must be the # base URL from which the finished HTML is served. # # html_use_opensearch = '' # This is the file name suffix for HTML files (e.g. ".xhtml"). # html_file_suffix = None # Language to be used for generating the HTML full-text search index. # Sphinx supports the following languages: # 'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja' # 'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr', 'zh' # # html_search_language = 'en' # A dictionary with options for the search language support, empty by default. # 'ja' uses this config value. # 'zh' user can custom change `jieba` dictionary path. # # html_search_options = {'type': 'default'} # The name of a javascript file (relative to the configuration directory) that # implements a search results scorer. If empty, the default will be used. # # html_search_scorer = 'scorer.js' # Output file base name for HTML help builder. htmlhelp_basename = 'Pysllodoc' # -- Options for LaTeX output --------------------------------------------- latex_elements = { # The paper size ('letterpaper' or 'a4paper'). # # 'papersize': 'letterpaper', # The font size ('10pt', '11pt' or '12pt'). # # 'pointsize': '10pt', # Additional stuff for the LaTeX preamble. # # 'preamble': '', # Latex figure (float) alignment # # 'figure_align': 'htbp', } # Grouping the document tree into LaTeX files. List of tuples # (source start file, target name, title, # author, documentclass [howto, manual, or own class]). latex_documents = [ (master_doc, 'Pysllo.tex', u'Pysllo Documentation', u'Marcin', 'manual'), ] # The name of an image file (relative to this directory) to place at the top of # the title page. # # latex_logo = None # For "manual" documents, if this is true, then toplevel headings are parts, # not chapters. # # latex_use_parts = False # If true, show page references after internal links. # # latex_show_pagerefs = False # If true, show URL addresses after external links. # # latex_show_urls = False # Documents to append as an appendix to all manuals. # # latex_appendices = [] # If false, no module index is generated. # # latex_domain_indices = True # -- Options for manual page output --------------------------------------- # One entry per manual page. List of tuples # (source start file, name, description, authors, manual section). man_pages = [ (master_doc, 'pysllo', u'Pysllo Documentation', [author], 1) ] # If true, show URL addresses after external links. # # man_show_urls = False # -- Options for Texinfo output ------------------------------------------- # Grouping the document tree into Texinfo files. List of tuples # (source start file, target name, title, author, # dir menu entry, description, category) texinfo_documents = [ (master_doc, 'Pysllo', u'Pysllo Documentation', author, 'Pysllo', 'One line description of project.', 'Miscellaneous'), ] # Documents to append as an appendix to all manuals. # # texinfo_appendices = [] # If false, no module index is generated. # # texinfo_domain_indices = True # How to display URL addresses: 'footnote', 'no', or 'inline'. # # texinfo_show_urls = 'footnote' # If true, do not generate a @detailmenu in the "Top" node's menu. # # texinfo_no_detailmenu = False
import datetime import decimal import calendar from django.template import loader from django.http import HttpResponseNotFound from django.core.serializers.json import DjangoJSONEncoder from django.http import HttpResponse from django.utils.encoding import smart_unicode from django.db import models from django.utils.http import urlencode from django.utils.translation import ugettext_lazy as _, ugettext from xadmin.sites import site from xadmin.views import BaseAdminPlugin, ListAdminView from xadmin.views.dashboard import ModelBaseWidget, widget_manager from xadmin.util import lookup_field, label_for_field, force_unicode, json @widget_manager.register class ChartWidget(ModelBaseWidget): widget_type = 'chart' description = _('Show models simple chart.') template = 'xadmin/widgets/chart.html' widget_icon = 'fa fa-bar-chart-o' def convert(self, data): self.list_params = data.pop('params', {}) self.chart = data.pop('chart', None) def setup(self): super(ChartWidget, self).setup() self.charts = {} self.one_chart = False model_admin = self.admin_site._registry[self.model] chart = self.chart if hasattr(model_admin, 'data_charts'): if chart and chart in model_admin.data_charts: self.charts = {chart: model_admin.data_charts[chart]} self.one_chart = True if self.title is None: self.title = model_admin.data_charts[chart].get('title') else: self.charts = model_admin.data_charts if self.title is None: self.title = ugettext( "%s Charts") % self.model._meta.verbose_name_plural def filte_choices_model(self, model, modeladmin): return bool(getattr(modeladmin, 'data_charts', None)) and \ super(ChartWidget, self).filte_choices_model(model, modeladmin) def get_chart_url(self, name, v): return self.model_admin_url('chart', name) + "?" + urlencode(self.list_params) def context(self, context): context.update({ 'charts': [{"name": name, "title": v['title'], 'url': self.get_chart_url(name, v)} for name, v in self.charts.items()], }) # Media def media(self): return self.vendor('flot.js', 'xadmin.plugin.charts.js') class JSONEncoder(DjangoJSONEncoder): def default(self, o): if isinstance(o, (datetime.date, datetime.datetime)): return calendar.timegm(o.timetuple()) * 1000 elif isinstance(o, decimal.Decimal): return str(o) else: try: return super(JSONEncoder, self).default(o) except Exception: return smart_unicode(o) class ChartsPlugin(BaseAdminPlugin): data_charts = {} def init_request(self, *args, **kwargs): return bool(self.data_charts) def get_chart_url(self, name, v): return self.admin_view.model_admin_url('chart', name) + self.admin_view.get_query_string() # Media def get_media(self, media): return media + self.vendor('flot.js', 'xadmin.plugin.charts.js') # Block Views def block_results_top(self, context, nodes): context.update({ 'charts': [{"name": name, "title": v['title'], 'url': self.get_chart_url(name, v)} for name, v in self.data_charts.items()], }) nodes.append(loader.render_to_string('xadmin/blocks/model_list.results_top.charts.html', context_instance=context)) class ChartsView(ListAdminView): data_charts = {} def get_ordering(self): if 'order' in self.chart: return self.chart['order'] else: return super(ChartsView, self).get_ordering() def get(self, request, name): if name not in self.data_charts: return HttpResponseNotFound() self.chart = self.data_charts[name] self.x_field = self.chart['x-field'] y_fields = self.chart['y-field'] self.y_fields = ( y_fields,) if type(y_fields) not in (list, tuple) else y_fields datas = [{"data":[], "label": force_unicode(label_for_field( i, self.model, model_admin=self))} for i in self.y_fields] self.make_result_list() for obj in self.result_list: xf, attrs, value = lookup_field(self.x_field, obj, self) for i, yfname in enumerate(self.y_fields): yf, yattrs, yv = lookup_field(yfname, obj, self) datas[i]["data"].append((value, yv)) option = {'series': {'lines': {'show': True}, 'points': {'show': False}}, 'grid': {'hoverable': True, 'clickable': True}} try: xfield = self.opts.get_field(self.x_field) if type(xfield) in (models.DateTimeField, models.DateField, models.TimeField): option['xaxis'] = {'mode': "time", 'tickLength': 5} if type(xfield) is models.DateField: option['xaxis']['timeformat'] = "%y/%m/%d" elif type(xfield) is models.TimeField: option['xaxis']['timeformat'] = "%H:%M:%S" else: option['xaxis']['timeformat'] = "%y/%m/%d %H:%M:%S" except Exception: pass option.update(self.chart.get('option', {})) content = {'data': datas, 'option': option} result = json.dumps(content, cls=JSONEncoder, ensure_ascii=False) return HttpResponse(result) site.register_plugin(ChartsPlugin, ListAdminView) site.register_modelview(r'^chart/(.+)/$', ChartsView, name='%s_%s_chart')
#!/usr/bin/python # Copyright 2013 Google Inc. # # This file is part of Ansible # # Ansible is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # Ansible is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Ansible. If not, see <http://www.gnu.org/licenses/>. DOCUMENTATION = ''' --- module: gce_pd version_added: "1.4" short_description: utilize GCE persistent disk resources description: - This module can create and destroy unformatted GCE persistent disks U(https://developers.google.com/compute/docs/disks#persistentdisks). It also supports attaching and detaching disks from running instances. Full install/configuration instructions for the gce* modules can be found in the comments of ansible/test/gce_tests.py. options: detach_only: description: - do not destroy the disk, merely detach it from an instance required: false default: "no" choices: ["yes", "no"] aliases: [] instance_name: description: - instance name if you wish to attach or detach the disk required: false default: null aliases: [] mode: description: - GCE mount mode of disk, READ_ONLY (default) or READ_WRITE required: false default: "READ_ONLY" choices: ["READ_WRITE", "READ_ONLY"] aliases: [] name: description: - name of the disk required: true default: null aliases: [] size_gb: description: - whole integer size of disk (in GB) to create, default is 10 GB required: false default: 10 aliases: [] image: description: - the source image to use for the disk required: false default: null aliases: [] version_added: "1.7" snapshot: description: - the source snapshot to use for the disk required: false default: null aliases: [] version_added: "1.7" state: description: - desired state of the persistent disk required: false default: "present" choices: ["active", "present", "absent", "deleted"] aliases: [] zone: description: - zone in which to create the disk required: false default: "us-central1-b" aliases: [] service_account_email: version_added: "1.6" description: - service account email required: false default: null aliases: [] pem_file: version_added: "1.6" description: - path to the pem file associated with the service account email required: false default: null aliases: [] project_id: version_added: "1.6" description: - your GCE project ID required: false default: null aliases: [] disk_type: version_added: "1.9" description: - type of disk provisioned required: false default: "pd-standard" choices: ["pd-standard", "pd-ssd"] aliases: [] requirements: - "python >= 2.6" - "apache-libcloud >= 0.13.3" author: "Eric Johnson (@erjohnso) <erjohnso@google.com>" ''' EXAMPLES = ''' # Simple attachment action to an existing instance - local_action: module: gce_pd instance_name: notlocalhost size_gb: 5 name: pd ''' try: from libcloud.compute.types import Provider from libcloud.compute.providers import get_driver from libcloud.common.google import GoogleBaseError, QuotaExceededError, \ ResourceExistsError, ResourceNotFoundError, ResourceInUseError _ = Provider.GCE HAS_LIBCLOUD = True except ImportError: HAS_LIBCLOUD = False def main(): module = AnsibleModule( argument_spec = dict( detach_only = dict(type='bool'), instance_name = dict(), mode = dict(default='READ_ONLY', choices=['READ_WRITE', 'READ_ONLY']), name = dict(required=True), size_gb = dict(default=10), disk_type = dict(default='pd-standard'), image = dict(), snapshot = dict(), state = dict(default='present'), zone = dict(default='us-central1-b'), service_account_email = dict(), pem_file = dict(), project_id = dict(), ) ) if not HAS_LIBCLOUD: module.fail_json(msg='libcloud with GCE support (0.13.3+) is required for this module') gce = gce_connect(module) detach_only = module.params.get('detach_only') instance_name = module.params.get('instance_name') mode = module.params.get('mode') name = module.params.get('name') size_gb = module.params.get('size_gb') disk_type = module.params.get('disk_type') image = module.params.get('image') snapshot = module.params.get('snapshot') state = module.params.get('state') zone = module.params.get('zone') if detach_only and not instance_name: module.fail_json( msg='Must specify an instance name when detaching a disk', changed=False) disk = inst = None changed = is_attached = False json_output = { 'name': name, 'zone': zone, 'state': state, 'disk_type': disk_type } if detach_only: json_output['detach_only'] = True json_output['detached_from_instance'] = instance_name if instance_name: # user wants to attach/detach from an existing instance try: inst = gce.ex_get_node(instance_name, zone) # is the disk attached? for d in inst.extra['disks']: if d['deviceName'] == name: is_attached = True json_output['attached_mode'] = d['mode'] json_output['attached_to_instance'] = inst.name except: pass # find disk if it already exists try: disk = gce.ex_get_volume(name) json_output['size_gb'] = int(disk.size) except ResourceNotFoundError: pass except Exception, e: module.fail_json(msg=unexpected_error_msg(e), changed=False) # user wants a disk to exist. If "instance_name" is supplied the user # also wants it attached if state in ['active', 'present']: if not size_gb: module.fail_json(msg="Must supply a size_gb", changed=False) try: size_gb = int(round(float(size_gb))) if size_gb < 1: raise Exception except: module.fail_json(msg="Must supply a size_gb larger than 1 GB", changed=False) if instance_name and inst is None: module.fail_json(msg='Instance %s does not exist in zone %s' % ( instance_name, zone), changed=False) if not disk: if image is not None and snapshot is not None: module.fail_json( msg='Cannot give both image (%s) and snapshot (%s)' % ( image, snapshot), changed=False) lc_image = None lc_snapshot = None if image is not None: lc_image = gce.ex_get_image(image) elif snapshot is not None: lc_snapshot = gce.ex_get_snapshot(snapshot) try: disk = gce.create_volume( size_gb, name, location=zone, image=lc_image, snapshot=lc_snapshot, ex_disk_type=disk_type) except ResourceExistsError: pass except QuotaExceededError: module.fail_json(msg='Requested disk size exceeds quota', changed=False) except Exception, e: module.fail_json(msg=unexpected_error_msg(e), changed=False) json_output['size_gb'] = size_gb if image is not None: json_output['image'] = image if snapshot is not None: json_output['snapshot'] = snapshot changed = True if inst and not is_attached: try: gce.attach_volume(inst, disk, device=name, ex_mode=mode) except Exception, e: module.fail_json(msg=unexpected_error_msg(e), changed=False) json_output['attached_to_instance'] = inst.name json_output['attached_mode'] = mode changed = True # user wants to delete a disk (or perhaps just detach it). if state in ['absent', 'deleted'] and disk: if inst and is_attached: try: gce.detach_volume(disk, ex_node=inst) except Exception, e: module.fail_json(msg=unexpected_error_msg(e), changed=False) changed = True if not detach_only: try: gce.destroy_volume(disk) except ResourceInUseError, e: module.fail_json(msg=str(e.value), changed=False) except Exception, e: module.fail_json(msg=unexpected_error_msg(e), changed=False) changed = True json_output['changed'] = changed module.exit_json(**json_output) # import module snippets from ansible.module_utils.basic import * from ansible.module_utils.gce import * if __name__ == '__main__': main()
# stdlib from hashlib import md5 import time # 3rd party import requests # project from checks import AgentCheck class Mesos(AgentCheck): SERVICE_CHECK_NAME = "mesos.can_connect" def check(self, instance): """ DEPRECATED: This generic Mesosphere check is deprecated not actively developed anymore. It will be removed in a future version of the Datadog Agent. Please head over to the Mesosphere master and slave specific checks. """ self.warning("This check is deprecated in favor of Mesos master and slave specific checks." " It will be removed in a future version of the Datadog Agent.") if 'url' not in instance: raise Exception('Mesos instance missing "url" value.') # Load values from the instance config url = instance['url'] instance_tags = instance.get('tags', []) default_timeout = self.init_config.get('default_timeout', 5) timeout = float(instance.get('timeout', default_timeout)) response = self.get_master_roles(url, timeout) if response is not None: for role in response['roles']: tags = ['role:' + role['name']] + instance_tags self.gauge('mesos.role.frameworks', len(role['frameworks']), tags=tags) self.gauge('mesos.role.weight', role['weight'], tags=tags) resources = role['resources'] for attr in ['cpus','mem']: if attr in resources: self.gauge('mesos.role.' + attr, resources[attr], tags=tags) response = self.get_master_stats(url, timeout) if response is not None: tags = instance_tags for key in iter(response): self.gauge('mesos.stats.' + key, response[key], tags=tags) response = self.get_master_state(url, timeout) if response is not None: tags = instance_tags for attr in ['deactivated_slaves','failed_tasks','finished_tasks','killed_tasks','lost_tasks','staged_tasks','started_tasks']: self.gauge('mesos.state.' + attr, response[attr], tags=tags) for framework in response['frameworks']: tags = ['framework:' + framework['id']] + instance_tags resources = framework['resources'] for attr in ['cpus','mem']: if attr in resources: self.gauge('mesos.state.framework.' + attr, resources[attr], tags=tags) for slave in response['slaves']: tags = ['mesos','slave:' + slave['id']] + instance_tags resources = slave['resources'] for attr in ['cpus','mem','disk']: if attr in resources: self.gauge('mesos.state.slave.' + attr, resources[attr], tags=tags) def get_master_roles(self, url, timeout): return self.get_json(url + "/master/roles.json", timeout) def get_master_stats(self, url, timeout): return self.get_json(url + "/master/stats.json", timeout) def get_master_state(self, url, timeout): return self.get_json(url + "/master/state.json", timeout) def get_json(self, url, timeout): # Use a hash of the URL as an aggregation key aggregation_key = md5(url).hexdigest() tags = ["url:%s" % url] msg = None status = None try: r = requests.get(url, timeout=timeout) if r.status_code != 200: self.status_code_event(url, r, aggregation_key) status = AgentCheck.CRITICAL msg = "Got %s when hitting %s" % (r.status_code, url) else: status = AgentCheck.OK msg = "Mesos master instance detected at %s " % url except requests.exceptions.Timeout as e: # If there's a timeout self.timeout_event(url, timeout, aggregation_key) msg = "%s seconds timeout when hitting %s" % (timeout, url) status = AgentCheck.CRITICAL except Exception as e: msg = str(e) status = AgentCheck.CRITICAL finally: self.service_check(self.SERVICE_CHECK_NAME, status, tags=tags, message=msg) if status is AgentCheck.CRITICAL: self.warning(msg) return None return r.json() def timeout_event(self, url, timeout, aggregation_key): self.event({ 'timestamp': int(time.time()), 'event_type': 'http_check', 'msg_title': 'URL timeout', 'msg_text': '%s timed out after %s seconds.' % (url, timeout), 'aggregation_key': aggregation_key }) def status_code_event(self, url, r, aggregation_key): self.event({ 'timestamp': int(time.time()), 'event_type': 'http_check', 'msg_title': 'Invalid reponse code for %s' % url, 'msg_text': '%s returned a status of %s' % (url, r.status_code), 'aggregation_key': aggregation_key })
"""Weak reference support for Python. This module is an implementation of PEP 205: http://www.python.org/dev/peps/pep-0205/ """ # Naming convention: Variables named "wr" are weak reference objects; # they are called this instead of "ref" to avoid name collisions with # the module-global ref() function imported from _weakref. from _weakref import ( getweakrefcount, getweakrefs, ref, proxy, CallableProxyType, ProxyType, ReferenceType) from _weakrefset import WeakSet, _IterationGuard import collections # Import after _weakref to avoid circular import. ProxyTypes = (ProxyType, CallableProxyType) __all__ = ["ref", "proxy", "getweakrefcount", "getweakrefs", "WeakKeyDictionary", "ReferenceType", "ProxyType", "CallableProxyType", "ProxyTypes", "WeakValueDictionary", "WeakSet"] class WeakValueDictionary(collections.MutableMapping): """Mapping class that references values weakly. Entries in the dictionary will be discarded when no strong reference to the value exists anymore """ # We inherit the constructor without worrying about the input # dictionary; since it uses our .update() method, we get the right # checks (if the other dictionary is a WeakValueDictionary, # objects are unwrapped on the way out, and we always wrap on the # way in). def __init__(self, *args, **kw): def remove(wr, selfref=ref(self)): self = selfref() if self is not None: if self._iterating: self._pending_removals.append(wr.key) else: del self.data[wr.key] self._remove = remove # A list of keys to be removed self._pending_removals = [] self._iterating = set() self.data = d = {} self.update(*args, **kw) def _commit_removals(self): l = self._pending_removals d = self.data # We shouldn't encounter any KeyError, because this method should # always be called *before* mutating the dict. while l: del d[l.pop()] def __getitem__(self, key): o = self.data[key]() if o is None: raise KeyError(key) else: return o def __delitem__(self, key): if self._pending_removals: self._commit_removals() del self.data[key] def __len__(self): return len(self.data) - len(self._pending_removals) def __contains__(self, key): try: o = self.data[key]() except KeyError: return False return o is not None def __repr__(self): return "<WeakValueDictionary at %s>" % id(self) def __setitem__(self, key, value): if self._pending_removals: self._commit_removals() self.data[key] = KeyedRef(value, self._remove, key) def copy(self): new = WeakValueDictionary() for key, wr in self.data.items(): o = wr() if o is not None: new[key] = o return new __copy__ = copy def __deepcopy__(self, memo): from copy import deepcopy new = self.__class__() for key, wr in self.data.items(): o = wr() if o is not None: new[deepcopy(key, memo)] = o return new def get(self, key, default=None): try: wr = self.data[key] except KeyError: return default else: o = wr() if o is None: # This should only happen return default else: return o def items(self): with _IterationGuard(self): for k, wr in self.data.items(): v = wr() if v is not None: yield k, v def keys(self): with _IterationGuard(self): for k, wr in self.data.items(): if wr() is not None: yield k __iter__ = keys def itervaluerefs(self): """Return an iterator that yields the weak references to the values. The references are not guaranteed to be 'live' at the time they are used, so the result of calling the references needs to be checked before being used. This can be used to avoid creating references that will cause the garbage collector to keep the values around longer than needed. """ with _IterationGuard(self): for wr in self.data.values(): yield wr def values(self): with _IterationGuard(self): for wr in self.data.values(): obj = wr() if obj is not None: yield obj def popitem(self): if self._pending_removals: self._commit_removals() while True: key, wr = self.data.popitem() o = wr() if o is not None: return key, o def pop(self, key, *args): if self._pending_removals: self._commit_removals() try: o = self.data.pop(key)() except KeyError: if args: return args[0] raise if o is None: raise KeyError(key) else: return o def setdefault(self, key, default=None): try: wr = self.data[key] except KeyError: if self._pending_removals: self._commit_removals() self.data[key] = KeyedRef(default, self._remove, key) return default else: return wr() def update(self, dict=None, **kwargs): if self._pending_removals: self._commit_removals() d = self.data if dict is not None: if not hasattr(dict, "items"): dict = type({})(dict) for key, o in dict.items(): d[key] = KeyedRef(o, self._remove, key) if len(kwargs): self.update(kwargs) def valuerefs(self): """Return a list of weak references to the values. The references are not guaranteed to be 'live' at the time they are used, so the result of calling the references needs to be checked before being used. This can be used to avoid creating references that will cause the garbage collector to keep the values around longer than needed. """ return list(self.data.values()) class KeyedRef(ref): """Specialized reference that includes a key corresponding to the value. This is used in the WeakValueDictionary to avoid having to create a function object for each key stored in the mapping. A shared callback object can use the 'key' attribute of a KeyedRef instead of getting a reference to the key from an enclosing scope. """ __slots__ = "key", def __new__(type, ob, callback, key): self = ref.__new__(type, ob, callback) self.key = key return self def __init__(self, ob, callback, key): super().__init__(ob, callback) class WeakKeyDictionary(collections.MutableMapping): """ Mapping class that references keys weakly. Entries in the dictionary will be discarded when there is no longer a strong reference to the key. This can be used to associate additional data with an object owned by other parts of an application without adding attributes to those objects. This can be especially useful with objects that override attribute accesses. """ def __init__(self, dict=None): self.data = {} def remove(k, selfref=ref(self)): self = selfref() if self is not None: if self._iterating: self._pending_removals.append(k) else: del self.data[k] self._remove = remove # A list of dead weakrefs (keys to be removed) self._pending_removals = [] self._iterating = set() if dict is not None: self.update(dict) def _commit_removals(self): # NOTE: We don't need to call this method before mutating the dict, # because a dead weakref never compares equal to a live weakref, # even if they happened to refer to equal objects. # However, it means keys may already have been removed. l = self._pending_removals d = self.data while l: try: del d[l.pop()] except KeyError: pass def __delitem__(self, key): del self.data[ref(key)] def __getitem__(self, key): return self.data[ref(key)] def __len__(self): return len(self.data) - len(self._pending_removals) def __repr__(self): return "<WeakKeyDictionary at %s>" % id(self) def __setitem__(self, key, value): self.data[ref(key, self._remove)] = value def copy(self): new = WeakKeyDictionary() for key, value in self.data.items(): o = key() if o is not None: new[o] = value return new __copy__ = copy def __deepcopy__(self, memo): from copy import deepcopy new = self.__class__() for key, value in self.data.items(): o = key() if o is not None: new[o] = deepcopy(value, memo) return new def get(self, key, default=None): return self.data.get(ref(key),default) def __contains__(self, key): try: wr = ref(key) except TypeError: return False return wr in self.data def items(self): with _IterationGuard(self): for wr, value in self.data.items(): key = wr() if key is not None: yield key, value def keys(self): with _IterationGuard(self): for wr in self.data: obj = wr() if obj is not None: yield obj __iter__ = keys def values(self): with _IterationGuard(self): for wr, value in self.data.items(): if wr() is not None: yield value def keyrefs(self): """Return a list of weak references to the keys. The references are not guaranteed to be 'live' at the time they are used, so the result of calling the references needs to be checked before being used. This can be used to avoid creating references that will cause the garbage collector to keep the keys around longer than needed. """ return list(self.data) def popitem(self): while True: key, value = self.data.popitem() o = key() if o is not None: return o, value def pop(self, key, *args): return self.data.pop(ref(key), *args) def setdefault(self, key, default=None): return self.data.setdefault(ref(key, self._remove),default) def update(self, dict=None, **kwargs): d = self.data if dict is not None: if not hasattr(dict, "items"): dict = type({})(dict) for key, value in dict.items(): d[ref(key, self._remove)] = value if len(kwargs): self.update(kwargs)
"""SCons.Scanner.Fortran This module implements the dependency scanner for Fortran code. """ # # Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010 The SCons Foundation # # Permission is hereby granted, free of charge, to any person obtaining # a copy of this software and associated documentation files (the # "Software"), to deal in the Software without restriction, including # without limitation the rights to use, copy, modify, merge, publish, # distribute, sublicense, and/or sell copies of the Software, and to # permit persons to whom the Software is furnished to do so, subject to # the following conditions: # # The above copyright notice and this permission notice shall be included # in all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY # KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE # WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND # NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE # LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION # OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION # WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. __revision__ = "src/engine/SCons/Scanner/Fortran.py 5134 2010/08/16 23:02:40 bdeegan" import re import SCons.Node import SCons.Node.FS import SCons.Scanner import SCons.Util import SCons.Warnings class F90Scanner(SCons.Scanner.Classic): """ A Classic Scanner subclass for Fortran source files which takes into account both USE and INCLUDE statements. This scanner will work for both F77 and F90 (and beyond) compilers. Currently, this scanner assumes that the include files do not contain USE statements. To enable the ability to deal with USE statements in include files, add logic right after the module names are found to loop over each include file, search for and locate each USE statement, and append each module name to the list of dependencies. Caching the search results in a common dictionary somewhere so that the same include file is not searched multiple times would be a smart thing to do. """ def __init__(self, name, suffixes, path_variable, use_regex, incl_regex, def_regex, *args, **kw): self.cre_use = re.compile(use_regex, re.M) self.cre_incl = re.compile(incl_regex, re.M) self.cre_def = re.compile(def_regex, re.M) def _scan(node, env, path, self=self): node = node.rfile() if not node.exists(): return [] return self.scan(node, env, path) kw['function'] = _scan kw['path_function'] = SCons.Scanner.FindPathDirs(path_variable) kw['recursive'] = 1 kw['skeys'] = suffixes kw['name'] = name SCons.Scanner.Current.__init__(self, *args, **kw) def scan(self, node, env, path=()): # cache the includes list in node so we only scan it once: if node.includes != None: mods_and_includes = node.includes else: # retrieve all included filenames includes = self.cre_incl.findall(node.get_text_contents()) # retrieve all USE'd module names modules = self.cre_use.findall(node.get_text_contents()) # retrieve all defined module names defmodules = self.cre_def.findall(node.get_text_contents()) # Remove all USE'd module names that are defined in the same file # (case-insensitively) d = {} for m in defmodules: d[m.lower()] = 1 modules = [m for m in modules if m.lower() not in d] # Convert module name to a .mod filename suffix = env.subst('$FORTRANMODSUFFIX') modules = [x.lower() + suffix for x in modules] # Remove unique items from the list mods_and_includes = SCons.Util.unique(includes+modules) node.includes = mods_and_includes # This is a hand-coded DSU (decorate-sort-undecorate, or # Schwartzian transform) pattern. The sort key is the raw name # of the file as specifed on the USE or INCLUDE line, which lets # us keep the sort order constant regardless of whether the file # is actually found in a Repository or locally. nodes = [] source_dir = node.get_dir() if callable(path): path = path() for dep in mods_and_includes: n, i = self.find_include(dep, source_dir, path) if n is None: SCons.Warnings.warn(SCons.Warnings.DependencyWarning, "No dependency generated for file: %s (referenced by: %s) -- file not found" % (i, node)) else: sortkey = self.sort_key(dep) nodes.append((sortkey, n)) return [pair[1] for pair in sorted(nodes)] def FortranScan(path_variable="FORTRANPATH"): """Return a prototype Scanner instance for scanning source files for Fortran USE & INCLUDE statements""" # The USE statement regex matches the following: # # USE module_name # USE :: module_name # USE, INTRINSIC :: module_name # USE, NON_INTRINSIC :: module_name # # Limitations # # -- While the regex can handle multiple USE statements on one line, # it cannot properly handle them if they are commented out. # In either of the following cases: # # ! USE mod_a ; USE mod_b [entire line is commented out] # USE mod_a ! ; USE mod_b [in-line comment of second USE statement] # # the second module name (mod_b) will be picked up as a dependency # even though it should be ignored. The only way I can see # to rectify this would be to modify the scanner to eliminate # the call to re.findall, read in the contents of the file, # treating the comment character as an end-of-line character # in addition to the normal linefeed, loop over each line, # weeding out the comments, and looking for the USE statements. # One advantage to this is that the regex passed to the scanner # would no longer need to match a semicolon. # # -- I question whether or not we need to detect dependencies to # INTRINSIC modules because these are built-in to the compiler. # If we consider them a dependency, will SCons look for them, not # find them, and kill the build? Or will we there be standard # compiler-specific directories we will need to point to so the # compiler and SCons can locate the proper object and mod files? # Here is a breakdown of the regex: # # (?i) : regex is case insensitive # ^ : start of line # (?: : group a collection of regex symbols without saving the match as a "group" # ^|; : matches either the start of the line or a semicolon - semicolon # ) : end the unsaved grouping # \s* : any amount of white space # USE : match the string USE, case insensitive # (?: : group a collection of regex symbols without saving the match as a "group" # \s+| : match one or more whitespace OR .... (the next entire grouped set of regex symbols) # (?: : group a collection of regex symbols without saving the match as a "group" # (?: : establish another unsaved grouping of regex symbols # \s* : any amount of white space # , : match a comma # \s* : any amount of white space # (?:NON_)? : optionally match the prefix NON_, case insensitive # INTRINSIC : match the string INTRINSIC, case insensitive # )? : optionally match the ", INTRINSIC/NON_INTRINSIC" grouped expression # \s* : any amount of white space # :: : match a double colon that must appear after the INTRINSIC/NON_INTRINSIC attribute # ) : end the unsaved grouping # ) : end the unsaved grouping # \s* : match any amount of white space # (\w+) : match the module name that is being USE'd # # use_regex = "(?i)(?:^|;)\s*USE(?:\s+|(?:(?:\s*,\s*(?:NON_)?INTRINSIC)?\s*::))\s*(\w+)" # The INCLUDE statement regex matches the following: # # INCLUDE 'some_Text' # INCLUDE "some_Text" # INCLUDE "some_Text" ; INCLUDE "some_Text" # INCLUDE kind_"some_Text" # INCLUDE kind_'some_Text" # # where some_Text can include any alphanumeric and/or special character # as defined by the Fortran 2003 standard. # # Limitations: # # -- The Fortran standard dictates that a " or ' in the INCLUDE'd # string must be represented as a "" or '', if the quotes that wrap # the entire string are either a ' or ", respectively. While the # regular expression below can detect the ' or " characters just fine, # the scanning logic, presently is unable to detect them and reduce # them to a single instance. This probably isn't an issue since, # in practice, ' or " are not generally used in filenames. # # -- This regex will not properly deal with multiple INCLUDE statements # when the entire line has been commented out, ala # # ! INCLUDE 'some_file' ; INCLUDE 'some_file' # # In such cases, it will properly ignore the first INCLUDE file, # but will actually still pick up the second. Interestingly enough, # the regex will properly deal with these cases: # # INCLUDE 'some_file' # INCLUDE 'some_file' !; INCLUDE 'some_file' # # To get around the above limitation, the FORTRAN programmer could # simply comment each INCLUDE statement separately, like this # # ! INCLUDE 'some_file' !; INCLUDE 'some_file' # # The way I see it, the only way to get around this limitation would # be to modify the scanning logic to replace the calls to re.findall # with a custom loop that processes each line separately, throwing # away fully commented out lines before attempting to match against # the INCLUDE syntax. # # Here is a breakdown of the regex: # # (?i) : regex is case insensitive # (?: : begin a non-saving group that matches the following: # ^ : either the start of the line # | : or # ['">]\s*; : a semicolon that follows a single quote, # double quote or greater than symbol (with any # amount of whitespace in between). This will # allow the regex to match multiple INCLUDE # statements per line (although it also requires # the positive lookahead assertion that is # used below). It will even properly deal with # (i.e. ignore) cases in which the additional # INCLUDES are part of an in-line comment, ala # " INCLUDE 'someFile' ! ; INCLUDE 'someFile2' " # ) : end of non-saving group # \s* : any amount of white space # INCLUDE : match the string INCLUDE, case insensitive # \s+ : match one or more white space characters # (?\w+_)? : match the optional "kind-param _" prefix allowed by the standard # [<"'] : match the include delimiter - an apostrophe, double quote, or less than symbol # (.+?) : match one or more characters that make up # the included path and file name and save it # in a group. The Fortran standard allows for # any non-control character to be used. The dot # operator will pick up any character, including # control codes, but I can't conceive of anyone # putting control codes in their file names. # The question mark indicates it is non-greedy so # that regex will match only up to the next quote, # double quote, or greater than symbol # (?=["'>]) : positive lookahead assertion to match the include # delimiter - an apostrophe, double quote, or # greater than symbol. This level of complexity # is required so that the include delimiter is # not consumed by the match, thus allowing the # sub-regex discussed above to uniquely match a # set of semicolon-separated INCLUDE statements # (as allowed by the F2003 standard) include_regex = """(?i)(?:^|['">]\s*;)\s*INCLUDE\s+(?:\w+_)?[<"'](.+?)(?=["'>])""" # The MODULE statement regex finds module definitions by matching # the following: # # MODULE module_name # # but *not* the following: # # MODULE PROCEDURE procedure_name # # Here is a breakdown of the regex: # # (?i) : regex is case insensitive # ^\s* : any amount of white space # MODULE : match the string MODULE, case insensitive # \s+ : match one or more white space characters # (?!PROCEDURE) : but *don't* match if the next word matches # PROCEDURE (negative lookahead assertion), # case insensitive # (\w+) : match one or more alphanumeric characters # that make up the defined module name and # save it in a group def_regex = """(?i)^\s*MODULE\s+(?!PROCEDURE)(\w+)""" scanner = F90Scanner("FortranScan", "$FORTRANSUFFIXES", path_variable, use_regex, include_regex, def_regex) return scanner # Local Variables: # tab-width:4 # indent-tabs-mode:nil # End: # vim: set expandtab tabstop=4 shiftwidth=4:
# -*- coding: utf-8 -*- """ The MIT License (MIT) Copyright (c) 2015-2016 Rapptz Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. """ from .user import User from .game import Game from .permissions import Permissions from . import utils from .enums import Status, ChannelType from .colour import Colour import copy class VoiceState: """Represents a Discord user's voice state. Attributes ------------ deaf: bool Indicates if the user is currently deafened by the server. mute: bool Indicates if the user is currently muted by the server. self_mute: bool Indicates if the user is currently muted by their own accord. self_deaf: bool Indicates if the user is currently deafened by their own accord. is_afk: bool Indicates if the user is currently in the AFK channel in the server. voice_channel: Optional[Union[:class:`Channel`, :class:`PrivateChannel`]] The voice channel that the user is currently connected to. None if the user is not currently in a voice channel. """ __slots__ = [ 'session_id', 'deaf', 'mute', 'self_mute', 'self_deaf', 'is_afk', 'voice_channel' ] def __init__(self, **kwargs): self.session_id = kwargs.get('session_id') self._update_voice_state(**kwargs) def _update_voice_state(self, **kwargs): self.self_mute = kwargs.get('self_mute', False) self.self_deaf = kwargs.get('self_deaf', False) self.is_afk = kwargs.get('suppress', False) self.mute = kwargs.get('mute', False) self.deaf = kwargs.get('deaf', False) self.voice_channel = kwargs.get('voice_channel') def flatten_voice_states(cls): for attr in VoiceState.__slots__: def getter(self, x=attr): return getattr(self.voice, x) setattr(cls, attr, property(getter)) return cls @flatten_voice_states class Member(User): """Represents a Discord member to a :class:`Server`. This is a subclass of :class:`User` that extends more functionality that server members have such as roles and permissions. Attributes ---------- voice: :class:`VoiceState` The member's voice state. Properties are defined to mirror access of the attributes. e.g. ``Member.is_afk`` is equivalent to `Member.voice.is_afk``. roles A list of :class:`Role` that the member belongs to. Note that the first element of this list is always the default '@everyone' role. joined_at : `datetime.datetime` A datetime object that specifies the date and time in UTC that the member joined the server for the first time. status : :class:`Status` The member's status. There is a chance that the status will be a ``str`` if it is a value that is not recognised by the enumerator. game : :class:`Game` The game that the user is currently playing. Could be None if no game is being played. server : :class:`Server` The server that the member belongs to. nick : Optional[str] The server specific nickname of the user. """ __slots__ = [ 'roles', 'joined_at', 'status', 'game', 'server', 'nick', 'voice' ] def __init__(self, **kwargs): super().__init__(**kwargs.get('user')) self.voice = VoiceState(**kwargs) self.joined_at = utils.parse_time(kwargs.get('joined_at')) self.roles = kwargs.get('roles', []) self.status = Status.offline game = kwargs.get('game', {}) self.game = Game(**game) if game else None self.server = kwargs.get('server', None) self.nick = kwargs.get('nick', None) def _update_voice_state(self, **kwargs): self.voice.self_mute = kwargs.get('self_mute', False) self.voice.self_deaf = kwargs.get('self_deaf', False) self.voice.is_afk = kwargs.get('suppress', False) self.voice.mute = kwargs.get('mute', False) self.voice.deaf = kwargs.get('deaf', False) old_channel = getattr(self, 'voice_channel', None) vc = kwargs.get('voice_channel') if old_channel is None and vc is not None: # we joined a channel vc.voice_members.append(self) elif old_channel is not None: try: # we either left a channel or we switched channels old_channel.voice_members.remove(self) except ValueError: pass finally: # we switched channels if vc is not None: vc.voice_members.append(self) self.voice.voice_channel = vc def _copy(self): ret = copy.copy(self) ret.voice = copy.copy(self.voice) return ret @property def colour(self): """A property that returns a :class:`Colour` denoting the rendered colour for the member. If the default colour is the one rendered then an instance of :meth:`Colour.default` is returned. There is an alias for this under ``color``. """ default_colour = Colour.default() # highest order of the colour is the one that gets rendered. # if the highest is the default colour then the next one with a colour # is chosen instead if self.roles: roles = sorted(self.roles, key=lambda r: r.position, reverse=True) for role in roles: if role.colour == default_colour: continue else: return role.colour return default_colour color = colour @property def mention(self): if self.nick: return '<@!{}>'.format(self.id) return '<@{}>'.format(self.id) def mentioned_in(self, message): mentioned = super().mentioned_in(message) if mentioned: return True for role in message.role_mentions: has_role = utils.get(self.roles, id=role.id) is not None if has_role: return True return False @property def top_role(self): """Returns the member's highest role. This is useful for figuring where a member stands in the role hierarchy chain. """ if self.roles: roles = sorted(self.roles, reverse=True) return roles[0] return None @property def server_permissions(self): """Returns the member's server permissions. This only takes into consideration the server permissions and not most of the implied permissions or any of the channel permission overwrites. For 100% accurate permission calculation, please use either :meth:`permissions_in` or :meth:`Channel.permissions_for`. This does take into consideration server ownership and the administrator implication. """ if self.server.owner == self: return Permissions.all() base = Permissions.none() for r in self.roles: base.value |= r.permissions.value if base.administrator: return Permissions.all() return base
# -*- coding: utf-8 -*- # # This file is part of Invenio. # Copyright (C) 2013 CERN. # # Invenio is free software; you can redistribute it and/or # modify it under the terms of the GNU General Public License as # published by the Free Software Foundation; either version 2 of the # License, or (at your option) any later version. # # Invenio is distributed in the hope that it will be useful, but # WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU # General Public License for more details. # # You should have received a copy of the GNU General Public License # along with Invenio; if not, write to the Free Software Foundation, Inc., # 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA. from invenio.legacy.dbquery import run_sql depends_on = ['invenio_release_1_1_0'] def info(): return "New bibcheck_rules table" def do_upgrade(): run_sql(""" CREATE TABLE IF NOT EXISTS bibcheck_rules ( name varchar(150) NOT NULL, last_run datetime NOT NULL default '0000-00-00', PRIMARY KEY (name) ) ENGINE=MyISAM; """) def estimate(): """ Estimate running time of upgrade in seconds (optional). """ return 1
# -*- coding: utf-8 -*- ''' #@summary: DbRowFactory is one common factory to convert db row tuple into user-defined class object. It is supported SqlAlchemy, and any database modules conformed to Python Database API Specification v2.0. e.g. cx_Oracle, zxJDBC #@note: Note 1: The DbRowFactory will create one row instance based on row class binding, and try to assign all fields' value to the new object. The DbRowFactory maps field and class setter_method/attribute by matching names. If both a setter_method and an attribute match the same field, the setter_method will be chosen eventually. Note 2: __init__() of the class must no other arguments rather than self #@see: http://www.python.org/dev/peps/pep-0249/ #Tested under: Python 2.7, Jython2.5.2 #Change log: version 1.0.1, 09 Nov. 2011, initial version version 1.0.2, 16 Feb. 2012, use pyObjectCreator to instantiate rowClass version 1.0.3, 08 Mar. 2012, fromSqlAlchemyResultProxy(), fetchAllRowObjects() functions added version 1.0.4, 31 May. 2013, bug fixed version, disable auto-close cursor if not created by SqlAlchemy version 1.0.5, 04 Feb. 2014, import pyObjectCreator in explicit relative importing ##====================sample begin======= #sample code , file: OracleJdbcSample.py from __future__ import with_statement from com.ziclix.python.sql import zxJDBC from pyDbRowFactory import DbRowFactory class rowClass2(object): def __init__(self): self.owner=None self.tablename=None def setOWNER(self, value): self.owner=value def print2(self): print("ownerName="+self.owner+",tablename="+self.tablename) if __name__=="__main__": #DB API 2.0 cursor sample jdbc_url="jdbc:oracle:thin:@127.0.0.1:1521:orcl"; username = "user1" password = "pwd1" driver = "oracle.jdbc.driver.OracleDriver" with zxJDBC.connect(jdbc_url, username, password, driver) as conn: with conn.cursor() as cursor : cursor.execute("""select tbl.owner, tbl.table_name tablename, tbl.tablespace_name from all_tables tbl""") #use DbRowFactory to bind rowClass2 class defined in pkg1.OracleJdbcSample.py rowFactory=DbRowFactory(cursor, "pkg1.OracleJdbcSample.rowClass2") for rowObject in rowFactory.fetchAllRowObjects(): rowObject.print2() #sqlalchemy sample from sqlalchemy import create_engine engine=create_engine("sqlite:///:memory:", echo=True) sql="""select tbl.owner, tbl.table_name tablename, tbl.tablespace_name from all_tables tbl""" resultProxy=engine.execute(sql) rowFactory=DbRowFactory.fromSqlAlchemyResultProxy(resultProxy, "pkg1.OracleJdbcSample.rowClass2") for rowObject in rowFactory.fetchAllRowObjects(): rowObject.print2() ##====================sample end======= ''' import inspect import sys __author__ = 'Harry Liu, <harrychinese@gmail.com>' __version__= '1.0.5' class DbRowFactory(object): ''' #@summary: DbRowFactory is one common row factory for any database module conformed to Python Database API Specification v2.0. e.g. cx_Oracle, zxJDBC #@note: Note 1: The DbRowFactory will create one row instance based on row class binding, and try to assign all fields' value to the new object. The DbRowFactory maps field and class setter_method/attribute by matching names. If both a setter_method and an attribute match the same field, the setter_method will be chosen eventually. Note 2: __init__() of the class must no other arguments rather than self #@see: http://www.python.org/dev/peps/pep-0249/ #@author: Harry Liu, harrychinese@gmail.com ''' FIELD_TO_SETTER=1 FIELD_TO_ATTRIBUTE=2 FIELD_TO_NONE=0 def __init__(self, cursor, rowClassFullName, setterPrefix="set", caseSensitive=False): ''' ##@summary: Constructor of DbRowFactory [arguments] cursor: Db API 2.0 cursor object rowClassFullName: full class name that you want to instantiate, included package and module name if has setterPrefix: settor method prefix caseSensitive: match fieldname with class setter_method/attribute in case sensitive or not ''' self._cursor=cursor self._setterPrefix=setterPrefix self._caseSensitive=caseSensitive self._fieldMemeberMapped=False self._allMethods=[] self._allAttributes=[] self._fieldMapList={} self._rowClassMeta = getClassMeta(rowClassFullName) self._resultProxy=None @classmethod def fromSqlAlchemyResultProxy(cls, resultProxy, rowClassFullName, setterPrefix="set", caseSensitive=False): ''' ##@summary: another constructor of DbRowFactory [arguments] resultProxy: SqlAlchemyResultProxy object, can returned after engine.execute("select 1") called, rowClassFullName: full class name that you want to instantiate, included package and module name if has setterPrefix: settor method prefix caseSensitive: match fieldname with class setter_method/attribute in case sensitive or not ''' factory= cls(resultProxy.cursor, rowClassFullName, setterPrefix, caseSensitive) factory._resultProxy=resultProxy return factory def createRowInstance(self, row ,*args,**kwargs): ''' #@summary: create one instance object, and try to assign all fields' value to the new object [arguments] row: row tuple in a _cursor *args: list style arguments in class constructor related to rowClassFullName *kwargs: dict style arguments in class constructor related to rowClassFullName ''' #step 1: initialize rowInstance before finding attributes. rowObject = self._rowClassMeta(*args,**kwargs) #mapping process run only once in order to gain better performance if self._fieldMemeberMapped==False: #dir() cannot list attributes before one class instantiation self._allAttributes=self._getAllMembers(rowObject) self._allMethods=self._getAllMembers(rowObject) self._fieldMapList=self._mapFieldAndMember() self._fieldMemeberMapped=True #step 2: assign field values i=0 #self._fieldMapList is [{Field1:(member1Flag,member1)},{Field2:(member2Flag,member2)}] for fieldMemberDict in self._fieldMapList: for field in fieldMemberDict: member=fieldMemberDict[field] if member[0]==self.FIELD_TO_NONE: pass else: fieldValue=row[i] if member[0]==self.FIELD_TO_SETTER: m=getattr(rowObject, member[1]) m(fieldValue) elif member[0]==self.FIELD_TO_ATTRIBUTE: setattr(rowObject, member[1], fieldValue) i=i+1 return rowObject def _getAllMembers(self,clazz) : ''' #@summary: extract all user-defined methods in given class #@param param clazz: class object ''' members=[member for member in dir(clazz)] sysMemberList=['__class__','__doc__','__init__','__new__','__subclasshook__','__dict__', '__module__','__delattr__', '__getattribute__', '__hash__', '__repr__', '__setattr__', '__str__','__format__', '__reduce__', '__reduce_ex__', '__sizeof__', '__weakref__'] members=[member for member in members if str(member) not in sysMemberList] return members def _mapFieldAndMember(self): ''' #@summary: create mapping between field and class setter_method/attribute, setter_method is preferred than attribute #field can be extract from cursor.description, e.g. sql: select 1 a, sysdate dt from dual cursor.description: [(u'A', 2, 22, None, 0, 0, 1), (u'DT', 91, 7, None, None, None, 1)] ''' #print(self._cursor.description) fields=[f[0] for f in self._cursor.description] mapList=[] #result is [{Field1:(member1Flag,member1)},{Field2:(member2Flag,member2)}] for f in fields: m= self._getSetterMethod(f) key=f if m: value=(self.FIELD_TO_SETTER,m) else: m= self._getAttribute(f) if m: value=(self.FIELD_TO_ATTRIBUTE,m) else: value=(self.FIELD_TO_NONE,None) mapList.append({key:value}) return mapList def _getAttribute(self, fieldName): ''' #@summary: get related attribute to given fieldname ''' if self._caseSensitive: if fieldName in self._allAttributes: return fieldName else: fieldNameUpper=fieldName.upper() allAttributesMap={} # attributeUpper=attribute for attr in self._allAttributes: allAttributesMap[attr.upper()]=attr if fieldNameUpper in allAttributesMap: return allAttributesMap[fieldNameUpper] def _getSetterMethod(self, fieldName): ''' ##@summary: get related setter method to given fieldname ''' if self._caseSensitive: setter=self._setterPrefix+fieldName if setter in self._allMethods: return setter else: setterUpper=self._setterPrefix+fieldName setterUpper=setterUpper.upper() allMethodMap={} # methodUpper=method for method in self._allMethods: allMethodMap[method.upper()]=method if setterUpper in allMethodMap: return allMethodMap[setterUpper] def _closeResultProxy(self): if self._resultProxy is not None: if self._resultProxy.closed==False: self._resultProxy.close() def _createdBySqlAlchemy(self): return self._resultProxy!=None def fetchAllRowObjects(self): """Fetch all rows, just like DB-API ``cursor.fetchall()``. the cursor is automatically closed after this is called """ result=[] rows=self._cursor.fetchall() for row in rows: rowObject=self.createRowInstance(row) result.append(rowObject) if self._createdBySqlAlchemy(): self._cursor.close() self._closeResultProxy() return result def fetchManyRowObjects(self, size=None): """Fetch many rows, just like DB-API ``cursor.fetchmany(size=cursor.arraysize)``. If rows are present, the cursor remains open after this is called. Else the cursor is automatically closed and an empty list is returned. """ result=[] rows=self._cursor.fetchmany(size) for row in rows: rowObject=self.createRowInstance(row) result.append(rowObject) if self._createdBySqlAlchemy(): if len(rows)==0: self._cursor.close() self._closeResultProxy() return result def fetchOneRowObject(self): """Fetch one row, just like DB-API ``cursor.fetchone()``. If a row is present, the cursor remains open after this is called. Else the cursor is automatically closed and None is returned. """ result=None row = self._cursor.fetchone() if row is not None: result=self.createRowInstance(row) else: if self._createdBySqlAlchemy(): self._cursor.close() self._closeResultProxy() return result ##reference doc #http://www.cnblogs.com/sevenyuan/archive/2010/12/06/1898056.html #http://stackoverflow.com/questions/4513192/python-dynamic-class-names #http://stackoverflow.com/questions/1796180/python-get-list-of-al-classes-within-current-module def createInstance(full_class_name,*args,**kwargs): ''' instantiate class dynamically [arguments] full_class_name: full class name that you want to instantiate, included package and module name if has *args: list style arguments in class constructor *kwargs: dict style arguments in class constructor [return] an instance of this full_class_name [example] import pyObjectCreator full_class_name="knightmade.logging.Logger" logger=pyObjectCreator.createInstance(full_class_name,'logname') ''' class_meta=getClassMeta(full_class_name) if class_meta!=None: obj=class_meta(*args,**kwargs) else: obj=None return obj def getClassMeta(full_class_name): ''' get class meta object of full_class_name, then we can use this meta object to instantiate full_class_name [arguments] full_class_name: full class name that you want to instantiate, included package and module name if has [return] an instance of this full_class_name [example] import pyObjectCreator full_class_name="knightmade.logging.Logger" loggerMeta=pyObjectCreator.getClassMeta(full_class_name) ''' namespace=full_class_name.strip().rsplit('.',1) if len(namespace)==1: class_name=namespace[0] class_meta=_getClassMetaFromCurrModule(class_name) else: module_name=namespace[0] class_name=namespace[1] class_meta=_getClassMetaFromOtherModule(class_name,module_name) return class_meta def _getClassMetaFromCurrModule(class_name): result=None module_name="__main__" for name, obj in inspect.getmembers(sys.modules[module_name]): if inspect.isclass(obj): if name==class_name: result=obj break return result def _getClassMetaFromOtherModule(class_name, module_name): module_meta=__import__(module_name,globals(), locals(),[class_name]) if module_meta!=None: class_meta=getattr(module_meta,class_name) else: class_meta=None return class_meta
"""Contains the Tilt mode code""" # tilt.py # Mission Pinball Framework # Written by Brian Madden & Gabe Knuth # Released under the MIT License. (See license info at the end of this file.) # Documentation and more info at http://missionpinball.com/mpf from mpf.system.config import CaseInsensitiveDict from mpf.system.mode import Mode from mpf.system.timing import Timing class Tilt(Mode): def mode_init(self): self._balls_to_collect = 0 self._last_warning_tick = 0 self.ball_ending_tilted_queue = None self.tilt_event_handlers = set() self.last_tilt_warning_switch_tick = 0 self.tilt_config = self.machine.config_processor.process_config2( config_spec='tilt', source=self._get_merged_settings('tilt'), section_name='tilt') def mode_start(self, **kwargs): self._register_switch_handlers() for event in self.tilt_config['reset_warnings_events']: self.add_mode_event_handler(event, self.reset_warnings) def mode_stop(self, **kwargs): self._remove_switch_handlers() self.reset_warnings_handlers = set() def _register_switch_handlers(self): for switch in self.machine.switches.items_tagged( self.tilt_config['tilt_warning_switch_tag']): self.machine.switch_controller.add_switch_handler( switch_name=switch.name, callback=self._tilt_warning_switch_handler) for switch in self.machine.switches.items_tagged( self.tilt_config['tilt_switch_tag']): self.machine.switch_controller.add_switch_handler( switch_name=switch.name, callback=self.tilt) for switch in self.machine.switches.items_tagged( self.tilt_config['slam_tilt_switch_tag']): self.machine.switch_controller.add_switch_handler( switch_name=switch.name, callback=self.slam_tilt) def _remove_switch_handlers(self): for switch in self.machine.switches.items_tagged( self.tilt_config['tilt_warning_switch_tag']): self.machine.switch_controller.remove_switch_handler( switch_name=switch.name, callback=self._tilt_warning_switch_handler) for switch in self.machine.switches.items_tagged( self.tilt_config['tilt_switch_tag']): self.machine.switch_controller.remove_switch_handler( switch_name=switch.name, callback=self.tilt) for switch in self.machine.switches.items_tagged( self.tilt_config['slam_tilt_switch_tag']): self.machine.switch_controller.remove_switch_handler( switch_name=switch.name, callback=self.slam_tilt) def tilt_warning(self): """Processes a tilt warning. If the number of warnings is the number to cause a tilt, a tilt will be processed. """ self.last_tilt_warning_switch_tick = self.machine.tick_num if not self.player: return self.log.debug("Tilt Warning") self._last_warning_tick = self.machine.tick_num self.player[self.tilt_config['tilt_warnings_player_var']] += 1 warnings = self.player[self.tilt_config['tilt_warnings_player_var']] if warnings >= self.tilt_config['warnings_to_tilt']: self.tilt() else: self.machine.events.post('tilt_warning', warnings=warnings, warnings_remaining=(self.tilt_config['warnings_to_tilt'] - warnings)) self.machine.events.post('tilt_warning_{}'.format(warnings)) def reset_warnings(self, **kwargs): """Resets the tilt warnings for the current player.""" try: self.player[self.tilt_config['tilt_warnings_player_var']] = 0 except AttributeError: pass def tilt(self, **kwargs): """Causes the ball to tilt.""" if not self.machine.game: return self._balls_to_collect = self.machine.playfield.balls # todo use collection self.log.debug("Processing Tilt. Balls to collect: %s", self._balls_to_collect) self.machine.game.tilted = True self.machine.events.post('tilt') self._disable_autofires() self._disable_flippers() self.tilt_event_handlers.add( self.machine.events.add_handler('ball_ending', self._ball_ending_tilted)) for device in self.machine.ball_devices: if 'drain' in device.tags: self.tilt_event_handlers.add( self.machine.events.add_handler( 'balldevice_{}_ball_enter'.format(device.name), self._tilted_ball_drain)) else: self.tilt_event_handlers.add( self.machine.events.add_handler( 'balldevice_{}_ball_enter'.format(device.name), self._tilted_ball_entered_non_drain_device)) self.machine.game.ball_ending() def _disable_flippers(self): for flipper in self.machine.flippers: flipper.disable() def _disable_autofires(self): for autofire in self.machine.autofires: autofire.disable() def _tilted_ball_drain(self, new_balls, unclaimed_balls, device): self._balls_to_collect -= unclaimed_balls self.log.debug("Tilted ball drain. Balls to collect: %s", self._balls_to_collect) if self._balls_to_collect <= 0: self._tilt_done() return {'unclaimed_balls': 0} def _tilted_ball_entered_non_drain_device(self, new_balls, unclaimed_balls, device): return {'unclaimed_balls': unclaimed_balls} def _tilt_switch_handler(self): self.tilt() def _tilt_warning_switch_handler(self): if (self._last_warning_tick + self.tilt_config['multiple_hit_window'] <= self.machine.tick_num): self.tilt_warning() def _ball_ending_tilted(self, queue): self.ball_ending_tilted_queue = queue queue.wait() if not self._balls_to_collect: self._tilt_done() def _tilt_done(self): if self.tilt_settle_ms_remaining(): self.delay.reset(ms=self.tilt_settle_ms_remaining(), callback=self._tilt_done, name='tilt') else: self.machine.game.tilted = False self.machine.events.post('tilt_clear') self.ball_ending_tilted_queue.clear() self.machine.events.remove_handlers_by_keys(self.tilt_event_handlers) self.tilt_event_handlers = set() def tilt_settle_ms_remaining(self): """Returns the amount of milliseconds remaining until the tilt settle time has cleared. """ ticks = (self.machine.tick_num - self.last_tilt_warning_switch_tick - self.tilt_config['settle_time']) if ticks >= 0: return 0 else: return abs(ticks * Timing.ms_per_tick) def slam_tilt(self): self.machine.events.post('slam_tilt') self.game_ended()
# -*- coding: utf-8 -*- # # documentation build configuration file, created by # sphinx-quickstart on Sat Sep 27 13:23:22 2008-2009. # # This file is execfile()d with the current directory set to its # containing dir. # # The contents of this file are pickled, so don't put values in the namespace # that aren't pickleable (module imports are okay, they're removed # automatically). # # All configuration values have a default value; values that are commented out # serve to show the default value. import sys import os # pip install sphinx_rtd_theme #import sphinx_rtd_theme #html_theme_path = [sphinx_rtd_theme.get_html_theme_path()] # If your extensions are in another directory, add it here. If the directory # is relative to the documentation root, use os.path.abspath to make it # absolute, like shown here. #sys.path.append(os.path.abspath('some/directory')) # sys.path.insert(0, os.path.join('ansible', 'lib')) sys.path.append(os.path.abspath('_themes')) VERSION='0.01' AUTHOR='AnsibleWorks' # General configuration # --------------------- # Add any Sphinx extension module names here, as strings. # They can be extensions # coming with Sphinx (named 'sphinx.ext.*') or your custom ones. extensions = ['sphinx.ext.autodoc'] # Later on, add 'sphinx.ext.viewcode' to the list if you want to have # colorized code generated too for references. # Add any paths that contain templates here, relative to this directory. templates_path = ['.templates'] # The suffix of source filenames. source_suffix = '.rst' # The master toctree document. master_doc = 'index' # General substitutions. project = 'Ansible Documentation' copyright = "2013 AnsibleWorks" # The default replacements for |version| and |release|, also used in various # other places throughout the built documents. # # The short X.Y version. version = VERSION # The full version, including alpha/beta/rc tags. release = VERSION # There are two options for replacing |today|: either, you set today to some # non-false value, then it is used: #today = '' # Else, today_fmt is used as the format for a strftime call. today_fmt = '%B %d, %Y' # List of documents that shouldn't be included in the build. #unused_docs = [] # List of directories, relative to source directories, that shouldn't be # searched for source files. #exclude_dirs = [] # A list of glob-style patterns that should be excluded when looking # for source files. exclude_patterns = ['modules'] # The reST default role (used for this markup: `text`) to use for all # documents. #default_role = None # If true, '()' will be appended to :func: etc. cross-reference text. #add_function_parentheses = True # If true, the current module name will be prepended to all description # unit titles (such as .. function::). #add_module_names = True # If true, sectionauthor and moduleauthor directives will be shown in the # output. They are ignored by default. #show_authors = False # The name of the Pygments (syntax highlighting) style to use. pygments_style = 'sphinx' # Options for HTML output # ----------------------- html_theme_path = ['_themes'] html_theme = 'srtd' html_short_title = 'Ansible Documentation' # The style sheet to use for HTML and HTML Help pages. A file of that name # must exist either in Sphinx' static/ path, or in one of the custom paths # given in html_static_path. #html_style = 'solar.css' # The name for this set of Sphinx documents. If None, it defaults to # "<project> v<release> documentation". html_title = 'Ansible Documentation' # A shorter title for the navigation bar. Default is the same as html_title. #html_short_title = None # The name of an image file (within the static path) to place at the top of # the sidebar. #html_logo = None # The name of an image file (within the static path) to use as favicon of the # docs. This file should be a Windows icon file (.ico) being 16x16 or 32x32 # pixels large. #html_favicon = 'favicon.ico' # Add any paths that contain custom static files (such as style sheets) here, # relative to this directory. They are copied after the builtin static files, # so a file named "default.css" will overwrite the builtin "default.css". #html_static_path = ['.static'] # If not '', a 'Last updated on:' timestamp is inserted at every page bottom, # using the given strftime format. html_last_updated_fmt = '%b %d, %Y' # If true, SmartyPants will be used to convert quotes and dashes to # typographically correct entities. #html_use_smartypants = True # Custom sidebar templates, maps document names to template names. #html_sidebars = {} # Additional templates that should be rendered to pages, maps page names to # template names. #html_additional_pages = {} # If false, no module index is generated. #html_use_modindex = True # If false, no index is generated. #html_use_index = True # If true, the index is split into individual pages for each letter. #html_split_index = False # If true, the reST sources are included in the HTML build as _sources/<name>. html_copy_source = False # If true, an OpenSearch description file will be output, and all pages will # contain a <link> tag referring to it. The value of this option must be the # base URL from which the finished HTML is served. #html_use_opensearch = '' # If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml"). #html_file_suffix = '' # Output file base name for HTML help builder. htmlhelp_basename = 'Poseidodoc' # Options for LaTeX output # ------------------------ # The paper size ('letter' or 'a4'). #latex_paper_size = 'letter' # The font size ('10pt', '11pt' or '12pt'). #latex_font_size = '10pt' # Grouping the document tree into LaTeX files. List of tuples # (source start file, target name, title, author, document class # [howto/manual]). latex_documents = [ ('index', 'ansible.tex', 'Ansible 1.2 Documentation', AUTHOR, 'manual'), ] # The name of an image file (relative to this directory) to place at the top of # the title page. #latex_logo = None # For "manual" documents, if this is true, then toplevel headings are parts, # not chapters. #latex_use_parts = False # Additional stuff for the LaTeX preamble. #latex_preamble = '' # Documents to append as an appendix to all manuals. #latex_appendices = [] # If false, no module index is generated. #latex_use_modindex = True autoclass_content = 'both'
from __future__ import unicode_literals import time from django.core.cache import cache class BaseThrottle(object): """ A simplified, swappable base class for throttling. Does nothing save for simulating the throttling API and implementing some common bits for the subclasses. Accepts a number of optional kwargs:: * ``throttle_at`` - the number of requests at which the user should be throttled. Default is 150 requests. * ``timeframe`` - the length of time (in seconds) in which the user make up to the ``throttle_at`` requests. Default is 3600 seconds ( 1 hour). * ``expiration`` - the length of time to retain the times the user has accessed the api in the cache. Default is 604800 (1 week). """ def __init__(self, throttle_at=150, timeframe=3600, expiration=None): self.throttle_at = throttle_at # In seconds, please. self.timeframe = timeframe if expiration is None: # Expire in a week. expiration = 604800 self.expiration = int(expiration) def convert_identifier_to_key(self, identifier): """ Takes an identifier (like a username or IP address) and converts it into a key usable by the cache system. """ bits = [] for char in identifier: if char.isalnum() or char in ['_', '.', '-']: bits.append(char) safe_string = ''.join(bits) return "%s_accesses" % safe_string def should_be_throttled(self, identifier, **kwargs): """ Returns whether or not the user has exceeded their throttle limit. If throttled, can return either True, and int specifying the number of seconds to wait, or a datetime object specifying when to retry the request. Always returns ``False``, as this implementation does not actually throttle the user. """ return False def accessed(self, identifier, **kwargs): """ Handles recording the user's access. Does nothing in this implementation. """ pass class CacheThrottle(BaseThrottle): """ A throttling mechanism that uses just the cache. """ def should_be_throttled(self, identifier, **kwargs): """ Returns whether or not the user has exceeded their throttle limit. If throttled, can return either True, and int specifying the number of seconds to wait, or a datetime object specifying when to retry the request. Maintains a list of timestamps when the user accessed the api within the cache. Returns ``False`` if the user should NOT be throttled or ``True`` if the user should be throttled. """ key = self.convert_identifier_to_key(identifier) # Weed out anything older than the timeframe. now = int(time.time()) timeframe = int(self.timeframe) throttle_at = int(self.throttle_at) minimum_time = now - timeframe times_accessed = [access for access in cache.get(key, []) if access >= minimum_time] cache.set(key, times_accessed, self.expiration) if len(times_accessed) >= throttle_at: # Throttle them. return timeframe - (now - times_accessed[-throttle_at]) # Let them through. return False def accessed(self, identifier, **kwargs): """ Handles recording the user's access. Stores the current timestamp in the "accesses" list within the cache. """ key = self.convert_identifier_to_key(identifier) times_accessed = cache.get(key, []) times_accessed.append(int(time.time())) cache.set(key, times_accessed, self.expiration) class CacheDBThrottle(CacheThrottle): """ A throttling mechanism that uses the cache for actual throttling but writes-through to the database. This is useful for tracking/aggregating usage through time, to possibly build a statistics interface or a billing mechanism. """ def accessed(self, identifier, **kwargs): """ Handles recording the user's access. Does everything the ``CacheThrottle`` class does, plus logs the access within the database using the ``ApiAccess`` model. """ # Do the import here, instead of top-level, so that the model is # only required when using this throttling mechanism. from tastypie.models import ApiAccess super(CacheDBThrottle, self).accessed(identifier, **kwargs) # Write out the access to the DB for logging purposes. ApiAccess.objects.create( identifier=identifier, url=kwargs.get('url', ''), request_method=kwargs.get('request_method', '') )
#! /usr/bin/env python """Tool for measuring execution time of small code snippets. This module avoids a number of common traps for measuring execution times. See also Tim Peters' introduction to the Algorithms chapter in the Python Cookbook, published by O'Reilly. Library usage: see the Timer class. Command line usage: python timeit.py [-n N] [-r N] [-s S] [-t] [-c] [-h] [--] [statement] Options: -n/--number N: how many times to execute 'statement' (default: see below) -r/--repeat N: how many times to repeat the timer (default 3) -s/--setup S: statement to be executed once initially (default 'pass') -t/--time: use time.time() (default on Unix) -c/--clock: use time.clock() (default on Windows) -v/--verbose: print raw timing results; repeat for more digits precision -h/--help: print this usage message and exit --: separate options from statement, use when statement starts with - statement: statement to be timed (default 'pass') A multi-line statement may be given by specifying each line as a separate argument; indented lines are possible by enclosing an argument in quotes and using leading spaces. Multiple -s options are treated similarly. If -n is not given, a suitable number of loops is calculated by trying successive powers of 10 until the total time is at least 0.2 seconds. The difference in default timer function is because on Windows, clock() has microsecond granularity but time()'s granularity is 1/60th of a second; on Unix, clock() has 1/100th of a second granularity and time() is much more precise. On either platform, the default timer functions measure wall clock time, not the CPU time. This means that other processes running on the same computer may interfere with the timing. The best thing to do when accurate timing is necessary is to repeat the timing a few times and use the best time. The -r option is good for this; the default of 3 repetitions is probably enough in most cases. On Unix, you can use clock() to measure CPU time. Note: there is a certain baseline overhead associated with executing a pass statement. The code here doesn't try to hide it, but you should be aware of it. The baseline overhead can be measured by invoking the program without arguments. The baseline overhead differs between Python versions! Also, to fairly compare older Python versions to Python 2.3, you may want to use python -O for the older versions to avoid timing SET_LINENO instructions. """ import gc import sys import time try: import itertools except ImportError: # Must be an older Python version (see timeit() below) itertools = None __all__ = ["Timer"] dummy_src_name = "<timeit-src>" default_number = 1000000 default_repeat = 3 if sys.platform == "win32": # On Windows, the best timer is time.clock() default_timer = time.clock else: # On most other platforms the best timer is time.time() default_timer = time.time # Don't change the indentation of the template; the reindent() calls # in Timer.__init__() depend on setup being indented 4 spaces and stmt # being indented 8 spaces. template = """ def inner(_it, _timer): %(setup)s _t0 = _timer() for _i in _it: %(stmt)s _t1 = _timer() return _t1 - _t0 """ def reindent(src, indent): """Helper to reindent a multi-line statement.""" return src.replace("\n", "\n" + " "*indent) def _template_func(setup, func): """Create a timer function. Used if the "statement" is a callable.""" def inner(_it, _timer, _func=func): setup() _t0 = _timer() for _i in _it: _func() _t1 = _timer() return _t1 - _t0 return inner class Timer: """Class for timing execution speed of small code snippets. The constructor takes a statement to be timed, an additional statement used for setup, and a timer function. Both statements default to 'pass'; the timer function is platform-dependent (see module doc string). To measure the execution time of the first statement, use the timeit() method. The repeat() method is a convenience to call timeit() multiple times and return a list of results. The statements may contain newlines, as long as they don't contain multi-line string literals. """ def __init__(self, stmt="pass", setup="pass", timer=default_timer): """Constructor. See class doc string.""" self.timer = timer ns = {} if isinstance(stmt, basestring): stmt = reindent(stmt, 8) if isinstance(setup, basestring): setup = reindent(setup, 4) src = template % {'stmt': stmt, 'setup': setup} elif hasattr(setup, '__call__'): src = template % {'stmt': stmt, 'setup': '_setup()'} ns['_setup'] = setup else: raise ValueError("setup is neither a string nor callable") self.src = src # Save for traceback display code = compile(src, dummy_src_name, "exec") exec code in globals(), ns self.inner = ns["inner"] elif hasattr(stmt, '__call__'): self.src = None if isinstance(setup, basestring): _setup = setup def setup(): exec _setup in globals(), ns elif not hasattr(setup, '__call__'): raise ValueError("setup is neither a string nor callable") self.inner = _template_func(setup, stmt) else: raise ValueError("stmt is neither a string nor callable") def print_exc(self, file=None): """Helper to print a traceback from the timed code. Typical use: t = Timer(...) # outside the try/except try: t.timeit(...) # or t.repeat(...) except: t.print_exc() The advantage over the standard traceback is that source lines in the compiled template will be displayed. The optional file argument directs where the traceback is sent; it defaults to sys.stderr. """ import linecache, traceback if self.src is not None: linecache.cache[dummy_src_name] = (len(self.src), None, self.src.split("\n"), dummy_src_name) # else the source is already stored somewhere else traceback.print_exc(file=file) def timeit(self, number=default_number): """Time 'number' executions of the main statement. To be precise, this executes the setup statement once, and then returns the time it takes to execute the main statement a number of times, as a float measured in seconds. The argument is the number of times through the loop, defaulting to one million. The main statement, the setup statement and the timer function to be used are passed to the constructor. """ if itertools: it = itertools.repeat(None, number) else: it = [None] * number gcold = gc.isenabled() gc.disable() timing = self.inner(it, self.timer) if gcold: gc.enable() return timing def repeat(self, repeat=default_repeat, number=default_number): """Call timeit() a few times. This is a convenience function that calls the timeit() repeatedly, returning a list of results. The first argument specifies how many times to call timeit(), defaulting to 3; the second argument specifies the timer argument, defaulting to one million. Note: it's tempting to calculate mean and standard deviation from the result vector and report these. However, this is not very useful. In a typical case, the lowest value gives a lower bound for how fast your machine can run the given code snippet; higher values in the result vector are typically not caused by variability in Python's speed, but by other processes interfering with your timing accuracy. So the min() of the result is probably the only number you should be interested in. After that, you should look at the entire vector and apply common sense rather than statistics. """ r = [] for i in range(repeat): t = self.timeit(number) r.append(t) return r def timeit(stmt="pass", setup="pass", timer=default_timer, number=default_number): """Convenience function to create Timer object and call timeit method.""" return Timer(stmt, setup, timer).timeit(number) def repeat(stmt="pass", setup="pass", timer=default_timer, repeat=default_repeat, number=default_number): """Convenience function to create Timer object and call repeat method.""" return Timer(stmt, setup, timer).repeat(repeat, number) def main(args=None): """Main program, used when run as a script. The optional argument specifies the command line to be parsed, defaulting to sys.argv[1:]. The return value is an exit code to be passed to sys.exit(); it may be None to indicate success. When an exception happens during timing, a traceback is printed to stderr and the return value is 1. Exceptions at other times (including the template compilation) are not caught. """ if args is None: args = sys.argv[1:] import getopt try: opts, args = getopt.getopt(args, "n:s:r:tcvh", ["number=", "setup=", "repeat=", "time", "clock", "verbose", "help"]) except getopt.error, err: print err print "use -h/--help for command line help" return 2 timer = default_timer stmt = "\n".join(args) or "pass" number = 0 # auto-determine setup = [] repeat = default_repeat verbose = 0 precision = 3 for o, a in opts: if o in ("-n", "--number"): number = int(a) if o in ("-s", "--setup"): setup.append(a) if o in ("-r", "--repeat"): repeat = int(a) if repeat <= 0: repeat = 1 if o in ("-t", "--time"): timer = time.time if o in ("-c", "--clock"): timer = time.clock if o in ("-v", "--verbose"): if verbose: precision += 1 verbose += 1 if o in ("-h", "--help"): print __doc__, return 0 setup = "\n".join(setup) or "pass" # Include the current directory, so that local imports work (sys.path # contains the directory of this script, rather than the current # directory) import os sys.path.insert(0, os.curdir) t = Timer(stmt, setup, timer) if number == 0: # determine number so that 0.2 <= total time < 2.0 for i in range(1, 10): number = 10**i try: x = t.timeit(number) except: t.print_exc() return 1 if verbose: print "%d loops -> %.*g secs" % (number, precision, x) if x >= 0.2: break try: r = t.repeat(repeat, number) except: t.print_exc() return 1 best = min(r) if verbose: print "raw times:", " ".join(["%.*g" % (precision, x) for x in r]) print "%d loops," % number, usec = best * 1e6 / number if usec < 1000: print "best of %d: %.*g usec per loop" % (repeat, precision, usec) else: msec = usec / 1000 if msec < 1000: print "best of %d: %.*g msec per loop" % (repeat, precision, msec) else: sec = msec / 1000 print "best of %d: %.*g sec per loop" % (repeat, precision, sec) return None if __name__ == "__main__": sys.exit(main())
import sys as s from parsingMatch import parseAllFact from parsingFasta import parseFasta def sanitizeNode(node): if not node or not (len(node) == 2): #It means this node cannot appear in the taxonomic tree return None else: return node #@allMatches is a dictionary of (key=sample ID,value=list of sequences ID matching a read in this sample) #@idSequences is a dictionary of (key=identifier of node,value=(name,rank of node)) #@filenames is the list of .match file names == list of samples ID /!\ #Returns a dictionary of (key=sample ID,value=list of nodes (name,rank) matching a read in this sample) def getMatchingNodes(allMatches,idSequences,filenames): matchingNodes = dict.fromkeys(filenames) for sample in filenames: matchingSequencesID = allMatches.get(sample) matchingNodesInThisSample = [] if not (matchingSequencesID == None): for sequenceID in matchingSequencesID: node = idSequences.get(sequenceID) cleanNode = sanitizeNode(node) if cleanNode: matchingNodesInThisSample.append(cleanNode) matchingNodes[sample] = matchingNodesInThisSample else: print "The sample \'",sample,"\' could not be processed." return matchingNodes #Returns @matchingNodes, dictionary of (key=sample ID,value=list of nodes matched in this sample -i.e. at least in one read of this sample), and @idSequences, which is a dictionary of (key=identifier of sequence,value=(name,rank) of the node associated to this sequence) #@filenames is the list of .match file names == list of samples ID /!\ #@fastaFileName is a string of the .fasta file name #@sampleIDList is the list of samples ID def featuresCreate(filenames,fastaFileName): print "/!\ Parsing .match files" print "[ You may have to wait a few seconds... ]" #@allMatches is a dictionary of (key=sample ID,value=list of sequences ID matching a read in this sample) import time start = time.time() allMatches = parseAllFact(filenames) end = time.time() print "TIME:",(end-start),"sec" print "/!\ Parsing .fasta files" print "[ You may have to wait a few seconds... ]" try: #@idSequences is a dictionary of (key=identifier,value=((name,rank)) #@paths is the list of paths from root to leaves #@nodesListTree is the list of all nodes (internal nodes and leaves) in the tree #We do not care for now of the OTU idSequences,paths,nodesListTree,_ = parseFasta(fastaFileName) except IOError: print "\nERROR: Maybe the filename",fastaFileName,".fasta does not exist in \"meta\" folder\n" s.exit(0) matchingNodes = getMatchingNodes(allMatches,idSequences,filenames) print "/!\ Matching nodes list done." return matchingNodes,idSequences,paths,nodesListTree
import os import sys import warnings import imp import opcode # opcode is not a virtualenv module, so we can use it to find the stdlib # Important! To work on pypy, this must be a module that resides in the # lib-python/modified-x.y.z directory dirname = os.path.dirname distutils_path = os.path.join(os.path.dirname(opcode.__file__), 'distutils') if os.path.normpath(distutils_path) == os.path.dirname(os.path.normpath(__file__)): warnings.warn( "The virtualenv distutils package at %s appears to be in the same location as the system distutils?") else: __path__.insert(0, distutils_path) real_distutils = imp.load_module("_virtualenv_distutils", None, distutils_path, ('', '', imp.PKG_DIRECTORY)) # Copy the relevant attributes try: __revision__ = real_distutils.__revision__ except AttributeError: pass __version__ = real_distutils.__version__ from distutils import dist, sysconfig try: basestring except NameError: basestring = str ## patch build_ext (distutils doesn't know how to get the libs directory ## path on windows - it hardcodes the paths around the patched sys.prefix) if sys.platform == 'win32': from distutils.command.build_ext import build_ext as old_build_ext class build_ext(old_build_ext): def finalize_options (self): if self.library_dirs is None: self.library_dirs = [] elif isinstance(self.library_dirs, basestring): self.library_dirs = self.library_dirs.split(os.pathsep) self.library_dirs.insert(0, os.path.join(sys.real_prefix, "Libs")) old_build_ext.finalize_options(self) from distutils.command import build_ext as build_ext_module build_ext_module.build_ext = build_ext ## distutils.dist patches: old_find_config_files = dist.Distribution.find_config_files def find_config_files(self): found = old_find_config_files(self) system_distutils = os.path.join(distutils_path, 'distutils.cfg') #if os.path.exists(system_distutils): # found.insert(0, system_distutils) # What to call the per-user config file if os.name == 'posix': user_filename = ".pydistutils.cfg" else: user_filename = "pydistutils.cfg" user_filename = os.path.join(sys.prefix, user_filename) if os.path.isfile(user_filename): for item in list(found): if item.endswith('pydistutils.cfg'): found.remove(item) found.append(user_filename) return found dist.Distribution.find_config_files = find_config_files ## distutils.sysconfig patches: old_get_python_inc = sysconfig.get_python_inc def sysconfig_get_python_inc(plat_specific=0, prefix=None): if prefix is None: prefix = sys.real_prefix return old_get_python_inc(plat_specific, prefix) sysconfig_get_python_inc.__doc__ = old_get_python_inc.__doc__ sysconfig.get_python_inc = sysconfig_get_python_inc old_get_python_lib = sysconfig.get_python_lib def sysconfig_get_python_lib(plat_specific=0, standard_lib=0, prefix=None): if standard_lib and prefix is None: prefix = sys.real_prefix return old_get_python_lib(plat_specific, standard_lib, prefix) sysconfig_get_python_lib.__doc__ = old_get_python_lib.__doc__ sysconfig.get_python_lib = sysconfig_get_python_lib old_get_config_vars = sysconfig.get_config_vars def sysconfig_get_config_vars(*args): real_vars = old_get_config_vars(*args) if sys.platform == 'win32': lib_dir = os.path.join(sys.real_prefix, "libs") if isinstance(real_vars, dict) and 'LIBDIR' not in real_vars: real_vars['LIBDIR'] = lib_dir # asked for all elif isinstance(real_vars, list) and 'LIBDIR' in args: real_vars = real_vars + [lib_dir] # asked for list return real_vars sysconfig_get_config_vars.__doc__ = old_get_config_vars.__doc__ sysconfig.get_config_vars = sysconfig_get_config_vars
# -*- coding: utf-8 -*- from django.contrib.auth.models import User from django.db import models from django.utils.encoding import python_2_unicode_compatible @python_2_unicode_compatible class Band(models.Model): name = models.CharField(max_length=100) bio = models.TextField() sign_date = models.DateField() class Meta: ordering = ('name',) def __str__(self): return self.name class Concert(models.Model): main_band = models.ForeignKey(Band, models.CASCADE, related_name='main_concerts') opening_band = models.ForeignKey(Band, models.CASCADE, related_name='opening_concerts', blank=True) day = models.CharField(max_length=3, choices=((1, 'Fri'), (2, 'Sat'))) transport = models.CharField(max_length=100, choices=( (1, 'Plane'), (2, 'Train'), (3, 'Bus') ), blank=True) class ValidationTestModel(models.Model): name = models.CharField(max_length=100) slug = models.SlugField() users = models.ManyToManyField(User) state = models.CharField(max_length=2, choices=(("CO", "Colorado"), ("WA", "Washington"))) is_active = models.BooleanField(default=False) pub_date = models.DateTimeField() band = models.ForeignKey(Band, models.CASCADE) # This field is intentionally 2 characters long (#16080). no = models.IntegerField(verbose_name="Number", blank=True, null=True) def decade_published_in(self): return self.pub_date.strftime('%Y')[:3] + "0's" class ValidationTestInlineModel(models.Model): parent = models.ForeignKey(ValidationTestModel, models.CASCADE)
import os def main(): import sys #Separate the nose params and the pydev params. pydev_params = [] other_test_framework_params = [] found_other_test_framework_param = None NOSE_PARAMS = '--nose-params' PY_TEST_PARAMS = '--py-test-params' for arg in sys.argv[1:]: if not found_other_test_framework_param and arg != NOSE_PARAMS and arg != PY_TEST_PARAMS: pydev_params.append(arg) else: if not found_other_test_framework_param: found_other_test_framework_param = arg else: other_test_framework_params.append(arg) #Here we'll run either with nose or with the pydev_runfiles. import pydev_runfiles import pydev_runfiles_xml_rpc import pydevd_constants from pydevd_file_utils import _NormFile DEBUG = 0 if DEBUG: sys.stdout.write('Received parameters: %s\n' % (sys.argv,)) sys.stdout.write('Params for pydev: %s\n' % (pydev_params,)) if found_other_test_framework_param: sys.stdout.write('Params for test framework: %s, %s\n' % (found_other_test_framework_param, other_test_framework_params)) try: configuration = pydev_runfiles.parse_cmdline([sys.argv[0]] + pydev_params) except: sys.stderr.write('Command line received: %s\n' % (sys.argv,)) raise pydev_runfiles_xml_rpc.InitializeServer(configuration.port) #Note that if the port is None, a Null server will be initialized. NOSE_FRAMEWORK = 1 PY_TEST_FRAMEWORK = 2 try: if found_other_test_framework_param: test_framework = 0 #Default (pydev) if found_other_test_framework_param == NOSE_PARAMS: import nose test_framework = NOSE_FRAMEWORK elif found_other_test_framework_param == PY_TEST_PARAMS: import pytest test_framework = PY_TEST_FRAMEWORK else: raise ImportError() else: raise ImportError() except ImportError: if found_other_test_framework_param: sys.stderr.write('Warning: Could not import the test runner: %s. Running with the default pydev unittest runner instead.\n' % ( found_other_test_framework_param,)) test_framework = 0 #Clear any exception that may be there so that clients don't see it. #See: https://sourceforge.net/tracker/?func=detail&aid=3408057&group_id=85796&atid=577329 if hasattr(sys, 'exc_clear'): sys.exc_clear() if test_framework == 0: return pydev_runfiles.main(configuration) #Note: still doesn't return a proper value. else: #We'll convert the parameters to what nose or py.test expects. #The supported parameters are: #runfiles.py --config-file|-t|--tests <Test.test1,Test2> dirs|files --nose-params xxx yyy zzz #(all after --nose-params should be passed directly to nose) #In java: #--tests = Constants.ATTR_UNITTEST_TESTS #--config-file = Constants.ATTR_UNITTEST_CONFIGURATION_FILE #The only thing actually handled here are the tests that we want to run, which we'll #handle and pass as what the test framework expects. py_test_accept_filter = {} files_to_tests = configuration.files_to_tests if files_to_tests: #Handling through the file contents (file where each line is a test) files_or_dirs = [] for file, tests in files_to_tests.items(): if test_framework == NOSE_FRAMEWORK: for test in tests: files_or_dirs.append(file + ':' + test) elif test_framework == PY_TEST_FRAMEWORK: file = _NormFile(file) py_test_accept_filter[file] = tests files_or_dirs.append(file) else: raise AssertionError('Cannot handle test framework: %s at this point.' % (test_framework,)) else: if configuration.tests: #Tests passed (works together with the files_or_dirs) files_or_dirs = [] for file in configuration.files_or_dirs: if test_framework == NOSE_FRAMEWORK: for t in configuration.tests: files_or_dirs.append(file + ':' + t) elif test_framework == PY_TEST_FRAMEWORK: file = _NormFile(file) py_test_accept_filter[file] = configuration.tests files_or_dirs.append(file) else: raise AssertionError('Cannot handle test framework: %s at this point.' % (test_framework,)) else: #Only files or dirs passed (let it do the test-loading based on those paths) files_or_dirs = configuration.files_or_dirs argv = other_test_framework_params + files_or_dirs if test_framework == NOSE_FRAMEWORK: #Nose usage: http://somethingaboutorange.com/mrl/projects/nose/0.11.2/usage.html #show_stdout_option = ['-s'] #processes_option = ['--processes=2'] argv.insert(0, sys.argv[0]) if DEBUG: sys.stdout.write('Final test framework args: %s\n' % (argv[1:],)) import pydev_runfiles_nose PYDEV_NOSE_PLUGIN_SINGLETON = pydev_runfiles_nose.StartPydevNosePluginSingleton(configuration) argv.append('--with-pydevplugin') # Return 'not' because it will return 'success' (so, exit == 0 if success) return not nose.run(argv=argv, addplugins=[PYDEV_NOSE_PLUGIN_SINGLETON]) elif test_framework == PY_TEST_FRAMEWORK: if DEBUG: sys.stdout.write('Final test framework args: %s\n' % (argv,)) sys.stdout.write('py_test_accept_filter: %s\n' % (py_test_accept_filter,)) import os try: xrange except: xrange = range for i in xrange(len(argv)): arg = argv[i] #Workaround bug in py.test: if we pass the full path it ends up importing conftest #more than once (so, always work with relative paths). if os.path.isfile(arg) or os.path.isdir(arg): from pydev_imports import relpath arg = relpath(arg) argv[i] = arg d = os.path.dirname(__file__) if d not in sys.path: sys.path.insert(0, d) import pickle, zlib, base64 # Update environment PYTHONPATH so that it finds our plugin if using xdist. os.environ['PYTHONPATH'] = os.pathsep.join(sys.path) # Set what should be skipped in the plugin through an environment variable s = base64.b64encode(zlib.compress(pickle.dumps(py_test_accept_filter))) if pydevd_constants.IS_PY3K: s = s.decode('ascii') # Must be str in py3. os.environ['PYDEV_PYTEST_SKIP'] = s # Identifies the main pid (i.e.: if it's not the main pid it has to connect back to the # main pid to give xml-rpc notifications). os.environ['PYDEV_MAIN_PID'] = str(os.getpid()) os.environ['PYDEV_PYTEST_SERVER'] = str(configuration.port) argv.append('-p') argv.append('pydev_runfiles_pytest2') if 'unittest' in sys.modules or 'unittest2' in sys.modules: sys.stderr.write('pydev test runner error: imported unittest before running pytest.main\n') return pytest.main(argv) else: raise AssertionError('Cannot handle test framework: %s at this point.' % (test_framework,)) if __name__ == '__main__': try: main() finally: try: #The server is not a daemon thread, so, we have to ask for it to be killed! import pydev_runfiles_xml_rpc pydev_runfiles_xml_rpc.forceServerKill() except: pass #Ignore any errors here import sys import threading if hasattr(sys, '_current_frames') and hasattr(threading, 'enumerate'): import time import traceback class DumpThreads(threading.Thread): def run(self): time.sleep(10) thread_id_to_name = {} try: for t in threading.enumerate(): thread_id_to_name[t.ident] = '%s (daemon: %s)' % (t.name, t.daemon) except: pass stack_trace = [ '===============================================================================', 'pydev pyunit runner: Threads still found running after tests finished', '================================= Thread Dump ================================='] for thread_id, stack in sys._current_frames().items(): stack_trace.append('\n-------------------------------------------------------------------------------') stack_trace.append(" Thread %s" % thread_id_to_name.get(thread_id, thread_id)) stack_trace.append('') if 'self' in stack.f_locals: sys.stderr.write(str(stack.f_locals['self'])+'\n') for filename, lineno, name, line in traceback.extract_stack(stack): stack_trace.append(' File "%s", line %d, in %s' % (filename, lineno, name)) if line: stack_trace.append(" %s" % (line.strip())) stack_trace.append('\n=============================== END Thread Dump ===============================') sys.stderr.write('\n'.join(stack_trace)) dump_current_frames_thread = DumpThreads() dump_current_frames_thread.setDaemon(True) # Daemon so that this thread doesn't halt it! dump_current_frames_thread.start()
""" A splash screen widget with support for positioning of the message text. """ from PyQt4.QtGui import ( QSplashScreen, QWidget, QPixmap, QPainter, QTextDocument, QTextBlockFormat, QTextCursor, QApplication ) from PyQt4.QtCore import Qt from .utils import is_transparency_supported class SplashScreen(QSplashScreen): """ Splash screen widget. Parameters ---------- parent : :class:`QWidget` Parent widget pixmap : :class:`QPixmap` Splash window pixmap. textRect : :class:`QRect` Bounding rectangle of the shown message on the widget. """ def __init__(self, parent=None, pixmap=None, textRect=None, **kwargs): QSplashScreen.__init__(self, parent, **kwargs) self.__textRect = textRect self.__message = "" self.__color = Qt.black self.__alignment = Qt.AlignLeft if pixmap is None: pixmap = QPixmap() self.setPixmap(pixmap) self.setAutoFillBackground(False) # Also set FramelesWindowHint (if not already set) self.setWindowFlags(self.windowFlags() | Qt.FramelessWindowHint) def setTextRect(self, rect): """ Set the rectangle (:class:`QRect`) in which to show the message text. """ if self.__textRect != rect: self.__textRect = rect self.update() def textRect(self): """ Return the text message rectangle. """ return self.__textRect def showEvent(self, event): QSplashScreen.showEvent(self, event) # Raise to top on show. self.raise_() def drawContents(self, painter): """ Reimplementation of drawContents to limit the drawing inside `textRext`. """ painter.setPen(self.__color) painter.setFont(self.font()) if self.__textRect: rect = self.__textRect else: rect = self.rect().adjusted(5, 5, -5, -5) if Qt.mightBeRichText(self.__message): doc = QTextDocument() doc.setHtml(self.__message) doc.setTextWidth(rect.width()) cursor = QTextCursor(doc) cursor.select(QTextCursor.Document) fmt = QTextBlockFormat() fmt.setAlignment(self.__alignment) cursor.mergeBlockFormat(fmt) painter.save() painter.translate(rect.topLeft()) doc.drawContents(painter) painter.restore() else: painter.drawText(rect, self.__alignment, self.__message) def showMessage(self, message, alignment=Qt.AlignLeft, color=Qt.black): """ Show the `message` with `color` and `alignment`. """ # Need to store all this arguments for drawContents (no access # methods) self.__alignment = alignment self.__color = color self.__message = message QSplashScreen.showMessage(self, message, alignment, color) QApplication.instance().processEvents() # Reimplemented to allow graceful fall back if the windowing system # does not support transparency. def setPixmap(self, pixmap): self.setAttribute(Qt.WA_TranslucentBackground, pixmap.hasAlpha() and \ is_transparency_supported()) self.__pixmap = pixmap QSplashScreen.setPixmap(self, pixmap) if pixmap.hasAlpha() and not is_transparency_supported(): self.setMask(pixmap.createHeuristicMask()) def repaint(self): QWidget.repaint(self) QApplication.flush() def event(self, event): if event.type() == event.Paint: pixmap = self.__pixmap painter = QPainter(self) if not pixmap.isNull(): painter.drawPixmap(0, 0, pixmap) self.drawContents(painter) return True return QSplashScreen.event(self, event)
######################################################################## # File : FC_Scaling_test # Author : Andrei Tsaregorodtsev ######################################################################## """ Test suite for a generic File Catalog scalability tests """ __RCSID__ = "$Id$" from DIRAC.Core.Base import Script from DIRAC import S_OK import sys, pprint, os, numpy Script.setUsageMessage( """ Test suite for a generic File Catalog scalability tests """ ) testType = 'noTest' def setTestType( value ): global testType testType = value return S_OK() testDir = '' def setTestDirectory( value ): global testDir testDir = value return S_OK() nClients = 1 def setNumberOfClients( value ): global nClients nClients = int( value ) return S_OK() nQueries = 100 def setNumberOfQueries( value ): global nQueries nQueries = int( value ) return S_OK() lfnListFile = 'lfns_100.txt' def setLFNListFile( value ): global lfnListFile lfnListFile = value return S_OK() outputFile = "output.txt" def setOutputFile( value ): global outputFile outputFile = value return S_OK() catalog = 'AugerTestFileCatalog' def setCatalog( value ): global catalog catalog = value return S_OK() fullTest = False def setFullTest( value ): global fullTest fullTest = True return S_OK() shortRange = False def setShortRange( value ): global shortRange shortRange = True return S_OK() verbosity = 0 def setVerbosity( value ): global verbosity verbosity += 1 return S_OK() Script.registerSwitch( "t:", "type=", "test type", setTestType ) Script.registerSwitch( "D:", "directory=", "test directory", setTestDirectory ) Script.registerSwitch( "N:", "clients=", "number of parallel clients", setNumberOfClients ) Script.registerSwitch( "Q:", "queries=", "number of queries in one test", setNumberOfQueries ) Script.registerSwitch( "C:", "catalog=", "catalog to use", setCatalog ) Script.registerSwitch( "L:", "lfnList=", "file with a list of LFNs", setLFNListFile ) Script.registerSwitch( "F", "fullTest", "run the full test", setFullTest ) Script.registerSwitch( "O:", "output=", "file with output result", setOutputFile ) Script.registerSwitch( "v", "verbose", "file with output result", setVerbosity ) Script.registerSwitch( "S", "shortRange", "run short parameter range", setShortRange ) Script.parseCommandLine( ignoreErrors = True ) from DIRAC.Resources.Catalog.FileCatalog import FileCatalog from DIRAC.Core.Utilities.ProcessPool import ProcessPool from DIRAC import S_OK import time fc = FileCatalog( catalogs=[catalog] ) resultTest = [] def listDirectory( n_queries ): global testDir start = time.time() sCount = 0 fCount = 0 resultList = [] startTotal = time.time() for i in xrange( n_queries ) : start = time.time() result = fc.listDirectory( testDir ) resultList.append( time.time() - start ) if result['OK']: sCount += 1 else: fCount += 1 total = time.time() - startTotal average, error = doStats( resultList ) if verbosity >= 1: print "getReplicas: Total time", total, 'Success', sCount, 'Failure', \ fCount, 'Average', average, 'Stdvar', error result = S_OK( (resultList, sCount, fCount) ) return result def getBulkReplicas( n_queries ): global lfnListFile, verbosity lFile = open(lfnListFile) lfnList = [ l.strip().replace('//','/') for l in lFile.read().strip().split() ] lFile.close() start = time.time() sCount = 0 fCount = 0 resultList = [] startTotal = time.time() for i in xrange( n_queries ) : start = time.time() result = fc.getReplicas( lfnList ) resultList.append( time.time() - start ) if verbosity >= 2: print "getReplicas: received lfns", len(result['Value']['Successful']) for lfn in result['Value']['Successful']: print result['Value']['Successful'][lfn] if verbosity >= 3: for lfn,res in result['Value']['Successful'].items(): print lfn print res break if result['OK']: sCount += 1 else: fCount += 1 total = time.time() - startTotal average, error = doStats( resultList ) if verbosity >= 1: print "getReplicas: Total time", total, 'Success', sCount, 'Failure', \ fCount, 'Average', average, 'Stdvar', error result = S_OK( (resultList, sCount, fCount) ) return result def getDirectoryReplicas( n_queries ): global testDir, verbosity sCount = 0 fCount = 0 resultList = [] startTotal = time.time() for i in xrange( n_queries ) : start = time.time() result = fc.getDirectoryReplicas( testDir ) resultList.append( time.time() - start ) if verbosity >= 2: print "Returned values", len(result['Value']['Successful'][testDir]) for lfn,res in result['Value']['Successful'][testDir].items(): print lfn print res break if result['OK']: sCount += 1 else: fCount += 1 total = time.time() - startTotal average, error = doStats( resultList ) if verbosity >= 1: print "getDirectoryReplicas: Total time", total, 'Success', sCount, 'Failure', \ fCount, '\nAverage', average, 'Stdvar', error result = S_OK( (resultList, sCount, fCount) ) return result def finalize(task,result): global resultTest, verbosity if verbosity >= 2: if result['OK']: print "Test time ", result['Value'], task.getTaskID() else: print "Error:", result['Message'] resultTest.append( result['Value'] ) def doException( expt ): print "Exception", expt def runTest( ): global nClients, nQueries, testType, resultTest, testDir, lfnListFile resultTest = [] pp = ProcessPool( nClients ) testFunction = eval( testType ) for c in xrange( nClients ): pp.createAndQueueTask( testFunction, [nQueries], callback=finalize, exceptionCallback=doException ) pp.processAllResults(3600) pp.finalize(0) timeResult = [] for testTime,success,failure in resultTest: #print testTime,success,failure timeResult += testTime averageTime, errorTime = doStats( timeResult ) rateResult = [ nClients/t for t in timeResult ] averageRate, errorRate = doStats( rateResult ) if testDir: print "\nTest results for clients %d, %s" % ( nClients, testDir ) else: print "\nTest results for clients %d, %s" % ( nClients, lfnListFile ) print "Query time: %.2f +/- %.2f" % (averageTime, errorTime) print "Query rate: %.2f +/- %.2f" % (averageRate, errorRate) return( (averageTime, errorTime), (averageRate, errorRate) ) def doStats( testArray ): array = list( testArray ) # Delete min and max value first del array[ array.index(max(array)) ] del array[ array.index(min(array)) ] numArray = numpy.array( array ) average = numpy.mean( numArray ) stddev = numpy.std( numArray ) return (average, stddev) numberOfFilesList = [ 10, 100, 500, 1000, 2000, 5000, 10000, 15000, 20000 ] numberOfFilesList_short = [ 100, 1000, 5000, 10000, 20000 ] numberOfClientsList = [1,2,3,5,7,10,12,15,20,30,50,75] numberOfClientsList_short = [1,5,10,20] directoriesList = [ (35455, "/auger/prod/QGSjetII_gr20_simADSTv2r5p1/en18.000/th0.65/2008/11/12"), (24024, "/auger/prod/QGSjetII_gr20/2008/09/04/en17.500/th0.65"), #(15205, "/auger/generated/2012-09-03"), (18391,"/auger/prod/QGSjetII_gr20_simADSTv2r5p1/en17.500/th0.65/2008/11/11"), (9907, "/auger/prod/QGSjetII_gr20/2008/09/03/en17.500/th0.65"), (5157, "/auger/prod/QGSjetII_gr20/2008/09/04/en20.000/th0.65"), (2538, "/auger/prod/QGSjetII_gr21/2009/01/12/en18.500/th0.65"), (1500, "/auger/prod/epos_gr03_sim/en17.500/th26.000"), (502, "/auger/prod/REPLICATED20081014/epos_gr08/en21.250/th26.000") ] directoriesList_short = [ (35455, "/auger/prod/QGSjetII_gr20_simADSTv2r5p1/en18.000/th0.65/2008/11/12"), (18391,"/auger/prod/QGSjetII_gr20_simADSTv2r5p1/en17.500/th0.65/2008/11/11"), (5157, "/auger/prod/QGSjetII_gr20/2008/09/04/en20.000/th0.65"), (1000, "/auger/prod/PhotonLib_gr22/2009/02/27/en17.500/th26.000") ] directoriesList.reverse() directoriesList_short.reverse() def executeTest( nc, nf, queryDict, rateDict, queryDict_r, rateDict_r ): global nClients nClients = nc t1,t2 = runTest() query,querys = t1 rate, rates = t2 fileLabel = "%d files" % nf queryDict.setdefault( fileLabel, {} ) queryDict[fileLabel][nc] = (query,querys) rateDict.setdefault( fileLabel, {} ) rateDict[fileLabel][nc] = (rate,rates) clientLabel = "%d clients" % nc queryDict_r.setdefault( clientLabel, {} ) queryDict_r[clientLabel][nf] = (query,querys) rateDict_r.setdefault( clientLabel, {} ) rateDict_r[clientLabel][nf] = (rate,rates) def runFullTest(): global outputFile, nClients, testDir, lfnListFile, shortRange queryDict = {} rateDict = {} queryDict_r = {} rateDict_r = {} ncList = numberOfClientsList if shortRange: ncList = numberOfClientsList_short nfList = numberOfFilesList if shortRange: nfList = numberOfFilesList_short ndList = directoriesList if shortRange: ndList = directoriesList_short for nc in ncList: if testType in ['getBulkReplicas']: for nf in nfList: lfnListFile = "lfns_%d.txt" % nf executeTest( nc, nf, queryDict, rateDict, queryDict_r, rateDict_r ) elif testType in ['getDirectoryReplicas', "listDirectory"]: for nf, directory in ndList: testDir = directory executeTest( nc, nf, queryDict, rateDict, queryDict_r, rateDict_r ) # Writing out result outFile = open( outputFile, "w" ) outFile.write( "Test type %s \n" % testType ) outFile.write( "Number of queries per unit test %d \n" % nQueries ) outFile.write( "Results: \n\n\n" ) outFile.write( 'data_f = ' + str( queryDict ) + '\n\n\n' ) outFile.write( 'data_f_r = ' + str( rateDict ) + '\n\n\n' ) outFile.write( 'data_c = ' + str( queryDict_r ) + '\n\n\n' ) outFile.write( 'data_c_r = ' + str( rateDict_r ) + '\n\n\n' ) outFile.close() pprint.pprint( queryDict ) pprint.pprint( rateDict ) pprint.pprint( queryDict_r ) pprint.pprint( rateDict_r ) ######################################################################### if os.path.exists( outputFile ): print "Output file %s already exists, exiting ..." sys.exit(-1) if fullTest: runFullTest() else: runTest()
# -*- encoding: utf-8 -*- ############################################################################## # # OpenERP, Open Source Management Solution # Copyright (C) 2004-TODAY OpenERP S.A. <http://www.openerp.com> # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as # published by the Free Software Foundation, either version 3 of the # License, or (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # ############################################################################## from openerp.osv import fields, osv from openerp.tools.translate import _ class survey_print_statistics(osv.osv_memory): _name = 'survey.print.statistics' _columns = { 'survey_ids': fields.many2many('survey', string="Survey", required="1"), } def action_next(self, cr, uid, ids, context=None): """ Print Survey Statistics in pdf format. """ if context is None: context = {} datas = {'ids': context.get('active_ids', [])} res = self.read(cr, uid, ids, ['survey_ids'], context=context) res = res and res[0] or {} datas['form'] = res datas['model'] = 'survey.print.statistics' return { 'type': 'ir.actions.report.xml', 'report_name': 'survey.analysis', 'datas': datas, } # vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
"""Test correct treatment of hex/oct constants. This is complex because of changes due to PEP 237. """ import unittest class TestHexOctBin(unittest.TestCase): def test_hex_baseline(self): # A few upper/lowercase tests self.assertEqual(0x0, 0X0) self.assertEqual(0x1, 0X1) self.assertEqual(0x123456789abcdef, 0X123456789abcdef) # Baseline tests self.assertEqual(0x0, 0) self.assertEqual(0x10, 16) self.assertEqual(0x7fffffff, 2147483647) self.assertEqual(0x7fffffffffffffff, 9223372036854775807) # Ditto with a minus sign and parentheses self.assertEqual(-(0x0), 0) self.assertEqual(-(0x10), -16) self.assertEqual(-(0x7fffffff), -2147483647) self.assertEqual(-(0x7fffffffffffffff), -9223372036854775807) # Ditto with a minus sign and NO parentheses self.assertEqual(-0x0, 0) self.assertEqual(-0x10, -16) self.assertEqual(-0x7fffffff, -2147483647) self.assertEqual(-0x7fffffffffffffff, -9223372036854775807) def test_hex_unsigned(self): # Positive constants self.assertEqual(0x80000000, 2147483648) self.assertEqual(0xffffffff, 4294967295) # Ditto with a minus sign and parentheses self.assertEqual(-(0x80000000), -2147483648) self.assertEqual(-(0xffffffff), -4294967295) # Ditto with a minus sign and NO parentheses # This failed in Python 2.2 through 2.2.2 and in 2.3a1 self.assertEqual(-0x80000000, -2147483648) self.assertEqual(-0xffffffff, -4294967295) # Positive constants self.assertEqual(0x8000000000000000, 9223372036854775808) self.assertEqual(0xffffffffffffffff, 18446744073709551615) # Ditto with a minus sign and parentheses self.assertEqual(-(0x8000000000000000), -9223372036854775808) self.assertEqual(-(0xffffffffffffffff), -18446744073709551615) # Ditto with a minus sign and NO parentheses # This failed in Python 2.2 through 2.2.2 and in 2.3a1 self.assertEqual(-0x8000000000000000, -9223372036854775808) self.assertEqual(-0xffffffffffffffff, -18446744073709551615) def test_oct_baseline(self): # A few upper/lowercase tests self.assertEqual(0o0, 0O0) self.assertEqual(0o1, 0O1) self.assertEqual(0o1234567, 0O1234567) # Baseline tests self.assertEqual(0o0, 0) self.assertEqual(0o20, 16) self.assertEqual(0o17777777777, 2147483647) self.assertEqual(0o777777777777777777777, 9223372036854775807) # Ditto with a minus sign and parentheses self.assertEqual(-(0o0), 0) self.assertEqual(-(0o20), -16) self.assertEqual(-(0o17777777777), -2147483647) self.assertEqual(-(0o777777777777777777777), -9223372036854775807) # Ditto with a minus sign and NO parentheses self.assertEqual(-0o0, 0) self.assertEqual(-0o20, -16) self.assertEqual(-0o17777777777, -2147483647) self.assertEqual(-0o777777777777777777777, -9223372036854775807) def test_oct_unsigned(self): # Positive constants self.assertEqual(0o20000000000, 2147483648) self.assertEqual(0o37777777777, 4294967295) # Ditto with a minus sign and parentheses self.assertEqual(-(0o20000000000), -2147483648) self.assertEqual(-(0o37777777777), -4294967295) # Ditto with a minus sign and NO parentheses # This failed in Python 2.2 through 2.2.2 and in 2.3a1 self.assertEqual(-0o20000000000, -2147483648) self.assertEqual(-0o37777777777, -4294967295) # Positive constants self.assertEqual(0o1000000000000000000000, 9223372036854775808) self.assertEqual(0o1777777777777777777777, 18446744073709551615) # Ditto with a minus sign and parentheses self.assertEqual(-(0o1000000000000000000000), -9223372036854775808) self.assertEqual(-(0o1777777777777777777777), -18446744073709551615) # Ditto with a minus sign and NO parentheses # This failed in Python 2.2 through 2.2.2 and in 2.3a1 self.assertEqual(-0o1000000000000000000000, -9223372036854775808) self.assertEqual(-0o1777777777777777777777, -18446744073709551615) def test_bin_baseline(self): # A few upper/lowercase tests self.assertEqual(0b0, 0B0) self.assertEqual(0b1, 0B1) self.assertEqual(0b10101010101, 0B10101010101) # Baseline tests self.assertEqual(0b0, 0) self.assertEqual(0b10000, 16) self.assertEqual(0b1111111111111111111111111111111, 2147483647) self.assertEqual(0b111111111111111111111111111111111111111111111111111111111111111, 9223372036854775807) # Ditto with a minus sign and parentheses self.assertEqual(-(0b0), 0) self.assertEqual(-(0b10000), -16) self.assertEqual(-(0b1111111111111111111111111111111), -2147483647) self.assertEqual(-(0b111111111111111111111111111111111111111111111111111111111111111), -9223372036854775807) # Ditto with a minus sign and NO parentheses self.assertEqual(-0b0, 0) self.assertEqual(-0b10000, -16) self.assertEqual(-0b1111111111111111111111111111111, -2147483647) self.assertEqual(-0b111111111111111111111111111111111111111111111111111111111111111, -9223372036854775807) def test_bin_unsigned(self): # Positive constants self.assertEqual(0b10000000000000000000000000000000, 2147483648) self.assertEqual(0b11111111111111111111111111111111, 4294967295) # Ditto with a minus sign and parentheses self.assertEqual(-(0b10000000000000000000000000000000), -2147483648) self.assertEqual(-(0b11111111111111111111111111111111), -4294967295) # Ditto with a minus sign and NO parentheses # This failed in Python 2.2 through 2.2.2 and in 2.3a1 self.assertEqual(-0b10000000000000000000000000000000, -2147483648) self.assertEqual(-0b11111111111111111111111111111111, -4294967295) # Positive constants self.assertEqual(0b1000000000000000000000000000000000000000000000000000000000000000, 9223372036854775808) self.assertEqual(0b1111111111111111111111111111111111111111111111111111111111111111, 18446744073709551615) # Ditto with a minus sign and parentheses self.assertEqual(-(0b1000000000000000000000000000000000000000000000000000000000000000), -9223372036854775808) self.assertEqual(-(0b1111111111111111111111111111111111111111111111111111111111111111), -18446744073709551615) # Ditto with a minus sign and NO parentheses # This failed in Python 2.2 through 2.2.2 and in 2.3a1 self.assertEqual(-0b1000000000000000000000000000000000000000000000000000000000000000, -9223372036854775808) self.assertEqual(-0b1111111111111111111111111111111111111111111111111111111111111111, -18446744073709551615) if __name__ == "__main__": unittest.main()
# -*- coding: utf-8 -*- """ ====================================================================== Structure generators (:mod:`sknano.generators`) ====================================================================== .. currentmodule:: sknano.generators Contents ======== Nanostructure generators ------------------------ .. autosummary:: :toctree: generated/ GeneratorBase FullereneGenerator GrapheneGenerator PrimitiveCellGrapheneGenerator ConventionalCellGrapheneGenerator BilayerGrapheneGenerator MWNTGenerator MWNTBundleGenerator SWNTGenerator SWNTBundleGenerator UnrolledSWNTGenerator Bulk structure generators -------------------------- .. autosummary:: :toctree: generated/ BulkGeneratorBase AlphaQuartzGenerator GoldGenerator CopperGenerator MoS2Generator CaesiumChlorideStructureGenerator DiamondStructureGenerator BCCStructureGenerator FCCStructureGenerator RocksaltStructureGenerator ZincblendeStructureGenerator Other ----- .. autodata:: STRUCTURE_GENERATORS :annotation: = tuple of recognized generator classes. """ from __future__ import absolute_import, division, print_function from __future__ import unicode_literals __docformat__ = 'restructuredtext en' from ._base import * from ._bulk_structure_generator import * from ._mixins import * from ._fullerene_generator import * from ._graphene_generator import * from ._bilayer_graphene_generator import * from ._swnt_generator import * from ._mwnt_generator import * from ._nanotube_bundle_generator import * from ._swnt_bundle_generator import * from ._mwnt_bundle_generator import * from ._unrolled_swnt_generator import * # from ._defect_generators import * __all__ = [s for s in dir() if not s.startswith('_')]
# coding=utf-8 # -------------------------------------------------------------------------- # Copyright (c) Microsoft Corporation. All rights reserved. # Licensed under the MIT License. See License.txt in the project root for # license information. # # Code generated by Microsoft (R) AutoRest Code Generator. # Changes may cause incorrect behavior and will be lost if the code is # regenerated. # -------------------------------------------------------------------------- from msrest.serialization import Model class NodeFile(Model): """Information about a file or directory on a compute node. :param name: The file path. :type name: str :param url: The URL of the file. :type url: str :param is_directory: Whether the object represents a directory. :type is_directory: bool :param properties: The file properties. :type properties: :class:`FileProperties <azure.batch.models.FileProperties>` """ _attribute_map = { 'name': {'key': 'name', 'type': 'str'}, 'url': {'key': 'url', 'type': 'str'}, 'is_directory': {'key': 'isDirectory', 'type': 'bool'}, 'properties': {'key': 'properties', 'type': 'FileProperties'}, } def __init__(self, name=None, url=None, is_directory=None, properties=None): self.name = name self.url = url self.is_directory = is_directory self.properties = properties
"""distutils.spawn Provides the 'spawn()' function, a front-end to various platform- specific functions for launching another program in a sub-process. Also provides the 'find_executable()' to search the path for a given executable name. """ import sys import os from distutils.errors import DistutilsPlatformError, DistutilsExecError from distutils.debug import DEBUG from distutils import log def spawn(cmd, search_path=1, verbose=0, dry_run=0): """Run another program, specified as a command list 'cmd', in a new process. 'cmd' is just the argument list for the new process, ie. cmd[0] is the program to run and cmd[1:] are the rest of its arguments. There is no way to run a program with a name different from that of its executable. If 'search_path' is true (the default), the system's executable search path will be used to find the program; otherwise, cmd[0] must be the exact path to the executable. If 'dry_run' is true, the command will not actually be run. Raise DistutilsExecError if running the program fails in any way; just return on success. """ # cmd is documented as a list, but just in case some code passes a tuple # in, protect our %-formatting code against horrible death cmd = list(cmd) if os.name == 'posix': _spawn_posix(cmd, search_path, dry_run=dry_run) elif os.name == 'nt': _spawn_nt(cmd, search_path, dry_run=dry_run) else: raise DistutilsPlatformError( "don't know how to spawn programs on platform '%s'" % os.name) def _nt_quote_args(args): """Quote command-line arguments for DOS/Windows conventions. Just wraps every argument which contains blanks in double quotes, and returns a new argument list. """ # XXX this doesn't seem very robust to me -- but if the Windows guys # say it'll work, I guess I'll have to accept it. (What if an arg # contains quotes? What other magic characters, other than spaces, # have to be escaped? Is there an escaping mechanism other than # quoting?) for i, arg in enumerate(args): if ' ' in arg: args[i] = '"%s"' % arg return args def _spawn_nt(cmd, search_path=1, verbose=0, dry_run=0): executable = cmd[0] cmd = _nt_quote_args(cmd) if search_path: # either we find one or it stays the same executable = find_executable(executable) or executable log.info(' '.join([executable] + cmd[1:])) if not dry_run: # spawn for NT requires a full path to the .exe try: rc = os.spawnv(os.P_WAIT, executable, cmd) except OSError as exc: # this seems to happen when the command isn't found if not DEBUG: cmd = executable raise DistutilsExecError( "command %r failed: %s" % (cmd, exc.args[-1])) if rc != 0: # and this reflects the command running but failing if not DEBUG: cmd = executable raise DistutilsExecError( "command %r failed with exit status %d" % (cmd, rc)) if sys.platform == 'darwin': from distutils import sysconfig _cfg_target = None _cfg_target_split = None def _spawn_posix(cmd, search_path=1, verbose=0, dry_run=0): log.info(' '.join(cmd)) if dry_run: return executable = cmd[0] exec_fn = search_path and os.execvp or os.execv env = None if sys.platform == 'darwin': global _cfg_target, _cfg_target_split if _cfg_target is None: _cfg_target = sysconfig.get_config_var( 'MACOSX_DEPLOYMENT_TARGET') or '' if _cfg_target: _cfg_target_split = [int(x) for x in _cfg_target.split('.')] if _cfg_target: # ensure that the deployment target of build process is not less # than that used when the interpreter was built. This ensures # extension modules are built with correct compatibility values cur_target = os.environ.get('MACOSX_DEPLOYMENT_TARGET', _cfg_target) if _cfg_target_split > [int(x) for x in cur_target.split('.')]: my_msg = ('$MACOSX_DEPLOYMENT_TARGET mismatch: ' 'now "%s" but "%s" during configure' % (cur_target, _cfg_target)) raise DistutilsPlatformError(my_msg) env = dict(os.environ, MACOSX_DEPLOYMENT_TARGET=cur_target) exec_fn = search_path and os.execvpe or os.execve pid = os.fork() if pid == 0: # in the child try: if env is None: exec_fn(executable, cmd) else: exec_fn(executable, cmd, env) except OSError as e: if not DEBUG: cmd = executable sys.stderr.write("unable to execute %r: %s\n" % (cmd, e.strerror)) os._exit(1) if not DEBUG: cmd = executable sys.stderr.write("unable to execute %r for unknown reasons" % cmd) os._exit(1) else: # in the parent # Loop until the child either exits or is terminated by a signal # (ie. keep waiting if it's merely stopped) while True: try: pid, status = os.waitpid(pid, 0) except OSError as exc: import errno if exc.errno == errno.EINTR: continue if not DEBUG: cmd = executable raise DistutilsExecError( "command %r failed: %s" % (cmd, exc.args[-1])) if os.WIFSIGNALED(status): if not DEBUG: cmd = executable raise DistutilsExecError( "command %r terminated by signal %d" % (cmd, os.WTERMSIG(status))) elif os.WIFEXITED(status): exit_status = os.WEXITSTATUS(status) if exit_status == 0: return # hey, it succeeded! else: if not DEBUG: cmd = executable raise DistutilsExecError( "command %r failed with exit status %d" % (cmd, exit_status)) elif os.WIFSTOPPED(status): continue else: if not DEBUG: cmd = executable raise DistutilsExecError( "unknown error executing %r: termination status %d" % (cmd, status)) def find_executable(executable, path=None): """Tries to find 'executable' in the directories listed in 'path'. A string listing directories separated by 'os.pathsep'; defaults to os.environ['PATH']. Returns the complete filename or None if not found. """ if path is None: path = os.environ['PATH'] paths = path.split(os.pathsep) base, ext = os.path.splitext(executable) if (sys.platform == 'win32') and (ext != '.exe'): executable = executable + '.exe' if not os.path.isfile(executable): for p in paths: f = os.path.join(p, executable) if os.path.isfile(f): # the file exists, we have a shot at spawn working return f return None else: return executable
#!/usr/bin/python # # Copyright: Ansible Project # GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt) from __future__ import absolute_import, division, print_function __metaclass__ = type ANSIBLE_METADATA = {'metadata_version': '1.1', 'status': ['preview'], 'supported_by': 'community'} DOCUMENTATION = """ --- module: onyx_mlag_ipl version_added: "2.5" author: "Samer Deeb (@samerd)" short_description: Manage IPL (inter-peer link) on Mellanox ONYX network devices description: - This module provides declarative management of IPL (inter-peer link) management on Mellanox ONYX network devices. notes: - Tested on ONYX 3.6.4000 options: name: description: - Name of the interface (port-channel) IPL should be configured on. required: true vlan_interface: description: - Name of the IPL vlan interface. state: description: - IPL state. default: present choices: ['present', 'absent'] peer_address: description: - IPL peer IP address. """ EXAMPLES = """ - name: run configure ipl onyx_mlag_ipl: name: Po1 vlan_interface: Vlan 322 state: present peer_address: 192.168.7.1 - name: run remove ipl onyx_mlag_ipl: name: Po1 state: absent """ RETURN = """ commands: description: The list of configuration mode commands to send to the device. returned: always type: list sample: - interface port-channel 1 ipl 1 - interface vlan 1024 ipl 1 peer-address 10.10.10.10 """ import re from ansible.module_utils.basic import AnsibleModule from ansible.module_utils.network.onyx.onyx import BaseOnyxModule from ansible.module_utils.network.onyx.onyx import show_cmd class OnyxMlagIplModule(BaseOnyxModule): VLAN_IF_REGEX = re.compile(r'^Vlan \d+') @classmethod def _get_element_spec(cls): return dict( name=dict(required=True), state=dict(default='present', choices=['present', 'absent']), peer_address=dict(), vlan_interface=dict(), ) def init_module(self): """ module initialization """ element_spec = self._get_element_spec() argument_spec = dict() argument_spec.update(element_spec) self._module = AnsibleModule( argument_spec=argument_spec, supports_check_mode=True) def get_required_config(self): module_params = self._module.params self._required_config = dict( name=module_params['name'], state=module_params['state'], peer_address=module_params['peer_address'], vlan_interface=module_params['vlan_interface']) self.validate_param_values(self._required_config) def _update_mlag_data(self, mlag_data): if not mlag_data: return mlag_summary = mlag_data.get("MLAG IPLs Summary", {}) ipl_id = "1" ipl_list = mlag_summary.get(ipl_id) if ipl_list: ipl_data = ipl_list[0] vlan_id = ipl_data.get("Vlan Interface") vlan_interface = "" if vlan_id != "N/A": vlan_interface = "Vlan %s" % vlan_id peer_address = ipl_data.get("Peer IP address") name = ipl_data.get("Group Port-Channel") self._current_config = dict( name=name, peer_address=peer_address, vlan_interface=vlan_interface) def _show_mlag_data(self): cmd = "show mlag" return show_cmd(self._module, cmd, json_fmt=True, fail_on_error=False) def load_current_config(self): # called in base class in run function self._current_config = dict() mlag_data = self._show_mlag_data() self._update_mlag_data(mlag_data) def _get_interface_cmd_name(self, if_name): if if_name.startswith('Po'): return if_name.replace("Po", "port-channel ") self._module.fail_json( msg='invalid interface name: %s' % if_name) def _generate_port_channel_command(self, if_name, enable): if_cmd_name = self._get_interface_cmd_name(if_name) if enable: ipl_cmd = 'ipl 1' else: ipl_cmd = "no ipl 1" cmd = "interface %s %s" % (if_cmd_name, ipl_cmd) return cmd def _generate_vlan_if_command(self, if_name, enable, peer_address): if_cmd_name = if_name.lower() if enable: ipl_cmd = 'ipl 1 peer-address %s' % peer_address else: ipl_cmd = "no ipl 1" cmd = "interface %s %s" % (if_cmd_name, ipl_cmd) return cmd def _generate_no_ipl_commands(self): curr_interface = self._current_config.get('name') req_interface = self._required_config.get('name') if curr_interface == req_interface: cmd = self._generate_port_channel_command( req_interface, enable=False) self._commands.append(cmd) def _generate_ipl_commands(self): curr_interface = self._current_config.get('name') req_interface = self._required_config.get('name') if curr_interface != req_interface: if curr_interface and curr_interface != 'N/A': cmd = self._generate_port_channel_command( curr_interface, enable=False) self._commands.append(cmd) cmd = self._generate_port_channel_command( req_interface, enable=True) self._commands.append(cmd) curr_vlan = self._current_config.get('vlan_interface') req_vlan = self._required_config.get('vlan_interface') add_peer = False if curr_vlan != req_vlan: add_peer = True if curr_vlan: cmd = self._generate_vlan_if_command(curr_vlan, enable=False, peer_address=None) self._commands.append(cmd) curr_peer = self._current_config.get('peer_address') req_peer = self._required_config.get('peer_address') if req_peer != curr_peer: add_peer = True if add_peer and req_peer: cmd = self._generate_vlan_if_command(req_vlan, enable=True, peer_address=req_peer) self._commands.append(cmd) def generate_commands(self): state = self._required_config['state'] if state == 'absent': self._generate_no_ipl_commands() else: self._generate_ipl_commands() def main(): """ main entry point for module execution """ OnyxMlagIplModule.main() if __name__ == '__main__': main()
# Copyright 2013 Intel. # # Licensed under the Apache License, Version 2.0 (the "License"); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import collections import mock from oslo_serialization import jsonutils import testtools from nova import db from nova import objects from nova.objects import pci_device_pool from nova.tests.functional.v3 import api_sample_base from nova.tests.functional.v3 import test_servers skip_msg = "Bug 1426241" fake_db_dev_1 = { 'created_at': None, 'updated_at': None, 'deleted_at': None, 'deleted': None, 'id': 1, 'compute_node_id': 1, 'address': '0000:04:10.0', 'vendor_id': '8086', 'numa_node': 0, 'product_id': '1520', 'dev_type': 'type-VF', 'status': 'available', 'dev_id': 'pci_0000_04_10_0', 'label': 'label_8086_1520', 'instance_uuid': '69ba1044-0766-4ec0-b60d-09595de034a1', 'request_id': None, 'extra_info': '{"key1": "value1", "key2": "value2"}' } fake_db_dev_2 = { 'created_at': None, 'updated_at': None, 'deleted_at': None, 'deleted': None, 'id': 2, 'compute_node_id': 1, 'address': '0000:04:10.1', 'vendor_id': '8086', 'numa_node': 1, 'product_id': '1520', 'dev_type': 'type-VF', 'status': 'available', 'dev_id': 'pci_0000_04_10_1', 'label': 'label_8086_1520', 'instance_uuid': 'd5b446a6-a1b4-4d01-b4f0-eac37b3a62fc', 'request_id': None, 'extra_info': '{"key3": "value3", "key4": "value4"}' } class ExtendedServerPciSampleJsonTest(test_servers.ServersSampleBase): extension_name = "os-pci" def setUp(self): raise testtools.TestCase.skipException(skip_msg) def test_show(self): uuid = self._post_server() response = self._do_get('servers/%s' % uuid) subs = self._get_regexes() subs['hostid'] = '[a-f0-9]+' self._verify_response('server-get-resp', subs, response, 200) def test_detail(self): self._post_server() response = self._do_get('servers/detail') subs = self._get_regexes() subs['hostid'] = '[a-f0-9]+' self._verify_response('servers-detail-resp', subs, response, 200) class ExtendedHyervisorPciSampleJsonTest(api_sample_base.ApiSampleTestBaseV3): ADMIN_API = True extra_extensions_to_load = ['os-hypervisors'] extension_name = 'os-pci' def setUp(self): raise testtools.TestCase.skipException(skip_msg) super(ExtendedHyervisorPciSampleJsonTest, self).setUp() cpu_info = collections.OrderedDict([ ('arch', 'x86_64'), ('model', 'Nehalem'), ('vendor', 'Intel'), ('features', ['pge', 'clflush']), ('topology', { 'cores': 1, 'threads': 1, 'sockets': 4, }), ]) self.fake_compute_node = objects.ComputeNode( cpu_info=jsonutils.dumps(cpu_info), current_workload=0, disk_available_least=0, host_ip="1.1.1.1", state="up", status="enabled", free_disk_gb=1028, free_ram_mb=7680, hypervisor_hostname="fake-mini", hypervisor_type="fake", hypervisor_version=1000, id=1, local_gb=1028, local_gb_used=0, memory_mb=8192, memory_mb_used=512, running_vms=0, vcpus=1, vcpus_used=0, service_id=2, host='043b3cacf6f34c90a7245151fc8ebcda', pci_device_pools=pci_device_pool.from_pci_stats( {"count": 5, "vendor_id": "8086", "product_id": "1520", "keya": "valuea", "extra_info": { "phys_function": '[["0x0000", ' '"0x04", "0x00",' ' "0x1"]]', "key1": "value1"}}),) self.fake_service = objects.Service( id=2, host='043b3cacf6f34c90a7245151fc8ebcda', disabled=False, disabled_reason=None) @mock.patch("nova.servicegroup.API.service_is_up", return_value=True) @mock.patch("nova.objects.Service.get_by_compute_host") @mock.patch("nova.objects.ComputeNode.get_by_id") def test_pci_show(self, mock_obj, mock_svc_get, mock_service): mock_obj.return_value = self.fake_compute_node mock_svc_get.return_value = self.fake_service hypervisor_id = 1 response = self._do_get('os-hypervisors/%s' % hypervisor_id) subs = { 'hypervisor_id': hypervisor_id, } subs.update(self._get_regexes()) self._verify_response('hypervisors-pci-show-resp', subs, response, 200) @mock.patch("nova.servicegroup.API.service_is_up", return_value=True) @mock.patch("nova.objects.Service.get_by_compute_host") @mock.patch("nova.objects.ComputeNodeList.get_all") def test_pci_detail(self, mock_obj, mock_svc_get, mock_service): mock_obj.return_value = [self.fake_compute_node] mock_svc_get.return_value = self.fake_service hypervisor_id = 1 subs = { 'hypervisor_id': hypervisor_id } response = self._do_get('os-hypervisors/detail') subs.update(self._get_regexes()) self._verify_response('hypervisors-pci-detail-resp', subs, response, 200) class PciSampleJsonTest(api_sample_base.ApiSampleTestBaseV3): ADMIN_API = True extension_name = "os-pci" def setUp(self): raise testtools.TestCase.skipException(skip_msg) def _fake_pci_device_get_by_id(self, context, id): return fake_db_dev_1 def _fake_pci_device_get_all_by_node(self, context, id): return [fake_db_dev_1, fake_db_dev_2] def test_pci_show(self): self.stubs.Set(db, 'pci_device_get_by_id', self._fake_pci_device_get_by_id) response = self._do_get('os-pci/1') subs = self._get_regexes() self._verify_response('pci-show-resp', subs, response, 200) def test_pci_index(self): self.stubs.Set(db, 'pci_device_get_all_by_node', self._fake_pci_device_get_all_by_node) response = self._do_get('os-pci') subs = self._get_regexes() self._verify_response('pci-index-resp', subs, response, 200) def test_pci_detail(self): self.stubs.Set(db, 'pci_device_get_all_by_node', self._fake_pci_device_get_all_by_node) response = self._do_get('os-pci/detail') subs = self._get_regexes() self._verify_response('pci-detail-resp', subs, response, 200)
"""Classes used to represent recipes.""" import env import action import io import os import os.path import sys file_db = { } # file database ext_db = { } # extension database # base classes class File(env.MapEnv): """Representation of files.""" path = None recipe = None is_goal = False is_target = False is_sticky = False actual_path = None def __init__(self, path): env.MapEnv.__init__(self, path.get_file() , env.cenv.path, env.cenv) self.path = path file_db[str(path)] = self def set_goal(self): """Mark a file as a goal.""" self.is_goal = True def set_target(self): """Mark a file as a target.""" self.is_target = True def set_sticky(self): """Mark a file as sticky, that is, a final target (not intermediate).""" self.sticky = True def actual(self): """Get the actual path of the file. For target file, this path is relative to BPATH variable.""" if not self.actual_path: if not self.is_target: self.actual_path = self.path else: bpath = self["BPATH"] if not bpath: self.actual_path = self.path else: bpath = env.topenv.path / bpath bpath = env.Path(bpath) if self.path.prefixed_by(env.topenv.path): self.actual_path = bpath / self.path.relative_to(env.topenv.path) else: self.actual_path = bpath / self.path return self.actual_path def __div__(self, arg): return self.path / str(arg) def time(self): """Get the last update time of the file.""" if self.is_goal: return 0 else: return self.actual().get_mod_time() def younger_than(self, f): """Test if the current file is younger than the given one.""" if self.is_goal: return True else: return self.time() < f.time() def __str__(self): path = self.actual() if path.prefixed_by(env.topdir) or path.prefixed_by(env.curdir()): return str(path.relative_to_cur()) else: return str(path) def get_file(path): """Get the file matching the given path in the DB. Apply localisation rules relative to a particular make.py if the path is not absolute.""" # apply localisation rule if not os.path.isabs(str(path)): path = env.cenv.path / path else: path = env.Path(path) path = path.norm() # find the file if file_db.has_key(str(path)): return file_db[str(path)] else: return File(path) def get_files(paths): """Apply get_file on straight arguments of recipes.""" if not paths: return [] if not isinstance(paths, list): paths = [ paths ] r = [] for p in paths: if not isinstance(p, File): p = get_file(p) r.append(p) return r class Recipe: """A recipe to build files.""" ress = None deps = None env = None cwd = None def __init__(self, ress, deps = None): ress = get_files(ress) deps = get_files(deps) self.ress = ress self.deps = deps for f in ress: f.recipe = self f.is_target = True self.env = env.cenv if hasattr(ress[0], 'cwd'): self.cwd = ress[0].cwd else: self.cwd = self.env.path def action(self, ctx): """Execute the receipe.""" pass def display_action(self, out): pass def display(self, out): out.write("%s: %s\n" % (" ".join([str(f) for f in self.ress]), " ".join([str(f) for f in self.deps]))) self.display_action(out) out.write("\n") class FunRecipe(Recipe): """A recipe that activates a function.""" fun = None def __init__(self, fun, ress, deps): Recipe.__init__(self, ress, deps) self.fun = fun def display_action(self, out): out.write("\t<internal>\n") def action(self, ctx): self.fun(self.ress, self.deps, ctx) class Ext: """Represent the support for a file extension.""" ext = None gens = None back = None def __init__(self, ext): self.ext = ext self.gens = { } self.backs = [] ext_db[ext] = self def update(self, ext, gen): """Update extension for the given generator and perform backward propagation.""" self.gens[ext] = gen for back in self.backs: back.dep.update(ext, back) def get_ext(ext): """Obtain an extension.""" if ext_db.has_key(ext): return ext_db[ext] else: return Ext(ext) class Gen: """A generator of recipe.""" res = None dep = None def __init__(self, res, dep): self.res = get_ext(res) self.dep = get_ext(dep) # update back link self.res.backs.append(self) # update current gens self.dep.update(res, self) for ext in self.dep.gens: self.res.update(ext, self) def gen(self, res, dep): """Generate a recipe to produce the given result from the given dependency.""" pass class FunGen(Gen): """A simple recipe generator from a function.""" fun = None def __init__(self, res, dep, fun): Gen.__init__(self, res, dep) self.fun = fun def gen(self, res, dep): return FunRecipe(self.fun, [res], [dep]) def gen(dir, rext, dep): """Generate recipes to build res. A generation string is found between file src and res. Each intermediate file has for name the kernel of res (generated files will be put in the res directory). """ dir = env.Path(dir) dep = env.Path(dep) # prepare the kernel b = dep.get_base() dext = dep.get_ext() #b, dext = os.path.splitext(dep) #_, n = os.path.split(b) n = b.get_file() kern = dir / n #os.path.join(dir, n) # initialize lookup process if not ext_db.has_key(dext): io.DEF.print_error("don't know how to build '%s' from '%s'" % (rext, dep)) exit(1) ext = ext_db[dext] prev = dep # end when dep is found while ext.ext <> rext: gen = ext.gens[rext] next = kern + gen.res.ext gen.gen(next, prev) prev = next ext = gen.res # return result return prev def fix(path): """Fix a path according to the current directory.""" if isinstance(path, list): return [str(get_file(p)) for p in path] else: return str(get_file(path)) class ActionRecipe(Recipe): """A recipe that supports an action. object for generation.""" act = None def __init__(self, ress, deps, actions): Recipe.__init__(self, ress, deps) self.act = action.make_actions(actions).instantiate(self) def action(self, ctx): if self.act: self.act.execute(ctx) def display_action(self, out): self.act.display(out) class ActionGen(Gen): """A recipe generator supporting simple actions.""" action = None def __init__(self, res, dep, action): Gen.__init__(self, res, dep) self.action = action def gen(self, res, dep): return ActionRecipe([res], [dep], self.action) def rule(ress, deps, *actions): """Build a rule with actions.""" ActionRecipe(ress, deps, make_actions(actions)) def goal(goal, deps, actions = action.Action()): """Build a goal with the following dependencies.""" path = env.Path(env.cenv.path) / goal file = get_file(str(path)) if file.recipe: raise env.ElfError("a goal already named '%s' already exist!" % goal) else: file.set_goal() file.recipe = ActionRecipe(goal, deps, actions) return
import re from django.db import connection from django.test import TestCase, skipUnlessDBFeature from django.utils.functional import cached_property test_srs = ({ 'srid': 4326, 'auth_name': ('EPSG', True), 'auth_srid': 4326, # Only the beginning, because there are differences depending on installed libs 'srtext': 'GEOGCS["WGS 84",DATUM["WGS_1984",SPHEROID["WGS 84"', # +ellps=WGS84 has been removed in the 4326 proj string in proj-4.8 'proj_re': r'\+proj=longlat (\+ellps=WGS84 )?(\+datum=WGS84 |\+towgs84=0,0,0,0,0,0,0 )\+no_defs ?', 'spheroid': 'WGS 84', 'name': 'WGS 84', 'geographic': True, 'projected': False, 'spatialite': True, # From proj's "cs2cs -le" and Wikipedia (semi-minor only) 'ellipsoid': (6378137.0, 6356752.3, 298.257223563), 'eprec': (1, 1, 9), 'wkt': re.sub(r'[\s+]', '', """ GEOGCS["WGS 84", DATUM["WGS_1984", SPHEROID["WGS 84",6378137,298.257223563, AUTHORITY["EPSG","7030"]], AUTHORITY["EPSG","6326"]], PRIMEM["Greenwich",0, AUTHORITY["EPSG","8901"]], UNIT["degree",0.01745329251994328, AUTHORITY["EPSG","9122"]], AUTHORITY["EPSG","4326"]] """) }, { 'srid': 32140, 'auth_name': ('EPSG', False), 'auth_srid': 32140, 'srtext': ( 'PROJCS["NAD83 / Texas South Central",GEOGCS["NAD83",' 'DATUM["North_American_Datum_1983",SPHEROID["GRS 1980"' ), 'proj_re': r'\+proj=lcc (\+lat_1=30.28333333333333? |\+lat_2=28.38333333333333? |\+lat_0=27.83333333333333? |' r'\+lon_0=-99 ){4}\+x_0=600000 \+y_0=4000000 (\+ellps=GRS80 )?' r'(\+datum=NAD83 |\+towgs84=0,0,0,0,0,0,0 )?\+units=m \+no_defs ?', 'spheroid': 'GRS 1980', 'name': 'NAD83 / Texas South Central', 'geographic': False, 'projected': True, 'spatialite': False, # From proj's "cs2cs -le" and Wikipedia (semi-minor only) 'ellipsoid': (6378137.0, 6356752.31414, 298.257222101), 'eprec': (1, 5, 10), }) @skipUnlessDBFeature("has_spatialrefsys_table") class SpatialRefSysTest(TestCase): @cached_property def SpatialRefSys(self): return connection.ops.connection.ops.spatial_ref_sys() def test_get_units(self): epsg_4326 = next(f for f in test_srs if f['srid'] == 4326) unit, unit_name = self.SpatialRefSys().get_units(epsg_4326['wkt']) self.assertEqual(unit_name, 'degree') self.assertAlmostEqual(unit, 0.01745329251994328) def test_retrieve(self): """ Test retrieval of SpatialRefSys model objects. """ for sd in test_srs: srs = self.SpatialRefSys.objects.get(srid=sd['srid']) self.assertEqual(sd['srid'], srs.srid) # Some of the authority names are borked on Oracle, e.g., SRID=32140. # also, Oracle Spatial seems to add extraneous info to fields, hence the # the testing with the 'startswith' flag. auth_name, oracle_flag = sd['auth_name'] # Compare case-insensitively because srs.auth_name is lowercase # ("epsg") on Spatialite. if not connection.ops.oracle or oracle_flag: self.assertIs(srs.auth_name.upper().startswith(auth_name), True) self.assertEqual(sd['auth_srid'], srs.auth_srid) # No PROJ and different srtext on Oracle. if not connection.ops.oracle: self.assertTrue(srs.wkt.startswith(sd['srtext'])) self.assertRegex(srs.proj4text, sd['proj_re']) def test_osr(self): """ Test getting OSR objects from SpatialRefSys model objects. """ for sd in test_srs: sr = self.SpatialRefSys.objects.get(srid=sd['srid']) self.assertTrue(sr.spheroid.startswith(sd['spheroid'])) self.assertEqual(sd['geographic'], sr.geographic) self.assertEqual(sd['projected'], sr.projected) self.assertIs(sr.name.startswith(sd['name']), True) # Testing the SpatialReference object directly. if not connection.ops.oracle: srs = sr.srs self.assertRegex(srs.proj, sd['proj_re']) self.assertTrue(srs.wkt.startswith(sd['srtext'])) def test_ellipsoid(self): """ Test the ellipsoid property. """ for sd in test_srs: # Getting the ellipsoid and precision parameters. ellps1 = sd['ellipsoid'] prec = sd['eprec'] # Getting our spatial reference and its ellipsoid srs = self.SpatialRefSys.objects.get(srid=sd['srid']) ellps2 = srs.ellipsoid for i in range(3): self.assertAlmostEqual(ellps1[i], ellps2[i], prec[i]) @skipUnlessDBFeature('supports_add_srs_entry') def test_add_entry(self): """ Test adding a new entry in the SpatialRefSys model using the add_srs_entry utility. """ from django.contrib.gis.utils import add_srs_entry add_srs_entry(3857) self.assertTrue( self.SpatialRefSys.objects.filter(srid=3857).exists() ) srs = self.SpatialRefSys.objects.get(srid=3857) self.assertTrue( self.SpatialRefSys.get_spheroid(srs.wkt).startswith('SPHEROID[') )
#!/usr/bin/env python # -*- coding: utf-8 -*- # # Licensed under the GNU LGPL v2.1 - http://www.gnu.org/licenses/lgpl.html """ This module replicates the miislita vector spaces from "A Linear Algebra Approach to the Vector Space Model -- A Fast Track Tutorial" by Dr. E. Garcia, admin@miislita.com See http://www.miislita.com for further details. """ from __future__ import division # always use floats from __future__ import with_statement import logging import tempfile import unittest import bz2 import os from gensim import utils, corpora, models, similarities # sample data files are located in the same folder module_path = os.path.dirname(__file__) datapath = lambda fname: os.path.join(module_path, 'test_data', fname) logger = logging.getLogger('test_miislita') def get_tmpfile(suffix): return os.path.join(tempfile.gettempdir(), suffix) class CorpusMiislita(corpora.TextCorpus): stoplist = set('for a of the and to in on'.split()) def get_texts(self): """ Parse documents from the .cor file provided in the constructor. Lowercase each document and ignore some stopwords. .cor format: one document per line, words separated by whitespace. """ with self.getstream() as stream: for doc in stream: yield [word for word in utils.to_unicode(doc).lower().split() if word not in CorpusMiislita.stoplist] def __len__(self): """Define this so we can use `len(corpus)`""" if 'length' not in self.__dict__: logger.info("caching corpus size (calculating number of documents)") self.length = sum(1 for doc in self.get_texts()) return self.length class TestMiislita(unittest.TestCase): def test_textcorpus(self): """Make sure TextCorpus can be serialized to disk. """ # construct corpus from file miislita = CorpusMiislita(datapath('head500.noblanks.cor.bz2')) # make sure serializing works ftmp = get_tmpfile('test_textcorpus.mm') corpora.MmCorpus.save_corpus(ftmp, miislita) self.assertTrue(os.path.exists(ftmp)) # make sure deserializing gives the same result miislita2 = corpora.MmCorpus(ftmp) self.assertEqual(list(miislita), list(miislita2)) def test_save_load_ability(self): """ Make sure we can save and load (un/pickle) TextCorpus objects (as long as the underlying input isn't a file-like object; we cannot pickle those). """ # construct corpus from file corpusname = datapath('miIslita.cor') miislita = CorpusMiislita(corpusname) # pickle to disk tmpf = get_tmpfile('tc_test.cpickle') miislita.save(tmpf) miislita2 = CorpusMiislita.load(tmpf) self.assertEqual(len(miislita), len(miislita2)) self.assertEqual(miislita.dictionary.token2id, miislita2.dictionary.token2id) def test_miislita_high_level(self): # construct corpus from file miislita = CorpusMiislita(datapath('miIslita.cor')) # initialize tfidf transformation and similarity index tfidf = models.TfidfModel(miislita, miislita.dictionary, normalize=False) index = similarities.SparseMatrixSimilarity(tfidf[miislita], num_features=len(miislita.dictionary)) # compare to query query = 'latent semantic indexing' vec_bow = miislita.dictionary.doc2bow(query.lower().split()) vec_tfidf = tfidf[vec_bow] # perform a similarity query against the corpus sims_tfidf = index[vec_tfidf] # for the expected results see the article expected = [0.0, 0.2560, 0.7022, 0.1524, 0.3334] for i, value in enumerate(expected): self.assertAlmostEqual(sims_tfidf[i], value, 2) if __name__ == '__main__': logging.basicConfig(level=logging.DEBUG) unittest.main()
# Copyright (c) 2013, Web Notes Technologies Pvt. Ltd. and Contributors # License: GNU General Public License v3. See license.txt # SETUP: # install pip install --upgrade dropbox # # Create new Dropbox App # # in conf.py, set oauth2 settings # dropbox_access_key # dropbox_access_secret from __future__ import unicode_literals import os import frappe from frappe.utils import get_request_site_address, cstr from frappe import _ @frappe.whitelist() def get_dropbox_authorize_url(): sess = get_dropbox_session() request_token = sess.obtain_request_token() return_address = get_request_site_address(True) \ + "?cmd=erpnext.setup.doctype.backup_manager.backup_dropbox.dropbox_callback" url = sess.build_authorize_url(request_token, return_address) return { "url": url, "key": request_token.key, "secret": request_token.secret, } @frappe.whitelist(allow_guest=True) def dropbox_callback(oauth_token=None, not_approved=False): from dropbox import client if not not_approved: if frappe.db.get_value("Backup Manager", None, "dropbox_access_key")==oauth_token: allowed = 1 message = "Dropbox access allowed." sess = get_dropbox_session() sess.set_request_token(frappe.db.get_value("Backup Manager", None, "dropbox_access_key"), frappe.db.get_value("Backup Manager", None, "dropbox_access_secret")) access_token = sess.obtain_access_token() frappe.db.set_value("Backup Manager", "Backup Manager", "dropbox_access_key", access_token.key) frappe.db.set_value("Backup Manager", "Backup Manager", "dropbox_access_secret", access_token.secret) frappe.db.set_value("Backup Manager", "Backup Manager", "dropbox_access_allowed", allowed) dropbox_client = client.DropboxClient(sess) try: dropbox_client.file_create_folder("files") except: pass else: allowed = 0 message = "Illegal Access Token Please try again." else: allowed = 0 message = "Dropbox Access not approved." frappe.local.message_title = "Dropbox Approval" frappe.local.message = "<h3>%s</h3><p>Please close this window.</p>" % message if allowed: frappe.local.message_success = True frappe.db.commit() frappe.response['type'] = 'page' frappe.response['page_name'] = 'message.html' def backup_to_dropbox(): from dropbox import client, session from frappe.utils.backups import new_backup from frappe.utils import get_files_path, get_backups_path if not frappe.db: frappe.connect() sess = session.DropboxSession(frappe.conf.dropbox_access_key, frappe.conf.dropbox_secret_key, "app_folder") sess.set_token(frappe.db.get_value("Backup Manager", None, "dropbox_access_key"), frappe.db.get_value("Backup Manager", None, "dropbox_access_secret")) dropbox_client = client.DropboxClient(sess) # upload database backup = new_backup() filename = os.path.join(get_backups_path(), os.path.basename(backup.backup_path_db)) upload_file_to_dropbox(filename, "/database", dropbox_client) frappe.db.close() response = dropbox_client.metadata("/files") # upload files to files folder did_not_upload = [] error_log = [] path = get_files_path() for filename in os.listdir(path): filename = cstr(filename) found = False filepath = os.path.join(path, filename) for file_metadata in response["contents"]: if os.path.basename(filepath) == os.path.basename(file_metadata["path"]) and os.stat(filepath).st_size == int(file_metadata["bytes"]): found = True break if not found: try: upload_file_to_dropbox(filepath, "/files", dropbox_client) except Exception: did_not_upload.append(filename) error_log.append(frappe.get_traceback()) frappe.connect() return did_not_upload, list(set(error_log)) def get_dropbox_session(): try: from dropbox import session except: frappe.msgprint(_("Please install dropbox python module"), raise_exception=1) if not (frappe.conf.dropbox_access_key or frappe.conf.dropbox_secret_key): frappe.throw(_("Please set Dropbox access keys in your site config")) sess = session.DropboxSession(frappe.conf.dropbox_access_key, frappe.conf.dropbox_secret_key, "app_folder") return sess def upload_file_to_dropbox(filename, folder, dropbox_client): from dropbox import rest size = os.stat(filename).st_size with open(filename, 'r') as f: # if max packet size reached, use chunked uploader max_packet_size = 4194304 if size > max_packet_size: uploader = dropbox_client.get_chunked_uploader(f, size) while uploader.offset < size: try: uploader.upload_chunked() uploader.finish(folder + "/" + os.path.basename(filename), overwrite=True) except rest.ErrorResponse: pass else: dropbox_client.put_file(folder + "/" + os.path.basename(filename), f, overwrite=True) if __name__=="__main__": backup_to_dropbox()
import pygame import os from random import randint UP = 3 DOWN = 7 RIGHT = 5 LEFT = 9 EXEC_DIR = os.path.dirname(__file__) class Monster(pygame.sprite.Sprite): """ This is our main monster class """ def __init__(self, initial_position, type, direction): pygame.sprite.Sprite.__init__(self) self.image = pygame.image.load(type + '.png') self.rect = self.image.get_rect() self.rect.topleft = initial_position self.next_update_time = 0 self.bottom = self.rect.bottom self.top = self.rect.top self.right = self.rect.right self.left = self.rect.left self.direction = direction self.type = type self.speed = randint(1,5) def update(self, plane, bounds): self.top = self.rect.top self.left = self.rect.left self.right = self.rect.right self.bottom = self.rect.bottom if plane == 'horizontal': if self.direction == RIGHT: self.rect.left += 1 * self.speed if self.right > bounds: self.reverse() elif self.direction == LEFT: self.rect.left -= 1 * self.speed if self.left < 0: self.reverse() elif plane == 'vertical': if self.direction == UP: self.rect.top -= 1 * self.speed if self.top < 30: self.reverse() elif self.direction == DOWN: self.rect.top += 1 * self.speed if self.bottom > bounds: self.reverse() def reverse(self): if self.direction == RIGHT: self.direction = LEFT elif self.direction == LEFT: self.direction = RIGHT elif self.direction == UP: self.direction = DOWN elif self.direction == DOWN: self.direction = UP
# Copyright (c) 2013 Chris Lucas, <chris@chrisjlucas.com> # Permission is hereby granted, free of charge, to any person obtaining # a copy of this software and associated documentation files (the # "Software"), to deal in the Software without restriction, including # without limitation the rights to use, copy, modify, merge, publish, # distribute, sublicense, and/or sell copies of the Software, and to # permit persons to whom the Software is furnished to do so, subject to # the following conditions: # # The above copyright notice and this permission notice shall be # included in all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, # EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF # MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND # NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE # LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION # OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION # WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. import inspect import rtorrent import re from rtorrent.common import bool_to_int, convert_version_tuple_to_str,\ safe_repr from rtorrent.err import MethodError from rtorrent.compat import xmlrpclib def get_varname(rpc_call): """Transform rpc method into variable name. @newfield example: Example @example: if the name of the rpc method is 'p.get_down_rate', the variable name will be 'down_rate' """ # extract variable name from xmlrpc func name r = re.search( "([ptdf]\.|system\.|get\_|is\_|set\_)+([^=]*)", rpc_call, re.I) if r: return(r.groups()[-1]) else: return(None) def _handle_unavailable_rpc_method(method, rt_obj): msg = "Method isn't available." if rt_obj._get_client_version_tuple() < method.min_version: msg = "This method is only available in " \ "RTorrent version v{0} or later".format( convert_version_tuple_to_str(method.min_version)) raise MethodError(msg) class DummyClass: def __init__(self): pass class Method: """Represents an individual RPC method""" def __init__(self, _class, method_name, rpc_call, docstring=None, varname=None, **kwargs): self._class = _class # : Class this method is associated with self.class_name = _class.__name__ self.method_name = method_name # : name of public-facing method self.rpc_call = rpc_call # : name of rpc method self.docstring = docstring # : docstring for rpc method (optional) self.varname = varname # : variable for the result of the method call, usually set to self.varname self.min_version = kwargs.get("min_version", ( 0, 0, 0)) # : Minimum version of rTorrent required self.boolean = kwargs.get("boolean", False) # : returns boolean value? self.post_process_func = kwargs.get( "post_process_func", None) # : custom post process function self.aliases = kwargs.get( "aliases", []) # : aliases for method (optional) self.required_args = [] #: Arguments required when calling the method (not utilized) self.method_type = self._get_method_type() if self.varname is None: self.varname = get_varname(self.rpc_call) assert self.varname is not None, "Couldn't get variable name." def __repr__(self): return safe_repr("Method(method_name='{0}', rpc_call='{1}')", self.method_name, self.rpc_call) def _get_method_type(self): """Determine whether method is a modifier or a retriever""" if self.method_name[:4] == "set_": return('m') # modifier else: return('r') # retriever def is_modifier(self): if self.method_type == 'm': return(True) else: return(False) def is_retriever(self): if self.method_type == 'r': return(True) else: return(False) def is_available(self, rt_obj): if rt_obj._get_client_version_tuple() < self.min_version or \ self.rpc_call not in rt_obj._get_rpc_methods(): return(False) else: return(True) class Multicall: def __init__(self, class_obj, **kwargs): self.class_obj = class_obj if class_obj.__class__.__name__ == "RTorrent": self.rt_obj = class_obj else: self.rt_obj = class_obj._rt_obj self.calls = [] def add(self, method, *args): """Add call to multicall @param method: L{Method} instance or name of raw RPC method @type method: Method or str @param args: call arguments """ # if a raw rpc method was given instead of a Method instance, # try and find the instance for it. And if all else fails, create a # dummy Method instance if isinstance(method, str): result = find_method(method) # if result not found if result == -1: method = Method(DummyClass, method, method) else: method = result # ensure method is available before adding if not method.is_available(self.rt_obj): _handle_unavailable_rpc_method(method, self.rt_obj) self.calls.append((method, args)) def list_calls(self): for c in self.calls: print(c) def call(self): """Execute added multicall calls @return: the results (post-processed), in the order they were added @rtype: tuple """ m = xmlrpclib.MultiCall(self.rt_obj._get_conn()) for call in self.calls: method, args = call rpc_call = getattr(method, "rpc_call") getattr(m, rpc_call)(*args) results = m() results = tuple(results) results_processed = [] for r, c in zip(results, self.calls): method = c[0] # Method instance result = process_result(method, r) results_processed.append(result) # assign result to class_obj exists = hasattr(self.class_obj, method.varname) if not exists or not inspect.ismethod(getattr(self.class_obj, method.varname)): setattr(self.class_obj, method.varname, result) return(tuple(results_processed)) def call_method(class_obj, method, *args): """Handles single RPC calls @param class_obj: Peer/File/Torrent/Tracker/RTorrent instance @type class_obj: object @param method: L{Method} instance or name of raw RPC method @type method: Method or str """ if method.is_retriever(): args = args[:-1] else: assert args[-1] is not None, "No argument given." if class_obj.__class__.__name__ == "RTorrent": rt_obj = class_obj else: rt_obj = class_obj._rt_obj # check if rpc method is even available if not method.is_available(rt_obj): _handle_unavailable_rpc_method(method, rt_obj) m = Multicall(class_obj) m.add(method, *args) # only added one method, only getting one result back ret_value = m.call()[0] ####### OBSOLETE ########################################################## # if method.is_retriever(): # #value = process_result(method, ret_value) # value = ret_value #MultiCall already processed the result # else: # # we're setting the user's input to method.varname # # but we'll return the value that xmlrpc gives us # value = process_result(method, args[-1]) ########################################################################## return(ret_value) def find_method(rpc_call): """Return L{Method} instance associated with given RPC call""" method_lists = [ rtorrent.methods, rtorrent.file.methods, rtorrent.tracker.methods, rtorrent.peer.methods, rtorrent.torrent.methods, ] for l in method_lists: for m in l: if m.rpc_call.lower() == rpc_call.lower(): return(m) return(-1) def process_result(method, result): """Process given C{B{result}} based on flags set in C{B{method}} @param method: L{Method} instance @type method: Method @param result: result to be processed (the result of given L{Method} instance) @note: Supported Processing: - boolean - convert ones and zeros returned by rTorrent and convert to python boolean values """ # handle custom post processing function if method.post_process_func is not None: result = method.post_process_func(result) # is boolean? if method.boolean: if result in [1, '1']: result = True elif result in [0, '0']: result = False return(result) def _build_rpc_methods(class_, method_list): """Build glorified aliases to raw RPC methods""" instance = None if not inspect.isclass(class_): instance = class_ class_ = instance.__class__ for m in method_list: class_name = m.class_name if class_name != class_.__name__: continue if class_name == "RTorrent": caller = lambda self, arg = None, method = m:\ call_method(self, method, bool_to_int(arg)) elif class_name == "Torrent": caller = lambda self, arg = None, method = m:\ call_method(self, method, self.rpc_id, bool_to_int(arg)) elif class_name in ["Tracker", "File"]: caller = lambda self, arg = None, method = m:\ call_method(self, method, self.rpc_id, bool_to_int(arg)) elif class_name == "Peer": caller = lambda self, arg = None, method = m:\ call_method(self, method, self.rpc_id, bool_to_int(arg)) elif class_name == "Group": caller = lambda arg = None, method = m: \ call_method(instance, method, bool_to_int(arg)) if m.docstring is None: m.docstring = "" # print(m) docstring = """{0} @note: Variable where the result for this method is stored: {1}.{2}""".format( m.docstring, class_name, m.varname) caller.__doc__ = docstring for method_name in [m.method_name] + list(m.aliases): if instance is None: setattr(class_, method_name, caller) else: setattr(instance, method_name, caller)
""" An implementation of a RequestCache. This cache is reset at the beginning and end of every request. """ import crum import threading class _RequestCache(threading.local): """ A thread-local for storing the per-request cache. """ def __init__(self): super(_RequestCache, self).__init__() self.data = {} REQUEST_CACHE = _RequestCache() class RequestCache(object): @classmethod def get_request_cache(cls, name=None): """ This method is deprecated. Please use :func:`request_cache.get_cache`. """ if name is None: return REQUEST_CACHE else: return REQUEST_CACHE.data.setdefault(name, {}) @classmethod def get_current_request(cls): """ This method is deprecated. Please use :func:`request_cache.get_request`. """ return crum.get_current_request() @classmethod def clear_request_cache(cls): """ Empty the request cache. """ REQUEST_CACHE.data = {} def process_request(self, request): self.clear_request_cache() return None def process_response(self, request, response): self.clear_request_cache() return response def process_exception(self, request, exception): # pylint: disable=unused-argument """ Clear the RequestCache after a failed request. """ self.clear_request_cache() return None def request_cached(f): """ A decorator for wrapping a function and automatically handles caching its return value, as well as returning that cached value for subsequent calls to the same function, with the same parameters, within a given request. Notes: - we convert arguments and keyword arguments to their string form to build the cache key, so if you have args/kwargs that can't be converted to strings, you're gonna have a bad time (don't do it) - cache key cardinality depends on the args/kwargs, so if you're caching a function that takes five arguments, you might have deceptively low cache efficiency. prefer function with fewer arguments. - we use the default request cache, not a named request cache (this shouldn't matter, but just mentioning it) - benchmark, benchmark, benchmark! if you never measure, how will you know you've improved? or regressed? Arguments: f (func): the function to wrap Returns: func: a wrapper function which will call the wrapped function, passing in the same args/kwargs, cache the value it returns, and return that cached value for subsequent calls with the same args/kwargs within a single request """ def wrapper(*args, **kwargs): """ Wrapper function to decorate with. """ # Build our cache key based on the module the function belongs to, the functions name, and a stringified # list of arguments and a query string-style stringified list of keyword arguments. converted_args = map(str, args) converted_kwargs = map(str, reduce(list.__add__, map(list, sorted(kwargs.iteritems())), [])) cache_keys = [f.__module__, f.func_name] + converted_args + converted_kwargs cache_key = '.'.join(cache_keys) # Check to see if we have a result in cache. If not, invoke our wrapped # function. Cache and return the result to the caller. rcache = RequestCache.get_request_cache() if cache_key in rcache.data: return rcache.data.get(cache_key) else: result = f(*args, **kwargs) rcache.data[cache_key] = result return result return wrapper
""" :class:`.OpenCage` is the Opencagedata geocoder. """ from geopy.compat import urlencode from geopy.geocoders.base import Geocoder, DEFAULT_TIMEOUT, DEFAULT_SCHEME from geopy.exc import ( GeocoderQueryError, GeocoderQuotaExceeded, ) from geopy.location import Location from geopy.util import logger __all__ = ("OpenCage", ) class OpenCage(Geocoder): """ Geocoder using the Open Cage Data API. Documentation at: http://geocoder.opencagedata.com/api.html ..versionadded:: 1.1.0 """ def __init__( self, api_key, domain='api.opencagedata.com', scheme=DEFAULT_SCHEME, timeout=DEFAULT_TIMEOUT, proxies=None, ): # pylint: disable=R0913 """ Initialize a customized Open Cage Data geocoder. :param string api_key: The API key required by Open Cage Data to perform geocoding requests. You can get your key here: https://developer.opencagedata.com/ :param string domain: Currently it is 'api.opencagedata.com', can be changed for testing purposes. :param string scheme: Use 'https' or 'http' as the API URL's scheme. Default is https. Note that SSL connections' certificates are not verified. :param dict proxies: If specified, routes this geocoder's requests through the specified proxy. E.g., {"https": "192.0.2.0"}. For more information, see documentation on :class:`urllib2.ProxyHandler`. """ super(OpenCage, self).__init__( scheme=scheme, timeout=timeout, proxies=proxies ) self.api_key = api_key self.domain = domain.strip('/') self.scheme = scheme self.api = '%s://%s/geocode/v1/json' % (self.scheme, self.domain) def geocode( self, query, bounds=None, country=None, language=None, exactly_one=True, timeout=None, ): # pylint: disable=W0221,R0913 """ Geocode a location query. :param string query: The query string to be geocoded; this must be URL encoded. :param string language: an IETF format language code (such as `es` for Spanish or pt-BR for Brazilian Portuguese); if this is omitted a code of `en` (English) will be assumed by the remote service. :param string bounds: Provides the geocoder with a hint to the region that the query resides in. This value will help the geocoder but will not restrict the possible results to the supplied region. The bounds parameter should be specified as 4 coordinate points forming the south-west and north-east corners of a bounding box. For example, `bounds=-0.563160,51.280430,0.278970,51.683979`. :param string country: Provides the geocoder with a hint to the country that the query resides in. This value will help the geocoder but will not restrict the possible results to the supplied country. The country code is a 3 character code as defined by the ISO 3166-1 Alpha 3 standard. :param bool exactly_one: Return one result or a list of results, if available. :param int timeout: Time, in seconds, to wait for the geocoding service to respond before raising a :class:`geopy.exc.GeocoderTimedOut` exception. Set this only if you wish to override, on this call only, the value set during the geocoder's initialization. """ params = { 'key': self.api_key, 'q': self.format_string % query, } if bounds: params['bounds'] = bounds if bounds: params['language'] = language if bounds: params['country'] = country url = "?".join((self.api, urlencode(params))) logger.debug("%s.geocode: %s", self.__class__.__name__, url) return self._parse_json( self._call_geocoder(url, timeout=timeout), exactly_one ) def reverse( self, query, language=None, exactly_one=False, timeout=None, ): # pylint: disable=W0221,R0913 """ Given a point, find an address. :param query: The coordinates for which you wish to obtain the closest human-readable addresses. :type query: :class:`geopy.point.Point`, list or tuple of (latitude, longitude), or string as "%(latitude)s, %(longitude)s" :param string language: The language in which to return results. :param boolean exactly_one: Return one result or a list of results, if available. :param int timeout: Time, in seconds, to wait for the geocoding service to respond before raising a :class:`geopy.exc.GeocoderTimedOut` exception. Set this only if you wish to override, on this call only, the value set during the geocoder's initialization. """ params = { 'key': self.api_key, 'q': self._coerce_point_to_string(query), } if language: params['language'] = language url = "?".join((self.api, urlencode(params))) logger.debug("%s.reverse: %s", self.__class__.__name__, url) return self._parse_json( self._call_geocoder(url, timeout=timeout), exactly_one ) def _parse_json(self, page, exactly_one=True): '''Returns location, (latitude, longitude) from json feed.''' places = page.get('results', []) if not len(places): self._check_status(page.get('status')) return None def parse_place(place): '''Get the location, lat, lng from a single json place.''' location = place.get('formatted') latitude = place['geometry']['lat'] longitude = place['geometry']['lng'] return Location(location, (latitude, longitude), place) if exactly_one: return parse_place(places[0]) else: return [parse_place(place) for place in places] @staticmethod def _check_status(status): """ Validates error statuses. """ status_code = status['code'] if status_code == 429: # Rate limit exceeded raise GeocoderQuotaExceeded( 'The given key has gone over the requests limit in the 24' ' hour period or has submitted too many requests in too' ' short a period of time.' ) if status_code == 200: # When there are no results, just return. return if status_code == 403: raise GeocoderQueryError( 'Your request was denied.' ) else: raise GeocoderQueryError('Unknown error.')
from django.db.transaction import non_atomic_requests from django.forms.models import modelformset_factory from django.shortcuts import redirect from olympia import amo from olympia.amo.utils import render from olympia.zadmin.decorators import admin_required from .forms import DiscoveryModuleForm from .models import DiscoveryModule from .modules import registry as module_registry @non_atomic_requests def promos(request, context, version, platform, compat_mode='strict'): if platform: platform = platform.lower() platform = amo.PLATFORM_DICT.get(platform, amo.PLATFORM_ALL) modules = get_modules(request, platform.api_name, version) return render(request, 'addons/impala/homepage_promos.html', {'modules': modules}) def get_modules(request, platform, version): lang = request.LANG qs = DiscoveryModule.objects.filter(app=request.APP.id) # Remove any modules without a registered backend or an ordering. modules = [m for m in qs if m.module in module_registry and m.ordering is not None] # Remove modules that specify a locales string we're not part of. modules = [m for m in modules if not m.locales or lang in m.locales.split()] modules = sorted(modules, key=lambda x: x.ordering) return [module_registry[m.module](request, platform, version) for m in modules] @admin_required @non_atomic_requests def module_admin(request): APP = request.APP # Custom sorting to drop ordering=NULL objects to the bottom. qs = DiscoveryModule.objects.raw(""" SELECT * from discovery_modules WHERE app_id = %s ORDER BY ordering IS NULL, ordering""", [APP.id]) qs.ordered = True # The formset looks for this. _sync_db_and_registry(qs, APP.id) Form = modelformset_factory(DiscoveryModule, form=DiscoveryModuleForm, can_delete=True, extra=0) formset = Form(request.POST or None, queryset=qs) if request.method == 'POST' and formset.is_valid(): formset.save() return redirect('discovery.module_admin') return render( request, 'legacy_discovery/module_admin.html', {'formset': formset}) def _sync_db_and_registry(qs, app_id): """Match up the module registry and DiscoveryModule rows in the db.""" existing = dict((m.module, m) for m in qs) to_add = [m for m in module_registry if m not in existing] to_delete = [m for m in existing if m not in module_registry] for m in to_add: DiscoveryModule.objects.get_or_create(module=m, app=app_id) DiscoveryModule.objects.filter(module__in=to_delete, app=app_id).delete() if to_add or to_delete: qs._result_cache = None
from django.db import models from .base import BaseModel class TagManager(models.Manager): """Manager that filters out system tags by default. """ def get_queryset(self): return super(TagManager, self).get_queryset().filter(system=False) class Tag(BaseModel): name = models.CharField(db_index=True, max_length=1024) system = models.BooleanField(default=False) objects = TagManager() all_tags = models.Manager() def __unicode__(self): if self.system: return 'System Tag: {}'.format(self.name) return u'{}'.format(self.name) def _natural_key(self): return hash(self.name + str(self.system)) @property def _id(self): return self.name.lower() @classmethod def load(cls, data, system=False): """For compatibility with v1: the tag name used to be the _id, so we make Tag.load('tagname') work as if `name` were the primary key. """ try: return cls.all_tags.get(system=system, name=data) except cls.DoesNotExist: return None class Meta: unique_together = ('name', 'system') ordering = ('name', )
# (C) Copyright David Abrahams 2001. Permission to copy, use, modify, sell and # distribute this software is granted provided this copyright notice appears in # all copies. This software is provided "as is" without express or implied # warranty, and with no claim as to its suitability for any purpose. from utility import to_seq def difference (b, a): """ Returns the elements of B that are not in A. """ result = [] for element in b: if not element in a: result.append (element) return result def intersection (set1, set2): """ Removes from set1 any items which don't appear in set2 and returns the result. """ result = [] for v in set1: if v in set2: result.append (v) return result def contains (small, large): """ Returns true iff all elements of 'small' exist in 'large'. """ small = to_seq (small) large = to_seq (large) for s in small: if not s in large: return False return True def equal (a, b): """ Returns True iff 'a' contains the same elements as 'b', irrespective of their order. # TODO: Python 2.4 has a proper set class. """ return contains (a, b) and contains (b, a)
#!/usr/bin/env python # # Copyright 2006, Google Inc. # All rights reserved. # # Redistribution and use in source and binary forms, with or without # modification, are permitted provided that the following conditions are # met: # # * Redistributions of source code must retain the above copyright # notice, this list of conditions and the following disclaimer. # * Redistributions in binary form must reproduce the above # copyright notice, this list of conditions and the following disclaimer # in the documentation and/or other materials provided with the # distribution. # * Neither the name of Google Inc. nor the names of its # contributors may be used to endorse or promote products derived from # this software without specific prior written permission. # # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS # "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT # LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR # A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT # OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, # SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT # LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, # DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY # THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT # (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE # OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. """Unit test utilities for Google C++ Testing Framework.""" __author__ = 'wan@google.com (Zhanyong Wan)' import atexit import os import shutil import sys import tempfile import unittest _test_module = unittest # Suppresses the 'Import not at the top of the file' lint complaint. # pylint: disable-msg=C6204 try: import subprocess _SUBPROCESS_MODULE_AVAILABLE = True except: import popen2 _SUBPROCESS_MODULE_AVAILABLE = False # pylint: enable-msg=C6204 GTEST_OUTPUT_VAR_NAME = 'GTEST_OUTPUT' IS_WINDOWS = os.name == 'nt' IS_CYGWIN = os.name == 'posix' and 'CYGWIN' in os.uname()[0] # Here we expose a class from a particular module, depending on the # environment. The comment suppresses the 'Invalid variable name' lint # complaint. TestCase = _test_module.TestCase # pylint: disable-msg=C6409 # Initially maps a flag to its default value. After # _ParseAndStripGTestFlags() is called, maps a flag to its actual value. _flag_map = {'gtest_source_dir': os.path.dirname(sys.argv[0]), 'gtest_build_dir': os.path.dirname(sys.argv[0])} _gtest_flags_are_parsed = False def _ParseAndStripGTestFlags(argv): """Parses and strips Google Test flags from argv. This is idempotent.""" # Suppresses the lint complaint about a global variable since we need it # here to maintain module-wide state. global _gtest_flags_are_parsed # pylint: disable-msg=W0603 if _gtest_flags_are_parsed: return _gtest_flags_are_parsed = True for flag in _flag_map: # The environment variable overrides the default value. if flag.upper() in os.environ: _flag_map[flag] = os.environ[flag.upper()] # The command line flag overrides the environment variable. i = 1 # Skips the program name. while i < len(argv): prefix = '--' + flag + '=' if argv[i].startswith(prefix): _flag_map[flag] = argv[i][len(prefix):] del argv[i] break else: # We don't increment i in case we just found a --gtest_* flag # and removed it from argv. i += 1 def GetFlag(flag): """Returns the value of the given flag.""" # In case GetFlag() is called before Main(), we always call # _ParseAndStripGTestFlags() here to make sure the --gtest_* flags # are parsed. _ParseAndStripGTestFlags(sys.argv) return _flag_map[flag] def GetSourceDir(): """Returns the absolute path of the directory where the .py files are.""" return os.path.abspath(GetFlag('gtest_source_dir')) def GetBuildDir(): """Returns the absolute path of the directory where the test binaries are.""" return os.path.abspath(GetFlag('gtest_build_dir')) _temp_dir = None def _RemoveTempDir(): if _temp_dir: shutil.rmtree(_temp_dir, ignore_errors=True) atexit.register(_RemoveTempDir) def GetTempDir(): """Returns a directory for temporary files.""" global _temp_dir if not _temp_dir: _temp_dir = tempfile.mkdtemp() return _temp_dir def GetTestExecutablePath(executable_name, build_dir=None): """Returns the absolute path of the test binary given its name. The function will print a message and abort the program if the resulting file doesn't exist. Args: executable_name: name of the test binary that the test script runs. build_dir: directory where to look for executables, by default the result of GetBuildDir(). Returns: The absolute path of the test binary. """ path = os.path.abspath(os.path.join(build_dir or GetBuildDir(), executable_name)) if (IS_WINDOWS or IS_CYGWIN) and not path.endswith('.exe'): path += '.exe' if not os.path.exists(path): message = ( 'Unable to find the test binary. Please make sure to provide path\n' 'to the binary via the --gtest_build_dir flag or the GTEST_BUILD_DIR\n' 'environment variable. For convenient use, invoke this script via\n' 'mk_test.py.\n' # TODO(vladl@google.com): change mk_test.py to test.py after renaming # the file. 'Please run mk_test.py -h for help.') print >> sys.stderr, message sys.exit(1) return path def GetExitStatus(exit_code): """Returns the argument to exit(), or -1 if exit() wasn't called. Args: exit_code: the result value of os.system(command). """ if os.name == 'nt': # On Windows, os.WEXITSTATUS() doesn't work and os.system() returns # the argument to exit() directly. return exit_code else: # On Unix, os.WEXITSTATUS() must be used to extract the exit status # from the result of os.system(). if os.WIFEXITED(exit_code): return os.WEXITSTATUS(exit_code) else: return -1 class Subprocess: def __init__(self, command, working_dir=None, capture_stderr=True, env=None): """Changes into a specified directory, if provided, and executes a command. Restores the old directory afterwards. Args: command: The command to run, in the form of sys.argv. working_dir: The directory to change into. capture_stderr: Determines whether to capture stderr in the output member or to discard it. env: Dictionary with environment to pass to the subprocess. Returns: An object that represents outcome of the executed process. It has the following attributes: terminated_by_signal True iff the child process has been terminated by a signal. signal Sygnal that terminated the child process. exited True iff the child process exited normally. exit_code The code with which the child process exited. output Child process's stdout and stderr output combined in a string. """ # The subprocess module is the preferrable way of running programs # since it is available and behaves consistently on all platforms, # including Windows. But it is only available starting in python 2.4. # In earlier python versions, we revert to the popen2 module, which is # available in python 2.0 and later but doesn't provide required # functionality (Popen4) under Windows. This allows us to support Mac # OS X 10.4 Tiger, which has python 2.3 installed. if _SUBPROCESS_MODULE_AVAILABLE: if capture_stderr: stderr = subprocess.STDOUT else: stderr = subprocess.PIPE p = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=stderr, cwd=working_dir, universal_newlines=True, env=env) # communicate returns a tuple with the file obect for the child's # output. self.output = p.communicate()[0] self._return_code = p.returncode else: old_dir = os.getcwd() def _ReplaceEnvDict(dest, src): # Changes made by os.environ.clear are not inheritable by child # processes until Python 2.6. To produce inheritable changes we have # to delete environment items with the del statement. for key in dest: del dest[key] dest.update(src) # When 'env' is not None, backup the environment variables and replace # them with the passed 'env'. When 'env' is None, we simply use the # current 'os.environ' for compatibility with the subprocess.Popen # semantics used above. if env is not None: old_environ = os.environ.copy() _ReplaceEnvDict(os.environ, env) try: if working_dir is not None: os.chdir(working_dir) if capture_stderr: p = popen2.Popen4(command) else: p = popen2.Popen3(command) p.tochild.close() self.output = p.fromchild.read() ret_code = p.wait() finally: os.chdir(old_dir) # Restore the old environment variables # if they were replaced. if env is not None: _ReplaceEnvDict(os.environ, old_environ) # Converts ret_code to match the semantics of # subprocess.Popen.returncode. if os.WIFSIGNALED(ret_code): self._return_code = -os.WTERMSIG(ret_code) else: # os.WIFEXITED(ret_code) should return True here. self._return_code = os.WEXITSTATUS(ret_code) if self._return_code < 0: self.terminated_by_signal = True self.exited = False self.signal = -self._return_code else: self.terminated_by_signal = False self.exited = True self.exit_code = self._return_code def Main(): """Runs the unit test.""" # We must call _ParseAndStripGTestFlags() before calling # unittest.main(). Otherwise the latter will be confused by the # --gtest_* flags. _ParseAndStripGTestFlags(sys.argv) # The tested binaries should not be writing XML output files unless the # script explicitly instructs them to. # TODO(vladl@google.com): Move this into Subprocess when we implement # passing environment into it as a parameter. if GTEST_OUTPUT_VAR_NAME in os.environ: del os.environ[GTEST_OUTPUT_VAR_NAME] _test_module.main()
# -*- coding: utf-8 -*- # $Id: da.py 7678 2013-07-03 09:57:36Z milde $ # Author: E D # Copyright: This module has been placed in the public domain. # New language mappings are welcome. Before doing a new translation, please # read <http://docutils.sf.net/docs/howto/i18n.html>. Two files must be # translated for each language: one in docutils/languages, the other in # docutils/parsers/rst/languages. """ Danish-language mappings for language-dependent features of Docutils. """ __docformat__ = 'reStructuredText' labels = { # fixed: language-dependent 'author': 'Forfatter', 'authors': 'Forfattere', 'organization': 'Organisation', 'address': 'Adresse', 'contact': 'Kontakt', 'version': 'Version', 'revision': 'Revision', 'status': 'Status', 'date': 'Dato', 'copyright': 'Copyright', 'dedication': 'Dedikation', 'abstract': 'Resum', 'attention': 'Giv agt!', 'caution': 'Pas p!', 'danger': '!FARE!', 'error': 'Fejl', 'hint': 'Vink', 'important': 'Vigtigt', 'note': 'Bemrk', 'tip': 'Tips', 'warning': 'Advarsel', 'contents': 'Indhold'} """Mapping of node class name to label text.""" bibliographic_fields = { # language-dependent: fixed 'forfatter': 'author', 'forfattere': 'authors', 'organisation': 'organization', 'adresse': 'address', 'kontakt': 'contact', 'version': 'version', 'revision': 'revision', 'status': 'status', 'dato': 'date', 'copyright': 'copyright', 'dedikation': 'dedication', 'resume': 'abstract', 'resum': 'abstract'} """Danish (lowcased) to canonical name mapping for bibliographic fields.""" author_separators = [';', ','] """List of separator strings for the 'Authors' bibliographic field. Tried in order."""
