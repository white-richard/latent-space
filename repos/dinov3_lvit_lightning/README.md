# DINOv3 PyTorch Lightning

![Python](https://img.shields.io/badge/python-3.11-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.8.0-red.svg)
![Lightning](https://img.shields.io/badge/Lightning-2.5.0-purple.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

A PyTorch Lightning implementation of DINOv3 self-supervised learning, providing an easy-to-use, scalable, and well-documented framework for training DINOv3 models on custom datasets.

> **Built upon the original [DINOv3](https://github.com/facebookresearch/dinov3) by Meta AI Research** - This implementation extends the original Facebook Research DINOv3 with PyTorch Lightning integration, GRAM loss support, and enhanced training capabilities while maintaining full compatibility with official pretrained weights.

## ðŸš€ Features

- **PyTorch Lightning Integration**: Clean, modular code with automatic multi-GPU support
- **GRAM Loss Support**: Gradient-based Regularization with Auxiliary Model for enhanced training
- **Hybrid DataLoader System**: Optimized data loading for different sampling strategies
- **Multiple Dataset Support**: Custom TIFF datasets, HuggingFace datasets, and standard vision datasets  
- **Flexible Sampling**: Infinite, distributed, epoch-based, and sharded-infinite samplers
- **Advanced Progress Tracking**: Real-time loss monitoring with rich progress bars including GRAM loss
- **Multi-GPU Training**: DDP support with automatic gradient synchronization
- **Robust Checkpoint Loading**: Compatible with both DINOv3 pretrained weights and training checkpoints
- **Comprehensive Logging**: TensorBoard, CSV, and WandB integration
- **Easy Configuration**: YAML-based config system with sensible defaults

## ðŸ“‹ Quick Start

### 1. Environment Setup

Create and activate the conda environment:

```bash
# Clone the repository with submodules
git clone --recurse-submodules https://github.com/marjanstoimchev/DinoV3LightningTraining.git
cd DinoV3LightningTraining

# Create conda environment 
conda env create -f environment.yml
conda activate dinov3_lightning

# If you didn't use --recurse-submodules, initialize submodule:
# git submodule update --init --recursive
```

### 2. Basic Training

```bash
# Train on HuggingFace dataset (recommended for testing)
python src/training/train_dinov3_lightning.py \
    --config-file configs/ssl_default_config.yaml \
    --checkpoint-path dinov3_official_weights/dinov3_vits16_pretrain_lvd1689m-08c60483.pth \
    --output-dir ./output \
    --gpus 1 \
    --sampler-type infinite
  

python3 src/training/train_dinov3_lightning.py \
    --config-file configs/ssl_lvit_small.yaml \
    --output-dir ./output_tmp \
    --gpus 1 \
    --sampler-type infinite

# Multi-GPU training with GRAM loss
python src/training/train_dinov3_lightning.py \
    --config-file configs/config_lightning_finetuning_v2.yaml \
    --checkpoint-path dinov3_official_weights/dinov3_vits16_pretrain_lvd1689m-08c60483.pth \
    --output-dir ./output_multigpu_gram \
    --gpus 4 \
    --strategy ddp \
    --sampler-type distributed \
    --batch-size 128
```

### 3. Custom Dataset Training

Edit `configs/config_lightning_finetuning.yaml`:

```yaml
train:
  # For custom TIFF images
  dataset_path: CustomTIFF:root=/path/to/your/images/
  
  # For HuggingFace datasets  
  dataset_path: HuggingFace:name=food101
  
  # For custom HuggingFace configs
  dataset_path: HuggingFace:name=your-dataset:split=train:image_key=image
```

## ðŸ—ï¸ Repository Structure

```
DinoV3Lightning_modified/
â”œâ”€â”€ src/                          # Main source code
â”‚   â”œâ”€â”€ callbacks/               # Custom Lightning callbacks
â”‚   â”œâ”€â”€ checkpointing/          # Model checkpointing utilities  
â”‚   â”œâ”€â”€ models/                 # Lightning modules and data modules
â”‚   â””â”€â”€ training/               # Training scripts
â”œâ”€â”€ configs/                     # Configuration files
â”œâ”€â”€ data/                       # Dataset implementations
â”œâ”€â”€ docs/                       # Documentation
â”œâ”€â”€ dinov3/                     # DINOv3 submodule
â”œâ”€â”€ scripts/                    # Utility scripts
â”œâ”€â”€ notebooks/                  # Jupyter notebooks for analysis
â”œâ”€â”€ environment.yml             # Conda environment
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ setup.sh                   # Installation script
```

## âš™ï¸ Configuration

### Sampler Types

The framework supports multiple sampling strategies:

| Sampler | Use Case | Multi-GPU | Performance |
|---------|----------|-----------|-------------|
| `infinite` | Continuous streaming, SSL training | âœ“ | Fastest for infinite datasets |
| `distributed` | Standard distributed training | âœ“ | Best for multi-GPU efficiency |  
| `epoch` | Traditional epoch-based training | âœ“ | Good for finite datasets |
| `sharded_infinite` | Sharded infinite streaming | âœ“ | Memory efficient streaming |

### Dataset Formats

**HuggingFace Datasets** (Recommended):
```yaml
dataset_path: HuggingFace:name=jonathancui/oxford-pets
dataset_path: HuggingFace:name=food101:split=train  
dataset_path: HuggingFace:name=imagenet-1k:streaming=true
```

**Custom TIFF/Image Datasets**:
```yaml  
dataset_path: CustomTIFF:root=/path/to/images/
```

**Local ImageNet-1k (official tarballs)**:

If you downloaded the official ImageNet-1k archives, note that `ILSVRC2012_img_train.tar` is a â€œtar of per-class tar filesâ€. You must extract it into real image files on disk before training.

Example extraction (train split):

```bash
scripts/extract_imagenet_train.sh \
  /home/richw/.code/datasets/imagenet/ILSVRC2012_img_train.tar \
  /home/richw/.code/datasets/imagenet_extracted
```

Then point your config at the extracted folder:

```yaml
train:
  dataset_path: CustomTIFF:root=/home/richw/.code/datasets/imagenet_extracted/train
```

### Key Configuration Options

```yaml
train:
  batch_size_per_gpu: 8          # Batch size per GPU
  num_workers: 8                 # DataLoader workers
  OFFICIAL_EPOCH_LENGTH: 600     # Steps per epoch for infinite samplers
  
student:
  arch: vit_small                # Model architecture (vit_small, vit_base, vit_large)
  patch_size: 16                 # Vision transformer patch size
  
optim:
  lr: 0.0001                     # Learning rate
  epochs: 100                    # Number of training epochs
  weight_decay: 0.02             # Weight decay
```

## ðŸ§  GRAM Loss (Gradient-based Regularization with Auxiliary Model)

### What is GRAM Loss?

GRAM Loss is an advanced regularization technique that uses an auxiliary teacher model to provide gradient-based guidance during training. This enhances the learning process by leveraging pre-trained knowledge while allowing the student model to adapt to new domains.

### Key Benefits

- **Enhanced Training Stability**: Gradient regularization improves convergence
- **Knowledge Transfer**: Leverages pretrained DINOv3 teacher for better representations
- **Domain Adaptation**: Maintains general features while learning domain-specific patterns
- **Real-time Monitoring**: GRAM loss is displayed in progress bars across all training regimes

### Enabling GRAM Loss

**1. Configuration Setup**

Use the GRAM-enabled configuration file:

```bash
# configs/config_lightning_finetuning_v2.yaml
gram:
  use_loss: true              # Enable GRAM loss
  teacher_momentum: 0.999     # Teacher EMA momentum
  warmup_teacher_temp: 0.04   # Teacher temperature warmup
  teacher_temp: 0.05          # Final teacher temperature
  warmup_teacher_temp_epochs: 30

# Architecture requirements (must match pretrained checkpoints)
student:
  mask_k_bias: true           # Required for DINOv3 pretrained weights
  n_storage_tokens: 4         # Storage tokens for teacher model
```

**2. Training with GRAM**

```bash
# Single GPU with GRAM
python src/training/train_dinov3_lightning.py \
    --config-file configs/config_lightning_finetuning_v2.yaml \
    --checkpoint-path dinov3_official_weights/dinov3_vits16_pretrain_lvd1689m-08c60483.pth \
    --output-dir ./output_gram \
    --gpus 1

# Multi-GPU with GRAM
python src/training/train_dinov3_lightning.py \
    --config-file configs/config_lightning_finetuning_v2.yaml \
    --checkpoint-path dinov3_official_weights/dinov3_vits16_pretrain_lvd1689m-08c60483.pth \
    --output-dir ./output_multigpu_gram \
    --gpus 4 \
    --strategy ddp \
    --sampler-type distributed
```

**3. Progress Monitoring**

GRAM loss is automatically displayed in all training regimes:

```
Epoch 1/100 | Step 384/832 (46.2%) | ETA: 2.1h | Speed: 5.72 it/s
DINO_L: 9.000 | DINO_G: 9.000 | KOLEO: -0.274 | IBOT: 2.254 | GRAM: 0.845 | total_loss: 12.095
```

### Architecture Compatibility

**Important**: GRAM functionality requires specific architecture settings to match DINOv3 pretrained checkpoints:

- `mask_k_bias: true` - Creates LinearKMaskedBias layers (required for pretrained weights)
- `n_storage_tokens: 4` - Storage tokens for teacher model (matches official checkpoints)

These settings ensure seamless loading of official DINOv3 pretrained weights as teacher models.

### Checkpoint Loading

The framework automatically handles both types of checkpoints:

| Checkpoint Type | Content | GRAM Usage |
|----------------|---------|------------|
| **Pretrained (.pth)** | Model weights only | âœ… Loaded as teacher |
| **Training (.ckpt)** | Full training state | âœ… Continues GRAM training |

## ðŸ”§ Advanced Usage

### Custom Training Script

```python
from src.models.dinov3_lightning_model import DINOv3LightningModule
from src.models.dinov3_lightning_datamodule import DINOv3DataModule
import pytorch_lightning as pl

# Load configuration
cfg = OmegaConf.load('configs/config_lightning_finetuning.yaml')

# Create model and data module
model = DINOv3LightningModule(cfg_path=cfg)
datamodule = DINOv3DataModule(cfg, sampler_type='distributed')

# Create trainer
trainer = pl.Trainer(
    accelerator='auto',
    devices=4,
    strategy='ddp',
    precision='bf16-mixed',
    max_epochs=100
)

# Train
trainer.fit(model, datamodule)
```

### Performance Optimization Tips

1. **Use appropriate sampler**: 
   - `distributed` for multi-GPU training
   - `infinite` for continuous streaming
   - `epoch` for traditional training

2. **Batch size tuning**:
   - Start with `batch_size_per_gpu=8` for ViT-Small
   - Scale proportionally with number of GPUs
   - Monitor GPU memory usage

3. **DataLoader optimization**:
   - Use `num_workers=8-16` depending on CPU cores
   - Enable `persistent_workers=True` (automatic in our implementation)
   - Use `pin_memory=True` (automatic in our implementation)

## ðŸ“ Monitoring and Logging

### Progress Tracking
The framework provides detailed real-time progress information:

```
Epoch 1/100 | Step 384/832 (46.2%) | ETA: 2.1h | Speed: 5.72 it/s | Elapsed: 1:10
DINO_L: 9.000 | DINO_G: 9.000 | KOLEO: -0.274 | IBOT: 2.254 | GRAM: 0.845 | total_loss: 12.095
```

### Logging Options
- **TensorBoard**: `tensorboard --logdir ./output/tensorboard_logs`
- **CSV Logs**: Available in `./output/csv_logs/`  
- **WandB**: Configure in your config file

## ðŸ” Troubleshooting

### Common Issues

**ImportError: No module named 'dinov3'**
```bash
# Ensure DINOv3 submodule is initialized
git submodule update --init --recursive
```

**CUDA out of memory**
```bash  
# Reduce batch size
--batch-size 64  # Instead of 128

# Or reduce batch_size_per_gpu in config
batch_size_per_gpu: 4  # Instead of 8
```

**Slow training speed**
- Use `--sampler-type distributed` for multi-GPU
- Use HuggingFace datasets instead of custom TIFF datasets
- Ensure sufficient `num_workers` (8-16)

**DataLoader hangs**
- Reduce `num_workers` if CPU limited
- Check dataset path accessibility
- Verify sufficient disk space

**GRAM Loss Issues**
```bash
# Error: "Unexpected key(s) in state_dict: bias_mask"
# Fix: Enable mask_k_bias in config
mask_k_bias: true  # In student section

# Error: "Unexpected key(s) in state_dict: storage_tokens"  
# Fix: Set storage tokens to match pretrained checkpoint
n_storage_tokens: 4  # In student section

# GRAM loss not showing in progress bar
# Check: Ensure using config_lightning_finetuning_v2.yaml with gram.use_loss: true
```

## ðŸ› ï¸ Setup

### 1. Dataset Configuration
Update `config_lightning_finetuning.yaml` with your dataset:

#### Custom Datasets
```yaml
train:
  dataset_path: CustomTIFF:root=../Datasets/composite/
  batch_size_per_gpu: 8  # Adjust based on GPU memory
```

#### HuggingFace Datasets
```yaml
train:
  # Examples:
  # dataset_path: HuggingFace:name=jonathancui/oxford-pets
  # dataset_path: HuggingFace:name=food101:split=train
  dataset_path: HuggingFace:name=your-dataset-name
  batch_size_per_gpu: 8
```

### 2. Training Parameters  
Modify `run.sh` for your setup:
```bash
GPUS=4                    # Number of GPUs
SAVE_EVERY_N_STEPS=50     # Checkpoint frequency
MAX_EPOCHS=30             # Training duration
```

## ðŸ“Š Monitoring & Analysis

### Real-time Training Status
```bash
python show_training_status.py    # Live metrics and progress
python plot_training_losses.py    # Loss visualizations
```

### TensorBoard Dashboard
```bash
tensorboard --logdir=output_multi_gpu/tensorboard_logs
```

### Analysis Notebooks
```bash
# Unified feature extraction and analysis
jupyter notebook notebooks/feature_extraction_unified.ipynb

# Training configuration comparison
jupyter notebook notebooks/compare_training_configs.ipynb

# Image retrieval and similarity analysis
jupyter notebook notebooks/image_retrieval.ipynb
```
**Features:**
- **Feature Extraction**: Compare original vs fine-tuned model features
- **Visualizations**: PCA, t-SNE, UMAP with comprehensive plotting
- **Training Analysis**: Compare different training configurations
- **Image Retrieval**: Similarity search and nearest neighbor analysis
- **Statistical Analysis**: Feature distributions, correlations, and metrics

## ðŸ“ Output Structure

```
output_multi_gpu/
â”œâ”€â”€ checkpoints/                          # Model checkpoints
â”‚   â”œâ”€â”€ model_epoch_01_step_000050_loss_11.312500.ckpt
â”‚   â”œâ”€â”€ model_epoch_01_step_000100_loss_10.456789.ckpt
â”‚   â””â”€â”€ last.ckpt                         # Most recent checkpoint
â”œâ”€â”€ tensorboard_logs/                     # TensorBoard logs
â”œâ”€â”€ csv_logs/                            # CSV metrics
â”œâ”€â”€ training.log                         # Detailed training log
â””â”€â”€ final_ssl_model.pth                  # Final SSL model (DINOv3 compatible)
```

## ðŸ”§ Configuration Guide

Key fine-tuning optimizations in `config_lightning_finetuning.yaml`:

```yaml
# Learning rates optimized for fine-tuning
schedules:
  lr: 
    peak: 0.0001         # Reduced from pretraining (0.001)
    end: 1.0e-07         # Gentle learning rate decay

# Training parameters  
optim:
  epochs: 30             # Fewer epochs than pretraining
  weight_decay: 0.02     # Lower than pretraining (0.04)

# Teacher EMA updates
ema:
  momentum: 0.999        # Slower updates (vs 0.996 pretraining)

# Data augmentation (less aggressive)
crops:
  global_crops_scale: [0.32, 1.0]  # Less aggressive cropping
  local_crops_scale: [0.05, 0.32]  
```

## ðŸ”„ Checkpoint Management & Progressive Training

### **Using Previous Checkpoints as Pretrained Weights**

This framework supports seamless continuation and transfer learning using any previously saved checkpoint:

#### **Continue from Lightning Checkpoints (.ckpt)**
```bash
# Resume exact training state (recommended for same dataset/config)
python train_dinov3_lightning.py \
  --config-file config_lightning_finetuning.yaml \
  --checkpoint-path output_multi_gpu/checkpoints/model_epoch_01_step_000100_loss_10.456789.ckpt \
  --output-dir ./output_continued \
  --gpus 4
```

#### **Transfer from SSL Model (.pth)**  
```bash
# Start fresh training with pretrained weights (new dataset/config)
python train_dinov3_lightning.py \
  --config-file config_lightning_finetuning.yaml \
  --checkpoint-path ./final_ssl_model.pth \
  --output-dir ./output_transfer \
  --gpus 4
```

### **Checkpoint Type Auto-Detection**

| Checkpoint Type | What's Loaded | Use Case |
|----------------|---------------|----------|
| **Lightning (.ckpt)** | Full state: model + optimizer + scheduler + counters | Continue training seamlessly |
| **SSL Model (.pth)** | Model weights only | Transfer learning, new domains |

### **Progressive Training Workflow**

1. **Initial Training**: Start with original DINOv3 pretrained weights
   ```bash
   ./run.sh  # Uses dinov3_vits16_pretrain_lvd1689m-08c60483.pth
   ```

2. **Domain Fine-tuning**: Use best checkpoint for new domain/dataset
   ```bash
   # Extract SSL model first
   python extract_ssl_model.py --checkpoint output_multi_gpu/checkpoints/best_model.ckpt
   
   # Update config for new dataset, then train
   python train_dinov3_lightning.py --checkpoint-path ./final_ssl_model.pth
   ```

3. **Iterative Refinement**: Chain multiple fine-tuning stages
   ```bash
   # Stage 1 â†’ Stage 2 â†’ Stage 3...
   --checkpoint-path output_stage1/checkpoints/best_model.ckpt
   ```

## ðŸš¨ Troubleshooting

### Memory Issues
```bash
# Reduce batch size in run.sh
BATCH_SIZE=16          # or smaller

# Or in config file
train:
  batch_size_per_gpu: 4
```

### Checkpoint Loading Issues
```bash
# Verify checkpoint exists
ls -la output_multi_gpu/checkpoints/

# Check checkpoint contents
python extract_ssl_model.py --checkpoint path/to/checkpoint.ckpt --info-only
```

### Training Hanging
- Fixed in this version with improved synchronization
- Checkpoints now save without blocking training loop

## ðŸ“š Documentation

- [SLURM Usage Guide](docs/SLURM_USAGE.md) - Running on HPC clusters
- [Configuration Reference](docs/CONFIG.md) - Detailed config options  
- [Usage Examples](docs/EXAMPLES.md) - Comprehensive training examples
- [API Documentation](docs/API.md) - Code API reference
- [Analysis Notebooks](notebooks/) - Feature extraction, training comparison, and image retrieval

## ðŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ðŸ™ Acknowledgments

- [DINOv3 original implementation](https://github.com/facebookresearch/dinov3) by Meta AI
- [PyTorch Lightning](https://lightning.ai/) framework
- The self-supervised learning community

## ðŸ“ž Support

- ðŸ“§ Create an issue for bug reports or feature requests
- ðŸ’¬ Join our discussions for questions and community support
- ðŸ“† Check the documentation for detailed guides

---

**Happy Training! ðŸš€**

## ðŸ”Œ Using A Custom Encoder (e.g. `LViT_small`)

This repo builds the student/teacher backbone inside the DINOv3 meta-architecture (`SSLMetaArch`) via `dinov3.models.build_model_from_cfg(cfg)`.

To use a custom encoder, you must provide a backbone that matches DINOv3â€™s expected training-time interface:

- `backbone(x, masks=..., is_training=True)` returns a dict (or list of dicts) containing:
  - `x_norm_clstoken`: `(B, D)`
  - `x_storage_tokens`: `(B, R, D)` (can be `R=0`)
  - `x_norm_patchtokens`: `(B, P, D)`

### `LViT_small` support

There is an optional integration path for `arch: lvit_small` implemented in [dinov3/dinov3/models/lvit.py](dinov3/dinov3/models/lvit.py).

This adapter is designed for the `hypercore` implementation you referenced (it extracts patch tokens from `patch_embedding -> encoder` and creates a CLS-like token using the Lorentzian centroid).

1) Install the optional dependency that provides `hypercore.models.LViT.LViT_small`.

2) Set your config:

```yaml
student:
  arch: lvit_small
  patch_size: 16
  n_storage_tokens: 0
  # Required if `LViT_small` doesnâ€™t expose `.embed_dim`
  embed_dim: 384
  # Optional: kwargs forwarded to `LViT_small(**lvit_kwargs)`
  lvit_kwargs: {}
```

Notes:
- `LViT_small` is instantiated with `image_size` taken from the DINO crop size, so your crop size must be fixed and divisible by `patch_size`.
- Official DINOv3 pretrained `.pth` checkpoints are **not compatible** with `lvit_small` (different parameter names/structures).

3) If your LViT implementation doesnâ€™t support DINO/iBOT masking, disable iBOT (simplest starting point):

```yaml
ibot:
  loss_weight: 0.0
  mask_sample_probability: 0.0
```

If you *do* want iBOT masking, youâ€™ll need to implement mask-token replacement in `LViTBackboneAdapter`.

The provided adapter already applies a best-effort mask-token replacement at the patch-token level before the encoder; if you need exact parity with DINOv3-ViT masking semantics, you may need to refine that logic.